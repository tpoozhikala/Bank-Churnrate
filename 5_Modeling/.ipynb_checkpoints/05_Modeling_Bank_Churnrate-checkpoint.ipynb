{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bank_Churnrate_Modeling<a id='2_Data_wrangling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Table of Contents<a id='2.1_Contents'></a>\n",
    "* 5. Bank_Churnrate_EDA\n",
    "  * 5.1 Table of Contents\n",
    "  * 5.2 Introduction\n",
    "  * 5.3 Imports\n",
    "  * 5.4 Load Scaled Train Test Bank Churn Data\n",
    "  * 5.5 Modeling\n",
    "    * 5.5.1 Logistic Regression \n",
    "    * 5.5.2 Random Forest Classifier\n",
    "    * 5.5.3 Gradient Boosting Classifier\n",
    "    * 5.5.4 Support Vector Classifier (SVC)\n",
    "    * 5.5.5 Improve Random Forest and Gradient Boosting with AdaBoost Classifier\n",
    "    * 5.5.6 Neural Network Classifier (Multi-Layer Perceptron)\n",
    "  * 5.6 Compile and Assess Model Performance\n",
    "      * 5.6.1 Retrieve ROC AUC Scores and Plot Curves\n",
    "      * 5.6.2 Create and Compare Results DataFrame\n",
    "  * 5.7 Save Data & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different predictive models for classification will be built and trained with the 70/30 train test sets and then assessed to determine best model performer. Models tested will be Logistic Regression, RandomForestClassifier with Bagging, Gradient Boosting Classifier which is random forest but with boosting, Support Vector Machines, Ada Boost Classifier, and Neural Network Multi-Layer Perceptron Classifier models. Also assessing model's performance based on dataset where the missing valued rows were dropped versus being imputed with the 'missing' categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Imports<a id='2.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all appropriate packages in order to perform modeling and appropriate assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas, matplotlib.pyplot, time, and sklearn models and ensemble and metrics packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score , precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import time\n",
    "import os\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Load Scaled Train Test Bank Churn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Train Test Data for both missing and dropped bank churn datasets\n",
    "X_train_missing = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_train_scaled_missing.csv', index_col=0)\n",
    "X_train_dropped = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_train_scaled_dropped.csv')\n",
    "X_test_missing = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_test_scaled_missing.csv')\n",
    "X_test_dropped = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/X_test_scaled_dropped.csv')\n",
    "y_train_missing = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_train_missing.csv')\n",
    "y_train_dropped = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_train_dropped.csv')\n",
    "y_test_missing = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_test_missing.csv')\n",
    "y_test_dropped = pd.read_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/Test_Train_Sets/y_test_dropped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auditing the datasets shape and .head() displaying the first few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5670, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply shape and .head() for all X and y Train and Test missing and dropped datasets\n",
    "X_train_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.413491</td>\n",
       "      <td>1.287980</td>\n",
       "      <td>-1.352932</td>\n",
       "      <td>-0.517792</td>\n",
       "      <td>-0.351164</td>\n",
       "      <td>1.401509</td>\n",
       "      <td>0.475754</td>\n",
       "      <td>0.721852</td>\n",
       "      <td>0.412086</td>\n",
       "      <td>-0.417043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246617</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>2.509442</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>-0.348800</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.455723</td>\n",
       "      <td>-1.039496</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>-0.351164</td>\n",
       "      <td>0.492604</td>\n",
       "      <td>-0.436921</td>\n",
       "      <td>-1.422478</td>\n",
       "      <td>-0.311172</td>\n",
       "      <td>0.672648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>2.866969</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.334195</td>\n",
       "      <td>1.287980</td>\n",
       "      <td>-0.859517</td>\n",
       "      <td>-1.160639</td>\n",
       "      <td>-0.351164</td>\n",
       "      <td>-0.416301</td>\n",
       "      <td>2.266265</td>\n",
       "      <td>1.663444</td>\n",
       "      <td>2.120213</td>\n",
       "      <td>0.039323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>3.572413</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>2.342523</td>\n",
       "      <td>-0.348800</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.288877</td>\n",
       "      <td>2.063805</td>\n",
       "      <td>-0.489456</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>-0.416301</td>\n",
       "      <td>0.662110</td>\n",
       "      <td>-0.347248</td>\n",
       "      <td>0.693244</td>\n",
       "      <td>0.053294</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246617</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>2.342523</td>\n",
       "      <td>-0.348800</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.331109</td>\n",
       "      <td>0.512154</td>\n",
       "      <td>1.854266</td>\n",
       "      <td>-1.160639</td>\n",
       "      <td>2.564066</td>\n",
       "      <td>-0.416301</td>\n",
       "      <td>-0.800048</td>\n",
       "      <td>-1.422478</td>\n",
       "      <td>-0.674506</td>\n",
       "      <td>-0.128321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>2.866969</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.413491  1.287980 -1.352932 -0.517792 -0.351164  1.401509  0.475754   \n",
       "1  1.455723 -1.039496  0.003960  0.767902 -0.351164  0.492604 -0.436921   \n",
       "2  0.334195  1.287980 -0.859517 -1.160639 -0.351164 -0.416301  2.266265   \n",
       "3 -0.288877  2.063805 -0.489456  0.767902  0.620579 -0.416301  0.662110   \n",
       "4  1.331109  0.512154  1.854266 -1.160639  2.564066 -0.416301 -0.800048   \n",
       "\n",
       "          7         8         9  ...        22        23       24        25  \\\n",
       "0  0.721852  0.412086 -0.417043  ...  1.246617 -0.279923 -0.28538 -0.465551   \n",
       "1 -1.422478 -0.311172  0.672648  ... -0.802171 -0.279923 -0.28538 -0.465551   \n",
       "2  1.663444  2.120213  0.039323  ... -0.802171  3.572413 -0.28538 -0.465551   \n",
       "3 -0.347248  0.693244  0.053294  ...  1.246617 -0.279923 -0.28538 -0.465551   \n",
       "4 -1.422478 -0.674506 -0.128321  ... -0.802171 -0.279923 -0.28538 -0.465551   \n",
       "\n",
       "         26        27        28        29        30        31  \n",
       "0  2.509442 -0.426890 -0.348800 -0.110175 -0.044089 -0.240901  \n",
       "1 -0.398495 -0.426890  2.866969 -0.110175 -0.044089 -0.240901  \n",
       "2 -0.398495  2.342523 -0.348800 -0.110175 -0.044089 -0.240901  \n",
       "3 -0.398495  2.342523 -0.348800 -0.110175 -0.044089 -0.240901  \n",
       "4 -0.398495 -0.426890  2.866969 -0.110175 -0.044089 -0.240901  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983, 29)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.508894</td>\n",
       "      <td>0.527369</td>\n",
       "      <td>-0.985376</td>\n",
       "      <td>-1.182990</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>-0.411583</td>\n",
       "      <td>-0.523637</td>\n",
       "      <td>-1.409925</td>\n",
       "      <td>-0.397529</td>\n",
       "      <td>0.176166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4493</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>1.200858</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>2.004089</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.638935</td>\n",
       "      <td>-1.029141</td>\n",
       "      <td>-0.483361</td>\n",
       "      <td>-0.531732</td>\n",
       "      <td>1.636781</td>\n",
       "      <td>-0.411583</td>\n",
       "      <td>0.240640</td>\n",
       "      <td>0.228737</td>\n",
       "      <td>0.220310</td>\n",
       "      <td>-0.262182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4493</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>2.004089</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.355304</td>\n",
       "      <td>2.083878</td>\n",
       "      <td>-0.106850</td>\n",
       "      <td>-1.834248</td>\n",
       "      <td>-0.368540</td>\n",
       "      <td>-0.411583</td>\n",
       "      <td>2.866147</td>\n",
       "      <td>-0.102927</td>\n",
       "      <td>2.877769</td>\n",
       "      <td>0.176166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4493</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.498980</td>\n",
       "      <td>2.356053</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>9.572247</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.763215</td>\n",
       "      <td>-0.250886</td>\n",
       "      <td>-0.985376</td>\n",
       "      <td>1.422041</td>\n",
       "      <td>-0.368540</td>\n",
       "      <td>0.494127</td>\n",
       "      <td>-0.533551</td>\n",
       "      <td>0.029739</td>\n",
       "      <td>-0.536664</td>\n",
       "      <td>0.838350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4493</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.498980</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>2.215738</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.514655</td>\n",
       "      <td>1.305623</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>0.770783</td>\n",
       "      <td>-0.368540</td>\n",
       "      <td>0.494127</td>\n",
       "      <td>0.225328</td>\n",
       "      <td>-1.409925</td>\n",
       "      <td>0.352059</td>\n",
       "      <td>-1.162193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4493</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>1.200858</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>2.004089</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.508894  0.527369 -0.985376 -1.182990  0.634121 -0.411583 -0.523637   \n",
       "1 -0.638935 -1.029141 -0.483361 -0.531732  1.636781 -0.411583  0.240640   \n",
       "2  0.355304  2.083878 -0.106850 -1.834248 -0.368540 -0.411583  2.866147   \n",
       "3 -0.763215 -0.250886 -0.985376  1.422041 -0.368540  0.494127 -0.533551   \n",
       "4 -0.514655  1.305623  0.018654  0.770783 -0.368540  0.494127  0.225328   \n",
       "\n",
       "          7         8         9  ...      19        20        21        22  \\\n",
       "0 -1.409925 -0.397529  0.176166  ... -0.4493 -0.302048  1.200858 -0.297071   \n",
       "1  0.228737  0.220310 -0.262182  ... -0.4493 -0.302048 -0.832738 -0.297071   \n",
       "2 -0.102927  2.877769  0.176166  ... -0.4493 -0.302048 -0.832738 -0.297071   \n",
       "3  0.029739 -0.536664  0.838350  ... -0.4493 -0.302048 -0.832738 -0.297071   \n",
       "4 -1.409925  0.352059 -1.162193  ... -0.4493 -0.302048  1.200858 -0.297071   \n",
       "\n",
       "         23        24        25        26        27        28  \n",
       "0  2.004089 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "1  2.004089 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "2 -0.498980  2.356053 -0.451317  9.572247 -0.038842 -0.235327  \n",
       "3 -0.498980 -0.424439  2.215738 -0.104469 -0.038842 -0.235327  \n",
       "4  2.004089 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2431, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.331109</td>\n",
       "      <td>-0.263671</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>-0.517792</td>\n",
       "      <td>-2.294651</td>\n",
       "      <td>-1.325206</td>\n",
       "      <td>-0.661807</td>\n",
       "      <td>0.349138</td>\n",
       "      <td>-0.693108</td>\n",
       "      <td>0.272163</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>-0.3488</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.413491</td>\n",
       "      <td>1.287980</td>\n",
       "      <td>-0.242748</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.492604</td>\n",
       "      <td>-0.742232</td>\n",
       "      <td>-0.222193</td>\n",
       "      <td>-0.722973</td>\n",
       "      <td>1.226807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>3.572413</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>-0.3488</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.908862</td>\n",
       "      <td>-1.039496</td>\n",
       "      <td>-1.722994</td>\n",
       "      <td>0.767902</td>\n",
       "      <td>-1.322908</td>\n",
       "      <td>2.310414</td>\n",
       "      <td>-0.233091</td>\n",
       "      <td>-1.422478</td>\n",
       "      <td>-0.107226</td>\n",
       "      <td>0.807695</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246617</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>-0.3488</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832652</td>\n",
       "      <td>0.512154</td>\n",
       "      <td>0.497375</td>\n",
       "      <td>0.125055</td>\n",
       "      <td>1.592322</td>\n",
       "      <td>1.401509</td>\n",
       "      <td>-0.299298</td>\n",
       "      <td>0.686297</td>\n",
       "      <td>-0.360258</td>\n",
       "      <td>-0.361161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>2.342523</td>\n",
       "      <td>-0.3488</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.455723</td>\n",
       "      <td>-1.039496</td>\n",
       "      <td>1.484205</td>\n",
       "      <td>0.125055</td>\n",
       "      <td>-0.351164</td>\n",
       "      <td>-0.416301</td>\n",
       "      <td>-0.800048</td>\n",
       "      <td>-0.440426</td>\n",
       "      <td>-0.761492</td>\n",
       "      <td>-0.770960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802171</td>\n",
       "      <td>-0.279923</td>\n",
       "      <td>-0.28538</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.398495</td>\n",
       "      <td>-0.426890</td>\n",
       "      <td>-0.3488</td>\n",
       "      <td>-0.110175</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>-0.240901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.331109 -0.263671  0.003960 -0.517792 -2.294651 -1.325206 -0.661807   \n",
       "1 -0.413491  1.287980 -0.242748  0.767902  0.620579  0.492604 -0.742232   \n",
       "2 -1.908862 -1.039496 -1.722994  0.767902 -1.322908  2.310414 -0.233091   \n",
       "3  0.832652  0.512154  0.497375  0.125055  1.592322  1.401509 -0.299298   \n",
       "4  1.455723 -1.039496  1.484205  0.125055 -0.351164 -0.416301 -0.800048   \n",
       "\n",
       "          7         8         9  ...        22        23       24        25  \\\n",
       "0  0.349138 -0.693108  0.272163  ... -0.802171 -0.279923 -0.28538 -0.465551   \n",
       "1 -0.222193 -0.722973  1.226807  ... -0.802171  3.572413 -0.28538 -0.465551   \n",
       "2 -1.422478 -0.107226  0.807695  ...  1.246617 -0.279923 -0.28538 -0.465551   \n",
       "3  0.686297 -0.360258 -0.361161  ... -0.802171 -0.279923 -0.28538 -0.465551   \n",
       "4 -0.440426 -0.761492 -0.770960  ... -0.802171 -0.279923 -0.28538 -0.465551   \n",
       "\n",
       "         26        27      28        29        30        31  \n",
       "0 -0.398495 -0.426890 -0.3488 -0.110175 -0.044089 -0.240901  \n",
       "1 -0.398495 -0.426890 -0.3488 -0.110175 -0.044089 -0.240901  \n",
       "2 -0.398495 -0.426890 -0.3488 -0.110175 -0.044089 -0.240901  \n",
       "3 -0.398495  2.342523 -0.3488 -0.110175 -0.044089 -0.240901  \n",
       "4 -0.398495 -0.426890 -0.3488 -0.110175 -0.044089 -0.240901  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 29)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.136055</td>\n",
       "      <td>1.305623</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>1.422041</td>\n",
       "      <td>-0.368540</td>\n",
       "      <td>0.494127</td>\n",
       "      <td>-0.777628</td>\n",
       "      <td>-1.409925</td>\n",
       "      <td>-0.651732</td>\n",
       "      <td>-0.887060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449300</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>1.200858</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.49898</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.355304</td>\n",
       "      <td>1.305623</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>0.770783</td>\n",
       "      <td>-1.371201</td>\n",
       "      <td>0.494127</td>\n",
       "      <td>-0.719829</td>\n",
       "      <td>0.452302</td>\n",
       "      <td>-0.761022</td>\n",
       "      <td>0.530574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449300</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>1.200858</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.49898</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>2.215738</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.852423</td>\n",
       "      <td>0.527369</td>\n",
       "      <td>1.399194</td>\n",
       "      <td>0.119525</td>\n",
       "      <td>-1.371201</td>\n",
       "      <td>-0.411583</td>\n",
       "      <td>-0.764002</td>\n",
       "      <td>-0.487410</td>\n",
       "      <td>-0.720891</td>\n",
       "      <td>0.777728</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449300</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.49898</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.976703</td>\n",
       "      <td>-0.250886</td>\n",
       "      <td>0.897180</td>\n",
       "      <td>1.422041</td>\n",
       "      <td>-1.371201</td>\n",
       "      <td>0.494127</td>\n",
       "      <td>-0.777628</td>\n",
       "      <td>-0.034137</td>\n",
       "      <td>-0.775212</td>\n",
       "      <td>0.274094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449300</td>\n",
       "      <td>3.310734</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.49898</td>\n",
       "      <td>-0.424439</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.141816</td>\n",
       "      <td>-0.250886</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>0.770783</td>\n",
       "      <td>0.634121</td>\n",
       "      <td>-0.411583</td>\n",
       "      <td>-0.691408</td>\n",
       "      <td>0.239792</td>\n",
       "      <td>-0.713505</td>\n",
       "      <td>0.516585</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225682</td>\n",
       "      <td>-0.302048</td>\n",
       "      <td>-0.832738</td>\n",
       "      <td>-0.297071</td>\n",
       "      <td>-0.49898</td>\n",
       "      <td>2.356053</td>\n",
       "      <td>-0.451317</td>\n",
       "      <td>-0.104469</td>\n",
       "      <td>-0.038842</td>\n",
       "      <td>-0.235327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.136055  1.305623  0.018654  1.422041 -0.368540  0.494127 -0.777628   \n",
       "1  0.355304  1.305623  0.018654  0.770783 -1.371201  0.494127 -0.719829   \n",
       "2  0.852423  0.527369  1.399194  0.119525 -1.371201 -0.411583 -0.764002   \n",
       "3  0.976703 -0.250886  0.897180  1.422041 -1.371201  0.494127 -0.777628   \n",
       "4 -0.141816 -0.250886  0.018654  0.770783  0.634121 -0.411583 -0.691408   \n",
       "\n",
       "          7         8         9  ...        19        20        21        22  \\\n",
       "0 -1.409925 -0.651732 -0.887060  ... -0.449300 -0.302048  1.200858 -0.297071   \n",
       "1  0.452302 -0.761022  0.530574  ... -0.449300 -0.302048  1.200858 -0.297071   \n",
       "2 -0.487410 -0.720891  0.777728  ... -0.449300 -0.302048 -0.832738 -0.297071   \n",
       "3 -0.034137 -0.775212  0.274094  ... -0.449300  3.310734 -0.832738 -0.297071   \n",
       "4  0.239792 -0.713505  0.516585  ...  2.225682 -0.302048 -0.832738 -0.297071   \n",
       "\n",
       "        23        24        25        26        27        28  \n",
       "0 -0.49898 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "1 -0.49898 -0.424439  2.215738 -0.104469 -0.038842 -0.235327  \n",
       "2 -0.49898 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "3 -0.49898 -0.424439 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "4 -0.49898  2.356053 -0.451317 -0.104469 -0.038842 -0.235327  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5670, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attrition_Flag\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               1\n",
       "4               0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3983, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attrition_Flag\n",
       "0               0\n",
       "1               1\n",
       "2               1\n",
       "3               1\n",
       "4               0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2431, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attrition_Flag\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               1\n",
       "4               1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1707, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attrition_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attrition_Flag\n",
       "0               1\n",
       "1               1\n",
       "2               1\n",
       "3               1\n",
       "4               1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Logistic Regression model and determine performance to see if applying assumption of datasets being linear are proper performer for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Logistic Regression model with 'saga' solver in order to test each regularization methods with GridSearchCV with standard 5 fold cv.\n",
    "#Referred to stackoverflow regarding receiving warnings when applying trained y values to model: https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected \n",
    "def logregmodel(X_train, y_train):\n",
    "    start = time.time()\n",
    "    logreg = LogisticRegression(random_state=random_state, solver='saga')\n",
    "    params_log = {'penalty':['l1', 'l2', 'none'], 'C':[0.001, 0.01, 0.1, 0.5, 1,10]}\n",
    "    logreg_GS = GridSearchCV(estimator = logreg, param_grid = params_log, cv=5)\n",
    "    logreg_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return logreg_GS.best_params_ , logreg_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "log_reg_missing_params , logreg_missing_score, time_log_missing = logregmodel(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Logistic Regression Model Build Time with GridSearch CV : 9.15 seconds\n",
      "Logistic Regression Model Best Parameters given Missing Dataset: {'C': 1, 'penalty': 'l1'}\n",
      "Logistic Regression Model Best Parameter Score given Missing Dataset: 0.9037037037037038\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Logistic Regression Model Build Time with GridSearch CV : {round(time_log_missing , 2)} seconds\")\n",
    "print(f\"Logistic Regression Model Best Parameters given Missing Dataset: {log_reg_missing_params}\")\n",
    "print(f\"Logistic Regression Model Best Parameter Score given Missing Dataset: {logreg_missing_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying GridSearchCV for Hyperparameter tuning with Logistic Regression best params for 'saga' solver of a C value of 1 and having a regularization paramter applied of 'l1' to suppress values being too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9062114356232003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.73      0.58      0.65       360\n",
      " not_churned       0.93      0.96      0.95      2071\n",
      "\n",
      "    accuracy                           0.91      2431\n",
      "   macro avg       0.83      0.77      0.80      2431\n",
      "weighted avg       0.90      0.91      0.90      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_missing_best = LogisticRegression(random_state=random_state, solver='saga', C=1, penalty='l1')\n",
    "log_reg_missing_best.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_log_missing = log_reg_missing_best.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_log_missing))\n",
    "print(classification_report(y_test_missing, y_pred_log_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Apply same parameters to build Logistic Regression Model for Dropped Bank Churn datasets to assess performance\n",
    "log_reg_dropped_params , logreg_dropped_score, time_log_dropped = logregmodel(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Logistic Regression Model Build Time with GridSearch CV : 9.02 seconds\n",
      "Logistic Regression Model Best Parameters given Dropped Dataset: {'C': 0.001, 'penalty': 'none'}\n",
      "Logistic Regression Model Best Parameter Score given Dropped Dataset: 0.8993215765149462\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Logistic Regression Model Build Time with GridSearch CV : {round(time_log_dropped , 2)} seconds\")\n",
    "print(f\"Logistic Regression Model Best Parameters given Dropped Dataset: {log_reg_dropped_params}\")\n",
    "print(f\"Logistic Regression Model Best Parameter Score given Dropped Dataset: {logreg_dropped_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9144698301113063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.76      0.59      0.66       244\n",
      " not_churned       0.93      0.97      0.95      1463\n",
      "\n",
      "    accuracy                           0.91      1707\n",
      "   macro avg       0.85      0.78      0.81      1707\n",
      "weighted avg       0.91      0.91      0.91      1707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "log_reg_dropped_best = LogisticRegression(random_state=random_state, solver='saga', C=0.001, penalty='none')\n",
    "log_reg_dropped_best.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_log_dropped = log_reg_dropped_best.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_log_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_log_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above assessment, it seems the Logistic Regression model has high accuracy however it is not the best at predicting when the user has churned. This might be due to the reality that there is class imbalance where there are siginificatly more not churned users than churned in the dataset. Therefore applying a logistic regression model might not be the best in terms of determing an overall generalizable predictive model to determine bank churn rate. \n",
    "\n",
    "However, the dataset where the null valued rows were dropped, had a slighlty better precision and recall in terms of predicting churn regarding customers in the dataset. This might be due to the reality that having less data might incur less imbalance in the dataset, thereby increasing the precision and recall. It also seemed accuracy increased as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidling Random Forest Classifier model and fitting with X_train and y_train datasets for missing and dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build RandomForestClassifier model with list of n # of tress, and different criterion to split with max_depth being a range from 0 to 1000 with 5 fold cv.\n",
    "def randforestmodel(X_train, y_train):\n",
    "    start = time.time()\n",
    "    randForest = RandomForestClassifier(random_state=random_state)\n",
    "    params_randForest = {'n_estimators':[1,10,100], 'criterion':['gini', 'entropy', 'log_loss'], 'max_depth':list(range(1,10))}\n",
    "    randForest_GS = GridSearchCV(estimator = randForest, param_grid = params_randForest, cv=5)\n",
    "    randForest_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return randForest_GS.best_params_ , randForest_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied Random Forest Classifier model to 'missing' bank churn X and y training data\n",
    "randForestparams_missing , randForestScore_missing , randForestTime_missing = randforestmodel(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Random Forest Classifier Model Build Time with GridSearchCV : 60.82 seconds\n",
      "Random Forest Classifier Model Best Parameters given Missing Dataset: {'criterion': 'entropy', 'max_depth': 9, 'n_estimators': 100}\n",
      "Random Forest Classifier Model Best Parameter Score given Missing Dataset: 0.9409171075837742\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Random Forest Classifier Model Build Time with GridSearchCV : {round(randForestTime_missing , 2)} seconds\")\n",
    "print(f\"Random Forest Classifier Model Best Parameters given Missing Dataset: {randForestparams_missing}\")\n",
    "print(f\"Random Forest Classifier Model Best Parameter Score given Missing Dataset: {randForestScore_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9391197038255862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.89      0.68      0.77       360\n",
      " not_churned       0.95      0.99      0.96      2071\n",
      "\n",
      "    accuracy                           0.94      2431\n",
      "   macro avg       0.92      0.83      0.87      2431\n",
      "weighted avg       0.94      0.94      0.94      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for RandomForestClassifier for X_train_missing and y_train_missing\n",
    "randForest_missing = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "randForest_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_randForest_missing = randForest_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_randForest_missing))\n",
    "print(classification_report(y_test_missing, y_pred_randForest_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied Random Forest Classifier model to 'dropped' bank churn X and y training data\n",
    "randForestparams_dropped , randForestScore_dropped , randForestTime_dropped = randforestmodel(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Random Forest Classifier Model Build Time with GridSearchCV : 50.89 seconds\n",
      "Random Forest Classifier Model Best Parameters given Missing Dataset: {'criterion': 'entropy', 'max_depth': 9, 'n_estimators': 100}\n",
      "Random Forest Classifier Model Best Parameter Score given Missing Dataset: 0.9399932535954554\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Random Forest Classifier Model Build Time with GridSearchCV : {round(randForestTime_dropped , 2)} seconds\")\n",
    "print(f\"Random Forest Classifier Model Best Parameters given Missing Dataset: {randForestparams_dropped}\")\n",
    "print(f\"Random Forest Classifier Model Best Parameter Score given Missing Dataset: {randForestScore_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496192149970709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.96      0.68      0.79       244\n",
      " not_churned       0.95      1.00      0.97      1463\n",
      "\n",
      "    accuracy                           0.95      1707\n",
      "   macro avg       0.95      0.84      0.88      1707\n",
      "weighted avg       0.95      0.95      0.95      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for RandomForestClassifier for X_train_dropped and y_train_dropped\n",
    "randForest_dropped = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "randForest_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_randForest_dropped = randForest_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_randForest_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_randForest_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above results regarding the RandomForestClassifier model for both missing and dropped datasets, it seems that the model as a whole siginificantly improved in prediction with the RandomForestClassifier model as compared with logistic regression. And it seems that overall precision for the 'dropped' dataset seemed to be better than the missing. This might be again due to size of the respective datasets as the 'dropped' being smaller might lead to having a larger prediction score than just imputing the null values as 'missing.' Again the accuracy for the 'dropped' dataset is slighltly higher than the 'missing' dataset for potentially the same reason of having less data to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidling Support Vector Classifier model and fitting with X_train and y_train datasets for missing and dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build GradientBoostingClassifier model with list learning rates and of n # of tress, and different criterion to split with max_depth being a range from 0 to 10 with 5 fold cv.\n",
    "def gradboostmodel(X_train, y_train):\n",
    "    start = time.time()\n",
    "    gradBoost = GradientBoostingClassifier(random_state=random_state)\n",
    "    params_gradBoost = {'learning_rate':[0.001, 0.1, 1, 10], 'n_estimators':[1,10,100], 'criterion':['friedman_mse', 'squared_error'], 'max_depth':list(range(1,10))}\n",
    "    gradBoost_GS = GridSearchCV(estimator = gradBoost, param_grid = params_gradBoost, cv=5)\n",
    "    gradBoost_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return gradBoost_GS.best_params_ , gradBoost_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied GradientBoosting Classifier model to 'missing' bank churn X and y training data\n",
    "gradBoostparams_missing , gradBoostScore_missing , gradBoostTime_missing = gradboostmodel(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Gradient Boosting Classifier Model Build Time with GridSearchCV : 798.64 seconds\n",
      "Gradient Boosting Classifier Model Best Parameters given Missing Dataset: {'criterion': 'squared_error', 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
      "Gradient Boosting Classifier Model Best Parameter Score given Missing Dataset: 0.9682539682539681\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Gradient Boosting Classifier Model Build Time with GridSearchCV : {round(gradBoostTime_missing , 2)} seconds\")\n",
    "print(f\"Gradient Boosting Classifier Model Best Parameters given Missing Dataset: {gradBoostparams_missing}\")\n",
    "print(f\"Gradient Boosting Classifier Model Best Parameter Score given Missing Dataset: {gradBoostScore_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679144385026738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.93      0.85      0.89       360\n",
      " not_churned       0.97      0.99      0.98      2071\n",
      "\n",
      "    accuracy                           0.97      2431\n",
      "   macro avg       0.95      0.92      0.93      2431\n",
      "weighted avg       0.97      0.97      0.97      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for GradientBoostingClassifier model for X_train_missing and y_train_missing\n",
    "gradBoost_missing = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='squared_error', max_depth=6)\n",
    "gradBoost_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_gradBoost_missing = gradBoost_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_gradBoost_missing))\n",
    "print(classification_report(y_test_missing, y_pred_gradBoost_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied GradientBoosting Classifier model to 'dropped' bank churn X and y training data\n",
    "gradBoostparams_dropped , gradBoostScore_dropped , gradBoostTime_dropped = gradboostmodel(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Gradient Boosting Classifier Model Build Time with GridSearchCV : 572.62 seconds\n",
      "Gradient Boosting Classifier Model Best Parameters given Dropped Dataset: {'criterion': 'friedman_mse', 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Gradient Boosting Classifier Model Best Parameter Score given Dropped Dataset: 0.9671109625921325\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Gradient Boosting Classifier Model Build Time with GridSearchCV : {round(gradBoostTime_dropped , 2)} seconds\")\n",
    "print(f\"Gradient Boosting Classifier Model Best Parameters given Dropped Dataset: {gradBoostparams_dropped}\")\n",
    "print(f\"Gradient Boosting Classifier Model Best Parameter Score given Dropped Dataset: {gradBoostScore_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968365553602812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.94      0.84      0.88       244\n",
      " not_churned       0.97      0.99      0.98      1463\n",
      "\n",
      "    accuracy                           0.97      1707\n",
      "   macro avg       0.95      0.91      0.93      1707\n",
      "weighted avg       0.97      0.97      0.97      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for GradientBoostingClassifier model for X_train_dropped and y_train_dropped\n",
    "gradBoost_dropped = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='friedman_mse', max_depth=5)\n",
    "gradBoost_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_gradBoost_dropped = gradBoost_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_gradBoost_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_gradBoost_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above results, it seems GradientBoosting Classifier though has longer computation time, was able to improve predictability over just the RandomForestClassifier Model in terms of precision and recall for predicting churn in the datasets. And the overall accuracy increased as well from the RandomForestClassifer.  \n",
    "\n",
    "Between the 'missing' and 'dropped' datasets it seems again that in dropping the missing values, there is a higher precision but recall is slightly less for the dropped versus the dataset. This might be due to the size of the dropped dataset being smaller so there is less confidence in being able to predict values for churn in the Attrition_Flag column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4 Support Vector Classifier (SVC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buidling Support Vector Classifier model and fitting with X_train and y_train datasets for missing and dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Support Vector Machine model with list of C values as well as kernel and gamma methods with 5 fold cv.\n",
    "#Assistance regarding appropriate params to test SVC found on https://stackoverflow.com/questions/36306555/scikit-learn-grid-search-with-svm-regression\n",
    "def svcmodel(X_train, y_train):\n",
    "    start = time.time()\n",
    "    svcmodel = SVC(random_state=random_state)\n",
    "    params_SVC = {'C':[0.001, 0.1, 1, 10], 'kernel':['linear', 'poly', 'rbf', 'sigmoid'], 'gamma':['scale', 'auto']}\n",
    "    svc_GS = GridSearchCV(estimator = svcmodel, param_grid = params_SVC, cv=5)\n",
    "    svc_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return svc_GS.best_params_ , svc_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applied Support Vector Classifier model to 'missing' bank churn X and y training data\n",
    "svcparams_missing , svcScore_missing , svcTime_missing = svcmodel(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Support Vector Classifier Model Build Time with GridSearchCV : 79.55 seconds\n",
      "Support Vector Classifier Model Best Parameters given Missing Dataset: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Support Vector Classifier Model Best Parameter Score given Missing Dataset: 0.9171075837742505\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Support Vector Classifier Model Build Time with GridSearchCV : {round(svcTime_missing , 2)} seconds\")\n",
    "print(f\"Support Vector Classifier Model Best Parameters given Missing Dataset: {svcparams_missing}\")\n",
    "print(f\"Support Vector Classifier Model Best Parameter Score given Missing Dataset: {svcScore_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9156725627313863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.73      0.69      0.71       360\n",
      " not_churned       0.95      0.95      0.95      2071\n",
      "\n",
      "    accuracy                           0.92      2431\n",
      "   macro avg       0.84      0.82      0.83      2431\n",
      "weighted avg       0.91      0.92      0.91      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for Support Vector Classifier model for X_train_missing and y_train_missing\n",
    "svc_missing = SVC(C=10, gamma = 'scale', kernel='rbf', random_state=random_state)\n",
    "svc_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_svc_missing = svc_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_svc_missing))\n",
    "print(classification_report(y_test_missing, y_pred_svc_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied Support Vector Classifier model to 'dropped' bank churn X and y training data\n",
    "svcparams_dropped , svcScore_dropped , svcTime_dropped = svcmodel(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Support Vector Classifier Model Build Time with GridSearchCV : 37.78 seconds\n",
      "Support Vector Classifier Model Best Parameters given Dropped Dataset: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Support Vector Classifier Model Best Parameter Score given Dropped Dataset: 0.9088633254099859\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate Support Vector Classifier Model Build Time with GridSearchCV : {round(svcTime_dropped , 2)} seconds\")\n",
    "print(f\"Support Vector Classifier Model Best Parameters given Dropped Dataset: {svcparams_dropped}\")\n",
    "print(f\"Support Vector Classifier Model Best Parameter Score given Dropped Dataset: {svcScore_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9302870533099004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.80      0.69      0.74       244\n",
      " not_churned       0.95      0.97      0.96      1463\n",
      "\n",
      "    accuracy                           0.93      1707\n",
      "   macro avg       0.87      0.83      0.85      1707\n",
      "weighted avg       0.93      0.93      0.93      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for Support Vector Classifier model for X_train_missing and y_train_missing\n",
    "svc_dropped = SVC(C=10, gamma = 'scale', kernel='rbf', random_state=random_state)\n",
    "svc_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_svc_dropped = svc_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_svc_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_svc_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above results regarding the Support Vector Classifier model, it seems overall it performs not as well as the Gradient Boosting Classifier model and also performance is slightly lower than the Random Forest Model as per precision and recall for predictability. The accuracy is similar to the Random Forest model. \n",
    "\n",
    "Looking at both the dropped and missing dataset results, it seems for the Support Vector Classifier the trend returns of the dropped dataset having a better accuracy and precision which again might be due to the size of the dataset being smaller in terms of determing overall level of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.5 Improve Random Forest and GradientBoosting with AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build AdaBoost Classifier models with each RandomForest and GradientBoosting Models to improve existing models with 5 fold cv with list of estimaros and learning rates.\n",
    "#Each of the AdaBoost Models have the intial base model being the most optimized of each Random Forest or Gradient Boosting \n",
    "#depending on the dropped or missing datasets.\n",
    "def adaboost_decision_missing(X_train, y_train):\n",
    "    start = time.time()\n",
    "    randForest_missing = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "    adaboostmodel = AdaBoostClassifier(estimator = randForest_missing, random_state=random_state)\n",
    "    params_adaboost = {'n_estimators':[1, 10, 50, 100], 'learning_rate':[0.01, 0.1, 1, 10]}\n",
    "    adaboost_GS = GridSearchCV(estimator = adaboostmodel, param_grid = params_adaboost, cv=5)\n",
    "    adaboost_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return adaboost_GS.best_params_ , adaboost_GS.best_score_ , end\n",
    "\n",
    "def adaboost_decision_dropped(X_train, y_train):\n",
    "    start = time.time()\n",
    "    randForest_dropped = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "    adaboostmodel = AdaBoostClassifier(estimator = randForest_dropped, random_state=random_state)\n",
    "    params_adaboost = {'n_estimators':[1, 10, 50, 100], 'learning_rate':[0.01, 0.1, 1, 10]}\n",
    "    adaboost_GS = GridSearchCV(estimator = adaboostmodel, param_grid = params_adaboost, cv=5)\n",
    "    adaboost_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return adaboost_GS.best_params_ , adaboost_GS.best_score_ , end\n",
    "\n",
    "def adaboost_gradient_missing(X_train, y_train):\n",
    "    start = time.time()\n",
    "    gradBoost_missing = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='squared_error', max_depth=6)\n",
    "    adaboostmodel = AdaBoostClassifier(estimator = gradBoost_missing, random_state=random_state)\n",
    "    params_adaboost = {'n_estimators':[1, 10, 50, 100], 'learning_rate':[0.01, 0.1, 1, 10]}\n",
    "    adaboost_GS = GridSearchCV(estimator = adaboostmodel, param_grid = params_adaboost, cv=5)\n",
    "    adaboost_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return adaboost_GS.best_params_ , adaboost_GS.best_score_ , end\n",
    "\n",
    "def adaboost_gradient_dropped(X_train, y_train):\n",
    "    start = time.time()\n",
    "    gradBoost_dropped = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='friedman_mse', max_depth=5)\n",
    "    adaboostmodel = AdaBoostClassifier(estimator = gradBoost_dropped, random_state=random_state)\n",
    "    params_adaboost = {'n_estimators':[1, 10, 50, 100], 'learning_rate':[0.01, 0.1, 1, 10]}\n",
    "    adaboost_GS = GridSearchCV(estimator = adaboostmodel, param_grid = params_adaboost, cv=5)\n",
    "    adaboost_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return adaboost_GS.best_params_ , adaboost_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applied AdaBoost Classifier model with RandomForest Classifier Base Model to 'missing' bank churn X and y training data\n",
    "adaboostparams_decision_missing , adaboostScore_decision_missing , adaBoostTime_decision_missing = adaboost_decision_missing(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate AdaBoost Classifier Model with Random Forest Base Build Time with GridSearchCV : 1461.56 seconds\n",
      "AdaBoost Classifier with Random Forest Base Model Best Parameters given Missing Dataset: {'learning_rate': 1, 'n_estimators': 10}\n",
      "AdaBoost Classifier with RandomForest Base Model Best Parameter Score given Missing Dataset: 0.9624338624338625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate AdaBoost Classifier Model with Random Forest Base Build Time with GridSearchCV : {round(adaBoostTime_decision_missing , 2)} seconds\")\n",
    "print(f\"AdaBoost Classifier with Random Forest Base Model Best Parameters given Missing Dataset: {adaboostparams_decision_missing}\")\n",
    "print(f\"AdaBoost Classifier with RandomForest Base Model Best Parameter Score given Missing Dataset: {adaboostScore_decision_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9638009049773756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.94      0.80      0.87       360\n",
      " not_churned       0.97      0.99      0.98      2071\n",
      "\n",
      "    accuracy                           0.96      2431\n",
      "   macro avg       0.96      0.90      0.92      2431\n",
      "weighted avg       0.96      0.96      0.96      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for AdaBoostClassifier model for X_train_missing and y_train_missing with RandformForestClassifier Base\n",
    "randForest_missing = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "adaBoost_decision_missing = AdaBoostClassifier(estimator = randForest_missing, learning_rate=1, n_estimators = 10, random_state=random_state)\n",
    "adaBoost_decision_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_adaBoost_decision_missing = adaBoost_decision_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_adaBoost_decision_missing))\n",
    "print(classification_report(y_test_missing, y_pred_adaBoost_decision_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applied AdaBoost Classifier model for RandomForest Classifier Base Model with 'dropped' bank churn X and y training data\n",
    "adaboostparams_decision_dropped , adaboostScore_decision_dropped , adaBoostTime_decision_dropped = adaboost_decision_dropped(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate AdaBoost Classifier with RandomForest Base Model Build Time with GridSearchCV : 1109.92 seconds\n",
      "AdaBoost Classifier Model Best Parameters with RandomForest Base given Dropped Dataset: {'learning_rate': 1, 'n_estimators': 10}\n",
      "AdaBoost Classifier Model with RandomForest Base Best Parameter Score given Dropped Dataset: 0.9598298897246584\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate AdaBoost Classifier with RandomForest Base Model Build Time with GridSearchCV : {round(adaBoostTime_decision_dropped , 2)} seconds\")\n",
    "print(f\"AdaBoost Classifier Model Best Parameters with RandomForest Base given Dropped Dataset: {adaboostparams_decision_dropped}\")\n",
    "print(f\"AdaBoost Classifier Model with RandomForest Base Best Parameter Score given Dropped Dataset: {adaboostScore_decision_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619214997070885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.96      0.76      0.85       244\n",
      " not_churned       0.96      1.00      0.98      1463\n",
      "\n",
      "    accuracy                           0.96      1707\n",
      "   macro avg       0.96      0.88      0.91      1707\n",
      "weighted avg       0.96      0.96      0.96      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for AdaBoostClassifier model for X_train_dropped and y_train_dropped with RandForestClassifier Base \n",
    "randForest_dropped = RandomForestClassifier(n_estimators = 100, random_state=random_state, criterion='entropy', max_depth=9)\n",
    "adaboost_decision_dropped = AdaBoostClassifier(estimator = randForest_dropped, learning_rate=1, n_estimators=10, random_state=random_state)\n",
    "adaboost_decision_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_adaBoost_decision_dropped = adaboost_decision_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_adaBoost_decision_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_adaBoost_decision_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied AdaBoost Classifier model to 'missing' bank churn X and y training data for GradientBoosting Base Model\n",
    "adaboostparams_gradient_missing , adaboostScore_gradient_missing , adaBoostTime_gradient_missing = adaboost_gradient_missing(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate AdaBoost Classifier Model Build Time with GridSearchCV : 202.42 seconds\n",
      "AdaBoost Classifier Model Best Parameters given Missing Dataset and Gradient Boosting Base: {'learning_rate': 0.01, 'n_estimators': 1}\n",
      "AdaBoost Classifier Model Best Parameter Score given Missing Dataset and Gradient Boosting Base: 0.9679012345679012\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate AdaBoost Classifier Model Build Time with GridSearchCV : {round(adaBoostTime_gradient_missing , 2)} seconds\")\n",
    "print(f\"AdaBoost Classifier Model Best Parameters given Missing Dataset and Gradient Boosting Base: {adaboostparams_gradient_missing}\")\n",
    "print(f\"AdaBoost Classifier Model Best Parameter Score given Missing Dataset and Gradient Boosting Base: {adaboostScore_gradient_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9691484985602633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.93      0.86      0.89       360\n",
      " not_churned       0.98      0.99      0.98      2071\n",
      "\n",
      "    accuracy                           0.97      2431\n",
      "   macro avg       0.95      0.92      0.94      2431\n",
      "weighted avg       0.97      0.97      0.97      2431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for AdaBoost Classifier model for X_train_missing and y_train_missing with GradientBoosting Base\n",
    "gradBoost_missing = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='squared_error', max_depth=6)\n",
    "adaBoost_gradient_missing = AdaBoostClassifier(estimator=gradBoost_missing, learning_rate=0.01, n_estimators=1, random_state=random_state)\n",
    "adaBoost_gradient_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_adaBoost_gradient_missing = adaBoost_gradient_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_adaBoost_gradient_missing))\n",
    "print(classification_report(y_test_missing, y_pred_adaBoost_gradient_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applied AdaBoost Classifier model to 'dropped' bank churn X and y training data for GradientBoosting Base Model\n",
    "adaboostparams_gradient_dropped , adaboostScore_gradient_dropped , adaBoostTime_gradient_dropped = adaboost_gradient_dropped(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate AdaBoost Classifier Model with Gradient Boosting Base Build Time with GridSearchCV : 877.93 seconds\n",
      "AdaBoost Classifier Model Best Parameters given Dropped Dataset and Gradient Boosting Base: {'learning_rate': 1, 'n_estimators': 10}\n",
      "AdaBoost Classifier Model Best Parameter Score given Dropped Dataset and Gradient Boosting Base: 0.968869441309433\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate AdaBoost Classifier Model with Gradient Boosting Base Build Time with GridSearchCV : {round(adaBoostTime_gradient_dropped , 2)} seconds\")\n",
    "print(f\"AdaBoost Classifier Model Best Parameters given Dropped Dataset and Gradient Boosting Base: {adaboostparams_gradient_dropped}\")\n",
    "print(f\"AdaBoost Classifier Model Best Parameter Score given Dropped Dataset and Gradient Boosting Base: {adaboostScore_gradient_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying best params for AdaBoost Classifier model model for X_train_missing and y_train_missing with GradientBoosting Base \n",
    "gradBoost_dropped = GradientBoostingClassifier(n_estimators = 100, learning_rate=0.1, random_state=random_state, criterion='friedman_mse', max_depth=5)\n",
    "adaBoost_gradient_dropped = AdaBoostClassifier(estimator=gradBoost_dropped, learning_rate=1, n_estimators=10, random_state=random_state)\n",
    "adaBoost_gradient_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_adaBoost_gradient_dropped = adaBoost_gradient_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_adaBoost_gradient_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_adaBoost_gradient_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems adding the AdaBoost Classifier for the respective RandomForest and GradientBoosting Models improved the respective models slightly. And based on the analysis for each of the models, it seems that applying a GradientBoosting Model with AdaBoost Classifer lead to a better performing model that seems to predict churn at a reasonable level of above 0.90 for precision and 0.86 for recall. The model being applied to both the imputed missing and dropped missing datasets gives further confidence that the model seems to perform well in each enviornment of having a 0.95 precision for the dropped dataset and 0.85 regarding recall for predicting churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.6 Neural Network Classifier (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Neural Network Classifier model for X_train and y_train datasets for both missing and dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build and retrieve best values for neural network model inputting X_train and y_train datasets for 5 fold cv.\n",
    "def mlpmodel(X_train, y_train):\n",
    "    start = time.time()\n",
    "    mlpmodel = MLPClassifier(random_state=random_state, max_iter=200 , early_stopping=False, verbose=True)\n",
    "    params_mlpmodel = {'hidden_layer_sizes':[(100,)], 'activation':['identity', 'logistic', 'tanh', 'relu'], 'solver':['lbfgs', 'sgd', 'adam'], 'alpha':[0.0001], 'learning_rate':['constant'], 'learning_rate_init':[0.001], 'power_t':[0.1, 0.5, 1], 'beta_1':[0.9], 'beta_2':[0.999]}\n",
    "    mlpmodel_GS = GridSearchCV(estimator = mlpmodel, param_grid = params_mlpmodel, cv=5)\n",
    "    mlpmodel_GS.fit(X_train, y_train.values.ravel())\n",
    "    end = time.time() - start\n",
    "    return mlpmodel_GS.best_params_ , mlpmodel_GS.best_score_ , end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88556097\n",
      "Iteration 2, loss = 0.70328294\n",
      "Iteration 3, loss = 0.57008677\n",
      "Iteration 4, loss = 0.48712322\n",
      "Iteration 5, loss = 0.43474580\n",
      "Iteration 6, loss = 0.39897719\n",
      "Iteration 7, loss = 0.37310783\n",
      "Iteration 8, loss = 0.35389019\n",
      "Iteration 9, loss = 0.33898799\n",
      "Iteration 10, loss = 0.32710566\n",
      "Iteration 11, loss = 0.31739883\n",
      "Iteration 12, loss = 0.30955543\n",
      "Iteration 13, loss = 0.30291018\n",
      "Iteration 14, loss = 0.29734783\n",
      "Iteration 15, loss = 0.29255580\n",
      "Iteration 16, loss = 0.28839603\n",
      "Iteration 17, loss = 0.28491861\n",
      "Iteration 18, loss = 0.28164867\n",
      "Iteration 19, loss = 0.27884589\n",
      "Iteration 20, loss = 0.27635250\n",
      "Iteration 21, loss = 0.27413679\n",
      "Iteration 22, loss = 0.27211307\n",
      "Iteration 23, loss = 0.27035117\n",
      "Iteration 24, loss = 0.26863662\n",
      "Iteration 25, loss = 0.26713367\n",
      "Iteration 26, loss = 0.26575598\n",
      "Iteration 27, loss = 0.26444587\n",
      "Iteration 28, loss = 0.26327483\n",
      "Iteration 29, loss = 0.26213130\n",
      "Iteration 30, loss = 0.26115015\n",
      "Iteration 31, loss = 0.26017097\n",
      "Iteration 32, loss = 0.25924883\n",
      "Iteration 33, loss = 0.25843177\n",
      "Iteration 34, loss = 0.25759605\n",
      "Iteration 35, loss = 0.25679891\n",
      "Iteration 36, loss = 0.25608912\n",
      "Iteration 37, loss = 0.25540106\n",
      "Iteration 38, loss = 0.25481982\n",
      "Iteration 39, loss = 0.25412806\n",
      "Iteration 40, loss = 0.25356766\n",
      "Iteration 41, loss = 0.25296355\n",
      "Iteration 42, loss = 0.25240178\n",
      "Iteration 43, loss = 0.25186592\n",
      "Iteration 44, loss = 0.25134728\n",
      "Iteration 45, loss = 0.25086605\n",
      "Iteration 46, loss = 0.25042087\n",
      "Iteration 47, loss = 0.24995317\n",
      "Iteration 48, loss = 0.24956810\n",
      "Iteration 49, loss = 0.24910189\n",
      "Iteration 50, loss = 0.24870775\n",
      "Iteration 51, loss = 0.24828680\n",
      "Iteration 52, loss = 0.24791496\n",
      "Iteration 53, loss = 0.24756154\n",
      "Iteration 54, loss = 0.24719839\n",
      "Iteration 55, loss = 0.24680902\n",
      "Iteration 56, loss = 0.24645998\n",
      "Iteration 57, loss = 0.24611190\n",
      "Iteration 58, loss = 0.24582514\n",
      "Iteration 59, loss = 0.24547923\n",
      "Iteration 60, loss = 0.24514974\n",
      "Iteration 61, loss = 0.24488897\n",
      "Iteration 62, loss = 0.24458157\n",
      "Iteration 63, loss = 0.24435060\n",
      "Iteration 64, loss = 0.24403362\n",
      "Iteration 65, loss = 0.24375275\n",
      "Iteration 66, loss = 0.24353022\n",
      "Iteration 67, loss = 0.24322527\n",
      "Iteration 68, loss = 0.24302871\n",
      "Iteration 69, loss = 0.24271749\n",
      "Iteration 70, loss = 0.24249282\n",
      "Iteration 71, loss = 0.24229709\n",
      "Iteration 72, loss = 0.24205529\n",
      "Iteration 73, loss = 0.24180730\n",
      "Iteration 74, loss = 0.24162217\n",
      "Iteration 75, loss = 0.24139047\n",
      "Iteration 76, loss = 0.24118432\n",
      "Iteration 77, loss = 0.24099720\n",
      "Iteration 78, loss = 0.24078375\n",
      "Iteration 79, loss = 0.24064081\n",
      "Iteration 80, loss = 0.24038977\n",
      "Iteration 81, loss = 0.24022718\n",
      "Iteration 82, loss = 0.24003566\n",
      "Iteration 83, loss = 0.23988003\n",
      "Iteration 84, loss = 0.23973784\n",
      "Iteration 85, loss = 0.23959212\n",
      "Iteration 86, loss = 0.23939646\n",
      "Iteration 87, loss = 0.23923662\n",
      "Iteration 88, loss = 0.23906995\n",
      "Iteration 89, loss = 0.23895082\n",
      "Iteration 90, loss = 0.23877311\n",
      "Iteration 91, loss = 0.23865314\n",
      "Iteration 92, loss = 0.23846632\n",
      "Iteration 93, loss = 0.23834868\n",
      "Iteration 94, loss = 0.23822930\n",
      "Iteration 95, loss = 0.23807858\n",
      "Iteration 96, loss = 0.23795733\n",
      "Iteration 97, loss = 0.23784475\n",
      "Iteration 98, loss = 0.23773499\n",
      "Iteration 99, loss = 0.23761296\n",
      "Iteration 100, loss = 0.23746870\n",
      "Iteration 101, loss = 0.23736833\n",
      "Iteration 102, loss = 0.23729934\n",
      "Iteration 103, loss = 0.23714508\n",
      "Iteration 104, loss = 0.23705498\n",
      "Iteration 105, loss = 0.23696745\n",
      "Iteration 106, loss = 0.23690212\n",
      "Iteration 107, loss = 0.23672607\n",
      "Iteration 108, loss = 0.23666555\n",
      "Iteration 109, loss = 0.23657914\n",
      "Iteration 110, loss = 0.23648063\n",
      "Iteration 111, loss = 0.23637742\n",
      "Iteration 112, loss = 0.23630443\n",
      "Iteration 113, loss = 0.23626326\n",
      "Iteration 114, loss = 0.23614017\n",
      "Iteration 115, loss = 0.23602426\n",
      "Iteration 116, loss = 0.23596700\n",
      "Iteration 117, loss = 0.23590495\n",
      "Iteration 118, loss = 0.23577668\n",
      "Iteration 119, loss = 0.23572606\n",
      "Iteration 120, loss = 0.23565587\n",
      "Iteration 121, loss = 0.23562712\n",
      "Iteration 122, loss = 0.23554144\n",
      "Iteration 123, loss = 0.23545733\n",
      "Iteration 124, loss = 0.23538451\n",
      "Iteration 125, loss = 0.23534057\n",
      "Iteration 126, loss = 0.23525056\n",
      "Iteration 127, loss = 0.23520310\n",
      "Iteration 128, loss = 0.23518991\n",
      "Iteration 129, loss = 0.23514079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88432351\n",
      "Iteration 2, loss = 0.70191667\n",
      "Iteration 3, loss = 0.56868948\n",
      "Iteration 4, loss = 0.48574577\n",
      "Iteration 5, loss = 0.43301317\n",
      "Iteration 6, loss = 0.39720539\n",
      "Iteration 7, loss = 0.37112188\n",
      "Iteration 8, loss = 0.35178336\n",
      "Iteration 9, loss = 0.33674845\n",
      "Iteration 10, loss = 0.32475020\n",
      "Iteration 11, loss = 0.31501095\n",
      "Iteration 12, loss = 0.30715312\n",
      "Iteration 13, loss = 0.30048246\n",
      "Iteration 14, loss = 0.29475581\n",
      "Iteration 15, loss = 0.28994956\n",
      "Iteration 16, loss = 0.28576439\n",
      "Iteration 17, loss = 0.28221413\n",
      "Iteration 18, loss = 0.27893544\n",
      "Iteration 19, loss = 0.27611841\n",
      "Iteration 20, loss = 0.27357464\n",
      "Iteration 21, loss = 0.27133762\n",
      "Iteration 22, loss = 0.26926142\n",
      "Iteration 23, loss = 0.26745385\n",
      "Iteration 24, loss = 0.26576726\n",
      "Iteration 25, loss = 0.26417969\n",
      "Iteration 26, loss = 0.26276914\n",
      "Iteration 27, loss = 0.26145268\n",
      "Iteration 28, loss = 0.26024892\n",
      "Iteration 29, loss = 0.25910262\n",
      "Iteration 30, loss = 0.25806000\n",
      "Iteration 31, loss = 0.25703533\n",
      "Iteration 32, loss = 0.25609342\n",
      "Iteration 33, loss = 0.25520132\n",
      "Iteration 34, loss = 0.25438596\n",
      "Iteration 35, loss = 0.25354516\n",
      "Iteration 36, loss = 0.25283108\n",
      "Iteration 37, loss = 0.25210202\n",
      "Iteration 38, loss = 0.25143548\n",
      "Iteration 39, loss = 0.25076905\n",
      "Iteration 40, loss = 0.25015302\n",
      "Iteration 41, loss = 0.24955799\n",
      "Iteration 42, loss = 0.24899755\n",
      "Iteration 43, loss = 0.24841727\n",
      "Iteration 44, loss = 0.24792250\n",
      "Iteration 45, loss = 0.24736994\n",
      "Iteration 46, loss = 0.24686088\n",
      "Iteration 47, loss = 0.24640147\n",
      "Iteration 48, loss = 0.24594256\n",
      "Iteration 49, loss = 0.24549890\n",
      "Iteration 50, loss = 0.24504055\n",
      "Iteration 51, loss = 0.24465783\n",
      "Iteration 52, loss = 0.24423997\n",
      "Iteration 53, loss = 0.24381751\n",
      "Iteration 54, loss = 0.24344698\n",
      "Iteration 55, loss = 0.24305451\n",
      "Iteration 56, loss = 0.24269904\n",
      "Iteration 57, loss = 0.24235004\n",
      "Iteration 58, loss = 0.24201384\n",
      "Iteration 59, loss = 0.24165489\n",
      "Iteration 60, loss = 0.24131823\n",
      "Iteration 61, loss = 0.24101364\n",
      "Iteration 62, loss = 0.24067488\n",
      "Iteration 63, loss = 0.24045212\n",
      "Iteration 64, loss = 0.24012329\n",
      "Iteration 65, loss = 0.23987039\n",
      "Iteration 66, loss = 0.23959420\n",
      "Iteration 67, loss = 0.23925824\n",
      "Iteration 68, loss = 0.23907006\n",
      "Iteration 69, loss = 0.23873308\n",
      "Iteration 70, loss = 0.23847699\n",
      "Iteration 71, loss = 0.23827868\n",
      "Iteration 72, loss = 0.23800978\n",
      "Iteration 73, loss = 0.23776982\n",
      "Iteration 74, loss = 0.23753420\n",
      "Iteration 75, loss = 0.23732942\n",
      "Iteration 76, loss = 0.23706316\n",
      "Iteration 77, loss = 0.23686497\n",
      "Iteration 78, loss = 0.23666819\n",
      "Iteration 79, loss = 0.23648123\n",
      "Iteration 80, loss = 0.23618868\n",
      "Iteration 81, loss = 0.23603822\n",
      "Iteration 82, loss = 0.23584155\n",
      "Iteration 83, loss = 0.23566424\n",
      "Iteration 84, loss = 0.23547095\n",
      "Iteration 85, loss = 0.23534064\n",
      "Iteration 86, loss = 0.23512474\n",
      "Iteration 87, loss = 0.23499076\n",
      "Iteration 88, loss = 0.23476299\n",
      "Iteration 89, loss = 0.23467628\n",
      "Iteration 90, loss = 0.23447426\n",
      "Iteration 91, loss = 0.23431458\n",
      "Iteration 92, loss = 0.23413251\n",
      "Iteration 93, loss = 0.23401635\n",
      "Iteration 94, loss = 0.23388597\n",
      "Iteration 95, loss = 0.23371997\n",
      "Iteration 96, loss = 0.23357268\n",
      "Iteration 97, loss = 0.23346980\n",
      "Iteration 98, loss = 0.23331219\n",
      "Iteration 99, loss = 0.23319983\n",
      "Iteration 100, loss = 0.23305704\n",
      "Iteration 101, loss = 0.23292961\n",
      "Iteration 102, loss = 0.23282262\n",
      "Iteration 103, loss = 0.23273773\n",
      "Iteration 104, loss = 0.23257086\n",
      "Iteration 105, loss = 0.23247098\n",
      "Iteration 106, loss = 0.23242071\n",
      "Iteration 107, loss = 0.23225625\n",
      "Iteration 108, loss = 0.23214968\n",
      "Iteration 109, loss = 0.23206749\n",
      "Iteration 110, loss = 0.23194642\n",
      "Iteration 111, loss = 0.23186288\n",
      "Iteration 112, loss = 0.23175040\n",
      "Iteration 113, loss = 0.23167574\n",
      "Iteration 114, loss = 0.23164322\n",
      "Iteration 115, loss = 0.23143817\n",
      "Iteration 116, loss = 0.23139425\n",
      "Iteration 117, loss = 0.23128923\n",
      "Iteration 118, loss = 0.23120818\n",
      "Iteration 119, loss = 0.23113771\n",
      "Iteration 120, loss = 0.23109854\n",
      "Iteration 121, loss = 0.23097760\n",
      "Iteration 122, loss = 0.23095187\n",
      "Iteration 123, loss = 0.23081082\n",
      "Iteration 124, loss = 0.23075418\n",
      "Iteration 125, loss = 0.23066508\n",
      "Iteration 126, loss = 0.23062080\n",
      "Iteration 127, loss = 0.23055185\n",
      "Iteration 128, loss = 0.23055551\n",
      "Iteration 129, loss = 0.23043766\n",
      "Iteration 130, loss = 0.23034947\n",
      "Iteration 131, loss = 0.23029346\n",
      "Iteration 132, loss = 0.23026931\n",
      "Iteration 133, loss = 0.23018677\n",
      "Iteration 134, loss = 0.23015900\n",
      "Iteration 135, loss = 0.23009900\n",
      "Iteration 136, loss = 0.23002267\n",
      "Iteration 137, loss = 0.22994316\n",
      "Iteration 138, loss = 0.22992731\n",
      "Iteration 139, loss = 0.22984978\n",
      "Iteration 140, loss = 0.22983555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89256025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.70565267\n",
      "Iteration 3, loss = 0.57045223\n",
      "Iteration 4, loss = 0.48759725\n",
      "Iteration 5, loss = 0.43508989\n",
      "Iteration 6, loss = 0.39945058\n",
      "Iteration 7, loss = 0.37388406\n",
      "Iteration 8, loss = 0.35488306\n",
      "Iteration 9, loss = 0.34001467\n",
      "Iteration 10, loss = 0.32826842\n",
      "Iteration 11, loss = 0.31875170\n",
      "Iteration 12, loss = 0.31102225\n",
      "Iteration 13, loss = 0.30448319\n",
      "Iteration 14, loss = 0.29890660\n",
      "Iteration 15, loss = 0.29418924\n",
      "Iteration 16, loss = 0.29009659\n",
      "Iteration 17, loss = 0.28660235\n",
      "Iteration 18, loss = 0.28343741\n",
      "Iteration 19, loss = 0.28069827\n",
      "Iteration 20, loss = 0.27818588\n",
      "Iteration 21, loss = 0.27601602\n",
      "Iteration 22, loss = 0.27404925\n",
      "Iteration 23, loss = 0.27226136\n",
      "Iteration 24, loss = 0.27060285\n",
      "Iteration 25, loss = 0.26906419\n",
      "Iteration 26, loss = 0.26768619\n",
      "Iteration 27, loss = 0.26642678\n",
      "Iteration 28, loss = 0.26520664\n",
      "Iteration 29, loss = 0.26412989\n",
      "Iteration 30, loss = 0.26310191\n",
      "Iteration 31, loss = 0.26214096\n",
      "Iteration 32, loss = 0.26117872\n",
      "Iteration 33, loss = 0.26030872\n",
      "Iteration 34, loss = 0.25950118\n",
      "Iteration 35, loss = 0.25874216\n",
      "Iteration 36, loss = 0.25799670\n",
      "Iteration 37, loss = 0.25729814\n",
      "Iteration 38, loss = 0.25665366\n",
      "Iteration 39, loss = 0.25599405\n",
      "Iteration 40, loss = 0.25542710\n",
      "Iteration 41, loss = 0.25480617\n",
      "Iteration 42, loss = 0.25426107\n",
      "Iteration 43, loss = 0.25370060\n",
      "Iteration 44, loss = 0.25320600\n",
      "Iteration 45, loss = 0.25269892\n",
      "Iteration 46, loss = 0.25219576\n",
      "Iteration 47, loss = 0.25172782\n",
      "Iteration 48, loss = 0.25127159\n",
      "Iteration 49, loss = 0.25084952\n",
      "Iteration 50, loss = 0.25036349\n",
      "Iteration 51, loss = 0.25000678\n",
      "Iteration 52, loss = 0.24958207\n",
      "Iteration 53, loss = 0.24917613\n",
      "Iteration 54, loss = 0.24881839\n",
      "Iteration 55, loss = 0.24842540\n",
      "Iteration 56, loss = 0.24808286\n",
      "Iteration 57, loss = 0.24772741\n",
      "Iteration 58, loss = 0.24741461\n",
      "Iteration 59, loss = 0.24703506\n",
      "Iteration 60, loss = 0.24673605\n",
      "Iteration 61, loss = 0.24641796\n",
      "Iteration 62, loss = 0.24610007\n",
      "Iteration 63, loss = 0.24588924\n",
      "Iteration 64, loss = 0.24551520\n",
      "Iteration 65, loss = 0.24526592\n",
      "Iteration 66, loss = 0.24497154\n",
      "Iteration 67, loss = 0.24472457\n",
      "Iteration 68, loss = 0.24447173\n",
      "Iteration 69, loss = 0.24417434\n",
      "Iteration 70, loss = 0.24394411\n",
      "Iteration 71, loss = 0.24369283\n",
      "Iteration 72, loss = 0.24343865\n",
      "Iteration 73, loss = 0.24320061\n",
      "Iteration 74, loss = 0.24297508\n",
      "Iteration 75, loss = 0.24280598\n",
      "Iteration 76, loss = 0.24255035\n",
      "Iteration 77, loss = 0.24230637\n",
      "Iteration 78, loss = 0.24212532\n",
      "Iteration 79, loss = 0.24195679\n",
      "Iteration 80, loss = 0.24171853\n",
      "Iteration 81, loss = 0.24151585\n",
      "Iteration 82, loss = 0.24133526\n",
      "Iteration 83, loss = 0.24117047\n",
      "Iteration 84, loss = 0.24098804\n",
      "Iteration 85, loss = 0.24081970\n",
      "Iteration 86, loss = 0.24062529\n",
      "Iteration 87, loss = 0.24048019\n",
      "Iteration 88, loss = 0.24027857\n",
      "Iteration 89, loss = 0.24013710\n",
      "Iteration 90, loss = 0.23999485\n",
      "Iteration 91, loss = 0.23984329\n",
      "Iteration 92, loss = 0.23965508\n",
      "Iteration 93, loss = 0.23955865\n",
      "Iteration 94, loss = 0.23940640\n",
      "Iteration 95, loss = 0.23926446\n",
      "Iteration 96, loss = 0.23914709\n",
      "Iteration 97, loss = 0.23896645\n",
      "Iteration 98, loss = 0.23887892\n",
      "Iteration 99, loss = 0.23874136\n",
      "Iteration 100, loss = 0.23863476\n",
      "Iteration 101, loss = 0.23848944\n",
      "Iteration 102, loss = 0.23836527\n",
      "Iteration 103, loss = 0.23830284\n",
      "Iteration 104, loss = 0.23815064\n",
      "Iteration 105, loss = 0.23805091\n",
      "Iteration 106, loss = 0.23795747\n",
      "Iteration 107, loss = 0.23784207\n",
      "Iteration 108, loss = 0.23771760\n",
      "Iteration 109, loss = 0.23771432\n",
      "Iteration 110, loss = 0.23752712\n",
      "Iteration 111, loss = 0.23744323\n",
      "Iteration 112, loss = 0.23733707\n",
      "Iteration 113, loss = 0.23728982\n",
      "Iteration 114, loss = 0.23720281\n",
      "Iteration 115, loss = 0.23708599\n",
      "Iteration 116, loss = 0.23701516\n",
      "Iteration 117, loss = 0.23694218\n",
      "Iteration 118, loss = 0.23681531\n",
      "Iteration 119, loss = 0.23677082\n",
      "Iteration 120, loss = 0.23671110\n",
      "Iteration 121, loss = 0.23660378\n",
      "Iteration 122, loss = 0.23655960\n",
      "Iteration 123, loss = 0.23649580\n",
      "Iteration 124, loss = 0.23642874\n",
      "Iteration 125, loss = 0.23631810\n",
      "Iteration 126, loss = 0.23631141\n",
      "Iteration 127, loss = 0.23620030\n",
      "Iteration 128, loss = 0.23627770\n",
      "Iteration 129, loss = 0.23611818\n",
      "Iteration 130, loss = 0.23600210\n",
      "Iteration 131, loss = 0.23594895\n",
      "Iteration 132, loss = 0.23594147\n",
      "Iteration 133, loss = 0.23583956\n",
      "Iteration 134, loss = 0.23584925\n",
      "Iteration 135, loss = 0.23577007\n",
      "Iteration 136, loss = 0.23570727\n",
      "Iteration 137, loss = 0.23570399\n",
      "Iteration 138, loss = 0.23564981\n",
      "Iteration 139, loss = 0.23555230\n",
      "Iteration 140, loss = 0.23555140\n",
      "Iteration 141, loss = 0.23549726\n",
      "Iteration 142, loss = 0.23543491\n",
      "Iteration 143, loss = 0.23539194\n",
      "Iteration 144, loss = 0.23540971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88641611\n",
      "Iteration 2, loss = 0.70323650\n",
      "Iteration 3, loss = 0.56971522\n",
      "Iteration 4, loss = 0.48662580\n",
      "Iteration 5, loss = 0.43394699\n",
      "Iteration 6, loss = 0.39785771\n",
      "Iteration 7, loss = 0.37219961\n",
      "Iteration 8, loss = 0.35291949\n",
      "Iteration 9, loss = 0.33792891\n",
      "Iteration 10, loss = 0.32607349\n",
      "Iteration 11, loss = 0.31643983\n",
      "Iteration 12, loss = 0.30859551\n",
      "Iteration 13, loss = 0.30199897\n",
      "Iteration 14, loss = 0.29634405\n",
      "Iteration 15, loss = 0.29156216\n",
      "Iteration 16, loss = 0.28735640\n",
      "Iteration 17, loss = 0.28380539\n",
      "Iteration 18, loss = 0.28058177\n",
      "Iteration 19, loss = 0.27776582\n",
      "Iteration 20, loss = 0.27520229\n",
      "Iteration 21, loss = 0.27297978\n",
      "Iteration 22, loss = 0.27095962\n",
      "Iteration 23, loss = 0.26915053\n",
      "Iteration 24, loss = 0.26742765\n",
      "Iteration 25, loss = 0.26587845\n",
      "Iteration 26, loss = 0.26447117\n",
      "Iteration 27, loss = 0.26324799\n",
      "Iteration 28, loss = 0.26199012\n",
      "Iteration 29, loss = 0.26086242\n",
      "Iteration 30, loss = 0.25981325\n",
      "Iteration 31, loss = 0.25884457\n",
      "Iteration 32, loss = 0.25789534\n",
      "Iteration 33, loss = 0.25702730\n",
      "Iteration 34, loss = 0.25623456\n",
      "Iteration 35, loss = 0.25546387\n",
      "Iteration 36, loss = 0.25475946\n",
      "Iteration 37, loss = 0.25402890\n",
      "Iteration 38, loss = 0.25337135\n",
      "Iteration 39, loss = 0.25274507\n",
      "Iteration 40, loss = 0.25217845\n",
      "Iteration 41, loss = 0.25159273\n",
      "Iteration 42, loss = 0.25101718\n",
      "Iteration 43, loss = 0.25047317\n",
      "Iteration 44, loss = 0.24996616\n",
      "Iteration 45, loss = 0.24947926\n",
      "Iteration 46, loss = 0.24899634\n",
      "Iteration 47, loss = 0.24855145\n",
      "Iteration 48, loss = 0.24809837\n",
      "Iteration 49, loss = 0.24769900\n",
      "Iteration 50, loss = 0.24725038\n",
      "Iteration 51, loss = 0.24686011\n",
      "Iteration 52, loss = 0.24647935\n",
      "Iteration 53, loss = 0.24607027\n",
      "Iteration 54, loss = 0.24570952\n",
      "Iteration 55, loss = 0.24535053\n",
      "Iteration 56, loss = 0.24502173\n",
      "Iteration 57, loss = 0.24467629\n",
      "Iteration 58, loss = 0.24434018\n",
      "Iteration 59, loss = 0.24399758\n",
      "Iteration 60, loss = 0.24370177\n",
      "Iteration 61, loss = 0.24337674\n",
      "Iteration 62, loss = 0.24308415\n",
      "Iteration 63, loss = 0.24284648\n",
      "Iteration 64, loss = 0.24252019\n",
      "Iteration 65, loss = 0.24223455\n",
      "Iteration 66, loss = 0.24198295\n",
      "Iteration 67, loss = 0.24174699\n",
      "Iteration 68, loss = 0.24145793\n",
      "Iteration 69, loss = 0.24122276\n",
      "Iteration 70, loss = 0.24097627\n",
      "Iteration 71, loss = 0.24071660\n",
      "Iteration 72, loss = 0.24051658\n",
      "Iteration 73, loss = 0.24024739\n",
      "Iteration 74, loss = 0.24002887\n",
      "Iteration 75, loss = 0.23988115\n",
      "Iteration 76, loss = 0.23958204\n",
      "Iteration 77, loss = 0.23936453\n",
      "Iteration 78, loss = 0.23919288\n",
      "Iteration 79, loss = 0.23901846\n",
      "Iteration 80, loss = 0.23877996\n",
      "Iteration 81, loss = 0.23856013\n",
      "Iteration 82, loss = 0.23837170\n",
      "Iteration 83, loss = 0.23819851\n",
      "Iteration 84, loss = 0.23802402\n",
      "Iteration 85, loss = 0.23782817\n",
      "Iteration 86, loss = 0.23770915\n",
      "Iteration 87, loss = 0.23752849\n",
      "Iteration 88, loss = 0.23731179\n",
      "Iteration 89, loss = 0.23720135\n",
      "Iteration 90, loss = 0.23705272\n",
      "Iteration 91, loss = 0.23686618\n",
      "Iteration 92, loss = 0.23669262\n",
      "Iteration 93, loss = 0.23656983\n",
      "Iteration 94, loss = 0.23642488\n",
      "Iteration 95, loss = 0.23624749\n",
      "Iteration 96, loss = 0.23612645\n",
      "Iteration 97, loss = 0.23597510\n",
      "Iteration 98, loss = 0.23588284\n",
      "Iteration 99, loss = 0.23577644\n",
      "Iteration 100, loss = 0.23560891\n",
      "Iteration 101, loss = 0.23550445\n",
      "Iteration 102, loss = 0.23534648\n",
      "Iteration 103, loss = 0.23527033\n",
      "Iteration 104, loss = 0.23514409\n",
      "Iteration 105, loss = 0.23503311\n",
      "Iteration 106, loss = 0.23488519\n",
      "Iteration 107, loss = 0.23481409\n",
      "Iteration 108, loss = 0.23470042\n",
      "Iteration 109, loss = 0.23463417\n",
      "Iteration 110, loss = 0.23447675\n",
      "Iteration 111, loss = 0.23437549\n",
      "Iteration 112, loss = 0.23426515\n",
      "Iteration 113, loss = 0.23419893\n",
      "Iteration 114, loss = 0.23407424\n",
      "Iteration 115, loss = 0.23399421\n",
      "Iteration 116, loss = 0.23394834\n",
      "Iteration 117, loss = 0.23382980\n",
      "Iteration 118, loss = 0.23373008\n",
      "Iteration 119, loss = 0.23365716\n",
      "Iteration 120, loss = 0.23360238\n",
      "Iteration 121, loss = 0.23349196\n",
      "Iteration 122, loss = 0.23345622\n",
      "Iteration 123, loss = 0.23335400\n",
      "Iteration 124, loss = 0.23330406\n",
      "Iteration 125, loss = 0.23318196\n",
      "Iteration 126, loss = 0.23315672\n",
      "Iteration 127, loss = 0.23306858\n",
      "Iteration 128, loss = 0.23303829\n",
      "Iteration 129, loss = 0.23293029\n",
      "Iteration 130, loss = 0.23282980\n",
      "Iteration 131, loss = 0.23281025\n",
      "Iteration 132, loss = 0.23273919\n",
      "Iteration 133, loss = 0.23266175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 134, loss = 0.23263984\n",
      "Iteration 135, loss = 0.23255569\n",
      "Iteration 136, loss = 0.23250334\n",
      "Iteration 137, loss = 0.23246433\n",
      "Iteration 138, loss = 0.23244531\n",
      "Iteration 139, loss = 0.23233346\n",
      "Iteration 140, loss = 0.23231758\n",
      "Iteration 141, loss = 0.23231105\n",
      "Iteration 142, loss = 0.23227407\n",
      "Iteration 143, loss = 0.23217533\n",
      "Iteration 144, loss = 0.23217285\n",
      "Iteration 145, loss = 0.23206079\n",
      "Iteration 146, loss = 0.23200406\n",
      "Iteration 147, loss = 0.23196192\n",
      "Iteration 148, loss = 0.23192897\n",
      "Iteration 149, loss = 0.23186524\n",
      "Iteration 150, loss = 0.23186857\n",
      "Iteration 151, loss = 0.23182260\n",
      "Iteration 152, loss = 0.23180355\n",
      "Iteration 153, loss = 0.23169308\n",
      "Iteration 154, loss = 0.23163806\n",
      "Iteration 155, loss = 0.23162844\n",
      "Iteration 156, loss = 0.23163063\n",
      "Iteration 157, loss = 0.23160610\n",
      "Iteration 158, loss = 0.23156015\n",
      "Iteration 159, loss = 0.23154819\n",
      "Iteration 160, loss = 0.23148580\n",
      "Iteration 161, loss = 0.23145206\n",
      "Iteration 162, loss = 0.23142120\n",
      "Iteration 163, loss = 0.23138391\n",
      "Iteration 164, loss = 0.23136876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89324538\n",
      "Iteration 2, loss = 0.70890846\n",
      "Iteration 3, loss = 0.57312111\n",
      "Iteration 4, loss = 0.48958888\n",
      "Iteration 5, loss = 0.43573465\n",
      "Iteration 6, loss = 0.39891712\n",
      "Iteration 7, loss = 0.37264163\n",
      "Iteration 8, loss = 0.35289778\n",
      "Iteration 9, loss = 0.33724561\n",
      "Iteration 10, loss = 0.32509986\n",
      "Iteration 11, loss = 0.31505746\n",
      "Iteration 12, loss = 0.30686524\n",
      "Iteration 13, loss = 0.30003444\n",
      "Iteration 14, loss = 0.29418867\n",
      "Iteration 15, loss = 0.28921552\n",
      "Iteration 16, loss = 0.28489693\n",
      "Iteration 17, loss = 0.28119034\n",
      "Iteration 18, loss = 0.27792282\n",
      "Iteration 19, loss = 0.27496169\n",
      "Iteration 20, loss = 0.27233377\n",
      "Iteration 21, loss = 0.27004423\n",
      "Iteration 22, loss = 0.26792841\n",
      "Iteration 23, loss = 0.26607085\n",
      "Iteration 24, loss = 0.26426905\n",
      "Iteration 25, loss = 0.26269169\n",
      "Iteration 26, loss = 0.26121040\n",
      "Iteration 27, loss = 0.25992556\n",
      "Iteration 28, loss = 0.25863586\n",
      "Iteration 29, loss = 0.25748551\n",
      "Iteration 30, loss = 0.25640853\n",
      "Iteration 31, loss = 0.25541805\n",
      "Iteration 32, loss = 0.25438170\n",
      "Iteration 33, loss = 0.25352907\n",
      "Iteration 34, loss = 0.25265790\n",
      "Iteration 35, loss = 0.25188249\n",
      "Iteration 36, loss = 0.25110424\n",
      "Iteration 37, loss = 0.25038825\n",
      "Iteration 38, loss = 0.24971162\n",
      "Iteration 39, loss = 0.24907560\n",
      "Iteration 40, loss = 0.24850005\n",
      "Iteration 41, loss = 0.24784728\n",
      "Iteration 42, loss = 0.24727612\n",
      "Iteration 43, loss = 0.24672633\n",
      "Iteration 44, loss = 0.24618120\n",
      "Iteration 45, loss = 0.24570496\n",
      "Iteration 46, loss = 0.24520852\n",
      "Iteration 47, loss = 0.24475202\n",
      "Iteration 48, loss = 0.24432859\n",
      "Iteration 49, loss = 0.24387949\n",
      "Iteration 50, loss = 0.24340955\n",
      "Iteration 51, loss = 0.24303566\n",
      "Iteration 52, loss = 0.24263837\n",
      "Iteration 53, loss = 0.24222366\n",
      "Iteration 54, loss = 0.24184682\n",
      "Iteration 55, loss = 0.24150242\n",
      "Iteration 56, loss = 0.24114073\n",
      "Iteration 57, loss = 0.24078927\n",
      "Iteration 58, loss = 0.24048852\n",
      "Iteration 59, loss = 0.24011819\n",
      "Iteration 60, loss = 0.23982539\n",
      "Iteration 61, loss = 0.23954486\n",
      "Iteration 62, loss = 0.23922828\n",
      "Iteration 63, loss = 0.23891517\n",
      "Iteration 64, loss = 0.23869076\n",
      "Iteration 65, loss = 0.23840742\n",
      "Iteration 66, loss = 0.23811765\n",
      "Iteration 67, loss = 0.23786744\n",
      "Iteration 68, loss = 0.23758637\n",
      "Iteration 69, loss = 0.23736254\n",
      "Iteration 70, loss = 0.23716822\n",
      "Iteration 71, loss = 0.23683356\n",
      "Iteration 72, loss = 0.23665735\n",
      "Iteration 73, loss = 0.23641431\n",
      "Iteration 74, loss = 0.23618358\n",
      "Iteration 75, loss = 0.23601380\n",
      "Iteration 76, loss = 0.23577048\n",
      "Iteration 77, loss = 0.23555614\n",
      "Iteration 78, loss = 0.23536533\n",
      "Iteration 79, loss = 0.23517707\n",
      "Iteration 80, loss = 0.23497922\n",
      "Iteration 81, loss = 0.23477143\n",
      "Iteration 82, loss = 0.23459386\n",
      "Iteration 83, loss = 0.23437694\n",
      "Iteration 84, loss = 0.23423125\n",
      "Iteration 85, loss = 0.23401932\n",
      "Iteration 86, loss = 0.23393085\n",
      "Iteration 87, loss = 0.23371236\n",
      "Iteration 88, loss = 0.23355771\n",
      "Iteration 89, loss = 0.23343951\n",
      "Iteration 90, loss = 0.23321851\n",
      "Iteration 91, loss = 0.23311204\n",
      "Iteration 92, loss = 0.23294682\n",
      "Iteration 93, loss = 0.23279077\n",
      "Iteration 94, loss = 0.23267172\n",
      "Iteration 95, loss = 0.23253059\n",
      "Iteration 96, loss = 0.23237522\n",
      "Iteration 97, loss = 0.23224286\n",
      "Iteration 98, loss = 0.23211119\n",
      "Iteration 99, loss = 0.23200284\n",
      "Iteration 100, loss = 0.23186314\n",
      "Iteration 101, loss = 0.23175723\n",
      "Iteration 102, loss = 0.23161874\n",
      "Iteration 103, loss = 0.23151520\n",
      "Iteration 104, loss = 0.23144714\n",
      "Iteration 105, loss = 0.23126691\n",
      "Iteration 106, loss = 0.23116993\n",
      "Iteration 107, loss = 0.23109245\n",
      "Iteration 108, loss = 0.23101012\n",
      "Iteration 109, loss = 0.23084947\n",
      "Iteration 110, loss = 0.23078109\n",
      "Iteration 111, loss = 0.23069749\n",
      "Iteration 112, loss = 0.23057542\n",
      "Iteration 113, loss = 0.23050328\n",
      "Iteration 114, loss = 0.23036433\n",
      "Iteration 115, loss = 0.23031875\n",
      "Iteration 116, loss = 0.23024892\n",
      "Iteration 117, loss = 0.23012638\n",
      "Iteration 118, loss = 0.23004114\n",
      "Iteration 119, loss = 0.22998825\n",
      "Iteration 120, loss = 0.22989266\n",
      "Iteration 121, loss = 0.22983825\n",
      "Iteration 122, loss = 0.22973573\n",
      "Iteration 123, loss = 0.22968113\n",
      "Iteration 124, loss = 0.22961063\n",
      "Iteration 125, loss = 0.22949745\n",
      "Iteration 126, loss = 0.22946423\n",
      "Iteration 127, loss = 0.22938802\n",
      "Iteration 128, loss = 0.22933289\n",
      "Iteration 129, loss = 0.22925112\n",
      "Iteration 130, loss = 0.22917380\n",
      "Iteration 131, loss = 0.22915106\n",
      "Iteration 132, loss = 0.22911291\n",
      "Iteration 133, loss = 0.22900591\n",
      "Iteration 134, loss = 0.22899499\n",
      "Iteration 135, loss = 0.22889345\n",
      "Iteration 136, loss = 0.22887979\n",
      "Iteration 137, loss = 0.22877425\n",
      "Iteration 138, loss = 0.22880081\n",
      "Iteration 139, loss = 0.22869279\n",
      "Iteration 140, loss = 0.22866567\n",
      "Iteration 141, loss = 0.22867246\n",
      "Iteration 142, loss = 0.22859484\n",
      "Iteration 143, loss = 0.22855416\n",
      "Iteration 144, loss = 0.22852261\n",
      "Iteration 145, loss = 0.22844134\n",
      "Iteration 146, loss = 0.22841308\n",
      "Iteration 147, loss = 0.22831805\n",
      "Iteration 148, loss = 0.22830858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73098371\n",
      "Iteration 2, loss = 0.48479169\n",
      "Iteration 3, loss = 0.38315967\n",
      "Iteration 4, loss = 0.32689555\n",
      "Iteration 5, loss = 0.29410025\n",
      "Iteration 6, loss = 0.27442266\n",
      "Iteration 7, loss = 0.26086390\n",
      "Iteration 8, loss = 0.25357945\n",
      "Iteration 9, loss = 0.24853431\n",
      "Iteration 10, loss = 0.24525197\n",
      "Iteration 11, loss = 0.24125787\n",
      "Iteration 12, loss = 0.23995927\n",
      "Iteration 13, loss = 0.23835283\n",
      "Iteration 14, loss = 0.23785220\n",
      "Iteration 15, loss = 0.23719740\n",
      "Iteration 16, loss = 0.23677779\n",
      "Iteration 17, loss = 0.23656306\n",
      "Iteration 18, loss = 0.23650748\n",
      "Iteration 19, loss = 0.23598080\n",
      "Iteration 20, loss = 0.23529723\n",
      "Iteration 21, loss = 0.23532176\n",
      "Iteration 22, loss = 0.23495442\n",
      "Iteration 23, loss = 0.23564359\n",
      "Iteration 24, loss = 0.23553508\n",
      "Iteration 25, loss = 0.23548774\n",
      "Iteration 26, loss = 0.23562878\n",
      "Iteration 27, loss = 0.23540206\n",
      "Iteration 28, loss = 0.23515715\n",
      "Iteration 29, loss = 0.23528438\n",
      "Iteration 30, loss = 0.23588526\n",
      "Iteration 31, loss = 0.23522492\n",
      "Iteration 32, loss = 0.23502474\n",
      "Iteration 33, loss = 0.23601686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73166792\n",
      "Iteration 2, loss = 0.48348983\n",
      "Iteration 3, loss = 0.38193879\n",
      "Iteration 4, loss = 0.32628096\n",
      "Iteration 5, loss = 0.29272008\n",
      "Iteration 6, loss = 0.27274807\n",
      "Iteration 7, loss = 0.25945813\n",
      "Iteration 8, loss = 0.25139805\n",
      "Iteration 9, loss = 0.24620021\n",
      "Iteration 10, loss = 0.24138148\n",
      "Iteration 11, loss = 0.23761596\n",
      "Iteration 12, loss = 0.23708076\n",
      "Iteration 13, loss = 0.23496284\n",
      "Iteration 14, loss = 0.23324242\n",
      "Iteration 15, loss = 0.23264719\n",
      "Iteration 16, loss = 0.23191143\n",
      "Iteration 17, loss = 0.23130667\n",
      "Iteration 18, loss = 0.23132143\n",
      "Iteration 19, loss = 0.23119239\n",
      "Iteration 20, loss = 0.23079971\n",
      "Iteration 21, loss = 0.23071909\n",
      "Iteration 22, loss = 0.23013856\n",
      "Iteration 23, loss = 0.23062271\n",
      "Iteration 24, loss = 0.23082663\n",
      "Iteration 25, loss = 0.23048860\n",
      "Iteration 26, loss = 0.22988198\n",
      "Iteration 27, loss = 0.23007453\n",
      "Iteration 28, loss = 0.23036593\n",
      "Iteration 29, loss = 0.23065110\n",
      "Iteration 30, loss = 0.23068856\n",
      "Iteration 31, loss = 0.23003925\n",
      "Iteration 32, loss = 0.23004973\n",
      "Iteration 33, loss = 0.22998435\n",
      "Iteration 34, loss = 0.23089692\n",
      "Iteration 35, loss = 0.22973781\n",
      "Iteration 36, loss = 0.23025740\n",
      "Iteration 37, loss = 0.23059655\n",
      "Iteration 38, loss = 0.23054422\n",
      "Iteration 39, loss = 0.23014665\n",
      "Iteration 40, loss = 0.23052936\n",
      "Iteration 41, loss = 0.23060476\n",
      "Iteration 42, loss = 0.23107695\n",
      "Iteration 43, loss = 0.23015960\n",
      "Iteration 44, loss = 0.23043754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.23050537\n",
      "Iteration 46, loss = 0.23027303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73473933\n",
      "Iteration 2, loss = 0.48434968\n",
      "Iteration 3, loss = 0.38316836\n",
      "Iteration 4, loss = 0.32780616\n",
      "Iteration 5, loss = 0.29562681\n",
      "Iteration 6, loss = 0.27558949\n",
      "Iteration 7, loss = 0.26364934\n",
      "Iteration 8, loss = 0.25588993\n",
      "Iteration 9, loss = 0.24975894\n",
      "Iteration 10, loss = 0.24578602\n",
      "Iteration 11, loss = 0.24314972\n",
      "Iteration 12, loss = 0.24218812\n",
      "Iteration 13, loss = 0.23954639\n",
      "Iteration 14, loss = 0.23815988\n",
      "Iteration 15, loss = 0.23840793\n",
      "Iteration 16, loss = 0.23743035\n",
      "Iteration 17, loss = 0.23709208\n",
      "Iteration 18, loss = 0.23733780\n",
      "Iteration 19, loss = 0.23720475\n",
      "Iteration 20, loss = 0.23637652\n",
      "Iteration 21, loss = 0.23623242\n",
      "Iteration 22, loss = 0.23631179\n",
      "Iteration 23, loss = 0.23671983\n",
      "Iteration 24, loss = 0.23682372\n",
      "Iteration 25, loss = 0.23649758\n",
      "Iteration 26, loss = 0.23608133\n",
      "Iteration 27, loss = 0.23619184\n",
      "Iteration 28, loss = 0.23633322\n",
      "Iteration 29, loss = 0.23669552\n",
      "Iteration 30, loss = 0.23662080\n",
      "Iteration 31, loss = 0.23632486\n",
      "Iteration 32, loss = 0.23604921\n",
      "Iteration 33, loss = 0.23571661\n",
      "Iteration 34, loss = 0.23584300\n",
      "Iteration 35, loss = 0.23615239\n",
      "Iteration 36, loss = 0.23568567\n",
      "Iteration 37, loss = 0.23643463\n",
      "Iteration 38, loss = 0.23611771\n",
      "Iteration 39, loss = 0.23587798\n",
      "Iteration 40, loss = 0.23669157\n",
      "Iteration 41, loss = 0.23641061\n",
      "Iteration 42, loss = 0.23680284\n",
      "Iteration 43, loss = 0.23620750\n",
      "Iteration 44, loss = 0.23615601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72773083\n",
      "Iteration 2, loss = 0.48180831\n",
      "Iteration 3, loss = 0.38144959\n",
      "Iteration 4, loss = 0.32529411\n",
      "Iteration 5, loss = 0.29310423\n",
      "Iteration 6, loss = 0.27273885\n",
      "Iteration 7, loss = 0.26063824\n",
      "Iteration 8, loss = 0.25236573\n",
      "Iteration 9, loss = 0.24662374\n",
      "Iteration 10, loss = 0.24203405\n",
      "Iteration 11, loss = 0.24025770\n",
      "Iteration 12, loss = 0.23862668\n",
      "Iteration 13, loss = 0.23581464\n",
      "Iteration 14, loss = 0.23538796\n",
      "Iteration 15, loss = 0.23449547\n",
      "Iteration 16, loss = 0.23361389\n",
      "Iteration 17, loss = 0.23339568\n",
      "Iteration 18, loss = 0.23381429\n",
      "Iteration 19, loss = 0.23343664\n",
      "Iteration 20, loss = 0.23292300\n",
      "Iteration 21, loss = 0.23264149\n",
      "Iteration 22, loss = 0.23264843\n",
      "Iteration 23, loss = 0.23325971\n",
      "Iteration 24, loss = 0.23281423\n",
      "Iteration 25, loss = 0.23251442\n",
      "Iteration 26, loss = 0.23262414\n",
      "Iteration 27, loss = 0.23319753\n",
      "Iteration 28, loss = 0.23383057\n",
      "Iteration 29, loss = 0.23356300\n",
      "Iteration 30, loss = 0.23266607\n",
      "Iteration 31, loss = 0.23310424\n",
      "Iteration 32, loss = 0.23240987\n",
      "Iteration 33, loss = 0.23246565\n",
      "Iteration 34, loss = 0.23261204\n",
      "Iteration 35, loss = 0.23260464\n",
      "Iteration 36, loss = 0.23281583\n",
      "Iteration 37, loss = 0.23292613\n",
      "Iteration 38, loss = 0.23236859\n",
      "Iteration 39, loss = 0.23239811\n",
      "Iteration 40, loss = 0.23342281\n",
      "Iteration 41, loss = 0.23322972\n",
      "Iteration 42, loss = 0.23311962\n",
      "Iteration 43, loss = 0.23232341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73345883\n",
      "Iteration 2, loss = 0.48525626\n",
      "Iteration 3, loss = 0.38229654\n",
      "Iteration 4, loss = 0.32420694\n",
      "Iteration 5, loss = 0.29054943\n",
      "Iteration 6, loss = 0.26976559\n",
      "Iteration 7, loss = 0.25680933\n",
      "Iteration 8, loss = 0.24864399\n",
      "Iteration 9, loss = 0.24268985\n",
      "Iteration 10, loss = 0.23863462\n",
      "Iteration 11, loss = 0.23594764\n",
      "Iteration 12, loss = 0.23402162\n",
      "Iteration 13, loss = 0.23255300\n",
      "Iteration 14, loss = 0.23115024\n",
      "Iteration 15, loss = 0.23050859\n",
      "Iteration 16, loss = 0.23024422\n",
      "Iteration 17, loss = 0.22997592\n",
      "Iteration 18, loss = 0.23078871\n",
      "Iteration 19, loss = 0.23013840\n",
      "Iteration 20, loss = 0.22957206\n",
      "Iteration 21, loss = 0.22942052\n",
      "Iteration 22, loss = 0.22905388\n",
      "Iteration 23, loss = 0.23003999\n",
      "Iteration 24, loss = 0.22943601\n",
      "Iteration 25, loss = 0.22899622\n",
      "Iteration 26, loss = 0.22891783\n",
      "Iteration 27, loss = 0.22954472\n",
      "Iteration 28, loss = 0.22946159\n",
      "Iteration 29, loss = 0.23012194\n",
      "Iteration 30, loss = 0.22951260\n",
      "Iteration 31, loss = 0.22991427\n",
      "Iteration 32, loss = 0.22922604\n",
      "Iteration 33, loss = 0.22926863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88556097\n",
      "Iteration 2, loss = 0.70328294\n",
      "Iteration 3, loss = 0.57008677\n",
      "Iteration 4, loss = 0.48712322\n",
      "Iteration 5, loss = 0.43474580\n",
      "Iteration 6, loss = 0.39897719\n",
      "Iteration 7, loss = 0.37310783\n",
      "Iteration 8, loss = 0.35389019\n",
      "Iteration 9, loss = 0.33898799\n",
      "Iteration 10, loss = 0.32710566\n",
      "Iteration 11, loss = 0.31739883\n",
      "Iteration 12, loss = 0.30955543\n",
      "Iteration 13, loss = 0.30291018\n",
      "Iteration 14, loss = 0.29734783\n",
      "Iteration 15, loss = 0.29255580\n",
      "Iteration 16, loss = 0.28839603\n",
      "Iteration 17, loss = 0.28491861\n",
      "Iteration 18, loss = 0.28164867\n",
      "Iteration 19, loss = 0.27884589\n",
      "Iteration 20, loss = 0.27635250\n",
      "Iteration 21, loss = 0.27413679\n",
      "Iteration 22, loss = 0.27211307\n",
      "Iteration 23, loss = 0.27035117\n",
      "Iteration 24, loss = 0.26863662\n",
      "Iteration 25, loss = 0.26713367\n",
      "Iteration 26, loss = 0.26575598\n",
      "Iteration 27, loss = 0.26444587\n",
      "Iteration 28, loss = 0.26327483\n",
      "Iteration 29, loss = 0.26213130\n",
      "Iteration 30, loss = 0.26115015\n",
      "Iteration 31, loss = 0.26017097\n",
      "Iteration 32, loss = 0.25924883\n",
      "Iteration 33, loss = 0.25843177\n",
      "Iteration 34, loss = 0.25759605\n",
      "Iteration 35, loss = 0.25679891\n",
      "Iteration 36, loss = 0.25608912\n",
      "Iteration 37, loss = 0.25540106\n",
      "Iteration 38, loss = 0.25481982\n",
      "Iteration 39, loss = 0.25412806\n",
      "Iteration 40, loss = 0.25356766\n",
      "Iteration 41, loss = 0.25296355\n",
      "Iteration 42, loss = 0.25240178\n",
      "Iteration 43, loss = 0.25186592\n",
      "Iteration 44, loss = 0.25134728\n",
      "Iteration 45, loss = 0.25086605\n",
      "Iteration 46, loss = 0.25042087\n",
      "Iteration 47, loss = 0.24995317\n",
      "Iteration 48, loss = 0.24956810\n",
      "Iteration 49, loss = 0.24910189\n",
      "Iteration 50, loss = 0.24870775\n",
      "Iteration 51, loss = 0.24828680\n",
      "Iteration 52, loss = 0.24791496\n",
      "Iteration 53, loss = 0.24756154\n",
      "Iteration 54, loss = 0.24719839\n",
      "Iteration 55, loss = 0.24680902\n",
      "Iteration 56, loss = 0.24645998\n",
      "Iteration 57, loss = 0.24611190\n",
      "Iteration 58, loss = 0.24582514\n",
      "Iteration 59, loss = 0.24547923\n",
      "Iteration 60, loss = 0.24514974\n",
      "Iteration 61, loss = 0.24488897\n",
      "Iteration 62, loss = 0.24458157\n",
      "Iteration 63, loss = 0.24435060\n",
      "Iteration 64, loss = 0.24403362\n",
      "Iteration 65, loss = 0.24375275\n",
      "Iteration 66, loss = 0.24353022\n",
      "Iteration 67, loss = 0.24322527\n",
      "Iteration 68, loss = 0.24302871\n",
      "Iteration 69, loss = 0.24271749\n",
      "Iteration 70, loss = 0.24249282\n",
      "Iteration 71, loss = 0.24229709\n",
      "Iteration 72, loss = 0.24205529\n",
      "Iteration 73, loss = 0.24180730\n",
      "Iteration 74, loss = 0.24162217\n",
      "Iteration 75, loss = 0.24139047\n",
      "Iteration 76, loss = 0.24118432\n",
      "Iteration 77, loss = 0.24099720\n",
      "Iteration 78, loss = 0.24078375\n",
      "Iteration 79, loss = 0.24064081\n",
      "Iteration 80, loss = 0.24038977\n",
      "Iteration 81, loss = 0.24022718\n",
      "Iteration 82, loss = 0.24003566\n",
      "Iteration 83, loss = 0.23988003\n",
      "Iteration 84, loss = 0.23973784\n",
      "Iteration 85, loss = 0.23959212\n",
      "Iteration 86, loss = 0.23939646\n",
      "Iteration 87, loss = 0.23923662\n",
      "Iteration 88, loss = 0.23906995\n",
      "Iteration 89, loss = 0.23895082\n",
      "Iteration 90, loss = 0.23877311\n",
      "Iteration 91, loss = 0.23865314\n",
      "Iteration 92, loss = 0.23846632\n",
      "Iteration 93, loss = 0.23834868\n",
      "Iteration 94, loss = 0.23822930\n",
      "Iteration 95, loss = 0.23807858\n",
      "Iteration 96, loss = 0.23795733\n",
      "Iteration 97, loss = 0.23784475\n",
      "Iteration 98, loss = 0.23773499\n",
      "Iteration 99, loss = 0.23761296\n",
      "Iteration 100, loss = 0.23746870\n",
      "Iteration 101, loss = 0.23736833\n",
      "Iteration 102, loss = 0.23729934\n",
      "Iteration 103, loss = 0.23714508\n",
      "Iteration 104, loss = 0.23705498\n",
      "Iteration 105, loss = 0.23696745\n",
      "Iteration 106, loss = 0.23690212\n",
      "Iteration 107, loss = 0.23672607\n",
      "Iteration 108, loss = 0.23666555\n",
      "Iteration 109, loss = 0.23657914\n",
      "Iteration 110, loss = 0.23648063\n",
      "Iteration 111, loss = 0.23637742\n",
      "Iteration 112, loss = 0.23630443\n",
      "Iteration 113, loss = 0.23626326\n",
      "Iteration 114, loss = 0.23614017\n",
      "Iteration 115, loss = 0.23602426\n",
      "Iteration 116, loss = 0.23596700\n",
      "Iteration 117, loss = 0.23590495\n",
      "Iteration 118, loss = 0.23577668\n",
      "Iteration 119, loss = 0.23572606\n",
      "Iteration 120, loss = 0.23565587\n",
      "Iteration 121, loss = 0.23562712\n",
      "Iteration 122, loss = 0.23554144\n",
      "Iteration 123, loss = 0.23545733\n",
      "Iteration 124, loss = 0.23538451\n",
      "Iteration 125, loss = 0.23534057\n",
      "Iteration 126, loss = 0.23525056\n",
      "Iteration 127, loss = 0.23520310\n",
      "Iteration 128, loss = 0.23518991\n",
      "Iteration 129, loss = 0.23514079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88432351\n",
      "Iteration 2, loss = 0.70191667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.56868948\n",
      "Iteration 4, loss = 0.48574577\n",
      "Iteration 5, loss = 0.43301317\n",
      "Iteration 6, loss = 0.39720539\n",
      "Iteration 7, loss = 0.37112188\n",
      "Iteration 8, loss = 0.35178336\n",
      "Iteration 9, loss = 0.33674845\n",
      "Iteration 10, loss = 0.32475020\n",
      "Iteration 11, loss = 0.31501095\n",
      "Iteration 12, loss = 0.30715312\n",
      "Iteration 13, loss = 0.30048246\n",
      "Iteration 14, loss = 0.29475581\n",
      "Iteration 15, loss = 0.28994956\n",
      "Iteration 16, loss = 0.28576439\n",
      "Iteration 17, loss = 0.28221413\n",
      "Iteration 18, loss = 0.27893544\n",
      "Iteration 19, loss = 0.27611841\n",
      "Iteration 20, loss = 0.27357464\n",
      "Iteration 21, loss = 0.27133762\n",
      "Iteration 22, loss = 0.26926142\n",
      "Iteration 23, loss = 0.26745385\n",
      "Iteration 24, loss = 0.26576726\n",
      "Iteration 25, loss = 0.26417969\n",
      "Iteration 26, loss = 0.26276914\n",
      "Iteration 27, loss = 0.26145268\n",
      "Iteration 28, loss = 0.26024892\n",
      "Iteration 29, loss = 0.25910262\n",
      "Iteration 30, loss = 0.25806000\n",
      "Iteration 31, loss = 0.25703533\n",
      "Iteration 32, loss = 0.25609342\n",
      "Iteration 33, loss = 0.25520132\n",
      "Iteration 34, loss = 0.25438596\n",
      "Iteration 35, loss = 0.25354516\n",
      "Iteration 36, loss = 0.25283108\n",
      "Iteration 37, loss = 0.25210202\n",
      "Iteration 38, loss = 0.25143548\n",
      "Iteration 39, loss = 0.25076905\n",
      "Iteration 40, loss = 0.25015302\n",
      "Iteration 41, loss = 0.24955799\n",
      "Iteration 42, loss = 0.24899755\n",
      "Iteration 43, loss = 0.24841727\n",
      "Iteration 44, loss = 0.24792250\n",
      "Iteration 45, loss = 0.24736994\n",
      "Iteration 46, loss = 0.24686088\n",
      "Iteration 47, loss = 0.24640147\n",
      "Iteration 48, loss = 0.24594256\n",
      "Iteration 49, loss = 0.24549890\n",
      "Iteration 50, loss = 0.24504055\n",
      "Iteration 51, loss = 0.24465783\n",
      "Iteration 52, loss = 0.24423997\n",
      "Iteration 53, loss = 0.24381751\n",
      "Iteration 54, loss = 0.24344698\n",
      "Iteration 55, loss = 0.24305451\n",
      "Iteration 56, loss = 0.24269904\n",
      "Iteration 57, loss = 0.24235004\n",
      "Iteration 58, loss = 0.24201384\n",
      "Iteration 59, loss = 0.24165489\n",
      "Iteration 60, loss = 0.24131823\n",
      "Iteration 61, loss = 0.24101364\n",
      "Iteration 62, loss = 0.24067488\n",
      "Iteration 63, loss = 0.24045212\n",
      "Iteration 64, loss = 0.24012329\n",
      "Iteration 65, loss = 0.23987039\n",
      "Iteration 66, loss = 0.23959420\n",
      "Iteration 67, loss = 0.23925824\n",
      "Iteration 68, loss = 0.23907006\n",
      "Iteration 69, loss = 0.23873308\n",
      "Iteration 70, loss = 0.23847699\n",
      "Iteration 71, loss = 0.23827868\n",
      "Iteration 72, loss = 0.23800978\n",
      "Iteration 73, loss = 0.23776982\n",
      "Iteration 74, loss = 0.23753420\n",
      "Iteration 75, loss = 0.23732942\n",
      "Iteration 76, loss = 0.23706316\n",
      "Iteration 77, loss = 0.23686497\n",
      "Iteration 78, loss = 0.23666819\n",
      "Iteration 79, loss = 0.23648123\n",
      "Iteration 80, loss = 0.23618868\n",
      "Iteration 81, loss = 0.23603822\n",
      "Iteration 82, loss = 0.23584155\n",
      "Iteration 83, loss = 0.23566424\n",
      "Iteration 84, loss = 0.23547095\n",
      "Iteration 85, loss = 0.23534064\n",
      "Iteration 86, loss = 0.23512474\n",
      "Iteration 87, loss = 0.23499076\n",
      "Iteration 88, loss = 0.23476299\n",
      "Iteration 89, loss = 0.23467628\n",
      "Iteration 90, loss = 0.23447426\n",
      "Iteration 91, loss = 0.23431458\n",
      "Iteration 92, loss = 0.23413251\n",
      "Iteration 93, loss = 0.23401635\n",
      "Iteration 94, loss = 0.23388597\n",
      "Iteration 95, loss = 0.23371997\n",
      "Iteration 96, loss = 0.23357268\n",
      "Iteration 97, loss = 0.23346980\n",
      "Iteration 98, loss = 0.23331219\n",
      "Iteration 99, loss = 0.23319983\n",
      "Iteration 100, loss = 0.23305704\n",
      "Iteration 101, loss = 0.23292961\n",
      "Iteration 102, loss = 0.23282262\n",
      "Iteration 103, loss = 0.23273773\n",
      "Iteration 104, loss = 0.23257086\n",
      "Iteration 105, loss = 0.23247098\n",
      "Iteration 106, loss = 0.23242071\n",
      "Iteration 107, loss = 0.23225625\n",
      "Iteration 108, loss = 0.23214968\n",
      "Iteration 109, loss = 0.23206749\n",
      "Iteration 110, loss = 0.23194642\n",
      "Iteration 111, loss = 0.23186288\n",
      "Iteration 112, loss = 0.23175040\n",
      "Iteration 113, loss = 0.23167574\n",
      "Iteration 114, loss = 0.23164322\n",
      "Iteration 115, loss = 0.23143817\n",
      "Iteration 116, loss = 0.23139425\n",
      "Iteration 117, loss = 0.23128923\n",
      "Iteration 118, loss = 0.23120818\n",
      "Iteration 119, loss = 0.23113771\n",
      "Iteration 120, loss = 0.23109854\n",
      "Iteration 121, loss = 0.23097760\n",
      "Iteration 122, loss = 0.23095187\n",
      "Iteration 123, loss = 0.23081082\n",
      "Iteration 124, loss = 0.23075418\n",
      "Iteration 125, loss = 0.23066508\n",
      "Iteration 126, loss = 0.23062080\n",
      "Iteration 127, loss = 0.23055185\n",
      "Iteration 128, loss = 0.23055551\n",
      "Iteration 129, loss = 0.23043766\n",
      "Iteration 130, loss = 0.23034947\n",
      "Iteration 131, loss = 0.23029346\n",
      "Iteration 132, loss = 0.23026931\n",
      "Iteration 133, loss = 0.23018677\n",
      "Iteration 134, loss = 0.23015900\n",
      "Iteration 135, loss = 0.23009900\n",
      "Iteration 136, loss = 0.23002267\n",
      "Iteration 137, loss = 0.22994316\n",
      "Iteration 138, loss = 0.22992731\n",
      "Iteration 139, loss = 0.22984978\n",
      "Iteration 140, loss = 0.22983555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89256025\n",
      "Iteration 2, loss = 0.70565267\n",
      "Iteration 3, loss = 0.57045223\n",
      "Iteration 4, loss = 0.48759725\n",
      "Iteration 5, loss = 0.43508989\n",
      "Iteration 6, loss = 0.39945058\n",
      "Iteration 7, loss = 0.37388406\n",
      "Iteration 8, loss = 0.35488306\n",
      "Iteration 9, loss = 0.34001467\n",
      "Iteration 10, loss = 0.32826842\n",
      "Iteration 11, loss = 0.31875170\n",
      "Iteration 12, loss = 0.31102225\n",
      "Iteration 13, loss = 0.30448319\n",
      "Iteration 14, loss = 0.29890660\n",
      "Iteration 15, loss = 0.29418924\n",
      "Iteration 16, loss = 0.29009659\n",
      "Iteration 17, loss = 0.28660235\n",
      "Iteration 18, loss = 0.28343741\n",
      "Iteration 19, loss = 0.28069827\n",
      "Iteration 20, loss = 0.27818588\n",
      "Iteration 21, loss = 0.27601602\n",
      "Iteration 22, loss = 0.27404925\n",
      "Iteration 23, loss = 0.27226136\n",
      "Iteration 24, loss = 0.27060285\n",
      "Iteration 25, loss = 0.26906419\n",
      "Iteration 26, loss = 0.26768619\n",
      "Iteration 27, loss = 0.26642678\n",
      "Iteration 28, loss = 0.26520664\n",
      "Iteration 29, loss = 0.26412989\n",
      "Iteration 30, loss = 0.26310191\n",
      "Iteration 31, loss = 0.26214096\n",
      "Iteration 32, loss = 0.26117872\n",
      "Iteration 33, loss = 0.26030872\n",
      "Iteration 34, loss = 0.25950118\n",
      "Iteration 35, loss = 0.25874216\n",
      "Iteration 36, loss = 0.25799670\n",
      "Iteration 37, loss = 0.25729814\n",
      "Iteration 38, loss = 0.25665366\n",
      "Iteration 39, loss = 0.25599405\n",
      "Iteration 40, loss = 0.25542710\n",
      "Iteration 41, loss = 0.25480617\n",
      "Iteration 42, loss = 0.25426107\n",
      "Iteration 43, loss = 0.25370060\n",
      "Iteration 44, loss = 0.25320600\n",
      "Iteration 45, loss = 0.25269892\n",
      "Iteration 46, loss = 0.25219576\n",
      "Iteration 47, loss = 0.25172782\n",
      "Iteration 48, loss = 0.25127159\n",
      "Iteration 49, loss = 0.25084952\n",
      "Iteration 50, loss = 0.25036349\n",
      "Iteration 51, loss = 0.25000678\n",
      "Iteration 52, loss = 0.24958207\n",
      "Iteration 53, loss = 0.24917613\n",
      "Iteration 54, loss = 0.24881839\n",
      "Iteration 55, loss = 0.24842540\n",
      "Iteration 56, loss = 0.24808286\n",
      "Iteration 57, loss = 0.24772741\n",
      "Iteration 58, loss = 0.24741461\n",
      "Iteration 59, loss = 0.24703506\n",
      "Iteration 60, loss = 0.24673605\n",
      "Iteration 61, loss = 0.24641796\n",
      "Iteration 62, loss = 0.24610007\n",
      "Iteration 63, loss = 0.24588924\n",
      "Iteration 64, loss = 0.24551520\n",
      "Iteration 65, loss = 0.24526592\n",
      "Iteration 66, loss = 0.24497154\n",
      "Iteration 67, loss = 0.24472457\n",
      "Iteration 68, loss = 0.24447173\n",
      "Iteration 69, loss = 0.24417434\n",
      "Iteration 70, loss = 0.24394411\n",
      "Iteration 71, loss = 0.24369283\n",
      "Iteration 72, loss = 0.24343865\n",
      "Iteration 73, loss = 0.24320061\n",
      "Iteration 74, loss = 0.24297508\n",
      "Iteration 75, loss = 0.24280598\n",
      "Iteration 76, loss = 0.24255035\n",
      "Iteration 77, loss = 0.24230637\n",
      "Iteration 78, loss = 0.24212532\n",
      "Iteration 79, loss = 0.24195679\n",
      "Iteration 80, loss = 0.24171853\n",
      "Iteration 81, loss = 0.24151585\n",
      "Iteration 82, loss = 0.24133526\n",
      "Iteration 83, loss = 0.24117047\n",
      "Iteration 84, loss = 0.24098804\n",
      "Iteration 85, loss = 0.24081970\n",
      "Iteration 86, loss = 0.24062529\n",
      "Iteration 87, loss = 0.24048019\n",
      "Iteration 88, loss = 0.24027857\n",
      "Iteration 89, loss = 0.24013710\n",
      "Iteration 90, loss = 0.23999485\n",
      "Iteration 91, loss = 0.23984329\n",
      "Iteration 92, loss = 0.23965508\n",
      "Iteration 93, loss = 0.23955865\n",
      "Iteration 94, loss = 0.23940640\n",
      "Iteration 95, loss = 0.23926446\n",
      "Iteration 96, loss = 0.23914709\n",
      "Iteration 97, loss = 0.23896645\n",
      "Iteration 98, loss = 0.23887892\n",
      "Iteration 99, loss = 0.23874136\n",
      "Iteration 100, loss = 0.23863476\n",
      "Iteration 101, loss = 0.23848944\n",
      "Iteration 102, loss = 0.23836527\n",
      "Iteration 103, loss = 0.23830284\n",
      "Iteration 104, loss = 0.23815064\n",
      "Iteration 105, loss = 0.23805091\n",
      "Iteration 106, loss = 0.23795747\n",
      "Iteration 107, loss = 0.23784207\n",
      "Iteration 108, loss = 0.23771760\n",
      "Iteration 109, loss = 0.23771432\n",
      "Iteration 110, loss = 0.23752712\n",
      "Iteration 111, loss = 0.23744323\n",
      "Iteration 112, loss = 0.23733707\n",
      "Iteration 113, loss = 0.23728982\n",
      "Iteration 114, loss = 0.23720281\n",
      "Iteration 115, loss = 0.23708599\n",
      "Iteration 116, loss = 0.23701516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 117, loss = 0.23694218\n",
      "Iteration 118, loss = 0.23681531\n",
      "Iteration 119, loss = 0.23677082\n",
      "Iteration 120, loss = 0.23671110\n",
      "Iteration 121, loss = 0.23660378\n",
      "Iteration 122, loss = 0.23655960\n",
      "Iteration 123, loss = 0.23649580\n",
      "Iteration 124, loss = 0.23642874\n",
      "Iteration 125, loss = 0.23631810\n",
      "Iteration 126, loss = 0.23631141\n",
      "Iteration 127, loss = 0.23620030\n",
      "Iteration 128, loss = 0.23627770\n",
      "Iteration 129, loss = 0.23611818\n",
      "Iteration 130, loss = 0.23600210\n",
      "Iteration 131, loss = 0.23594895\n",
      "Iteration 132, loss = 0.23594147\n",
      "Iteration 133, loss = 0.23583956\n",
      "Iteration 134, loss = 0.23584925\n",
      "Iteration 135, loss = 0.23577007\n",
      "Iteration 136, loss = 0.23570727\n",
      "Iteration 137, loss = 0.23570399\n",
      "Iteration 138, loss = 0.23564981\n",
      "Iteration 139, loss = 0.23555230\n",
      "Iteration 140, loss = 0.23555140\n",
      "Iteration 141, loss = 0.23549726\n",
      "Iteration 142, loss = 0.23543491\n",
      "Iteration 143, loss = 0.23539194\n",
      "Iteration 144, loss = 0.23540971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88641611\n",
      "Iteration 2, loss = 0.70323650\n",
      "Iteration 3, loss = 0.56971522\n",
      "Iteration 4, loss = 0.48662580\n",
      "Iteration 5, loss = 0.43394699\n",
      "Iteration 6, loss = 0.39785771\n",
      "Iteration 7, loss = 0.37219961\n",
      "Iteration 8, loss = 0.35291949\n",
      "Iteration 9, loss = 0.33792891\n",
      "Iteration 10, loss = 0.32607349\n",
      "Iteration 11, loss = 0.31643983\n",
      "Iteration 12, loss = 0.30859551\n",
      "Iteration 13, loss = 0.30199897\n",
      "Iteration 14, loss = 0.29634405\n",
      "Iteration 15, loss = 0.29156216\n",
      "Iteration 16, loss = 0.28735640\n",
      "Iteration 17, loss = 0.28380539\n",
      "Iteration 18, loss = 0.28058177\n",
      "Iteration 19, loss = 0.27776582\n",
      "Iteration 20, loss = 0.27520229\n",
      "Iteration 21, loss = 0.27297978\n",
      "Iteration 22, loss = 0.27095962\n",
      "Iteration 23, loss = 0.26915053\n",
      "Iteration 24, loss = 0.26742765\n",
      "Iteration 25, loss = 0.26587845\n",
      "Iteration 26, loss = 0.26447117\n",
      "Iteration 27, loss = 0.26324799\n",
      "Iteration 28, loss = 0.26199012\n",
      "Iteration 29, loss = 0.26086242\n",
      "Iteration 30, loss = 0.25981325\n",
      "Iteration 31, loss = 0.25884457\n",
      "Iteration 32, loss = 0.25789534\n",
      "Iteration 33, loss = 0.25702730\n",
      "Iteration 34, loss = 0.25623456\n",
      "Iteration 35, loss = 0.25546387\n",
      "Iteration 36, loss = 0.25475946\n",
      "Iteration 37, loss = 0.25402890\n",
      "Iteration 38, loss = 0.25337135\n",
      "Iteration 39, loss = 0.25274507\n",
      "Iteration 40, loss = 0.25217845\n",
      "Iteration 41, loss = 0.25159273\n",
      "Iteration 42, loss = 0.25101718\n",
      "Iteration 43, loss = 0.25047317\n",
      "Iteration 44, loss = 0.24996616\n",
      "Iteration 45, loss = 0.24947926\n",
      "Iteration 46, loss = 0.24899634\n",
      "Iteration 47, loss = 0.24855145\n",
      "Iteration 48, loss = 0.24809837\n",
      "Iteration 49, loss = 0.24769900\n",
      "Iteration 50, loss = 0.24725038\n",
      "Iteration 51, loss = 0.24686011\n",
      "Iteration 52, loss = 0.24647935\n",
      "Iteration 53, loss = 0.24607027\n",
      "Iteration 54, loss = 0.24570952\n",
      "Iteration 55, loss = 0.24535053\n",
      "Iteration 56, loss = 0.24502173\n",
      "Iteration 57, loss = 0.24467629\n",
      "Iteration 58, loss = 0.24434018\n",
      "Iteration 59, loss = 0.24399758\n",
      "Iteration 60, loss = 0.24370177\n",
      "Iteration 61, loss = 0.24337674\n",
      "Iteration 62, loss = 0.24308415\n",
      "Iteration 63, loss = 0.24284648\n",
      "Iteration 64, loss = 0.24252019\n",
      "Iteration 65, loss = 0.24223455\n",
      "Iteration 66, loss = 0.24198295\n",
      "Iteration 67, loss = 0.24174699\n",
      "Iteration 68, loss = 0.24145793\n",
      "Iteration 69, loss = 0.24122276\n",
      "Iteration 70, loss = 0.24097627\n",
      "Iteration 71, loss = 0.24071660\n",
      "Iteration 72, loss = 0.24051658\n",
      "Iteration 73, loss = 0.24024739\n",
      "Iteration 74, loss = 0.24002887\n",
      "Iteration 75, loss = 0.23988115\n",
      "Iteration 76, loss = 0.23958204\n",
      "Iteration 77, loss = 0.23936453\n",
      "Iteration 78, loss = 0.23919288\n",
      "Iteration 79, loss = 0.23901846\n",
      "Iteration 80, loss = 0.23877996\n",
      "Iteration 81, loss = 0.23856013\n",
      "Iteration 82, loss = 0.23837170\n",
      "Iteration 83, loss = 0.23819851\n",
      "Iteration 84, loss = 0.23802402\n",
      "Iteration 85, loss = 0.23782817\n",
      "Iteration 86, loss = 0.23770915\n",
      "Iteration 87, loss = 0.23752849\n",
      "Iteration 88, loss = 0.23731179\n",
      "Iteration 89, loss = 0.23720135\n",
      "Iteration 90, loss = 0.23705272\n",
      "Iteration 91, loss = 0.23686618\n",
      "Iteration 92, loss = 0.23669262\n",
      "Iteration 93, loss = 0.23656983\n",
      "Iteration 94, loss = 0.23642488\n",
      "Iteration 95, loss = 0.23624749\n",
      "Iteration 96, loss = 0.23612645\n",
      "Iteration 97, loss = 0.23597510\n",
      "Iteration 98, loss = 0.23588284\n",
      "Iteration 99, loss = 0.23577644\n",
      "Iteration 100, loss = 0.23560891\n",
      "Iteration 101, loss = 0.23550445\n",
      "Iteration 102, loss = 0.23534648\n",
      "Iteration 103, loss = 0.23527033\n",
      "Iteration 104, loss = 0.23514409\n",
      "Iteration 105, loss = 0.23503311\n",
      "Iteration 106, loss = 0.23488519\n",
      "Iteration 107, loss = 0.23481409\n",
      "Iteration 108, loss = 0.23470042\n",
      "Iteration 109, loss = 0.23463417\n",
      "Iteration 110, loss = 0.23447675\n",
      "Iteration 111, loss = 0.23437549\n",
      "Iteration 112, loss = 0.23426515\n",
      "Iteration 113, loss = 0.23419893\n",
      "Iteration 114, loss = 0.23407424\n",
      "Iteration 115, loss = 0.23399421\n",
      "Iteration 116, loss = 0.23394834\n",
      "Iteration 117, loss = 0.23382980\n",
      "Iteration 118, loss = 0.23373008\n",
      "Iteration 119, loss = 0.23365716\n",
      "Iteration 120, loss = 0.23360238\n",
      "Iteration 121, loss = 0.23349196\n",
      "Iteration 122, loss = 0.23345622\n",
      "Iteration 123, loss = 0.23335400\n",
      "Iteration 124, loss = 0.23330406\n",
      "Iteration 125, loss = 0.23318196\n",
      "Iteration 126, loss = 0.23315672\n",
      "Iteration 127, loss = 0.23306858\n",
      "Iteration 128, loss = 0.23303829\n",
      "Iteration 129, loss = 0.23293029\n",
      "Iteration 130, loss = 0.23282980\n",
      "Iteration 131, loss = 0.23281025\n",
      "Iteration 132, loss = 0.23273919\n",
      "Iteration 133, loss = 0.23266175\n",
      "Iteration 134, loss = 0.23263984\n",
      "Iteration 135, loss = 0.23255569\n",
      "Iteration 136, loss = 0.23250334\n",
      "Iteration 137, loss = 0.23246433\n",
      "Iteration 138, loss = 0.23244531\n",
      "Iteration 139, loss = 0.23233346\n",
      "Iteration 140, loss = 0.23231758\n",
      "Iteration 141, loss = 0.23231105\n",
      "Iteration 142, loss = 0.23227407\n",
      "Iteration 143, loss = 0.23217533\n",
      "Iteration 144, loss = 0.23217285\n",
      "Iteration 145, loss = 0.23206079\n",
      "Iteration 146, loss = 0.23200406\n",
      "Iteration 147, loss = 0.23196192\n",
      "Iteration 148, loss = 0.23192897\n",
      "Iteration 149, loss = 0.23186524\n",
      "Iteration 150, loss = 0.23186857\n",
      "Iteration 151, loss = 0.23182260\n",
      "Iteration 152, loss = 0.23180355\n",
      "Iteration 153, loss = 0.23169308\n",
      "Iteration 154, loss = 0.23163806\n",
      "Iteration 155, loss = 0.23162844\n",
      "Iteration 156, loss = 0.23163063\n",
      "Iteration 157, loss = 0.23160610\n",
      "Iteration 158, loss = 0.23156015\n",
      "Iteration 159, loss = 0.23154819\n",
      "Iteration 160, loss = 0.23148580\n",
      "Iteration 161, loss = 0.23145206\n",
      "Iteration 162, loss = 0.23142120\n",
      "Iteration 163, loss = 0.23138391\n",
      "Iteration 164, loss = 0.23136876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89324538\n",
      "Iteration 2, loss = 0.70890846\n",
      "Iteration 3, loss = 0.57312111\n",
      "Iteration 4, loss = 0.48958888\n",
      "Iteration 5, loss = 0.43573465\n",
      "Iteration 6, loss = 0.39891712\n",
      "Iteration 7, loss = 0.37264163\n",
      "Iteration 8, loss = 0.35289778\n",
      "Iteration 9, loss = 0.33724561\n",
      "Iteration 10, loss = 0.32509986\n",
      "Iteration 11, loss = 0.31505746\n",
      "Iteration 12, loss = 0.30686524\n",
      "Iteration 13, loss = 0.30003444\n",
      "Iteration 14, loss = 0.29418867\n",
      "Iteration 15, loss = 0.28921552\n",
      "Iteration 16, loss = 0.28489693\n",
      "Iteration 17, loss = 0.28119034\n",
      "Iteration 18, loss = 0.27792282\n",
      "Iteration 19, loss = 0.27496169\n",
      "Iteration 20, loss = 0.27233377\n",
      "Iteration 21, loss = 0.27004423\n",
      "Iteration 22, loss = 0.26792841\n",
      "Iteration 23, loss = 0.26607085\n",
      "Iteration 24, loss = 0.26426905\n",
      "Iteration 25, loss = 0.26269169\n",
      "Iteration 26, loss = 0.26121040\n",
      "Iteration 27, loss = 0.25992556\n",
      "Iteration 28, loss = 0.25863586\n",
      "Iteration 29, loss = 0.25748551\n",
      "Iteration 30, loss = 0.25640853\n",
      "Iteration 31, loss = 0.25541805\n",
      "Iteration 32, loss = 0.25438170\n",
      "Iteration 33, loss = 0.25352907\n",
      "Iteration 34, loss = 0.25265790\n",
      "Iteration 35, loss = 0.25188249\n",
      "Iteration 36, loss = 0.25110424\n",
      "Iteration 37, loss = 0.25038825\n",
      "Iteration 38, loss = 0.24971162\n",
      "Iteration 39, loss = 0.24907560\n",
      "Iteration 40, loss = 0.24850005\n",
      "Iteration 41, loss = 0.24784728\n",
      "Iteration 42, loss = 0.24727612\n",
      "Iteration 43, loss = 0.24672633\n",
      "Iteration 44, loss = 0.24618120\n",
      "Iteration 45, loss = 0.24570496\n",
      "Iteration 46, loss = 0.24520852\n",
      "Iteration 47, loss = 0.24475202\n",
      "Iteration 48, loss = 0.24432859\n",
      "Iteration 49, loss = 0.24387949\n",
      "Iteration 50, loss = 0.24340955\n",
      "Iteration 51, loss = 0.24303566\n",
      "Iteration 52, loss = 0.24263837\n",
      "Iteration 53, loss = 0.24222366\n",
      "Iteration 54, loss = 0.24184682\n",
      "Iteration 55, loss = 0.24150242\n",
      "Iteration 56, loss = 0.24114073\n",
      "Iteration 57, loss = 0.24078927\n",
      "Iteration 58, loss = 0.24048852\n",
      "Iteration 59, loss = 0.24011819\n",
      "Iteration 60, loss = 0.23982539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 0.23954486\n",
      "Iteration 62, loss = 0.23922828\n",
      "Iteration 63, loss = 0.23891517\n",
      "Iteration 64, loss = 0.23869076\n",
      "Iteration 65, loss = 0.23840742\n",
      "Iteration 66, loss = 0.23811765\n",
      "Iteration 67, loss = 0.23786744\n",
      "Iteration 68, loss = 0.23758637\n",
      "Iteration 69, loss = 0.23736254\n",
      "Iteration 70, loss = 0.23716822\n",
      "Iteration 71, loss = 0.23683356\n",
      "Iteration 72, loss = 0.23665735\n",
      "Iteration 73, loss = 0.23641431\n",
      "Iteration 74, loss = 0.23618358\n",
      "Iteration 75, loss = 0.23601380\n",
      "Iteration 76, loss = 0.23577048\n",
      "Iteration 77, loss = 0.23555614\n",
      "Iteration 78, loss = 0.23536533\n",
      "Iteration 79, loss = 0.23517707\n",
      "Iteration 80, loss = 0.23497922\n",
      "Iteration 81, loss = 0.23477143\n",
      "Iteration 82, loss = 0.23459386\n",
      "Iteration 83, loss = 0.23437694\n",
      "Iteration 84, loss = 0.23423125\n",
      "Iteration 85, loss = 0.23401932\n",
      "Iteration 86, loss = 0.23393085\n",
      "Iteration 87, loss = 0.23371236\n",
      "Iteration 88, loss = 0.23355771\n",
      "Iteration 89, loss = 0.23343951\n",
      "Iteration 90, loss = 0.23321851\n",
      "Iteration 91, loss = 0.23311204\n",
      "Iteration 92, loss = 0.23294682\n",
      "Iteration 93, loss = 0.23279077\n",
      "Iteration 94, loss = 0.23267172\n",
      "Iteration 95, loss = 0.23253059\n",
      "Iteration 96, loss = 0.23237522\n",
      "Iteration 97, loss = 0.23224286\n",
      "Iteration 98, loss = 0.23211119\n",
      "Iteration 99, loss = 0.23200284\n",
      "Iteration 100, loss = 0.23186314\n",
      "Iteration 101, loss = 0.23175723\n",
      "Iteration 102, loss = 0.23161874\n",
      "Iteration 103, loss = 0.23151520\n",
      "Iteration 104, loss = 0.23144714\n",
      "Iteration 105, loss = 0.23126691\n",
      "Iteration 106, loss = 0.23116993\n",
      "Iteration 107, loss = 0.23109245\n",
      "Iteration 108, loss = 0.23101012\n",
      "Iteration 109, loss = 0.23084947\n",
      "Iteration 110, loss = 0.23078109\n",
      "Iteration 111, loss = 0.23069749\n",
      "Iteration 112, loss = 0.23057542\n",
      "Iteration 113, loss = 0.23050328\n",
      "Iteration 114, loss = 0.23036433\n",
      "Iteration 115, loss = 0.23031875\n",
      "Iteration 116, loss = 0.23024892\n",
      "Iteration 117, loss = 0.23012638\n",
      "Iteration 118, loss = 0.23004114\n",
      "Iteration 119, loss = 0.22998825\n",
      "Iteration 120, loss = 0.22989266\n",
      "Iteration 121, loss = 0.22983825\n",
      "Iteration 122, loss = 0.22973573\n",
      "Iteration 123, loss = 0.22968113\n",
      "Iteration 124, loss = 0.22961063\n",
      "Iteration 125, loss = 0.22949745\n",
      "Iteration 126, loss = 0.22946423\n",
      "Iteration 127, loss = 0.22938802\n",
      "Iteration 128, loss = 0.22933289\n",
      "Iteration 129, loss = 0.22925112\n",
      "Iteration 130, loss = 0.22917380\n",
      "Iteration 131, loss = 0.22915106\n",
      "Iteration 132, loss = 0.22911291\n",
      "Iteration 133, loss = 0.22900591\n",
      "Iteration 134, loss = 0.22899499\n",
      "Iteration 135, loss = 0.22889345\n",
      "Iteration 136, loss = 0.22887979\n",
      "Iteration 137, loss = 0.22877425\n",
      "Iteration 138, loss = 0.22880081\n",
      "Iteration 139, loss = 0.22869279\n",
      "Iteration 140, loss = 0.22866567\n",
      "Iteration 141, loss = 0.22867246\n",
      "Iteration 142, loss = 0.22859484\n",
      "Iteration 143, loss = 0.22855416\n",
      "Iteration 144, loss = 0.22852261\n",
      "Iteration 145, loss = 0.22844134\n",
      "Iteration 146, loss = 0.22841308\n",
      "Iteration 147, loss = 0.22831805\n",
      "Iteration 148, loss = 0.22830858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73098371\n",
      "Iteration 2, loss = 0.48479169\n",
      "Iteration 3, loss = 0.38315967\n",
      "Iteration 4, loss = 0.32689555\n",
      "Iteration 5, loss = 0.29410025\n",
      "Iteration 6, loss = 0.27442266\n",
      "Iteration 7, loss = 0.26086390\n",
      "Iteration 8, loss = 0.25357945\n",
      "Iteration 9, loss = 0.24853431\n",
      "Iteration 10, loss = 0.24525197\n",
      "Iteration 11, loss = 0.24125787\n",
      "Iteration 12, loss = 0.23995927\n",
      "Iteration 13, loss = 0.23835283\n",
      "Iteration 14, loss = 0.23785220\n",
      "Iteration 15, loss = 0.23719740\n",
      "Iteration 16, loss = 0.23677779\n",
      "Iteration 17, loss = 0.23656306\n",
      "Iteration 18, loss = 0.23650748\n",
      "Iteration 19, loss = 0.23598080\n",
      "Iteration 20, loss = 0.23529723\n",
      "Iteration 21, loss = 0.23532176\n",
      "Iteration 22, loss = 0.23495442\n",
      "Iteration 23, loss = 0.23564359\n",
      "Iteration 24, loss = 0.23553508\n",
      "Iteration 25, loss = 0.23548774\n",
      "Iteration 26, loss = 0.23562878\n",
      "Iteration 27, loss = 0.23540206\n",
      "Iteration 28, loss = 0.23515715\n",
      "Iteration 29, loss = 0.23528438\n",
      "Iteration 30, loss = 0.23588526\n",
      "Iteration 31, loss = 0.23522492\n",
      "Iteration 32, loss = 0.23502474\n",
      "Iteration 33, loss = 0.23601686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73166792\n",
      "Iteration 2, loss = 0.48348983\n",
      "Iteration 3, loss = 0.38193879\n",
      "Iteration 4, loss = 0.32628096\n",
      "Iteration 5, loss = 0.29272008\n",
      "Iteration 6, loss = 0.27274807\n",
      "Iteration 7, loss = 0.25945813\n",
      "Iteration 8, loss = 0.25139805\n",
      "Iteration 9, loss = 0.24620021\n",
      "Iteration 10, loss = 0.24138148\n",
      "Iteration 11, loss = 0.23761596\n",
      "Iteration 12, loss = 0.23708076\n",
      "Iteration 13, loss = 0.23496284\n",
      "Iteration 14, loss = 0.23324242\n",
      "Iteration 15, loss = 0.23264719\n",
      "Iteration 16, loss = 0.23191143\n",
      "Iteration 17, loss = 0.23130667\n",
      "Iteration 18, loss = 0.23132143\n",
      "Iteration 19, loss = 0.23119239\n",
      "Iteration 20, loss = 0.23079971\n",
      "Iteration 21, loss = 0.23071909\n",
      "Iteration 22, loss = 0.23013856\n",
      "Iteration 23, loss = 0.23062271\n",
      "Iteration 24, loss = 0.23082663\n",
      "Iteration 25, loss = 0.23048860\n",
      "Iteration 26, loss = 0.22988198\n",
      "Iteration 27, loss = 0.23007453\n",
      "Iteration 28, loss = 0.23036593\n",
      "Iteration 29, loss = 0.23065110\n",
      "Iteration 30, loss = 0.23068856\n",
      "Iteration 31, loss = 0.23003925\n",
      "Iteration 32, loss = 0.23004973\n",
      "Iteration 33, loss = 0.22998435\n",
      "Iteration 34, loss = 0.23089692\n",
      "Iteration 35, loss = 0.22973781\n",
      "Iteration 36, loss = 0.23025740\n",
      "Iteration 37, loss = 0.23059655\n",
      "Iteration 38, loss = 0.23054422\n",
      "Iteration 39, loss = 0.23014665\n",
      "Iteration 40, loss = 0.23052936\n",
      "Iteration 41, loss = 0.23060476\n",
      "Iteration 42, loss = 0.23107695\n",
      "Iteration 43, loss = 0.23015960\n",
      "Iteration 44, loss = 0.23043754\n",
      "Iteration 45, loss = 0.23050537\n",
      "Iteration 46, loss = 0.23027303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73473933\n",
      "Iteration 2, loss = 0.48434968\n",
      "Iteration 3, loss = 0.38316836\n",
      "Iteration 4, loss = 0.32780616\n",
      "Iteration 5, loss = 0.29562681\n",
      "Iteration 6, loss = 0.27558949\n",
      "Iteration 7, loss = 0.26364934\n",
      "Iteration 8, loss = 0.25588993\n",
      "Iteration 9, loss = 0.24975894\n",
      "Iteration 10, loss = 0.24578602\n",
      "Iteration 11, loss = 0.24314972\n",
      "Iteration 12, loss = 0.24218812\n",
      "Iteration 13, loss = 0.23954639\n",
      "Iteration 14, loss = 0.23815988\n",
      "Iteration 15, loss = 0.23840793\n",
      "Iteration 16, loss = 0.23743035\n",
      "Iteration 17, loss = 0.23709208\n",
      "Iteration 18, loss = 0.23733780\n",
      "Iteration 19, loss = 0.23720475\n",
      "Iteration 20, loss = 0.23637652\n",
      "Iteration 21, loss = 0.23623242\n",
      "Iteration 22, loss = 0.23631179\n",
      "Iteration 23, loss = 0.23671983\n",
      "Iteration 24, loss = 0.23682372\n",
      "Iteration 25, loss = 0.23649758\n",
      "Iteration 26, loss = 0.23608133\n",
      "Iteration 27, loss = 0.23619184\n",
      "Iteration 28, loss = 0.23633322\n",
      "Iteration 29, loss = 0.23669552\n",
      "Iteration 30, loss = 0.23662080\n",
      "Iteration 31, loss = 0.23632486\n",
      "Iteration 32, loss = 0.23604921\n",
      "Iteration 33, loss = 0.23571661\n",
      "Iteration 34, loss = 0.23584300\n",
      "Iteration 35, loss = 0.23615239\n",
      "Iteration 36, loss = 0.23568567\n",
      "Iteration 37, loss = 0.23643463\n",
      "Iteration 38, loss = 0.23611771\n",
      "Iteration 39, loss = 0.23587798\n",
      "Iteration 40, loss = 0.23669157\n",
      "Iteration 41, loss = 0.23641061\n",
      "Iteration 42, loss = 0.23680284\n",
      "Iteration 43, loss = 0.23620750\n",
      "Iteration 44, loss = 0.23615601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72773083\n",
      "Iteration 2, loss = 0.48180831\n",
      "Iteration 3, loss = 0.38144959\n",
      "Iteration 4, loss = 0.32529411\n",
      "Iteration 5, loss = 0.29310423\n",
      "Iteration 6, loss = 0.27273885\n",
      "Iteration 7, loss = 0.26063824\n",
      "Iteration 8, loss = 0.25236573\n",
      "Iteration 9, loss = 0.24662374\n",
      "Iteration 10, loss = 0.24203405\n",
      "Iteration 11, loss = 0.24025770\n",
      "Iteration 12, loss = 0.23862668\n",
      "Iteration 13, loss = 0.23581464\n",
      "Iteration 14, loss = 0.23538796\n",
      "Iteration 15, loss = 0.23449547\n",
      "Iteration 16, loss = 0.23361389\n",
      "Iteration 17, loss = 0.23339568\n",
      "Iteration 18, loss = 0.23381429\n",
      "Iteration 19, loss = 0.23343664\n",
      "Iteration 20, loss = 0.23292300\n",
      "Iteration 21, loss = 0.23264149\n",
      "Iteration 22, loss = 0.23264843\n",
      "Iteration 23, loss = 0.23325971\n",
      "Iteration 24, loss = 0.23281423\n",
      "Iteration 25, loss = 0.23251442\n",
      "Iteration 26, loss = 0.23262414\n",
      "Iteration 27, loss = 0.23319753\n",
      "Iteration 28, loss = 0.23383057\n",
      "Iteration 29, loss = 0.23356300\n",
      "Iteration 30, loss = 0.23266607\n",
      "Iteration 31, loss = 0.23310424\n",
      "Iteration 32, loss = 0.23240987\n",
      "Iteration 33, loss = 0.23246565\n",
      "Iteration 34, loss = 0.23261204\n",
      "Iteration 35, loss = 0.23260464\n",
      "Iteration 36, loss = 0.23281583\n",
      "Iteration 37, loss = 0.23292613\n",
      "Iteration 38, loss = 0.23236859\n",
      "Iteration 39, loss = 0.23239811\n",
      "Iteration 40, loss = 0.23342281\n",
      "Iteration 41, loss = 0.23322972\n",
      "Iteration 42, loss = 0.23311962\n",
      "Iteration 43, loss = 0.23232341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73345883\n",
      "Iteration 2, loss = 0.48525626\n",
      "Iteration 3, loss = 0.38229654\n",
      "Iteration 4, loss = 0.32420694\n",
      "Iteration 5, loss = 0.29054943\n",
      "Iteration 6, loss = 0.26976559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.25680933\n",
      "Iteration 8, loss = 0.24864399\n",
      "Iteration 9, loss = 0.24268985\n",
      "Iteration 10, loss = 0.23863462\n",
      "Iteration 11, loss = 0.23594764\n",
      "Iteration 12, loss = 0.23402162\n",
      "Iteration 13, loss = 0.23255300\n",
      "Iteration 14, loss = 0.23115024\n",
      "Iteration 15, loss = 0.23050859\n",
      "Iteration 16, loss = 0.23024422\n",
      "Iteration 17, loss = 0.22997592\n",
      "Iteration 18, loss = 0.23078871\n",
      "Iteration 19, loss = 0.23013840\n",
      "Iteration 20, loss = 0.22957206\n",
      "Iteration 21, loss = 0.22942052\n",
      "Iteration 22, loss = 0.22905388\n",
      "Iteration 23, loss = 0.23003999\n",
      "Iteration 24, loss = 0.22943601\n",
      "Iteration 25, loss = 0.22899622\n",
      "Iteration 26, loss = 0.22891783\n",
      "Iteration 27, loss = 0.22954472\n",
      "Iteration 28, loss = 0.22946159\n",
      "Iteration 29, loss = 0.23012194\n",
      "Iteration 30, loss = 0.22951260\n",
      "Iteration 31, loss = 0.22991427\n",
      "Iteration 32, loss = 0.22922604\n",
      "Iteration 33, loss = 0.22926863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88556097\n",
      "Iteration 2, loss = 0.70328294\n",
      "Iteration 3, loss = 0.57008677\n",
      "Iteration 4, loss = 0.48712322\n",
      "Iteration 5, loss = 0.43474580\n",
      "Iteration 6, loss = 0.39897719\n",
      "Iteration 7, loss = 0.37310783\n",
      "Iteration 8, loss = 0.35389019\n",
      "Iteration 9, loss = 0.33898799\n",
      "Iteration 10, loss = 0.32710566\n",
      "Iteration 11, loss = 0.31739883\n",
      "Iteration 12, loss = 0.30955543\n",
      "Iteration 13, loss = 0.30291018\n",
      "Iteration 14, loss = 0.29734783\n",
      "Iteration 15, loss = 0.29255580\n",
      "Iteration 16, loss = 0.28839603\n",
      "Iteration 17, loss = 0.28491861\n",
      "Iteration 18, loss = 0.28164867\n",
      "Iteration 19, loss = 0.27884589\n",
      "Iteration 20, loss = 0.27635250\n",
      "Iteration 21, loss = 0.27413679\n",
      "Iteration 22, loss = 0.27211307\n",
      "Iteration 23, loss = 0.27035117\n",
      "Iteration 24, loss = 0.26863662\n",
      "Iteration 25, loss = 0.26713367\n",
      "Iteration 26, loss = 0.26575598\n",
      "Iteration 27, loss = 0.26444587\n",
      "Iteration 28, loss = 0.26327483\n",
      "Iteration 29, loss = 0.26213130\n",
      "Iteration 30, loss = 0.26115015\n",
      "Iteration 31, loss = 0.26017097\n",
      "Iteration 32, loss = 0.25924883\n",
      "Iteration 33, loss = 0.25843177\n",
      "Iteration 34, loss = 0.25759605\n",
      "Iteration 35, loss = 0.25679891\n",
      "Iteration 36, loss = 0.25608912\n",
      "Iteration 37, loss = 0.25540106\n",
      "Iteration 38, loss = 0.25481982\n",
      "Iteration 39, loss = 0.25412806\n",
      "Iteration 40, loss = 0.25356766\n",
      "Iteration 41, loss = 0.25296355\n",
      "Iteration 42, loss = 0.25240178\n",
      "Iteration 43, loss = 0.25186592\n",
      "Iteration 44, loss = 0.25134728\n",
      "Iteration 45, loss = 0.25086605\n",
      "Iteration 46, loss = 0.25042087\n",
      "Iteration 47, loss = 0.24995317\n",
      "Iteration 48, loss = 0.24956810\n",
      "Iteration 49, loss = 0.24910189\n",
      "Iteration 50, loss = 0.24870775\n",
      "Iteration 51, loss = 0.24828680\n",
      "Iteration 52, loss = 0.24791496\n",
      "Iteration 53, loss = 0.24756154\n",
      "Iteration 54, loss = 0.24719839\n",
      "Iteration 55, loss = 0.24680902\n",
      "Iteration 56, loss = 0.24645998\n",
      "Iteration 57, loss = 0.24611190\n",
      "Iteration 58, loss = 0.24582514\n",
      "Iteration 59, loss = 0.24547923\n",
      "Iteration 60, loss = 0.24514974\n",
      "Iteration 61, loss = 0.24488897\n",
      "Iteration 62, loss = 0.24458157\n",
      "Iteration 63, loss = 0.24435060\n",
      "Iteration 64, loss = 0.24403362\n",
      "Iteration 65, loss = 0.24375275\n",
      "Iteration 66, loss = 0.24353022\n",
      "Iteration 67, loss = 0.24322527\n",
      "Iteration 68, loss = 0.24302871\n",
      "Iteration 69, loss = 0.24271749\n",
      "Iteration 70, loss = 0.24249282\n",
      "Iteration 71, loss = 0.24229709\n",
      "Iteration 72, loss = 0.24205529\n",
      "Iteration 73, loss = 0.24180730\n",
      "Iteration 74, loss = 0.24162217\n",
      "Iteration 75, loss = 0.24139047\n",
      "Iteration 76, loss = 0.24118432\n",
      "Iteration 77, loss = 0.24099720\n",
      "Iteration 78, loss = 0.24078375\n",
      "Iteration 79, loss = 0.24064081\n",
      "Iteration 80, loss = 0.24038977\n",
      "Iteration 81, loss = 0.24022718\n",
      "Iteration 82, loss = 0.24003566\n",
      "Iteration 83, loss = 0.23988003\n",
      "Iteration 84, loss = 0.23973784\n",
      "Iteration 85, loss = 0.23959212\n",
      "Iteration 86, loss = 0.23939646\n",
      "Iteration 87, loss = 0.23923662\n",
      "Iteration 88, loss = 0.23906995\n",
      "Iteration 89, loss = 0.23895082\n",
      "Iteration 90, loss = 0.23877311\n",
      "Iteration 91, loss = 0.23865314\n",
      "Iteration 92, loss = 0.23846632\n",
      "Iteration 93, loss = 0.23834868\n",
      "Iteration 94, loss = 0.23822930\n",
      "Iteration 95, loss = 0.23807858\n",
      "Iteration 96, loss = 0.23795733\n",
      "Iteration 97, loss = 0.23784475\n",
      "Iteration 98, loss = 0.23773499\n",
      "Iteration 99, loss = 0.23761296\n",
      "Iteration 100, loss = 0.23746870\n",
      "Iteration 101, loss = 0.23736833\n",
      "Iteration 102, loss = 0.23729934\n",
      "Iteration 103, loss = 0.23714508\n",
      "Iteration 104, loss = 0.23705498\n",
      "Iteration 105, loss = 0.23696745\n",
      "Iteration 106, loss = 0.23690212\n",
      "Iteration 107, loss = 0.23672607\n",
      "Iteration 108, loss = 0.23666555\n",
      "Iteration 109, loss = 0.23657914\n",
      "Iteration 110, loss = 0.23648063\n",
      "Iteration 111, loss = 0.23637742\n",
      "Iteration 112, loss = 0.23630443\n",
      "Iteration 113, loss = 0.23626326\n",
      "Iteration 114, loss = 0.23614017\n",
      "Iteration 115, loss = 0.23602426\n",
      "Iteration 116, loss = 0.23596700\n",
      "Iteration 117, loss = 0.23590495\n",
      "Iteration 118, loss = 0.23577668\n",
      "Iteration 119, loss = 0.23572606\n",
      "Iteration 120, loss = 0.23565587\n",
      "Iteration 121, loss = 0.23562712\n",
      "Iteration 122, loss = 0.23554144\n",
      "Iteration 123, loss = 0.23545733\n",
      "Iteration 124, loss = 0.23538451\n",
      "Iteration 125, loss = 0.23534057\n",
      "Iteration 126, loss = 0.23525056\n",
      "Iteration 127, loss = 0.23520310\n",
      "Iteration 128, loss = 0.23518991\n",
      "Iteration 129, loss = 0.23514079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88432351\n",
      "Iteration 2, loss = 0.70191667\n",
      "Iteration 3, loss = 0.56868948\n",
      "Iteration 4, loss = 0.48574577\n",
      "Iteration 5, loss = 0.43301317\n",
      "Iteration 6, loss = 0.39720539\n",
      "Iteration 7, loss = 0.37112188\n",
      "Iteration 8, loss = 0.35178336\n",
      "Iteration 9, loss = 0.33674845\n",
      "Iteration 10, loss = 0.32475020\n",
      "Iteration 11, loss = 0.31501095\n",
      "Iteration 12, loss = 0.30715312\n",
      "Iteration 13, loss = 0.30048246\n",
      "Iteration 14, loss = 0.29475581\n",
      "Iteration 15, loss = 0.28994956\n",
      "Iteration 16, loss = 0.28576439\n",
      "Iteration 17, loss = 0.28221413\n",
      "Iteration 18, loss = 0.27893544\n",
      "Iteration 19, loss = 0.27611841\n",
      "Iteration 20, loss = 0.27357464\n",
      "Iteration 21, loss = 0.27133762\n",
      "Iteration 22, loss = 0.26926142\n",
      "Iteration 23, loss = 0.26745385\n",
      "Iteration 24, loss = 0.26576726\n",
      "Iteration 25, loss = 0.26417969\n",
      "Iteration 26, loss = 0.26276914\n",
      "Iteration 27, loss = 0.26145268\n",
      "Iteration 28, loss = 0.26024892\n",
      "Iteration 29, loss = 0.25910262\n",
      "Iteration 30, loss = 0.25806000\n",
      "Iteration 31, loss = 0.25703533\n",
      "Iteration 32, loss = 0.25609342\n",
      "Iteration 33, loss = 0.25520132\n",
      "Iteration 34, loss = 0.25438596\n",
      "Iteration 35, loss = 0.25354516\n",
      "Iteration 36, loss = 0.25283108\n",
      "Iteration 37, loss = 0.25210202\n",
      "Iteration 38, loss = 0.25143548\n",
      "Iteration 39, loss = 0.25076905\n",
      "Iteration 40, loss = 0.25015302\n",
      "Iteration 41, loss = 0.24955799\n",
      "Iteration 42, loss = 0.24899755\n",
      "Iteration 43, loss = 0.24841727\n",
      "Iteration 44, loss = 0.24792250\n",
      "Iteration 45, loss = 0.24736994\n",
      "Iteration 46, loss = 0.24686088\n",
      "Iteration 47, loss = 0.24640147\n",
      "Iteration 48, loss = 0.24594256\n",
      "Iteration 49, loss = 0.24549890\n",
      "Iteration 50, loss = 0.24504055\n",
      "Iteration 51, loss = 0.24465783\n",
      "Iteration 52, loss = 0.24423997\n",
      "Iteration 53, loss = 0.24381751\n",
      "Iteration 54, loss = 0.24344698\n",
      "Iteration 55, loss = 0.24305451\n",
      "Iteration 56, loss = 0.24269904\n",
      "Iteration 57, loss = 0.24235004\n",
      "Iteration 58, loss = 0.24201384\n",
      "Iteration 59, loss = 0.24165489\n",
      "Iteration 60, loss = 0.24131823\n",
      "Iteration 61, loss = 0.24101364\n",
      "Iteration 62, loss = 0.24067488\n",
      "Iteration 63, loss = 0.24045212\n",
      "Iteration 64, loss = 0.24012329\n",
      "Iteration 65, loss = 0.23987039\n",
      "Iteration 66, loss = 0.23959420\n",
      "Iteration 67, loss = 0.23925824\n",
      "Iteration 68, loss = 0.23907006\n",
      "Iteration 69, loss = 0.23873308\n",
      "Iteration 70, loss = 0.23847699\n",
      "Iteration 71, loss = 0.23827868\n",
      "Iteration 72, loss = 0.23800978\n",
      "Iteration 73, loss = 0.23776982\n",
      "Iteration 74, loss = 0.23753420\n",
      "Iteration 75, loss = 0.23732942\n",
      "Iteration 76, loss = 0.23706316\n",
      "Iteration 77, loss = 0.23686497\n",
      "Iteration 78, loss = 0.23666819\n",
      "Iteration 79, loss = 0.23648123\n",
      "Iteration 80, loss = 0.23618868\n",
      "Iteration 81, loss = 0.23603822\n",
      "Iteration 82, loss = 0.23584155\n",
      "Iteration 83, loss = 0.23566424\n",
      "Iteration 84, loss = 0.23547095\n",
      "Iteration 85, loss = 0.23534064\n",
      "Iteration 86, loss = 0.23512474\n",
      "Iteration 87, loss = 0.23499076\n",
      "Iteration 88, loss = 0.23476299\n",
      "Iteration 89, loss = 0.23467628\n",
      "Iteration 90, loss = 0.23447426\n",
      "Iteration 91, loss = 0.23431458\n",
      "Iteration 92, loss = 0.23413251\n",
      "Iteration 93, loss = 0.23401635\n",
      "Iteration 94, loss = 0.23388597\n",
      "Iteration 95, loss = 0.23371997\n",
      "Iteration 96, loss = 0.23357268\n",
      "Iteration 97, loss = 0.23346980\n",
      "Iteration 98, loss = 0.23331219\n",
      "Iteration 99, loss = 0.23319983\n",
      "Iteration 100, loss = 0.23305704\n",
      "Iteration 101, loss = 0.23292961\n",
      "Iteration 102, loss = 0.23282262\n",
      "Iteration 103, loss = 0.23273773\n",
      "Iteration 104, loss = 0.23257086\n",
      "Iteration 105, loss = 0.23247098\n",
      "Iteration 106, loss = 0.23242071\n",
      "Iteration 107, loss = 0.23225625\n",
      "Iteration 108, loss = 0.23214968\n",
      "Iteration 109, loss = 0.23206749\n",
      "Iteration 110, loss = 0.23194642\n",
      "Iteration 111, loss = 0.23186288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 112, loss = 0.23175040\n",
      "Iteration 113, loss = 0.23167574\n",
      "Iteration 114, loss = 0.23164322\n",
      "Iteration 115, loss = 0.23143817\n",
      "Iteration 116, loss = 0.23139425\n",
      "Iteration 117, loss = 0.23128923\n",
      "Iteration 118, loss = 0.23120818\n",
      "Iteration 119, loss = 0.23113771\n",
      "Iteration 120, loss = 0.23109854\n",
      "Iteration 121, loss = 0.23097760\n",
      "Iteration 122, loss = 0.23095187\n",
      "Iteration 123, loss = 0.23081082\n",
      "Iteration 124, loss = 0.23075418\n",
      "Iteration 125, loss = 0.23066508\n",
      "Iteration 126, loss = 0.23062080\n",
      "Iteration 127, loss = 0.23055185\n",
      "Iteration 128, loss = 0.23055551\n",
      "Iteration 129, loss = 0.23043766\n",
      "Iteration 130, loss = 0.23034947\n",
      "Iteration 131, loss = 0.23029346\n",
      "Iteration 132, loss = 0.23026931\n",
      "Iteration 133, loss = 0.23018677\n",
      "Iteration 134, loss = 0.23015900\n",
      "Iteration 135, loss = 0.23009900\n",
      "Iteration 136, loss = 0.23002267\n",
      "Iteration 137, loss = 0.22994316\n",
      "Iteration 138, loss = 0.22992731\n",
      "Iteration 139, loss = 0.22984978\n",
      "Iteration 140, loss = 0.22983555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89256025\n",
      "Iteration 2, loss = 0.70565267\n",
      "Iteration 3, loss = 0.57045223\n",
      "Iteration 4, loss = 0.48759725\n",
      "Iteration 5, loss = 0.43508989\n",
      "Iteration 6, loss = 0.39945058\n",
      "Iteration 7, loss = 0.37388406\n",
      "Iteration 8, loss = 0.35488306\n",
      "Iteration 9, loss = 0.34001467\n",
      "Iteration 10, loss = 0.32826842\n",
      "Iteration 11, loss = 0.31875170\n",
      "Iteration 12, loss = 0.31102225\n",
      "Iteration 13, loss = 0.30448319\n",
      "Iteration 14, loss = 0.29890660\n",
      "Iteration 15, loss = 0.29418924\n",
      "Iteration 16, loss = 0.29009659\n",
      "Iteration 17, loss = 0.28660235\n",
      "Iteration 18, loss = 0.28343741\n",
      "Iteration 19, loss = 0.28069827\n",
      "Iteration 20, loss = 0.27818588\n",
      "Iteration 21, loss = 0.27601602\n",
      "Iteration 22, loss = 0.27404925\n",
      "Iteration 23, loss = 0.27226136\n",
      "Iteration 24, loss = 0.27060285\n",
      "Iteration 25, loss = 0.26906419\n",
      "Iteration 26, loss = 0.26768619\n",
      "Iteration 27, loss = 0.26642678\n",
      "Iteration 28, loss = 0.26520664\n",
      "Iteration 29, loss = 0.26412989\n",
      "Iteration 30, loss = 0.26310191\n",
      "Iteration 31, loss = 0.26214096\n",
      "Iteration 32, loss = 0.26117872\n",
      "Iteration 33, loss = 0.26030872\n",
      "Iteration 34, loss = 0.25950118\n",
      "Iteration 35, loss = 0.25874216\n",
      "Iteration 36, loss = 0.25799670\n",
      "Iteration 37, loss = 0.25729814\n",
      "Iteration 38, loss = 0.25665366\n",
      "Iteration 39, loss = 0.25599405\n",
      "Iteration 40, loss = 0.25542710\n",
      "Iteration 41, loss = 0.25480617\n",
      "Iteration 42, loss = 0.25426107\n",
      "Iteration 43, loss = 0.25370060\n",
      "Iteration 44, loss = 0.25320600\n",
      "Iteration 45, loss = 0.25269892\n",
      "Iteration 46, loss = 0.25219576\n",
      "Iteration 47, loss = 0.25172782\n",
      "Iteration 48, loss = 0.25127159\n",
      "Iteration 49, loss = 0.25084952\n",
      "Iteration 50, loss = 0.25036349\n",
      "Iteration 51, loss = 0.25000678\n",
      "Iteration 52, loss = 0.24958207\n",
      "Iteration 53, loss = 0.24917613\n",
      "Iteration 54, loss = 0.24881839\n",
      "Iteration 55, loss = 0.24842540\n",
      "Iteration 56, loss = 0.24808286\n",
      "Iteration 57, loss = 0.24772741\n",
      "Iteration 58, loss = 0.24741461\n",
      "Iteration 59, loss = 0.24703506\n",
      "Iteration 60, loss = 0.24673605\n",
      "Iteration 61, loss = 0.24641796\n",
      "Iteration 62, loss = 0.24610007\n",
      "Iteration 63, loss = 0.24588924\n",
      "Iteration 64, loss = 0.24551520\n",
      "Iteration 65, loss = 0.24526592\n",
      "Iteration 66, loss = 0.24497154\n",
      "Iteration 67, loss = 0.24472457\n",
      "Iteration 68, loss = 0.24447173\n",
      "Iteration 69, loss = 0.24417434\n",
      "Iteration 70, loss = 0.24394411\n",
      "Iteration 71, loss = 0.24369283\n",
      "Iteration 72, loss = 0.24343865\n",
      "Iteration 73, loss = 0.24320061\n",
      "Iteration 74, loss = 0.24297508\n",
      "Iteration 75, loss = 0.24280598\n",
      "Iteration 76, loss = 0.24255035\n",
      "Iteration 77, loss = 0.24230637\n",
      "Iteration 78, loss = 0.24212532\n",
      "Iteration 79, loss = 0.24195679\n",
      "Iteration 80, loss = 0.24171853\n",
      "Iteration 81, loss = 0.24151585\n",
      "Iteration 82, loss = 0.24133526\n",
      "Iteration 83, loss = 0.24117047\n",
      "Iteration 84, loss = 0.24098804\n",
      "Iteration 85, loss = 0.24081970\n",
      "Iteration 86, loss = 0.24062529\n",
      "Iteration 87, loss = 0.24048019\n",
      "Iteration 88, loss = 0.24027857\n",
      "Iteration 89, loss = 0.24013710\n",
      "Iteration 90, loss = 0.23999485\n",
      "Iteration 91, loss = 0.23984329\n",
      "Iteration 92, loss = 0.23965508\n",
      "Iteration 93, loss = 0.23955865\n",
      "Iteration 94, loss = 0.23940640\n",
      "Iteration 95, loss = 0.23926446\n",
      "Iteration 96, loss = 0.23914709\n",
      "Iteration 97, loss = 0.23896645\n",
      "Iteration 98, loss = 0.23887892\n",
      "Iteration 99, loss = 0.23874136\n",
      "Iteration 100, loss = 0.23863476\n",
      "Iteration 101, loss = 0.23848944\n",
      "Iteration 102, loss = 0.23836527\n",
      "Iteration 103, loss = 0.23830284\n",
      "Iteration 104, loss = 0.23815064\n",
      "Iteration 105, loss = 0.23805091\n",
      "Iteration 106, loss = 0.23795747\n",
      "Iteration 107, loss = 0.23784207\n",
      "Iteration 108, loss = 0.23771760\n",
      "Iteration 109, loss = 0.23771432\n",
      "Iteration 110, loss = 0.23752712\n",
      "Iteration 111, loss = 0.23744323\n",
      "Iteration 112, loss = 0.23733707\n",
      "Iteration 113, loss = 0.23728982\n",
      "Iteration 114, loss = 0.23720281\n",
      "Iteration 115, loss = 0.23708599\n",
      "Iteration 116, loss = 0.23701516\n",
      "Iteration 117, loss = 0.23694218\n",
      "Iteration 118, loss = 0.23681531\n",
      "Iteration 119, loss = 0.23677082\n",
      "Iteration 120, loss = 0.23671110\n",
      "Iteration 121, loss = 0.23660378\n",
      "Iteration 122, loss = 0.23655960\n",
      "Iteration 123, loss = 0.23649580\n",
      "Iteration 124, loss = 0.23642874\n",
      "Iteration 125, loss = 0.23631810\n",
      "Iteration 126, loss = 0.23631141\n",
      "Iteration 127, loss = 0.23620030\n",
      "Iteration 128, loss = 0.23627770\n",
      "Iteration 129, loss = 0.23611818\n",
      "Iteration 130, loss = 0.23600210\n",
      "Iteration 131, loss = 0.23594895\n",
      "Iteration 132, loss = 0.23594147\n",
      "Iteration 133, loss = 0.23583956\n",
      "Iteration 134, loss = 0.23584925\n",
      "Iteration 135, loss = 0.23577007\n",
      "Iteration 136, loss = 0.23570727\n",
      "Iteration 137, loss = 0.23570399\n",
      "Iteration 138, loss = 0.23564981\n",
      "Iteration 139, loss = 0.23555230\n",
      "Iteration 140, loss = 0.23555140\n",
      "Iteration 141, loss = 0.23549726\n",
      "Iteration 142, loss = 0.23543491\n",
      "Iteration 143, loss = 0.23539194\n",
      "Iteration 144, loss = 0.23540971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88641611\n",
      "Iteration 2, loss = 0.70323650\n",
      "Iteration 3, loss = 0.56971522\n",
      "Iteration 4, loss = 0.48662580\n",
      "Iteration 5, loss = 0.43394699\n",
      "Iteration 6, loss = 0.39785771\n",
      "Iteration 7, loss = 0.37219961\n",
      "Iteration 8, loss = 0.35291949\n",
      "Iteration 9, loss = 0.33792891\n",
      "Iteration 10, loss = 0.32607349\n",
      "Iteration 11, loss = 0.31643983\n",
      "Iteration 12, loss = 0.30859551\n",
      "Iteration 13, loss = 0.30199897\n",
      "Iteration 14, loss = 0.29634405\n",
      "Iteration 15, loss = 0.29156216\n",
      "Iteration 16, loss = 0.28735640\n",
      "Iteration 17, loss = 0.28380539\n",
      "Iteration 18, loss = 0.28058177\n",
      "Iteration 19, loss = 0.27776582\n",
      "Iteration 20, loss = 0.27520229\n",
      "Iteration 21, loss = 0.27297978\n",
      "Iteration 22, loss = 0.27095962\n",
      "Iteration 23, loss = 0.26915053\n",
      "Iteration 24, loss = 0.26742765\n",
      "Iteration 25, loss = 0.26587845\n",
      "Iteration 26, loss = 0.26447117\n",
      "Iteration 27, loss = 0.26324799\n",
      "Iteration 28, loss = 0.26199012\n",
      "Iteration 29, loss = 0.26086242\n",
      "Iteration 30, loss = 0.25981325\n",
      "Iteration 31, loss = 0.25884457\n",
      "Iteration 32, loss = 0.25789534\n",
      "Iteration 33, loss = 0.25702730\n",
      "Iteration 34, loss = 0.25623456\n",
      "Iteration 35, loss = 0.25546387\n",
      "Iteration 36, loss = 0.25475946\n",
      "Iteration 37, loss = 0.25402890\n",
      "Iteration 38, loss = 0.25337135\n",
      "Iteration 39, loss = 0.25274507\n",
      "Iteration 40, loss = 0.25217845\n",
      "Iteration 41, loss = 0.25159273\n",
      "Iteration 42, loss = 0.25101718\n",
      "Iteration 43, loss = 0.25047317\n",
      "Iteration 44, loss = 0.24996616\n",
      "Iteration 45, loss = 0.24947926\n",
      "Iteration 46, loss = 0.24899634\n",
      "Iteration 47, loss = 0.24855145\n",
      "Iteration 48, loss = 0.24809837\n",
      "Iteration 49, loss = 0.24769900\n",
      "Iteration 50, loss = 0.24725038\n",
      "Iteration 51, loss = 0.24686011\n",
      "Iteration 52, loss = 0.24647935\n",
      "Iteration 53, loss = 0.24607027\n",
      "Iteration 54, loss = 0.24570952\n",
      "Iteration 55, loss = 0.24535053\n",
      "Iteration 56, loss = 0.24502173\n",
      "Iteration 57, loss = 0.24467629\n",
      "Iteration 58, loss = 0.24434018\n",
      "Iteration 59, loss = 0.24399758\n",
      "Iteration 60, loss = 0.24370177\n",
      "Iteration 61, loss = 0.24337674\n",
      "Iteration 62, loss = 0.24308415\n",
      "Iteration 63, loss = 0.24284648\n",
      "Iteration 64, loss = 0.24252019\n",
      "Iteration 65, loss = 0.24223455\n",
      "Iteration 66, loss = 0.24198295\n",
      "Iteration 67, loss = 0.24174699\n",
      "Iteration 68, loss = 0.24145793\n",
      "Iteration 69, loss = 0.24122276\n",
      "Iteration 70, loss = 0.24097627\n",
      "Iteration 71, loss = 0.24071660\n",
      "Iteration 72, loss = 0.24051658\n",
      "Iteration 73, loss = 0.24024739\n",
      "Iteration 74, loss = 0.24002887\n",
      "Iteration 75, loss = 0.23988115\n",
      "Iteration 76, loss = 0.23958204\n",
      "Iteration 77, loss = 0.23936453\n",
      "Iteration 78, loss = 0.23919288\n",
      "Iteration 79, loss = 0.23901846\n",
      "Iteration 80, loss = 0.23877996\n",
      "Iteration 81, loss = 0.23856013\n",
      "Iteration 82, loss = 0.23837170\n",
      "Iteration 83, loss = 0.23819851\n",
      "Iteration 84, loss = 0.23802402\n",
      "Iteration 85, loss = 0.23782817\n",
      "Iteration 86, loss = 0.23770915\n",
      "Iteration 87, loss = 0.23752849\n",
      "Iteration 88, loss = 0.23731179\n",
      "Iteration 89, loss = 0.23720135\n",
      "Iteration 90, loss = 0.23705272\n",
      "Iteration 91, loss = 0.23686618\n",
      "Iteration 92, loss = 0.23669262\n",
      "Iteration 93, loss = 0.23656983\n",
      "Iteration 94, loss = 0.23642488\n",
      "Iteration 95, loss = 0.23624749\n",
      "Iteration 96, loss = 0.23612645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 0.23597510\n",
      "Iteration 98, loss = 0.23588284\n",
      "Iteration 99, loss = 0.23577644\n",
      "Iteration 100, loss = 0.23560891\n",
      "Iteration 101, loss = 0.23550445\n",
      "Iteration 102, loss = 0.23534648\n",
      "Iteration 103, loss = 0.23527033\n",
      "Iteration 104, loss = 0.23514409\n",
      "Iteration 105, loss = 0.23503311\n",
      "Iteration 106, loss = 0.23488519\n",
      "Iteration 107, loss = 0.23481409\n",
      "Iteration 108, loss = 0.23470042\n",
      "Iteration 109, loss = 0.23463417\n",
      "Iteration 110, loss = 0.23447675\n",
      "Iteration 111, loss = 0.23437549\n",
      "Iteration 112, loss = 0.23426515\n",
      "Iteration 113, loss = 0.23419893\n",
      "Iteration 114, loss = 0.23407424\n",
      "Iteration 115, loss = 0.23399421\n",
      "Iteration 116, loss = 0.23394834\n",
      "Iteration 117, loss = 0.23382980\n",
      "Iteration 118, loss = 0.23373008\n",
      "Iteration 119, loss = 0.23365716\n",
      "Iteration 120, loss = 0.23360238\n",
      "Iteration 121, loss = 0.23349196\n",
      "Iteration 122, loss = 0.23345622\n",
      "Iteration 123, loss = 0.23335400\n",
      "Iteration 124, loss = 0.23330406\n",
      "Iteration 125, loss = 0.23318196\n",
      "Iteration 126, loss = 0.23315672\n",
      "Iteration 127, loss = 0.23306858\n",
      "Iteration 128, loss = 0.23303829\n",
      "Iteration 129, loss = 0.23293029\n",
      "Iteration 130, loss = 0.23282980\n",
      "Iteration 131, loss = 0.23281025\n",
      "Iteration 132, loss = 0.23273919\n",
      "Iteration 133, loss = 0.23266175\n",
      "Iteration 134, loss = 0.23263984\n",
      "Iteration 135, loss = 0.23255569\n",
      "Iteration 136, loss = 0.23250334\n",
      "Iteration 137, loss = 0.23246433\n",
      "Iteration 138, loss = 0.23244531\n",
      "Iteration 139, loss = 0.23233346\n",
      "Iteration 140, loss = 0.23231758\n",
      "Iteration 141, loss = 0.23231105\n",
      "Iteration 142, loss = 0.23227407\n",
      "Iteration 143, loss = 0.23217533\n",
      "Iteration 144, loss = 0.23217285\n",
      "Iteration 145, loss = 0.23206079\n",
      "Iteration 146, loss = 0.23200406\n",
      "Iteration 147, loss = 0.23196192\n",
      "Iteration 148, loss = 0.23192897\n",
      "Iteration 149, loss = 0.23186524\n",
      "Iteration 150, loss = 0.23186857\n",
      "Iteration 151, loss = 0.23182260\n",
      "Iteration 152, loss = 0.23180355\n",
      "Iteration 153, loss = 0.23169308\n",
      "Iteration 154, loss = 0.23163806\n",
      "Iteration 155, loss = 0.23162844\n",
      "Iteration 156, loss = 0.23163063\n",
      "Iteration 157, loss = 0.23160610\n",
      "Iteration 158, loss = 0.23156015\n",
      "Iteration 159, loss = 0.23154819\n",
      "Iteration 160, loss = 0.23148580\n",
      "Iteration 161, loss = 0.23145206\n",
      "Iteration 162, loss = 0.23142120\n",
      "Iteration 163, loss = 0.23138391\n",
      "Iteration 164, loss = 0.23136876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89324538\n",
      "Iteration 2, loss = 0.70890846\n",
      "Iteration 3, loss = 0.57312111\n",
      "Iteration 4, loss = 0.48958888\n",
      "Iteration 5, loss = 0.43573465\n",
      "Iteration 6, loss = 0.39891712\n",
      "Iteration 7, loss = 0.37264163\n",
      "Iteration 8, loss = 0.35289778\n",
      "Iteration 9, loss = 0.33724561\n",
      "Iteration 10, loss = 0.32509986\n",
      "Iteration 11, loss = 0.31505746\n",
      "Iteration 12, loss = 0.30686524\n",
      "Iteration 13, loss = 0.30003444\n",
      "Iteration 14, loss = 0.29418867\n",
      "Iteration 15, loss = 0.28921552\n",
      "Iteration 16, loss = 0.28489693\n",
      "Iteration 17, loss = 0.28119034\n",
      "Iteration 18, loss = 0.27792282\n",
      "Iteration 19, loss = 0.27496169\n",
      "Iteration 20, loss = 0.27233377\n",
      "Iteration 21, loss = 0.27004423\n",
      "Iteration 22, loss = 0.26792841\n",
      "Iteration 23, loss = 0.26607085\n",
      "Iteration 24, loss = 0.26426905\n",
      "Iteration 25, loss = 0.26269169\n",
      "Iteration 26, loss = 0.26121040\n",
      "Iteration 27, loss = 0.25992556\n",
      "Iteration 28, loss = 0.25863586\n",
      "Iteration 29, loss = 0.25748551\n",
      "Iteration 30, loss = 0.25640853\n",
      "Iteration 31, loss = 0.25541805\n",
      "Iteration 32, loss = 0.25438170\n",
      "Iteration 33, loss = 0.25352907\n",
      "Iteration 34, loss = 0.25265790\n",
      "Iteration 35, loss = 0.25188249\n",
      "Iteration 36, loss = 0.25110424\n",
      "Iteration 37, loss = 0.25038825\n",
      "Iteration 38, loss = 0.24971162\n",
      "Iteration 39, loss = 0.24907560\n",
      "Iteration 40, loss = 0.24850005\n",
      "Iteration 41, loss = 0.24784728\n",
      "Iteration 42, loss = 0.24727612\n",
      "Iteration 43, loss = 0.24672633\n",
      "Iteration 44, loss = 0.24618120\n",
      "Iteration 45, loss = 0.24570496\n",
      "Iteration 46, loss = 0.24520852\n",
      "Iteration 47, loss = 0.24475202\n",
      "Iteration 48, loss = 0.24432859\n",
      "Iteration 49, loss = 0.24387949\n",
      "Iteration 50, loss = 0.24340955\n",
      "Iteration 51, loss = 0.24303566\n",
      "Iteration 52, loss = 0.24263837\n",
      "Iteration 53, loss = 0.24222366\n",
      "Iteration 54, loss = 0.24184682\n",
      "Iteration 55, loss = 0.24150242\n",
      "Iteration 56, loss = 0.24114073\n",
      "Iteration 57, loss = 0.24078927\n",
      "Iteration 58, loss = 0.24048852\n",
      "Iteration 59, loss = 0.24011819\n",
      "Iteration 60, loss = 0.23982539\n",
      "Iteration 61, loss = 0.23954486\n",
      "Iteration 62, loss = 0.23922828\n",
      "Iteration 63, loss = 0.23891517\n",
      "Iteration 64, loss = 0.23869076\n",
      "Iteration 65, loss = 0.23840742\n",
      "Iteration 66, loss = 0.23811765\n",
      "Iteration 67, loss = 0.23786744\n",
      "Iteration 68, loss = 0.23758637\n",
      "Iteration 69, loss = 0.23736254\n",
      "Iteration 70, loss = 0.23716822\n",
      "Iteration 71, loss = 0.23683356\n",
      "Iteration 72, loss = 0.23665735\n",
      "Iteration 73, loss = 0.23641431\n",
      "Iteration 74, loss = 0.23618358\n",
      "Iteration 75, loss = 0.23601380\n",
      "Iteration 76, loss = 0.23577048\n",
      "Iteration 77, loss = 0.23555614\n",
      "Iteration 78, loss = 0.23536533\n",
      "Iteration 79, loss = 0.23517707\n",
      "Iteration 80, loss = 0.23497922\n",
      "Iteration 81, loss = 0.23477143\n",
      "Iteration 82, loss = 0.23459386\n",
      "Iteration 83, loss = 0.23437694\n",
      "Iteration 84, loss = 0.23423125\n",
      "Iteration 85, loss = 0.23401932\n",
      "Iteration 86, loss = 0.23393085\n",
      "Iteration 87, loss = 0.23371236\n",
      "Iteration 88, loss = 0.23355771\n",
      "Iteration 89, loss = 0.23343951\n",
      "Iteration 90, loss = 0.23321851\n",
      "Iteration 91, loss = 0.23311204\n",
      "Iteration 92, loss = 0.23294682\n",
      "Iteration 93, loss = 0.23279077\n",
      "Iteration 94, loss = 0.23267172\n",
      "Iteration 95, loss = 0.23253059\n",
      "Iteration 96, loss = 0.23237522\n",
      "Iteration 97, loss = 0.23224286\n",
      "Iteration 98, loss = 0.23211119\n",
      "Iteration 99, loss = 0.23200284\n",
      "Iteration 100, loss = 0.23186314\n",
      "Iteration 101, loss = 0.23175723\n",
      "Iteration 102, loss = 0.23161874\n",
      "Iteration 103, loss = 0.23151520\n",
      "Iteration 104, loss = 0.23144714\n",
      "Iteration 105, loss = 0.23126691\n",
      "Iteration 106, loss = 0.23116993\n",
      "Iteration 107, loss = 0.23109245\n",
      "Iteration 108, loss = 0.23101012\n",
      "Iteration 109, loss = 0.23084947\n",
      "Iteration 110, loss = 0.23078109\n",
      "Iteration 111, loss = 0.23069749\n",
      "Iteration 112, loss = 0.23057542\n",
      "Iteration 113, loss = 0.23050328\n",
      "Iteration 114, loss = 0.23036433\n",
      "Iteration 115, loss = 0.23031875\n",
      "Iteration 116, loss = 0.23024892\n",
      "Iteration 117, loss = 0.23012638\n",
      "Iteration 118, loss = 0.23004114\n",
      "Iteration 119, loss = 0.22998825\n",
      "Iteration 120, loss = 0.22989266\n",
      "Iteration 121, loss = 0.22983825\n",
      "Iteration 122, loss = 0.22973573\n",
      "Iteration 123, loss = 0.22968113\n",
      "Iteration 124, loss = 0.22961063\n",
      "Iteration 125, loss = 0.22949745\n",
      "Iteration 126, loss = 0.22946423\n",
      "Iteration 127, loss = 0.22938802\n",
      "Iteration 128, loss = 0.22933289\n",
      "Iteration 129, loss = 0.22925112\n",
      "Iteration 130, loss = 0.22917380\n",
      "Iteration 131, loss = 0.22915106\n",
      "Iteration 132, loss = 0.22911291\n",
      "Iteration 133, loss = 0.22900591\n",
      "Iteration 134, loss = 0.22899499\n",
      "Iteration 135, loss = 0.22889345\n",
      "Iteration 136, loss = 0.22887979\n",
      "Iteration 137, loss = 0.22877425\n",
      "Iteration 138, loss = 0.22880081\n",
      "Iteration 139, loss = 0.22869279\n",
      "Iteration 140, loss = 0.22866567\n",
      "Iteration 141, loss = 0.22867246\n",
      "Iteration 142, loss = 0.22859484\n",
      "Iteration 143, loss = 0.22855416\n",
      "Iteration 144, loss = 0.22852261\n",
      "Iteration 145, loss = 0.22844134\n",
      "Iteration 146, loss = 0.22841308\n",
      "Iteration 147, loss = 0.22831805\n",
      "Iteration 148, loss = 0.22830858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73098371\n",
      "Iteration 2, loss = 0.48479169\n",
      "Iteration 3, loss = 0.38315967\n",
      "Iteration 4, loss = 0.32689555\n",
      "Iteration 5, loss = 0.29410025\n",
      "Iteration 6, loss = 0.27442266\n",
      "Iteration 7, loss = 0.26086390\n",
      "Iteration 8, loss = 0.25357945\n",
      "Iteration 9, loss = 0.24853431\n",
      "Iteration 10, loss = 0.24525197\n",
      "Iteration 11, loss = 0.24125787\n",
      "Iteration 12, loss = 0.23995927\n",
      "Iteration 13, loss = 0.23835283\n",
      "Iteration 14, loss = 0.23785220\n",
      "Iteration 15, loss = 0.23719740\n",
      "Iteration 16, loss = 0.23677779\n",
      "Iteration 17, loss = 0.23656306\n",
      "Iteration 18, loss = 0.23650748\n",
      "Iteration 19, loss = 0.23598080\n",
      "Iteration 20, loss = 0.23529723\n",
      "Iteration 21, loss = 0.23532176\n",
      "Iteration 22, loss = 0.23495442\n",
      "Iteration 23, loss = 0.23564359\n",
      "Iteration 24, loss = 0.23553508\n",
      "Iteration 25, loss = 0.23548774\n",
      "Iteration 26, loss = 0.23562878\n",
      "Iteration 27, loss = 0.23540206\n",
      "Iteration 28, loss = 0.23515715\n",
      "Iteration 29, loss = 0.23528438\n",
      "Iteration 30, loss = 0.23588526\n",
      "Iteration 31, loss = 0.23522492\n",
      "Iteration 32, loss = 0.23502474\n",
      "Iteration 33, loss = 0.23601686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73166792\n",
      "Iteration 2, loss = 0.48348983\n",
      "Iteration 3, loss = 0.38193879\n",
      "Iteration 4, loss = 0.32628096\n",
      "Iteration 5, loss = 0.29272008\n",
      "Iteration 6, loss = 0.27274807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.25945813\n",
      "Iteration 8, loss = 0.25139805\n",
      "Iteration 9, loss = 0.24620021\n",
      "Iteration 10, loss = 0.24138148\n",
      "Iteration 11, loss = 0.23761596\n",
      "Iteration 12, loss = 0.23708076\n",
      "Iteration 13, loss = 0.23496284\n",
      "Iteration 14, loss = 0.23324242\n",
      "Iteration 15, loss = 0.23264719\n",
      "Iteration 16, loss = 0.23191143\n",
      "Iteration 17, loss = 0.23130667\n",
      "Iteration 18, loss = 0.23132143\n",
      "Iteration 19, loss = 0.23119239\n",
      "Iteration 20, loss = 0.23079971\n",
      "Iteration 21, loss = 0.23071909\n",
      "Iteration 22, loss = 0.23013856\n",
      "Iteration 23, loss = 0.23062271\n",
      "Iteration 24, loss = 0.23082663\n",
      "Iteration 25, loss = 0.23048860\n",
      "Iteration 26, loss = 0.22988198\n",
      "Iteration 27, loss = 0.23007453\n",
      "Iteration 28, loss = 0.23036593\n",
      "Iteration 29, loss = 0.23065110\n",
      "Iteration 30, loss = 0.23068856\n",
      "Iteration 31, loss = 0.23003925\n",
      "Iteration 32, loss = 0.23004973\n",
      "Iteration 33, loss = 0.22998435\n",
      "Iteration 34, loss = 0.23089692\n",
      "Iteration 35, loss = 0.22973781\n",
      "Iteration 36, loss = 0.23025740\n",
      "Iteration 37, loss = 0.23059655\n",
      "Iteration 38, loss = 0.23054422\n",
      "Iteration 39, loss = 0.23014665\n",
      "Iteration 40, loss = 0.23052936\n",
      "Iteration 41, loss = 0.23060476\n",
      "Iteration 42, loss = 0.23107695\n",
      "Iteration 43, loss = 0.23015960\n",
      "Iteration 44, loss = 0.23043754\n",
      "Iteration 45, loss = 0.23050537\n",
      "Iteration 46, loss = 0.23027303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73473933\n",
      "Iteration 2, loss = 0.48434968\n",
      "Iteration 3, loss = 0.38316836\n",
      "Iteration 4, loss = 0.32780616\n",
      "Iteration 5, loss = 0.29562681\n",
      "Iteration 6, loss = 0.27558949\n",
      "Iteration 7, loss = 0.26364934\n",
      "Iteration 8, loss = 0.25588993\n",
      "Iteration 9, loss = 0.24975894\n",
      "Iteration 10, loss = 0.24578602\n",
      "Iteration 11, loss = 0.24314972\n",
      "Iteration 12, loss = 0.24218812\n",
      "Iteration 13, loss = 0.23954639\n",
      "Iteration 14, loss = 0.23815988\n",
      "Iteration 15, loss = 0.23840793\n",
      "Iteration 16, loss = 0.23743035\n",
      "Iteration 17, loss = 0.23709208\n",
      "Iteration 18, loss = 0.23733780\n",
      "Iteration 19, loss = 0.23720475\n",
      "Iteration 20, loss = 0.23637652\n",
      "Iteration 21, loss = 0.23623242\n",
      "Iteration 22, loss = 0.23631179\n",
      "Iteration 23, loss = 0.23671983\n",
      "Iteration 24, loss = 0.23682372\n",
      "Iteration 25, loss = 0.23649758\n",
      "Iteration 26, loss = 0.23608133\n",
      "Iteration 27, loss = 0.23619184\n",
      "Iteration 28, loss = 0.23633322\n",
      "Iteration 29, loss = 0.23669552\n",
      "Iteration 30, loss = 0.23662080\n",
      "Iteration 31, loss = 0.23632486\n",
      "Iteration 32, loss = 0.23604921\n",
      "Iteration 33, loss = 0.23571661\n",
      "Iteration 34, loss = 0.23584300\n",
      "Iteration 35, loss = 0.23615239\n",
      "Iteration 36, loss = 0.23568567\n",
      "Iteration 37, loss = 0.23643463\n",
      "Iteration 38, loss = 0.23611771\n",
      "Iteration 39, loss = 0.23587798\n",
      "Iteration 40, loss = 0.23669157\n",
      "Iteration 41, loss = 0.23641061\n",
      "Iteration 42, loss = 0.23680284\n",
      "Iteration 43, loss = 0.23620750\n",
      "Iteration 44, loss = 0.23615601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72773083\n",
      "Iteration 2, loss = 0.48180831\n",
      "Iteration 3, loss = 0.38144959\n",
      "Iteration 4, loss = 0.32529411\n",
      "Iteration 5, loss = 0.29310423\n",
      "Iteration 6, loss = 0.27273885\n",
      "Iteration 7, loss = 0.26063824\n",
      "Iteration 8, loss = 0.25236573\n",
      "Iteration 9, loss = 0.24662374\n",
      "Iteration 10, loss = 0.24203405\n",
      "Iteration 11, loss = 0.24025770\n",
      "Iteration 12, loss = 0.23862668\n",
      "Iteration 13, loss = 0.23581464\n",
      "Iteration 14, loss = 0.23538796\n",
      "Iteration 15, loss = 0.23449547\n",
      "Iteration 16, loss = 0.23361389\n",
      "Iteration 17, loss = 0.23339568\n",
      "Iteration 18, loss = 0.23381429\n",
      "Iteration 19, loss = 0.23343664\n",
      "Iteration 20, loss = 0.23292300\n",
      "Iteration 21, loss = 0.23264149\n",
      "Iteration 22, loss = 0.23264843\n",
      "Iteration 23, loss = 0.23325971\n",
      "Iteration 24, loss = 0.23281423\n",
      "Iteration 25, loss = 0.23251442\n",
      "Iteration 26, loss = 0.23262414\n",
      "Iteration 27, loss = 0.23319753\n",
      "Iteration 28, loss = 0.23383057\n",
      "Iteration 29, loss = 0.23356300\n",
      "Iteration 30, loss = 0.23266607\n",
      "Iteration 31, loss = 0.23310424\n",
      "Iteration 32, loss = 0.23240987\n",
      "Iteration 33, loss = 0.23246565\n",
      "Iteration 34, loss = 0.23261204\n",
      "Iteration 35, loss = 0.23260464\n",
      "Iteration 36, loss = 0.23281583\n",
      "Iteration 37, loss = 0.23292613\n",
      "Iteration 38, loss = 0.23236859\n",
      "Iteration 39, loss = 0.23239811\n",
      "Iteration 40, loss = 0.23342281\n",
      "Iteration 41, loss = 0.23322972\n",
      "Iteration 42, loss = 0.23311962\n",
      "Iteration 43, loss = 0.23232341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73345883\n",
      "Iteration 2, loss = 0.48525626\n",
      "Iteration 3, loss = 0.38229654\n",
      "Iteration 4, loss = 0.32420694\n",
      "Iteration 5, loss = 0.29054943\n",
      "Iteration 6, loss = 0.26976559\n",
      "Iteration 7, loss = 0.25680933\n",
      "Iteration 8, loss = 0.24864399\n",
      "Iteration 9, loss = 0.24268985\n",
      "Iteration 10, loss = 0.23863462\n",
      "Iteration 11, loss = 0.23594764\n",
      "Iteration 12, loss = 0.23402162\n",
      "Iteration 13, loss = 0.23255300\n",
      "Iteration 14, loss = 0.23115024\n",
      "Iteration 15, loss = 0.23050859\n",
      "Iteration 16, loss = 0.23024422\n",
      "Iteration 17, loss = 0.22997592\n",
      "Iteration 18, loss = 0.23078871\n",
      "Iteration 19, loss = 0.23013840\n",
      "Iteration 20, loss = 0.22957206\n",
      "Iteration 21, loss = 0.22942052\n",
      "Iteration 22, loss = 0.22905388\n",
      "Iteration 23, loss = 0.23003999\n",
      "Iteration 24, loss = 0.22943601\n",
      "Iteration 25, loss = 0.22899622\n",
      "Iteration 26, loss = 0.22891783\n",
      "Iteration 27, loss = 0.22954472\n",
      "Iteration 28, loss = 0.22946159\n",
      "Iteration 29, loss = 0.23012194\n",
      "Iteration 30, loss = 0.22951260\n",
      "Iteration 31, loss = 0.22991427\n",
      "Iteration 32, loss = 0.22922604\n",
      "Iteration 33, loss = 0.22926863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58523567\n",
      "Iteration 2, loss = 0.46127749\n",
      "Iteration 3, loss = 0.45171443\n",
      "Iteration 4, loss = 0.45039341\n",
      "Iteration 5, loss = 0.44921194\n",
      "Iteration 6, loss = 0.44808802\n",
      "Iteration 7, loss = 0.44691545\n",
      "Iteration 8, loss = 0.44579091\n",
      "Iteration 9, loss = 0.44467065\n",
      "Iteration 10, loss = 0.44356352\n",
      "Iteration 11, loss = 0.44239826\n",
      "Iteration 12, loss = 0.44136226\n",
      "Iteration 13, loss = 0.44018194\n",
      "Iteration 14, loss = 0.43913207\n",
      "Iteration 15, loss = 0.43800279\n",
      "Iteration 16, loss = 0.43690682\n",
      "Iteration 17, loss = 0.43583948\n",
      "Iteration 18, loss = 0.43470876\n",
      "Iteration 19, loss = 0.43370254\n",
      "Iteration 20, loss = 0.43253598\n",
      "Iteration 21, loss = 0.43144047\n",
      "Iteration 22, loss = 0.43036923\n",
      "Iteration 23, loss = 0.42924995\n",
      "Iteration 24, loss = 0.42814489\n",
      "Iteration 25, loss = 0.42710785\n",
      "Iteration 26, loss = 0.42605620\n",
      "Iteration 27, loss = 0.42486354\n",
      "Iteration 28, loss = 0.42374932\n",
      "Iteration 29, loss = 0.42262193\n",
      "Iteration 30, loss = 0.42153926\n",
      "Iteration 31, loss = 0.42036607\n",
      "Iteration 32, loss = 0.41926031\n",
      "Iteration 33, loss = 0.41821412\n",
      "Iteration 34, loss = 0.41698299\n",
      "Iteration 35, loss = 0.41585315\n",
      "Iteration 36, loss = 0.41469930\n",
      "Iteration 37, loss = 0.41353175\n",
      "Iteration 38, loss = 0.41238878\n",
      "Iteration 39, loss = 0.41125539\n",
      "Iteration 40, loss = 0.41007222\n",
      "Iteration 41, loss = 0.40888429\n",
      "Iteration 42, loss = 0.40769905\n",
      "Iteration 43, loss = 0.40649301\n",
      "Iteration 44, loss = 0.40527762\n",
      "Iteration 45, loss = 0.40408024\n",
      "Iteration 46, loss = 0.40287599\n",
      "Iteration 47, loss = 0.40164690\n",
      "Iteration 48, loss = 0.40042757\n",
      "Iteration 49, loss = 0.39924697\n",
      "Iteration 50, loss = 0.39797053\n",
      "Iteration 51, loss = 0.39671916\n",
      "Iteration 52, loss = 0.39546519\n",
      "Iteration 53, loss = 0.39422463\n",
      "Iteration 54, loss = 0.39293552\n",
      "Iteration 55, loss = 0.39169790\n",
      "Iteration 56, loss = 0.39040532\n",
      "Iteration 57, loss = 0.38918183\n",
      "Iteration 58, loss = 0.38785404\n",
      "Iteration 59, loss = 0.38658292\n",
      "Iteration 60, loss = 0.38528676\n",
      "Iteration 61, loss = 0.38408391\n",
      "Iteration 62, loss = 0.38272369\n",
      "Iteration 63, loss = 0.38142659\n",
      "Iteration 64, loss = 0.38008638\n",
      "Iteration 65, loss = 0.37879923\n",
      "Iteration 66, loss = 0.37750820\n",
      "Iteration 67, loss = 0.37618865\n",
      "Iteration 68, loss = 0.37489898\n",
      "Iteration 69, loss = 0.37355466\n",
      "Iteration 70, loss = 0.37225574\n",
      "Iteration 71, loss = 0.37094772\n",
      "Iteration 72, loss = 0.36963796\n",
      "Iteration 73, loss = 0.36831064\n",
      "Iteration 74, loss = 0.36697732\n",
      "Iteration 75, loss = 0.36570840\n",
      "Iteration 76, loss = 0.36441023\n",
      "Iteration 77, loss = 0.36309920\n",
      "Iteration 78, loss = 0.36179007\n",
      "Iteration 79, loss = 0.36045224\n",
      "Iteration 80, loss = 0.35914495\n",
      "Iteration 81, loss = 0.35788442\n",
      "Iteration 82, loss = 0.35657539\n",
      "Iteration 83, loss = 0.35528684\n",
      "Iteration 84, loss = 0.35403091\n",
      "Iteration 85, loss = 0.35276396\n",
      "Iteration 86, loss = 0.35146993\n",
      "Iteration 87, loss = 0.35026181\n",
      "Iteration 88, loss = 0.34898946\n",
      "Iteration 89, loss = 0.34773410\n",
      "Iteration 90, loss = 0.34653914\n",
      "Iteration 91, loss = 0.34528129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 92, loss = 0.34404481\n",
      "Iteration 93, loss = 0.34284154\n",
      "Iteration 94, loss = 0.34164137\n",
      "Iteration 95, loss = 0.34043447\n",
      "Iteration 96, loss = 0.33930397\n",
      "Iteration 97, loss = 0.33811856\n",
      "Iteration 98, loss = 0.33694176\n",
      "Iteration 99, loss = 0.33581961\n",
      "Iteration 100, loss = 0.33465419\n",
      "Iteration 101, loss = 0.33354724\n",
      "Iteration 102, loss = 0.33244906\n",
      "Iteration 103, loss = 0.33127689\n",
      "Iteration 104, loss = 0.33020147\n",
      "Iteration 105, loss = 0.32914944\n",
      "Iteration 106, loss = 0.32814364\n",
      "Iteration 107, loss = 0.32700393\n",
      "Iteration 108, loss = 0.32599195\n",
      "Iteration 109, loss = 0.32490198\n",
      "Iteration 110, loss = 0.32390211\n",
      "Iteration 111, loss = 0.32289394\n",
      "Iteration 112, loss = 0.32189602\n",
      "Iteration 113, loss = 0.32095062\n",
      "Iteration 114, loss = 0.31993990\n",
      "Iteration 115, loss = 0.31901927\n",
      "Iteration 116, loss = 0.31801185\n",
      "Iteration 117, loss = 0.31711538\n",
      "Iteration 118, loss = 0.31617538\n",
      "Iteration 119, loss = 0.31525338\n",
      "Iteration 120, loss = 0.31438431\n",
      "Iteration 121, loss = 0.31349316\n",
      "Iteration 122, loss = 0.31263384\n",
      "Iteration 123, loss = 0.31177208\n",
      "Iteration 124, loss = 0.31092499\n",
      "Iteration 125, loss = 0.31007785\n",
      "Iteration 126, loss = 0.30927496\n",
      "Iteration 127, loss = 0.30847511\n",
      "Iteration 128, loss = 0.30766696\n",
      "Iteration 129, loss = 0.30689726\n",
      "Iteration 130, loss = 0.30611822\n",
      "Iteration 131, loss = 0.30536258\n",
      "Iteration 132, loss = 0.30460362\n",
      "Iteration 133, loss = 0.30387028\n",
      "Iteration 134, loss = 0.30316985\n",
      "Iteration 135, loss = 0.30243652\n",
      "Iteration 136, loss = 0.30173453\n",
      "Iteration 137, loss = 0.30104661\n",
      "Iteration 138, loss = 0.30036478\n",
      "Iteration 139, loss = 0.29972339\n",
      "Iteration 140, loss = 0.29904886\n",
      "Iteration 141, loss = 0.29838256\n",
      "Iteration 142, loss = 0.29779701\n",
      "Iteration 143, loss = 0.29711176\n",
      "Iteration 144, loss = 0.29651466\n",
      "Iteration 145, loss = 0.29589839\n",
      "Iteration 146, loss = 0.29533467\n",
      "Iteration 147, loss = 0.29469459\n",
      "Iteration 148, loss = 0.29414986\n",
      "Iteration 149, loss = 0.29358031\n",
      "Iteration 150, loss = 0.29300579\n",
      "Iteration 151, loss = 0.29249112\n",
      "Iteration 152, loss = 0.29191251\n",
      "Iteration 153, loss = 0.29141435\n",
      "Iteration 154, loss = 0.29087847\n",
      "Iteration 155, loss = 0.29033963\n",
      "Iteration 156, loss = 0.28983220\n",
      "Iteration 157, loss = 0.28933658\n",
      "Iteration 158, loss = 0.28885412\n",
      "Iteration 159, loss = 0.28836298\n",
      "Iteration 160, loss = 0.28788620\n",
      "Iteration 161, loss = 0.28740293\n",
      "Iteration 162, loss = 0.28695747\n",
      "Iteration 163, loss = 0.28648754\n",
      "Iteration 164, loss = 0.28605281\n",
      "Iteration 165, loss = 0.28561541\n",
      "Iteration 166, loss = 0.28518484\n",
      "Iteration 167, loss = 0.28477451\n",
      "Iteration 168, loss = 0.28435612\n",
      "Iteration 169, loss = 0.28391333\n",
      "Iteration 170, loss = 0.28352687\n",
      "Iteration 171, loss = 0.28312678\n",
      "Iteration 172, loss = 0.28273119\n",
      "Iteration 173, loss = 0.28234466\n",
      "Iteration 174, loss = 0.28194590\n",
      "Iteration 175, loss = 0.28163518\n",
      "Iteration 176, loss = 0.28123744\n",
      "Iteration 177, loss = 0.28083365\n",
      "Iteration 178, loss = 0.28047988\n",
      "Iteration 179, loss = 0.28013856\n",
      "Iteration 180, loss = 0.27978168\n",
      "Iteration 181, loss = 0.27944022\n",
      "Iteration 182, loss = 0.27911644\n",
      "Iteration 183, loss = 0.27878295\n",
      "Iteration 184, loss = 0.27846793\n",
      "Iteration 185, loss = 0.27812769\n",
      "Iteration 186, loss = 0.27781170\n",
      "Iteration 187, loss = 0.27749268\n",
      "Iteration 188, loss = 0.27718242\n",
      "Iteration 189, loss = 0.27689719\n",
      "Iteration 190, loss = 0.27656812\n",
      "Iteration 191, loss = 0.27627326\n",
      "Iteration 192, loss = 0.27602869\n",
      "Iteration 193, loss = 0.27571642\n",
      "Iteration 194, loss = 0.27542496\n",
      "Iteration 195, loss = 0.27514717\n",
      "Iteration 196, loss = 0.27486522\n",
      "Iteration 197, loss = 0.27460450\n",
      "Iteration 198, loss = 0.27433277\n",
      "Iteration 199, loss = 0.27408510\n",
      "Iteration 200, loss = 0.27383249\n",
      "Iteration 1, loss = 0.58528018\n",
      "Iteration 2, loss = 0.46121313\n",
      "Iteration 3, loss = 0.45181791\n",
      "Iteration 4, loss = 0.45054511\n",
      "Iteration 5, loss = 0.44930256\n",
      "Iteration 6, loss = 0.44812612\n",
      "Iteration 7, loss = 0.44693814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44576260\n",
      "Iteration 9, loss = 0.44468032\n",
      "Iteration 10, loss = 0.44358438\n",
      "Iteration 11, loss = 0.44230392\n",
      "Iteration 12, loss = 0.44126542\n",
      "Iteration 13, loss = 0.44005206\n",
      "Iteration 14, loss = 0.43893150\n",
      "Iteration 15, loss = 0.43778028\n",
      "Iteration 16, loss = 0.43667196\n",
      "Iteration 17, loss = 0.43554194\n",
      "Iteration 18, loss = 0.43441159\n",
      "Iteration 19, loss = 0.43330551\n",
      "Iteration 20, loss = 0.43216524\n",
      "Iteration 21, loss = 0.43106885\n",
      "Iteration 22, loss = 0.42991431\n",
      "Iteration 23, loss = 0.42879246\n",
      "Iteration 24, loss = 0.42766837\n",
      "Iteration 25, loss = 0.42657403\n",
      "Iteration 26, loss = 0.42545289\n",
      "Iteration 27, loss = 0.42430674\n",
      "Iteration 28, loss = 0.42310641\n",
      "Iteration 29, loss = 0.42198039\n",
      "Iteration 30, loss = 0.42084246\n",
      "Iteration 31, loss = 0.41966953\n",
      "Iteration 32, loss = 0.41853179\n",
      "Iteration 33, loss = 0.41741613\n",
      "Iteration 34, loss = 0.41616304\n",
      "Iteration 35, loss = 0.41499362\n",
      "Iteration 36, loss = 0.41381855\n",
      "Iteration 37, loss = 0.41262589\n",
      "Iteration 38, loss = 0.41145635\n",
      "Iteration 39, loss = 0.41026829\n",
      "Iteration 40, loss = 0.40905480\n",
      "Iteration 41, loss = 0.40784199\n",
      "Iteration 42, loss = 0.40669304\n",
      "Iteration 43, loss = 0.40537323\n",
      "Iteration 44, loss = 0.40421745\n",
      "Iteration 45, loss = 0.40292864\n",
      "Iteration 46, loss = 0.40163273\n",
      "Iteration 47, loss = 0.40039313\n",
      "Iteration 48, loss = 0.39911058\n",
      "Iteration 49, loss = 0.39786357\n",
      "Iteration 50, loss = 0.39660603\n",
      "Iteration 51, loss = 0.39531825\n",
      "Iteration 52, loss = 0.39400997\n",
      "Iteration 53, loss = 0.39270536\n",
      "Iteration 54, loss = 0.39140164\n",
      "Iteration 55, loss = 0.39013260\n",
      "Iteration 56, loss = 0.38878655\n",
      "Iteration 57, loss = 0.38756445\n",
      "Iteration 58, loss = 0.38621852\n",
      "Iteration 59, loss = 0.38485780\n",
      "Iteration 60, loss = 0.38351566\n",
      "Iteration 61, loss = 0.38222809\n",
      "Iteration 62, loss = 0.38086426\n",
      "Iteration 63, loss = 0.37954500\n",
      "Iteration 64, loss = 0.37817497\n",
      "Iteration 65, loss = 0.37686078\n",
      "Iteration 66, loss = 0.37547809\n",
      "Iteration 67, loss = 0.37416341\n",
      "Iteration 68, loss = 0.37284453\n",
      "Iteration 69, loss = 0.37145531\n",
      "Iteration 70, loss = 0.37012418\n",
      "Iteration 71, loss = 0.36879155\n",
      "Iteration 72, loss = 0.36742315\n",
      "Iteration 73, loss = 0.36611222\n",
      "Iteration 74, loss = 0.36476132\n",
      "Iteration 75, loss = 0.36340803\n",
      "Iteration 76, loss = 0.36206247\n",
      "Iteration 77, loss = 0.36074968\n",
      "Iteration 78, loss = 0.35939847\n",
      "Iteration 79, loss = 0.35803921\n",
      "Iteration 80, loss = 0.35672322\n",
      "Iteration 81, loss = 0.35540523\n",
      "Iteration 82, loss = 0.35411306\n",
      "Iteration 83, loss = 0.35277351\n",
      "Iteration 84, loss = 0.35148658\n",
      "Iteration 85, loss = 0.35020241\n",
      "Iteration 86, loss = 0.34889507\n",
      "Iteration 87, loss = 0.34768988\n",
      "Iteration 88, loss = 0.34634943\n",
      "Iteration 89, loss = 0.34508117\n",
      "Iteration 90, loss = 0.34385524\n",
      "Iteration 91, loss = 0.34258685\n",
      "Iteration 92, loss = 0.34134205\n",
      "Iteration 93, loss = 0.34012265\n",
      "Iteration 94, loss = 0.33889733\n",
      "Iteration 95, loss = 0.33768486\n",
      "Iteration 96, loss = 0.33653379\n",
      "Iteration 97, loss = 0.33535721\n",
      "Iteration 98, loss = 0.33415864\n",
      "Iteration 99, loss = 0.33303801\n",
      "Iteration 100, loss = 0.33182996\n",
      "Iteration 101, loss = 0.33070741\n",
      "Iteration 102, loss = 0.32958560\n",
      "Iteration 103, loss = 0.32846636\n",
      "Iteration 104, loss = 0.32736029\n",
      "Iteration 105, loss = 0.32630694\n",
      "Iteration 106, loss = 0.32527494\n",
      "Iteration 107, loss = 0.32414344\n",
      "Iteration 108, loss = 0.32310544\n",
      "Iteration 109, loss = 0.32205062\n",
      "Iteration 110, loss = 0.32104106\n",
      "Iteration 111, loss = 0.32003404\n",
      "Iteration 112, loss = 0.31902127\n",
      "Iteration 113, loss = 0.31808106\n",
      "Iteration 114, loss = 0.31709622\n",
      "Iteration 115, loss = 0.31611056\n",
      "Iteration 116, loss = 0.31517248\n",
      "Iteration 117, loss = 0.31421375\n",
      "Iteration 118, loss = 0.31330457\n",
      "Iteration 119, loss = 0.31238494\n",
      "Iteration 120, loss = 0.31151489\n",
      "Iteration 121, loss = 0.31061424\n",
      "Iteration 122, loss = 0.30975120\n",
      "Iteration 123, loss = 0.30890705\n",
      "Iteration 124, loss = 0.30805265\n",
      "Iteration 125, loss = 0.30724091\n",
      "Iteration 126, loss = 0.30645003\n",
      "Iteration 127, loss = 0.30561482\n",
      "Iteration 128, loss = 0.30482700\n",
      "Iteration 129, loss = 0.30406181\n",
      "Iteration 130, loss = 0.30327443\n",
      "Iteration 131, loss = 0.30252613\n",
      "Iteration 132, loss = 0.30178006\n",
      "Iteration 133, loss = 0.30106125\n",
      "Iteration 134, loss = 0.30033940\n",
      "Iteration 135, loss = 0.29962742\n",
      "Iteration 136, loss = 0.29892084\n",
      "Iteration 137, loss = 0.29823971\n",
      "Iteration 138, loss = 0.29760999\n",
      "Iteration 139, loss = 0.29690149\n",
      "Iteration 140, loss = 0.29625775\n",
      "Iteration 141, loss = 0.29560854\n",
      "Iteration 142, loss = 0.29496530\n",
      "Iteration 143, loss = 0.29431720\n",
      "Iteration 144, loss = 0.29372887\n",
      "Iteration 145, loss = 0.29312325\n",
      "Iteration 146, loss = 0.29252404\n",
      "Iteration 147, loss = 0.29194985\n",
      "Iteration 148, loss = 0.29140259\n",
      "Iteration 149, loss = 0.29081941\n",
      "Iteration 150, loss = 0.29024988\n",
      "Iteration 151, loss = 0.28971177\n",
      "Iteration 152, loss = 0.28917550\n",
      "Iteration 153, loss = 0.28866445\n",
      "Iteration 154, loss = 0.28812947\n",
      "Iteration 155, loss = 0.28761497\n",
      "Iteration 156, loss = 0.28710513\n",
      "Iteration 157, loss = 0.28662115\n",
      "Iteration 158, loss = 0.28612518\n",
      "Iteration 159, loss = 0.28565313\n",
      "Iteration 160, loss = 0.28520216\n",
      "Iteration 161, loss = 0.28471614\n",
      "Iteration 162, loss = 0.28424369\n",
      "Iteration 163, loss = 0.28379941\n",
      "Iteration 164, loss = 0.28335900\n",
      "Iteration 165, loss = 0.28293800\n",
      "Iteration 166, loss = 0.28249673\n",
      "Iteration 167, loss = 0.28206554\n",
      "Iteration 168, loss = 0.28165169\n",
      "Iteration 169, loss = 0.28124675\n",
      "Iteration 170, loss = 0.28083469\n",
      "Iteration 171, loss = 0.28043084\n",
      "Iteration 172, loss = 0.28005089\n",
      "Iteration 173, loss = 0.27967739\n",
      "Iteration 174, loss = 0.27927879\n",
      "Iteration 175, loss = 0.27891584\n",
      "Iteration 176, loss = 0.27858834\n",
      "Iteration 177, loss = 0.27817325\n",
      "Iteration 178, loss = 0.27781276\n",
      "Iteration 179, loss = 0.27748046\n",
      "Iteration 180, loss = 0.27712585\n",
      "Iteration 181, loss = 0.27678060\n",
      "Iteration 182, loss = 0.27643835\n",
      "Iteration 183, loss = 0.27610851\n",
      "Iteration 184, loss = 0.27578908\n",
      "Iteration 185, loss = 0.27545983\n",
      "Iteration 186, loss = 0.27516135\n",
      "Iteration 187, loss = 0.27483796\n",
      "Iteration 188, loss = 0.27452373\n",
      "Iteration 189, loss = 0.27426024\n",
      "Iteration 190, loss = 0.27391257\n",
      "Iteration 191, loss = 0.27362131\n",
      "Iteration 192, loss = 0.27333409\n",
      "Iteration 193, loss = 0.27304557\n",
      "Iteration 194, loss = 0.27276413\n",
      "Iteration 195, loss = 0.27247125\n",
      "Iteration 196, loss = 0.27219987\n",
      "Iteration 197, loss = 0.27193338\n",
      "Iteration 198, loss = 0.27166971\n",
      "Iteration 199, loss = 0.27139589\n",
      "Iteration 200, loss = 0.27114205\n",
      "Iteration 1, loss = 0.58586111\n",
      "Iteration 2, loss = 0.46113489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.45227479\n",
      "Iteration 4, loss = 0.45107143\n",
      "Iteration 5, loss = 0.44983544\n",
      "Iteration 6, loss = 0.44868253\n",
      "Iteration 7, loss = 0.44753273\n",
      "Iteration 8, loss = 0.44637702\n",
      "Iteration 9, loss = 0.44527894\n",
      "Iteration 10, loss = 0.44416994\n",
      "Iteration 11, loss = 0.44304191\n",
      "Iteration 12, loss = 0.44199592\n",
      "Iteration 13, loss = 0.44083446\n",
      "Iteration 14, loss = 0.43973805\n",
      "Iteration 15, loss = 0.43865971\n",
      "Iteration 16, loss = 0.43755073\n",
      "Iteration 17, loss = 0.43646254\n",
      "Iteration 18, loss = 0.43538052\n",
      "Iteration 19, loss = 0.43428091\n",
      "Iteration 20, loss = 0.43322083\n",
      "Iteration 21, loss = 0.43211851\n",
      "Iteration 22, loss = 0.43108922\n",
      "Iteration 23, loss = 0.42993203\n",
      "Iteration 24, loss = 0.42885693\n",
      "Iteration 25, loss = 0.42779830\n",
      "Iteration 26, loss = 0.42665043\n",
      "Iteration 27, loss = 0.42565081\n",
      "Iteration 28, loss = 0.42443879\n",
      "Iteration 29, loss = 0.42337200\n",
      "Iteration 30, loss = 0.42226672\n",
      "Iteration 31, loss = 0.42114629\n",
      "Iteration 32, loss = 0.42003090\n",
      "Iteration 33, loss = 0.41894929\n",
      "Iteration 34, loss = 0.41774802\n",
      "Iteration 35, loss = 0.41664075\n",
      "Iteration 36, loss = 0.41546381\n",
      "Iteration 37, loss = 0.41435639\n",
      "Iteration 38, loss = 0.41322333\n",
      "Iteration 39, loss = 0.41203518\n",
      "Iteration 40, loss = 0.41089587\n",
      "Iteration 41, loss = 0.40980437\n",
      "Iteration 42, loss = 0.40856252\n",
      "Iteration 43, loss = 0.40734457\n",
      "Iteration 44, loss = 0.40616745\n",
      "Iteration 45, loss = 0.40498782\n",
      "Iteration 46, loss = 0.40379814\n",
      "Iteration 47, loss = 0.40256998\n",
      "Iteration 48, loss = 0.40136571\n",
      "Iteration 49, loss = 0.40013454\n",
      "Iteration 50, loss = 0.39890695\n",
      "Iteration 51, loss = 0.39773658\n",
      "Iteration 52, loss = 0.39645723\n",
      "Iteration 53, loss = 0.39515820\n",
      "Iteration 54, loss = 0.39396542\n",
      "Iteration 55, loss = 0.39265303\n",
      "Iteration 56, loss = 0.39141167\n",
      "Iteration 57, loss = 0.39016056\n",
      "Iteration 58, loss = 0.38893674\n",
      "Iteration 59, loss = 0.38761481\n",
      "Iteration 60, loss = 0.38633359\n",
      "Iteration 61, loss = 0.38506617\n",
      "Iteration 62, loss = 0.38378413\n",
      "Iteration 63, loss = 0.38246714\n",
      "Iteration 64, loss = 0.38118234\n",
      "Iteration 65, loss = 0.37988434\n",
      "Iteration 66, loss = 0.37859157\n",
      "Iteration 67, loss = 0.37728849\n",
      "Iteration 68, loss = 0.37598693\n",
      "Iteration 69, loss = 0.37467868\n",
      "Iteration 70, loss = 0.37337790\n",
      "Iteration 71, loss = 0.37211077\n",
      "Iteration 72, loss = 0.37079647\n",
      "Iteration 73, loss = 0.36948297\n",
      "Iteration 74, loss = 0.36818163\n",
      "Iteration 75, loss = 0.36686806\n",
      "Iteration 76, loss = 0.36560186\n",
      "Iteration 77, loss = 0.36429340\n",
      "Iteration 78, loss = 0.36297918\n",
      "Iteration 79, loss = 0.36169882\n",
      "Iteration 80, loss = 0.36040824\n",
      "Iteration 81, loss = 0.35912531\n",
      "Iteration 82, loss = 0.35784817\n",
      "Iteration 83, loss = 0.35655550\n",
      "Iteration 84, loss = 0.35531166\n",
      "Iteration 85, loss = 0.35405025\n",
      "Iteration 86, loss = 0.35278415\n",
      "Iteration 87, loss = 0.35155132\n",
      "Iteration 88, loss = 0.35029675\n",
      "Iteration 89, loss = 0.34906168\n",
      "Iteration 90, loss = 0.34786428\n",
      "Iteration 91, loss = 0.34666576\n",
      "Iteration 92, loss = 0.34541168\n",
      "Iteration 93, loss = 0.34422177\n",
      "Iteration 94, loss = 0.34301132\n",
      "Iteration 95, loss = 0.34182460\n",
      "Iteration 96, loss = 0.34074113\n",
      "Iteration 97, loss = 0.33954807\n",
      "Iteration 98, loss = 0.33837469\n",
      "Iteration 99, loss = 0.33726412\n",
      "Iteration 100, loss = 0.33610039\n",
      "Iteration 101, loss = 0.33498783\n",
      "Iteration 102, loss = 0.33387904\n",
      "Iteration 103, loss = 0.33280159\n",
      "Iteration 104, loss = 0.33170096\n",
      "Iteration 105, loss = 0.33067389\n",
      "Iteration 106, loss = 0.32961287\n",
      "Iteration 107, loss = 0.32853958\n",
      "Iteration 108, loss = 0.32747977\n",
      "Iteration 109, loss = 0.32647576\n",
      "Iteration 110, loss = 0.32547893\n",
      "Iteration 111, loss = 0.32447747\n",
      "Iteration 112, loss = 0.32346138\n",
      "Iteration 113, loss = 0.32253451\n",
      "Iteration 114, loss = 0.32154415\n",
      "Iteration 115, loss = 0.32061089\n",
      "Iteration 116, loss = 0.31962910\n",
      "Iteration 117, loss = 0.31871680\n",
      "Iteration 118, loss = 0.31781107\n",
      "Iteration 119, loss = 0.31689281\n",
      "Iteration 120, loss = 0.31602601\n",
      "Iteration 121, loss = 0.31513865\n",
      "Iteration 122, loss = 0.31430998\n",
      "Iteration 123, loss = 0.31343729\n",
      "Iteration 124, loss = 0.31258586\n",
      "Iteration 125, loss = 0.31179135\n",
      "Iteration 126, loss = 0.31106114\n",
      "Iteration 127, loss = 0.31018579\n",
      "Iteration 128, loss = 0.30938646\n",
      "Iteration 129, loss = 0.30860460\n",
      "Iteration 130, loss = 0.30784750\n",
      "Iteration 131, loss = 0.30709131\n",
      "Iteration 132, loss = 0.30636413\n",
      "Iteration 133, loss = 0.30563670\n",
      "Iteration 134, loss = 0.30490969\n",
      "Iteration 135, loss = 0.30420267\n",
      "Iteration 136, loss = 0.30350451\n",
      "Iteration 137, loss = 0.30281089\n",
      "Iteration 138, loss = 0.30219631\n",
      "Iteration 139, loss = 0.30149085\n",
      "Iteration 140, loss = 0.30083687\n",
      "Iteration 141, loss = 0.30018647\n",
      "Iteration 142, loss = 0.29955602\n",
      "Iteration 143, loss = 0.29893054\n",
      "Iteration 144, loss = 0.29833994\n",
      "Iteration 145, loss = 0.29774686\n",
      "Iteration 146, loss = 0.29713506\n",
      "Iteration 147, loss = 0.29659387\n",
      "Iteration 148, loss = 0.29600657\n",
      "Iteration 149, loss = 0.29544640\n",
      "Iteration 150, loss = 0.29488482\n",
      "Iteration 151, loss = 0.29433944\n",
      "Iteration 152, loss = 0.29381815\n",
      "Iteration 153, loss = 0.29329705\n",
      "Iteration 154, loss = 0.29277482\n",
      "Iteration 155, loss = 0.29227421\n",
      "Iteration 156, loss = 0.29174984\n",
      "Iteration 157, loss = 0.29126349\n",
      "Iteration 158, loss = 0.29076926\n",
      "Iteration 159, loss = 0.29030508\n",
      "Iteration 160, loss = 0.28983492\n",
      "Iteration 161, loss = 0.28937290\n",
      "Iteration 162, loss = 0.28890907\n",
      "Iteration 163, loss = 0.28846356\n",
      "Iteration 164, loss = 0.28803382\n",
      "Iteration 165, loss = 0.28762411\n",
      "Iteration 166, loss = 0.28720122\n",
      "Iteration 167, loss = 0.28673607\n",
      "Iteration 168, loss = 0.28636132\n",
      "Iteration 169, loss = 0.28596565\n",
      "Iteration 170, loss = 0.28553442\n",
      "Iteration 171, loss = 0.28512799\n",
      "Iteration 172, loss = 0.28475710\n",
      "Iteration 173, loss = 0.28438981\n",
      "Iteration 174, loss = 0.28400802\n",
      "Iteration 175, loss = 0.28362476\n",
      "Iteration 176, loss = 0.28326892\n",
      "Iteration 177, loss = 0.28289330\n",
      "Iteration 178, loss = 0.28256239\n",
      "Iteration 179, loss = 0.28223218\n",
      "Iteration 180, loss = 0.28187073\n",
      "Iteration 181, loss = 0.28152074\n",
      "Iteration 182, loss = 0.28118573\n",
      "Iteration 183, loss = 0.28085627\n",
      "Iteration 184, loss = 0.28056941\n",
      "Iteration 185, loss = 0.28022267\n",
      "Iteration 186, loss = 0.27991942\n",
      "Iteration 187, loss = 0.27963807\n",
      "Iteration 188, loss = 0.27929411\n",
      "Iteration 189, loss = 0.27906103\n",
      "Iteration 190, loss = 0.27870788\n",
      "Iteration 191, loss = 0.27850040\n",
      "Iteration 192, loss = 0.27816299\n",
      "Iteration 193, loss = 0.27785289\n",
      "Iteration 194, loss = 0.27755898\n",
      "Iteration 195, loss = 0.27729216\n",
      "Iteration 196, loss = 0.27701481\n",
      "Iteration 197, loss = 0.27676646\n",
      "Iteration 198, loss = 0.27649823\n",
      "Iteration 199, loss = 0.27623588\n",
      "Iteration 200, loss = 0.27599328\n",
      "Iteration 1, loss = 0.58444695\n",
      "Iteration 2, loss = 0.46103485\n",
      "Iteration 3, loss = 0.45183497\n",
      "Iteration 4, loss = 0.45055889\n",
      "Iteration 5, loss = 0.44934938\n",
      "Iteration 6, loss = 0.44818902\n",
      "Iteration 7, loss = 0.44699667\n",
      "Iteration 8, loss = 0.44583497\n",
      "Iteration 9, loss = 0.44469881\n",
      "Iteration 10, loss = 0.44359255\n",
      "Iteration 11, loss = 0.44243379\n",
      "Iteration 12, loss = 0.44135626\n",
      "Iteration 13, loss = 0.44020636\n",
      "Iteration 14, loss = 0.43909576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.43797092\n",
      "Iteration 16, loss = 0.43682867\n",
      "Iteration 17, loss = 0.43571693\n",
      "Iteration 18, loss = 0.43462462\n",
      "Iteration 19, loss = 0.43349505\n",
      "Iteration 20, loss = 0.43240769\n",
      "Iteration 21, loss = 0.43127824\n",
      "Iteration 22, loss = 0.43018385\n",
      "Iteration 23, loss = 0.42904946\n",
      "Iteration 24, loss = 0.42794250\n",
      "Iteration 25, loss = 0.42680975\n",
      "Iteration 26, loss = 0.42567571\n",
      "Iteration 27, loss = 0.42465197\n",
      "Iteration 28, loss = 0.42343004\n",
      "Iteration 29, loss = 0.42233808\n",
      "Iteration 30, loss = 0.42117012\n",
      "Iteration 31, loss = 0.42004030\n",
      "Iteration 32, loss = 0.41893209\n",
      "Iteration 33, loss = 0.41776957\n",
      "Iteration 34, loss = 0.41660497\n",
      "Iteration 35, loss = 0.41544589\n",
      "Iteration 36, loss = 0.41429010\n",
      "Iteration 37, loss = 0.41311444\n",
      "Iteration 38, loss = 0.41193509\n",
      "Iteration 39, loss = 0.41072203\n",
      "Iteration 40, loss = 0.40955763\n",
      "Iteration 41, loss = 0.40837868\n",
      "Iteration 42, loss = 0.40712462\n",
      "Iteration 43, loss = 0.40591425\n",
      "Iteration 44, loss = 0.40468926\n",
      "Iteration 45, loss = 0.40349255\n",
      "Iteration 46, loss = 0.40223633\n",
      "Iteration 47, loss = 0.40104244\n",
      "Iteration 48, loss = 0.39975238\n",
      "Iteration 49, loss = 0.39858401\n",
      "Iteration 50, loss = 0.39727255\n",
      "Iteration 51, loss = 0.39597930\n",
      "Iteration 52, loss = 0.39469910\n",
      "Iteration 53, loss = 0.39340625\n",
      "Iteration 54, loss = 0.39217421\n",
      "Iteration 55, loss = 0.39080755\n",
      "Iteration 56, loss = 0.38954580\n",
      "Iteration 57, loss = 0.38826599\n",
      "Iteration 58, loss = 0.38695613\n",
      "Iteration 59, loss = 0.38565664\n",
      "Iteration 60, loss = 0.38435440\n",
      "Iteration 61, loss = 0.38306146\n",
      "Iteration 62, loss = 0.38166243\n",
      "Iteration 63, loss = 0.38035980\n",
      "Iteration 64, loss = 0.37905759\n",
      "Iteration 65, loss = 0.37772384\n",
      "Iteration 66, loss = 0.37638955\n",
      "Iteration 67, loss = 0.37504818\n",
      "Iteration 68, loss = 0.37371083\n",
      "Iteration 69, loss = 0.37238421\n",
      "Iteration 70, loss = 0.37104080\n",
      "Iteration 71, loss = 0.36973469\n",
      "Iteration 72, loss = 0.36838854\n",
      "Iteration 73, loss = 0.36704984\n",
      "Iteration 74, loss = 0.36570599\n",
      "Iteration 75, loss = 0.36438563\n",
      "Iteration 76, loss = 0.36304419\n",
      "Iteration 77, loss = 0.36169745\n",
      "Iteration 78, loss = 0.36038024\n",
      "Iteration 79, loss = 0.35906623\n",
      "Iteration 80, loss = 0.35771584\n",
      "Iteration 81, loss = 0.35641066\n",
      "Iteration 82, loss = 0.35511986\n",
      "Iteration 83, loss = 0.35380928\n",
      "Iteration 84, loss = 0.35254582\n",
      "Iteration 85, loss = 0.35123205\n",
      "Iteration 86, loss = 0.34996426\n",
      "Iteration 87, loss = 0.34869995\n",
      "Iteration 88, loss = 0.34740335\n",
      "Iteration 89, loss = 0.34616050\n",
      "Iteration 90, loss = 0.34492767\n",
      "Iteration 91, loss = 0.34372253\n",
      "Iteration 92, loss = 0.34242184\n",
      "Iteration 93, loss = 0.34119585\n",
      "Iteration 94, loss = 0.33996629\n",
      "Iteration 95, loss = 0.33876600\n",
      "Iteration 96, loss = 0.33760720\n",
      "Iteration 97, loss = 0.33642263\n",
      "Iteration 98, loss = 0.33525096\n",
      "Iteration 99, loss = 0.33408369\n",
      "Iteration 100, loss = 0.33292817\n",
      "Iteration 101, loss = 0.33179468\n",
      "Iteration 102, loss = 0.33064944\n",
      "Iteration 103, loss = 0.32956100\n",
      "Iteration 104, loss = 0.32845974\n",
      "Iteration 105, loss = 0.32742775\n",
      "Iteration 106, loss = 0.32628713\n",
      "Iteration 107, loss = 0.32521081\n",
      "Iteration 108, loss = 0.32418648\n",
      "Iteration 109, loss = 0.32313658\n",
      "Iteration 110, loss = 0.32210088\n",
      "Iteration 111, loss = 0.32109035\n",
      "Iteration 112, loss = 0.32005773\n",
      "Iteration 113, loss = 0.31906409\n",
      "Iteration 114, loss = 0.31808761\n",
      "Iteration 115, loss = 0.31716020\n",
      "Iteration 116, loss = 0.31622480\n",
      "Iteration 117, loss = 0.31524715\n",
      "Iteration 118, loss = 0.31434671\n",
      "Iteration 119, loss = 0.31342276\n",
      "Iteration 120, loss = 0.31253626\n",
      "Iteration 121, loss = 0.31161857\n",
      "Iteration 122, loss = 0.31078907\n",
      "Iteration 123, loss = 0.30989478\n",
      "Iteration 124, loss = 0.30904830\n",
      "Iteration 125, loss = 0.30822016\n",
      "Iteration 126, loss = 0.30741884\n",
      "Iteration 127, loss = 0.30660696\n",
      "Iteration 128, loss = 0.30579841\n",
      "Iteration 129, loss = 0.30500124\n",
      "Iteration 130, loss = 0.30423635\n",
      "Iteration 131, loss = 0.30349392\n",
      "Iteration 132, loss = 0.30274210\n",
      "Iteration 133, loss = 0.30198607\n",
      "Iteration 134, loss = 0.30127733\n",
      "Iteration 135, loss = 0.30055173\n",
      "Iteration 136, loss = 0.29985810\n",
      "Iteration 137, loss = 0.29916116\n",
      "Iteration 138, loss = 0.29852939\n",
      "Iteration 139, loss = 0.29778846\n",
      "Iteration 140, loss = 0.29717451\n",
      "Iteration 141, loss = 0.29650826\n",
      "Iteration 142, loss = 0.29587384\n",
      "Iteration 143, loss = 0.29524134\n",
      "Iteration 144, loss = 0.29463751\n",
      "Iteration 145, loss = 0.29401080\n",
      "Iteration 146, loss = 0.29341329\n",
      "Iteration 147, loss = 0.29290107\n",
      "Iteration 148, loss = 0.29231437\n",
      "Iteration 149, loss = 0.29168682\n",
      "Iteration 150, loss = 0.29112445\n",
      "Iteration 151, loss = 0.29058304\n",
      "Iteration 152, loss = 0.29003999\n",
      "Iteration 153, loss = 0.28951270\n",
      "Iteration 154, loss = 0.28900067\n",
      "Iteration 155, loss = 0.28846839\n",
      "Iteration 156, loss = 0.28796888\n",
      "Iteration 157, loss = 0.28746339\n",
      "Iteration 158, loss = 0.28697945\n",
      "Iteration 159, loss = 0.28650173\n",
      "Iteration 160, loss = 0.28601853\n",
      "Iteration 161, loss = 0.28555786\n",
      "Iteration 162, loss = 0.28508975\n",
      "Iteration 163, loss = 0.28463406\n",
      "Iteration 164, loss = 0.28421025\n",
      "Iteration 165, loss = 0.28376527\n",
      "Iteration 166, loss = 0.28337053\n",
      "Iteration 167, loss = 0.28288953\n",
      "Iteration 168, loss = 0.28251730\n",
      "Iteration 169, loss = 0.28214781\n",
      "Iteration 170, loss = 0.28166182\n",
      "Iteration 171, loss = 0.28125212\n",
      "Iteration 172, loss = 0.28087286\n",
      "Iteration 173, loss = 0.28047938\n",
      "Iteration 174, loss = 0.28011706\n",
      "Iteration 175, loss = 0.27972533\n",
      "Iteration 176, loss = 0.27937866\n",
      "Iteration 177, loss = 0.27899801\n",
      "Iteration 178, loss = 0.27864940\n",
      "Iteration 179, loss = 0.27833697\n",
      "Iteration 180, loss = 0.27795814\n",
      "Iteration 181, loss = 0.27761301\n",
      "Iteration 182, loss = 0.27727722\n",
      "Iteration 183, loss = 0.27694263\n",
      "Iteration 184, loss = 0.27665489\n",
      "Iteration 185, loss = 0.27630301\n",
      "Iteration 186, loss = 0.27599898\n",
      "Iteration 187, loss = 0.27569906\n",
      "Iteration 188, loss = 0.27539912\n",
      "Iteration 189, loss = 0.27511388\n",
      "Iteration 190, loss = 0.27476709\n",
      "Iteration 191, loss = 0.27456425\n",
      "Iteration 192, loss = 0.27426610\n",
      "Iteration 193, loss = 0.27390070\n",
      "Iteration 194, loss = 0.27363201\n",
      "Iteration 195, loss = 0.27334084\n",
      "Iteration 196, loss = 0.27308348\n",
      "Iteration 197, loss = 0.27279909\n",
      "Iteration 198, loss = 0.27254674\n",
      "Iteration 199, loss = 0.27226135\n",
      "Iteration 200, loss = 0.27201006\n",
      "Iteration 1, loss = 0.58466182\n",
      "Iteration 2, loss = 0.46112550\n",
      "Iteration 3, loss = 0.45219909\n",
      "Iteration 4, loss = 0.45081139\n",
      "Iteration 5, loss = 0.44960447\n",
      "Iteration 6, loss = 0.44844300\n",
      "Iteration 7, loss = 0.44730270\n",
      "Iteration 8, loss = 0.44616322\n",
      "Iteration 9, loss = 0.44501902\n",
      "Iteration 10, loss = 0.44387821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.44276072\n",
      "Iteration 12, loss = 0.44163116\n",
      "Iteration 13, loss = 0.44053272\n",
      "Iteration 14, loss = 0.43949072\n",
      "Iteration 15, loss = 0.43828924\n",
      "Iteration 16, loss = 0.43722307\n",
      "Iteration 17, loss = 0.43607247\n",
      "Iteration 18, loss = 0.43496678\n",
      "Iteration 19, loss = 0.43388504\n",
      "Iteration 20, loss = 0.43275913\n",
      "Iteration 21, loss = 0.43165205\n",
      "Iteration 22, loss = 0.43053578\n",
      "Iteration 23, loss = 0.42946383\n",
      "Iteration 24, loss = 0.42835080\n",
      "Iteration 25, loss = 0.42721804\n",
      "Iteration 26, loss = 0.42612164\n",
      "Iteration 27, loss = 0.42511641\n",
      "Iteration 28, loss = 0.42393579\n",
      "Iteration 29, loss = 0.42288102\n",
      "Iteration 30, loss = 0.42167925\n",
      "Iteration 31, loss = 0.42053462\n",
      "Iteration 32, loss = 0.41941187\n",
      "Iteration 33, loss = 0.41824650\n",
      "Iteration 34, loss = 0.41712691\n",
      "Iteration 35, loss = 0.41599709\n",
      "Iteration 36, loss = 0.41478857\n",
      "Iteration 37, loss = 0.41361054\n",
      "Iteration 38, loss = 0.41254669\n",
      "Iteration 39, loss = 0.41130955\n",
      "Iteration 40, loss = 0.41010335\n",
      "Iteration 41, loss = 0.40889114\n",
      "Iteration 42, loss = 0.40769497\n",
      "Iteration 43, loss = 0.40651708\n",
      "Iteration 44, loss = 0.40531532\n",
      "Iteration 45, loss = 0.40408099\n",
      "Iteration 46, loss = 0.40287850\n",
      "Iteration 47, loss = 0.40164271\n",
      "Iteration 48, loss = 0.40036058\n",
      "Iteration 49, loss = 0.39923393\n",
      "Iteration 50, loss = 0.39786877\n",
      "Iteration 51, loss = 0.39663257\n",
      "Iteration 52, loss = 0.39534015\n",
      "Iteration 53, loss = 0.39408110\n",
      "Iteration 54, loss = 0.39282394\n",
      "Iteration 55, loss = 0.39147587\n",
      "Iteration 56, loss = 0.39021710\n",
      "Iteration 57, loss = 0.38892185\n",
      "Iteration 58, loss = 0.38762145\n",
      "Iteration 59, loss = 0.38634306\n",
      "Iteration 60, loss = 0.38500697\n",
      "Iteration 61, loss = 0.38369341\n",
      "Iteration 62, loss = 0.38235616\n",
      "Iteration 63, loss = 0.38102838\n",
      "Iteration 64, loss = 0.37970886\n",
      "Iteration 65, loss = 0.37840371\n",
      "Iteration 66, loss = 0.37713234\n",
      "Iteration 67, loss = 0.37570015\n",
      "Iteration 68, loss = 0.37437568\n",
      "Iteration 69, loss = 0.37303863\n",
      "Iteration 70, loss = 0.37169946\n",
      "Iteration 71, loss = 0.37036849\n",
      "Iteration 72, loss = 0.36901700\n",
      "Iteration 73, loss = 0.36768602\n",
      "Iteration 74, loss = 0.36633675\n",
      "Iteration 75, loss = 0.36502639\n",
      "Iteration 76, loss = 0.36365751\n",
      "Iteration 77, loss = 0.36230073\n",
      "Iteration 78, loss = 0.36099999\n",
      "Iteration 79, loss = 0.35964818\n",
      "Iteration 80, loss = 0.35835263\n",
      "Iteration 81, loss = 0.35698443\n",
      "Iteration 82, loss = 0.35568173\n",
      "Iteration 83, loss = 0.35440651\n",
      "Iteration 84, loss = 0.35306784\n",
      "Iteration 85, loss = 0.35175092\n",
      "Iteration 86, loss = 0.35047487\n",
      "Iteration 87, loss = 0.34913333\n",
      "Iteration 88, loss = 0.34786251\n",
      "Iteration 89, loss = 0.34662547\n",
      "Iteration 90, loss = 0.34531167\n",
      "Iteration 91, loss = 0.34409061\n",
      "Iteration 92, loss = 0.34282424\n",
      "Iteration 93, loss = 0.34157083\n",
      "Iteration 94, loss = 0.34034353\n",
      "Iteration 95, loss = 0.33910283\n",
      "Iteration 96, loss = 0.33790046\n",
      "Iteration 97, loss = 0.33667552\n",
      "Iteration 98, loss = 0.33553296\n",
      "Iteration 99, loss = 0.33427658\n",
      "Iteration 100, loss = 0.33312430\n",
      "Iteration 101, loss = 0.33197039\n",
      "Iteration 102, loss = 0.33079359\n",
      "Iteration 103, loss = 0.32967045\n",
      "Iteration 104, loss = 0.32858070\n",
      "Iteration 105, loss = 0.32745588\n",
      "Iteration 106, loss = 0.32628898\n",
      "Iteration 107, loss = 0.32519944\n",
      "Iteration 108, loss = 0.32416208\n",
      "Iteration 109, loss = 0.32304483\n",
      "Iteration 110, loss = 0.32203403\n",
      "Iteration 111, loss = 0.32095021\n",
      "Iteration 112, loss = 0.31988045\n",
      "Iteration 113, loss = 0.31885443\n",
      "Iteration 114, loss = 0.31785171\n",
      "Iteration 115, loss = 0.31685472\n",
      "Iteration 116, loss = 0.31594388\n",
      "Iteration 117, loss = 0.31489420\n",
      "Iteration 118, loss = 0.31397556\n",
      "Iteration 119, loss = 0.31300325\n",
      "Iteration 120, loss = 0.31205834\n",
      "Iteration 121, loss = 0.31114140\n",
      "Iteration 122, loss = 0.31026889\n",
      "Iteration 123, loss = 0.30934037\n",
      "Iteration 124, loss = 0.30846236\n",
      "Iteration 125, loss = 0.30758727\n",
      "Iteration 126, loss = 0.30674490\n",
      "Iteration 127, loss = 0.30590650\n",
      "Iteration 128, loss = 0.30508714\n",
      "Iteration 129, loss = 0.30422350\n",
      "Iteration 130, loss = 0.30344775\n",
      "Iteration 131, loss = 0.30265108\n",
      "Iteration 132, loss = 0.30190689\n",
      "Iteration 133, loss = 0.30105207\n",
      "Iteration 134, loss = 0.30032900\n",
      "Iteration 135, loss = 0.29956614\n",
      "Iteration 136, loss = 0.29883740\n",
      "Iteration 137, loss = 0.29808722\n",
      "Iteration 138, loss = 0.29745195\n",
      "Iteration 139, loss = 0.29666149\n",
      "Iteration 140, loss = 0.29600324\n",
      "Iteration 141, loss = 0.29534260\n",
      "Iteration 142, loss = 0.29466946\n",
      "Iteration 143, loss = 0.29399818\n",
      "Iteration 144, loss = 0.29334328\n",
      "Iteration 145, loss = 0.29271438\n",
      "Iteration 146, loss = 0.29207174\n",
      "Iteration 147, loss = 0.29144133\n",
      "Iteration 148, loss = 0.29085439\n",
      "Iteration 149, loss = 0.29024411\n",
      "Iteration 150, loss = 0.28965300\n",
      "Iteration 151, loss = 0.28907411\n",
      "Iteration 152, loss = 0.28848596\n",
      "Iteration 153, loss = 0.28791274\n",
      "Iteration 154, loss = 0.28737434\n",
      "Iteration 155, loss = 0.28681673\n",
      "Iteration 156, loss = 0.28628402\n",
      "Iteration 157, loss = 0.28577407\n",
      "Iteration 158, loss = 0.28522476\n",
      "Iteration 159, loss = 0.28473129\n",
      "Iteration 160, loss = 0.28421430\n",
      "Iteration 161, loss = 0.28375101\n",
      "Iteration 162, loss = 0.28323782\n",
      "Iteration 163, loss = 0.28274995\n",
      "Iteration 164, loss = 0.28230289\n",
      "Iteration 165, loss = 0.28181746\n",
      "Iteration 166, loss = 0.28135935\n",
      "Iteration 167, loss = 0.28092955\n",
      "Iteration 168, loss = 0.28047524\n",
      "Iteration 169, loss = 0.28005021\n",
      "Iteration 170, loss = 0.27960686\n",
      "Iteration 171, loss = 0.27915661\n",
      "Iteration 172, loss = 0.27876187\n",
      "Iteration 173, loss = 0.27834606\n",
      "Iteration 174, loss = 0.27792289\n",
      "Iteration 175, loss = 0.27756939\n",
      "Iteration 176, loss = 0.27712486\n",
      "Iteration 177, loss = 0.27674554\n",
      "Iteration 178, loss = 0.27637978\n",
      "Iteration 179, loss = 0.27601361\n",
      "Iteration 180, loss = 0.27562363\n",
      "Iteration 181, loss = 0.27525944\n",
      "Iteration 182, loss = 0.27490507\n",
      "Iteration 183, loss = 0.27455628\n",
      "Iteration 184, loss = 0.27423441\n",
      "Iteration 185, loss = 0.27385047\n",
      "Iteration 186, loss = 0.27352778\n",
      "Iteration 187, loss = 0.27319650\n",
      "Iteration 188, loss = 0.27288600\n",
      "Iteration 189, loss = 0.27257439\n",
      "Iteration 190, loss = 0.27222138\n",
      "Iteration 191, loss = 0.27196466\n",
      "Iteration 192, loss = 0.27163241\n",
      "Iteration 193, loss = 0.27129321\n",
      "Iteration 194, loss = 0.27098273\n",
      "Iteration 195, loss = 0.27068996\n",
      "Iteration 196, loss = 0.27039636\n",
      "Iteration 197, loss = 0.27011798\n",
      "Iteration 198, loss = 0.26982401\n",
      "Iteration 199, loss = 0.26953982\n",
      "Iteration 200, loss = 0.26927854\n",
      "Iteration 1, loss = 0.55079558\n",
      "Iteration 2, loss = 0.41982747\n",
      "Iteration 3, loss = 0.39031789\n",
      "Iteration 4, loss = 0.36929628\n",
      "Iteration 5, loss = 0.35164028\n",
      "Iteration 6, loss = 0.33677530\n",
      "Iteration 7, loss = 0.32375707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.31305296\n",
      "Iteration 9, loss = 0.30420859\n",
      "Iteration 10, loss = 0.29702052\n",
      "Iteration 11, loss = 0.29047278\n",
      "Iteration 12, loss = 0.28569246\n",
      "Iteration 13, loss = 0.28094136\n",
      "Iteration 14, loss = 0.27734456\n",
      "Iteration 15, loss = 0.27383438\n",
      "Iteration 16, loss = 0.27106516\n",
      "Iteration 17, loss = 0.26879086\n",
      "Iteration 18, loss = 0.26593749\n",
      "Iteration 19, loss = 0.26372580\n",
      "Iteration 20, loss = 0.26179554\n",
      "Iteration 21, loss = 0.25991315\n",
      "Iteration 22, loss = 0.25816602\n",
      "Iteration 23, loss = 0.25660096\n",
      "Iteration 24, loss = 0.25499173\n",
      "Iteration 25, loss = 0.25364281\n",
      "Iteration 26, loss = 0.25226616\n",
      "Iteration 27, loss = 0.25054580\n",
      "Iteration 28, loss = 0.24937886\n",
      "Iteration 29, loss = 0.24814420\n",
      "Iteration 30, loss = 0.24743198\n",
      "Iteration 31, loss = 0.24579138\n",
      "Iteration 32, loss = 0.24471167\n",
      "Iteration 33, loss = 0.24416869\n",
      "Iteration 34, loss = 0.24281894\n",
      "Iteration 35, loss = 0.24164966\n",
      "Iteration 36, loss = 0.24093685\n",
      "Iteration 37, loss = 0.24015413\n",
      "Iteration 38, loss = 0.23964860\n",
      "Iteration 39, loss = 0.23870074\n",
      "Iteration 40, loss = 0.23811068\n",
      "Iteration 41, loss = 0.23735612\n",
      "Iteration 42, loss = 0.23676754\n",
      "Iteration 43, loss = 0.23626662\n",
      "Iteration 44, loss = 0.23562140\n",
      "Iteration 45, loss = 0.23534647\n",
      "Iteration 46, loss = 0.23504116\n",
      "Iteration 47, loss = 0.23450915\n",
      "Iteration 48, loss = 0.23440415\n",
      "Iteration 49, loss = 0.23419637\n",
      "Iteration 50, loss = 0.23376842\n",
      "Iteration 51, loss = 0.23319975\n",
      "Iteration 52, loss = 0.23313817\n",
      "Iteration 53, loss = 0.23299451\n",
      "Iteration 54, loss = 0.23259377\n",
      "Iteration 55, loss = 0.23209515\n",
      "Iteration 56, loss = 0.23203363\n",
      "Iteration 57, loss = 0.23169149\n",
      "Iteration 58, loss = 0.23163221\n",
      "Iteration 59, loss = 0.23132252\n",
      "Iteration 60, loss = 0.23099255\n",
      "Iteration 61, loss = 0.23113616\n",
      "Iteration 62, loss = 0.23072894\n",
      "Iteration 63, loss = 0.23092597\n",
      "Iteration 64, loss = 0.23040615\n",
      "Iteration 65, loss = 0.23010242\n",
      "Iteration 66, loss = 0.23031834\n",
      "Iteration 67, loss = 0.22969847\n",
      "Iteration 68, loss = 0.22971769\n",
      "Iteration 69, loss = 0.22895296\n",
      "Iteration 70, loss = 0.22914829\n",
      "Iteration 71, loss = 0.22907936\n",
      "Iteration 72, loss = 0.22856659\n",
      "Iteration 73, loss = 0.22807643\n",
      "Iteration 74, loss = 0.22799615\n",
      "Iteration 75, loss = 0.22774728\n",
      "Iteration 76, loss = 0.22797764\n",
      "Iteration 77, loss = 0.22703861\n",
      "Iteration 78, loss = 0.22675371\n",
      "Iteration 79, loss = 0.22675612\n",
      "Iteration 80, loss = 0.22592679\n",
      "Iteration 81, loss = 0.22589478\n",
      "Iteration 82, loss = 0.22547457\n",
      "Iteration 83, loss = 0.22493892\n",
      "Iteration 84, loss = 0.22489924\n",
      "Iteration 85, loss = 0.22456983\n",
      "Iteration 86, loss = 0.22377185\n",
      "Iteration 87, loss = 0.22354858\n",
      "Iteration 88, loss = 0.22297859\n",
      "Iteration 89, loss = 0.22286884\n",
      "Iteration 90, loss = 0.22208250\n",
      "Iteration 91, loss = 0.22187616\n",
      "Iteration 92, loss = 0.22096827\n",
      "Iteration 93, loss = 0.22059492\n",
      "Iteration 94, loss = 0.22012859\n",
      "Iteration 95, loss = 0.21960339\n",
      "Iteration 96, loss = 0.21902868\n",
      "Iteration 97, loss = 0.21854502\n",
      "Iteration 98, loss = 0.21812528\n",
      "Iteration 99, loss = 0.21782525\n",
      "Iteration 100, loss = 0.21701032\n",
      "Iteration 101, loss = 0.21643616\n",
      "Iteration 102, loss = 0.21648271\n",
      "Iteration 103, loss = 0.21524586\n",
      "Iteration 104, loss = 0.21451202\n",
      "Iteration 105, loss = 0.21399733\n",
      "Iteration 106, loss = 0.21440766\n",
      "Iteration 107, loss = 0.21258975\n",
      "Iteration 108, loss = 0.21214280\n",
      "Iteration 109, loss = 0.21137376\n",
      "Iteration 110, loss = 0.21080373\n",
      "Iteration 111, loss = 0.20995780\n",
      "Iteration 112, loss = 0.20938270\n",
      "Iteration 113, loss = 0.20898336\n",
      "Iteration 114, loss = 0.20819189\n",
      "Iteration 115, loss = 0.20740535\n",
      "Iteration 116, loss = 0.20658098\n",
      "Iteration 117, loss = 0.20586082\n",
      "Iteration 118, loss = 0.20499196\n",
      "Iteration 119, loss = 0.20431805\n",
      "Iteration 120, loss = 0.20374613\n",
      "Iteration 121, loss = 0.20315356\n",
      "Iteration 122, loss = 0.20243933\n",
      "Iteration 123, loss = 0.20145335\n",
      "Iteration 124, loss = 0.20094540\n",
      "Iteration 125, loss = 0.20002816\n",
      "Iteration 126, loss = 0.19915675\n",
      "Iteration 127, loss = 0.19875535\n",
      "Iteration 128, loss = 0.19781533\n",
      "Iteration 129, loss = 0.19760582\n",
      "Iteration 130, loss = 0.19634146\n",
      "Iteration 131, loss = 0.19568923\n",
      "Iteration 132, loss = 0.19483533\n",
      "Iteration 133, loss = 0.19381238\n",
      "Iteration 134, loss = 0.19361013\n",
      "Iteration 135, loss = 0.19254476\n",
      "Iteration 136, loss = 0.19198379\n",
      "Iteration 137, loss = 0.19092371\n",
      "Iteration 138, loss = 0.19021733\n",
      "Iteration 139, loss = 0.18966375\n",
      "Iteration 140, loss = 0.18882997\n",
      "Iteration 141, loss = 0.18809688\n",
      "Iteration 142, loss = 0.18777428\n",
      "Iteration 143, loss = 0.18660684\n",
      "Iteration 144, loss = 0.18615501\n",
      "Iteration 145, loss = 0.18526519\n",
      "Iteration 146, loss = 0.18444998\n",
      "Iteration 147, loss = 0.18364373\n",
      "Iteration 148, loss = 0.18294788\n",
      "Iteration 149, loss = 0.18224151\n",
      "Iteration 150, loss = 0.18152889\n",
      "Iteration 151, loss = 0.18118620\n",
      "Iteration 152, loss = 0.18036017\n",
      "Iteration 153, loss = 0.17975805\n",
      "Iteration 154, loss = 0.17969639\n",
      "Iteration 155, loss = 0.17844151\n",
      "Iteration 156, loss = 0.17777911\n",
      "Iteration 157, loss = 0.17705151\n",
      "Iteration 158, loss = 0.17628608\n",
      "Iteration 159, loss = 0.17587513\n",
      "Iteration 160, loss = 0.17531436\n",
      "Iteration 161, loss = 0.17462369\n",
      "Iteration 162, loss = 0.17388840\n",
      "Iteration 163, loss = 0.17318544\n",
      "Iteration 164, loss = 0.17258294\n",
      "Iteration 165, loss = 0.17214648\n",
      "Iteration 166, loss = 0.17149210\n",
      "Iteration 167, loss = 0.17138577\n",
      "Iteration 168, loss = 0.17133785\n",
      "Iteration 169, loss = 0.16978497\n",
      "Iteration 170, loss = 0.16973916\n",
      "Iteration 171, loss = 0.16888399\n",
      "Iteration 172, loss = 0.16819373\n",
      "Iteration 173, loss = 0.16759283\n",
      "Iteration 174, loss = 0.16692667\n",
      "Iteration 175, loss = 0.16679571\n",
      "Iteration 176, loss = 0.16644658\n",
      "Iteration 177, loss = 0.16533579\n",
      "Iteration 178, loss = 0.16489943\n",
      "Iteration 179, loss = 0.16438920\n",
      "Iteration 180, loss = 0.16360527\n",
      "Iteration 181, loss = 0.16306449\n",
      "Iteration 182, loss = 0.16291360\n",
      "Iteration 183, loss = 0.16240065\n",
      "Iteration 184, loss = 0.16189083\n",
      "Iteration 185, loss = 0.16110072\n",
      "Iteration 186, loss = 0.16066804\n",
      "Iteration 187, loss = 0.16016151\n",
      "Iteration 188, loss = 0.15976325\n",
      "Iteration 189, loss = 0.15898237\n",
      "Iteration 190, loss = 0.15862876\n",
      "Iteration 191, loss = 0.15811423\n",
      "Iteration 192, loss = 0.15779538\n",
      "Iteration 193, loss = 0.15733146\n",
      "Iteration 194, loss = 0.15650609\n",
      "Iteration 195, loss = 0.15607102\n",
      "Iteration 196, loss = 0.15571397\n",
      "Iteration 197, loss = 0.15555685\n",
      "Iteration 198, loss = 0.15453763\n",
      "Iteration 199, loss = 0.15459540\n",
      "Iteration 200, loss = 0.15440699\n",
      "Iteration 1, loss = 0.55197845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.41972768\n",
      "Iteration 3, loss = 0.38994650\n",
      "Iteration 4, loss = 0.36869049\n",
      "Iteration 5, loss = 0.35097130\n",
      "Iteration 6, loss = 0.33592083\n",
      "Iteration 7, loss = 0.32289487\n",
      "Iteration 8, loss = 0.31218614\n",
      "Iteration 9, loss = 0.30322555\n",
      "Iteration 10, loss = 0.29593045\n",
      "Iteration 11, loss = 0.28925784\n",
      "Iteration 12, loss = 0.28471594\n",
      "Iteration 13, loss = 0.27980390\n",
      "Iteration 14, loss = 0.27569199\n",
      "Iteration 15, loss = 0.27236249\n",
      "Iteration 16, loss = 0.26931185\n",
      "Iteration 17, loss = 0.26669405\n",
      "Iteration 18, loss = 0.26396632\n",
      "Iteration 19, loss = 0.26179766\n",
      "Iteration 20, loss = 0.25966993\n",
      "Iteration 21, loss = 0.25772896\n",
      "Iteration 22, loss = 0.25570793\n",
      "Iteration 23, loss = 0.25407058\n",
      "Iteration 24, loss = 0.25247332\n",
      "Iteration 25, loss = 0.25087995\n",
      "Iteration 26, loss = 0.24924412\n",
      "Iteration 27, loss = 0.24762140\n",
      "Iteration 28, loss = 0.24636811\n",
      "Iteration 29, loss = 0.24501002\n",
      "Iteration 30, loss = 0.24391174\n",
      "Iteration 31, loss = 0.24231192\n",
      "Iteration 32, loss = 0.24112096\n",
      "Iteration 33, loss = 0.24019535\n",
      "Iteration 34, loss = 0.23909177\n",
      "Iteration 35, loss = 0.23781057\n",
      "Iteration 36, loss = 0.23708117\n",
      "Iteration 37, loss = 0.23601049\n",
      "Iteration 38, loss = 0.23524460\n",
      "Iteration 39, loss = 0.23457552\n",
      "Iteration 40, loss = 0.23368416\n",
      "Iteration 41, loss = 0.23299641\n",
      "Iteration 42, loss = 0.23254329\n",
      "Iteration 43, loss = 0.23170209\n",
      "Iteration 44, loss = 0.23124027\n",
      "Iteration 45, loss = 0.23068345\n",
      "Iteration 46, loss = 0.23005722\n",
      "Iteration 47, loss = 0.22979065\n",
      "Iteration 48, loss = 0.22931537\n",
      "Iteration 49, loss = 0.22920631\n",
      "Iteration 50, loss = 0.22864810\n",
      "Iteration 51, loss = 0.22843742\n",
      "Iteration 52, loss = 0.22812308\n",
      "Iteration 53, loss = 0.22761658\n",
      "Iteration 54, loss = 0.22722215\n",
      "Iteration 55, loss = 0.22689684\n",
      "Iteration 56, loss = 0.22672085\n",
      "Iteration 57, loss = 0.22642402\n",
      "Iteration 58, loss = 0.22642117\n",
      "Iteration 59, loss = 0.22600957\n",
      "Iteration 60, loss = 0.22555853\n",
      "Iteration 61, loss = 0.22551230\n",
      "Iteration 62, loss = 0.22523052\n",
      "Iteration 63, loss = 0.22536531\n",
      "Iteration 64, loss = 0.22471193\n",
      "Iteration 65, loss = 0.22474005\n",
      "Iteration 66, loss = 0.22452905\n",
      "Iteration 67, loss = 0.22390108\n",
      "Iteration 68, loss = 0.22407661\n",
      "Iteration 69, loss = 0.22351975\n",
      "Iteration 70, loss = 0.22314030\n",
      "Iteration 71, loss = 0.22355775\n",
      "Iteration 72, loss = 0.22284861\n",
      "Iteration 73, loss = 0.22221254\n",
      "Iteration 74, loss = 0.22193699\n",
      "Iteration 75, loss = 0.22199012\n",
      "Iteration 76, loss = 0.22129325\n",
      "Iteration 77, loss = 0.22089234\n",
      "Iteration 78, loss = 0.22072519\n",
      "Iteration 79, loss = 0.22042702\n",
      "Iteration 80, loss = 0.21961903\n",
      "Iteration 81, loss = 0.21934909\n",
      "Iteration 82, loss = 0.21908670\n",
      "Iteration 83, loss = 0.21847894\n",
      "Iteration 84, loss = 0.21816051\n",
      "Iteration 85, loss = 0.21788059\n",
      "Iteration 86, loss = 0.21719333\n",
      "Iteration 87, loss = 0.21723656\n",
      "Iteration 88, loss = 0.21618753\n",
      "Iteration 89, loss = 0.21604823\n",
      "Iteration 90, loss = 0.21533341\n",
      "Iteration 91, loss = 0.21509764\n",
      "Iteration 92, loss = 0.21412303\n",
      "Iteration 93, loss = 0.21372298\n",
      "Iteration 94, loss = 0.21326733\n",
      "Iteration 95, loss = 0.21268832\n",
      "Iteration 96, loss = 0.21196415\n",
      "Iteration 97, loss = 0.21138861\n",
      "Iteration 98, loss = 0.21092769\n",
      "Iteration 99, loss = 0.21068388\n",
      "Iteration 100, loss = 0.20983252\n",
      "Iteration 101, loss = 0.20912368\n",
      "Iteration 102, loss = 0.20906828\n",
      "Iteration 103, loss = 0.20802038\n",
      "Iteration 104, loss = 0.20722492\n",
      "Iteration 105, loss = 0.20685238\n",
      "Iteration 106, loss = 0.20668545\n",
      "Iteration 107, loss = 0.20537316\n",
      "Iteration 108, loss = 0.20460973\n",
      "Iteration 109, loss = 0.20395963\n",
      "Iteration 110, loss = 0.20314793\n",
      "Iteration 111, loss = 0.20252802\n",
      "Iteration 112, loss = 0.20177343\n",
      "Iteration 113, loss = 0.20122996\n",
      "Iteration 114, loss = 0.20098215\n",
      "Iteration 115, loss = 0.19966660\n",
      "Iteration 116, loss = 0.19903486\n",
      "Iteration 117, loss = 0.19808804\n",
      "Iteration 118, loss = 0.19740864\n",
      "Iteration 119, loss = 0.19665924\n",
      "Iteration 120, loss = 0.19613871\n",
      "Iteration 121, loss = 0.19530555\n",
      "Iteration 122, loss = 0.19487546\n",
      "Iteration 123, loss = 0.19378853\n",
      "Iteration 124, loss = 0.19320731\n",
      "Iteration 125, loss = 0.19220771\n",
      "Iteration 126, loss = 0.19162077\n",
      "Iteration 127, loss = 0.19099864\n",
      "Iteration 128, loss = 0.19036062\n",
      "Iteration 129, loss = 0.18993707\n",
      "Iteration 130, loss = 0.18848758\n",
      "Iteration 131, loss = 0.18777061\n",
      "Iteration 132, loss = 0.18725624\n",
      "Iteration 133, loss = 0.18644960\n",
      "Iteration 134, loss = 0.18595649\n",
      "Iteration 135, loss = 0.18511009\n",
      "Iteration 136, loss = 0.18413677\n",
      "Iteration 137, loss = 0.18336684\n",
      "Iteration 138, loss = 0.18283417\n",
      "Iteration 139, loss = 0.18206992\n",
      "Iteration 140, loss = 0.18130191\n",
      "Iteration 141, loss = 0.18066083\n",
      "Iteration 142, loss = 0.18017795\n",
      "Iteration 143, loss = 0.17922989\n",
      "Iteration 144, loss = 0.17877185\n",
      "Iteration 145, loss = 0.17782437\n",
      "Iteration 146, loss = 0.17718010\n",
      "Iteration 147, loss = 0.17648916\n",
      "Iteration 148, loss = 0.17593550\n",
      "Iteration 149, loss = 0.17502433\n",
      "Iteration 150, loss = 0.17443869\n",
      "Iteration 151, loss = 0.17383027\n",
      "Iteration 152, loss = 0.17324838\n",
      "Iteration 153, loss = 0.17238934\n",
      "Iteration 154, loss = 0.17192632\n",
      "Iteration 155, loss = 0.17129478\n",
      "Iteration 156, loss = 0.17072194\n",
      "Iteration 157, loss = 0.16991760\n",
      "Iteration 158, loss = 0.16932008\n",
      "Iteration 159, loss = 0.16888340\n",
      "Iteration 160, loss = 0.16857023\n",
      "Iteration 161, loss = 0.16788402\n",
      "Iteration 162, loss = 0.16705984\n",
      "Iteration 163, loss = 0.16659523\n",
      "Iteration 164, loss = 0.16573096\n",
      "Iteration 165, loss = 0.16522207\n",
      "Iteration 166, loss = 0.16458236\n",
      "Iteration 167, loss = 0.16424344\n",
      "Iteration 168, loss = 0.16373229\n",
      "Iteration 169, loss = 0.16298538\n",
      "Iteration 170, loss = 0.16260165\n",
      "Iteration 171, loss = 0.16197727\n",
      "Iteration 172, loss = 0.16127184\n",
      "Iteration 173, loss = 0.16084923\n",
      "Iteration 174, loss = 0.16025489\n",
      "Iteration 175, loss = 0.15968679\n",
      "Iteration 176, loss = 0.15982826\n",
      "Iteration 177, loss = 0.15882887\n",
      "Iteration 178, loss = 0.15794156\n",
      "Iteration 179, loss = 0.15757047\n",
      "Iteration 180, loss = 0.15703400\n",
      "Iteration 181, loss = 0.15637548\n",
      "Iteration 182, loss = 0.15608647\n",
      "Iteration 183, loss = 0.15562200\n",
      "Iteration 184, loss = 0.15501500\n",
      "Iteration 185, loss = 0.15437595\n",
      "Iteration 186, loss = 0.15386878\n",
      "Iteration 187, loss = 0.15340110\n",
      "Iteration 188, loss = 0.15293332\n",
      "Iteration 189, loss = 0.15221571\n",
      "Iteration 190, loss = 0.15210342\n",
      "Iteration 191, loss = 0.15133824\n",
      "Iteration 192, loss = 0.15100686\n",
      "Iteration 193, loss = 0.15030908\n",
      "Iteration 194, loss = 0.14996109\n",
      "Iteration 195, loss = 0.14934010\n",
      "Iteration 196, loss = 0.14892156\n",
      "Iteration 197, loss = 0.14866904\n",
      "Iteration 198, loss = 0.14780898\n",
      "Iteration 199, loss = 0.14772799\n",
      "Iteration 200, loss = 0.14702298\n",
      "Iteration 1, loss = 0.55287046\n",
      "Iteration 2, loss = 0.41960997\n",
      "Iteration 3, loss = 0.39034349\n",
      "Iteration 4, loss = 0.36925972\n",
      "Iteration 5, loss = 0.35200990\n",
      "Iteration 6, loss = 0.33723050\n",
      "Iteration 7, loss = 0.32471665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.31430682\n",
      "Iteration 9, loss = 0.30572241\n",
      "Iteration 10, loss = 0.29862143\n",
      "Iteration 11, loss = 0.29262893\n",
      "Iteration 12, loss = 0.28814043\n",
      "Iteration 13, loss = 0.28359736\n",
      "Iteration 14, loss = 0.27966287\n",
      "Iteration 15, loss = 0.27667978\n",
      "Iteration 16, loss = 0.27366179\n",
      "Iteration 17, loss = 0.27123884\n",
      "Iteration 18, loss = 0.26873440\n",
      "Iteration 19, loss = 0.26656715\n",
      "Iteration 20, loss = 0.26439708\n",
      "Iteration 21, loss = 0.26247989\n",
      "Iteration 22, loss = 0.26078861\n",
      "Iteration 23, loss = 0.25901443\n",
      "Iteration 24, loss = 0.25738347\n",
      "Iteration 25, loss = 0.25571538\n",
      "Iteration 26, loss = 0.25408124\n",
      "Iteration 27, loss = 0.25269743\n",
      "Iteration 28, loss = 0.25122296\n",
      "Iteration 29, loss = 0.25005280\n",
      "Iteration 30, loss = 0.24887285\n",
      "Iteration 31, loss = 0.24734175\n",
      "Iteration 32, loss = 0.24605033\n",
      "Iteration 33, loss = 0.24495932\n",
      "Iteration 34, loss = 0.24390989\n",
      "Iteration 35, loss = 0.24287888\n",
      "Iteration 36, loss = 0.24195949\n",
      "Iteration 37, loss = 0.24103685\n",
      "Iteration 38, loss = 0.24044637\n",
      "Iteration 39, loss = 0.23947438\n",
      "Iteration 40, loss = 0.23899412\n",
      "Iteration 41, loss = 0.23823539\n",
      "Iteration 42, loss = 0.23781740\n",
      "Iteration 43, loss = 0.23701856\n",
      "Iteration 44, loss = 0.23643719\n",
      "Iteration 45, loss = 0.23619214\n",
      "Iteration 46, loss = 0.23564763\n",
      "Iteration 47, loss = 0.23518373\n",
      "Iteration 48, loss = 0.23469205\n",
      "Iteration 49, loss = 0.23442207\n",
      "Iteration 50, loss = 0.23389259\n",
      "Iteration 51, loss = 0.23407193\n",
      "Iteration 52, loss = 0.23364326\n",
      "Iteration 53, loss = 0.23303991\n",
      "Iteration 54, loss = 0.23286219\n",
      "Iteration 55, loss = 0.23250615\n",
      "Iteration 56, loss = 0.23254277\n",
      "Iteration 57, loss = 0.23198699\n",
      "Iteration 58, loss = 0.23210913\n",
      "Iteration 59, loss = 0.23179615\n",
      "Iteration 60, loss = 0.23138610\n",
      "Iteration 61, loss = 0.23125125\n",
      "Iteration 62, loss = 0.23119887\n",
      "Iteration 63, loss = 0.23104084\n",
      "Iteration 64, loss = 0.23024418\n",
      "Iteration 65, loss = 0.23016136\n",
      "Iteration 66, loss = 0.22986864\n",
      "Iteration 67, loss = 0.22970788\n",
      "Iteration 68, loss = 0.22953817\n",
      "Iteration 69, loss = 0.22912487\n",
      "Iteration 70, loss = 0.22895252\n",
      "Iteration 71, loss = 0.22909971\n",
      "Iteration 72, loss = 0.22818859\n",
      "Iteration 73, loss = 0.22779718\n",
      "Iteration 74, loss = 0.22735712\n",
      "Iteration 75, loss = 0.22743835\n",
      "Iteration 76, loss = 0.22680834\n",
      "Iteration 77, loss = 0.22637446\n",
      "Iteration 78, loss = 0.22606314\n",
      "Iteration 79, loss = 0.22594074\n",
      "Iteration 80, loss = 0.22523810\n",
      "Iteration 81, loss = 0.22468867\n",
      "Iteration 82, loss = 0.22456105\n",
      "Iteration 83, loss = 0.22399552\n",
      "Iteration 84, loss = 0.22352959\n",
      "Iteration 85, loss = 0.22307543\n",
      "Iteration 86, loss = 0.22249812\n",
      "Iteration 87, loss = 0.22210201\n",
      "Iteration 88, loss = 0.22151286\n",
      "Iteration 89, loss = 0.22091054\n",
      "Iteration 90, loss = 0.22039323\n",
      "Iteration 91, loss = 0.21995878\n",
      "Iteration 92, loss = 0.21903809\n",
      "Iteration 93, loss = 0.21867831\n",
      "Iteration 94, loss = 0.21801686\n",
      "Iteration 95, loss = 0.21742757\n",
      "Iteration 96, loss = 0.21674308\n",
      "Iteration 97, loss = 0.21580586\n",
      "Iteration 98, loss = 0.21554721\n",
      "Iteration 99, loss = 0.21474020\n",
      "Iteration 100, loss = 0.21437786\n",
      "Iteration 101, loss = 0.21322594\n",
      "Iteration 102, loss = 0.21288254\n",
      "Iteration 103, loss = 0.21181664\n",
      "Iteration 104, loss = 0.21090463\n",
      "Iteration 105, loss = 0.21040438\n",
      "Iteration 106, loss = 0.20985893\n",
      "Iteration 107, loss = 0.20858607\n",
      "Iteration 108, loss = 0.20781116\n",
      "Iteration 109, loss = 0.20751656\n",
      "Iteration 110, loss = 0.20605456\n",
      "Iteration 111, loss = 0.20536441\n",
      "Iteration 112, loss = 0.20458598\n",
      "Iteration 113, loss = 0.20414966\n",
      "Iteration 114, loss = 0.20308096\n",
      "Iteration 115, loss = 0.20220574\n",
      "Iteration 116, loss = 0.20125409\n",
      "Iteration 117, loss = 0.20038296\n",
      "Iteration 118, loss = 0.19935270\n",
      "Iteration 119, loss = 0.19870807\n",
      "Iteration 120, loss = 0.19787619\n",
      "Iteration 121, loss = 0.19690529\n",
      "Iteration 122, loss = 0.19629145\n",
      "Iteration 123, loss = 0.19560146\n",
      "Iteration 124, loss = 0.19458297\n",
      "Iteration 125, loss = 0.19342841\n",
      "Iteration 126, loss = 0.19293161\n",
      "Iteration 127, loss = 0.19191957\n",
      "Iteration 128, loss = 0.19140481\n",
      "Iteration 129, loss = 0.19051349\n",
      "Iteration 130, loss = 0.18928498\n",
      "Iteration 131, loss = 0.18841576\n",
      "Iteration 132, loss = 0.18790672\n",
      "Iteration 133, loss = 0.18699665\n",
      "Iteration 134, loss = 0.18638138\n",
      "Iteration 135, loss = 0.18534600\n",
      "Iteration 136, loss = 0.18455742\n",
      "Iteration 137, loss = 0.18384919\n",
      "Iteration 138, loss = 0.18319236\n",
      "Iteration 139, loss = 0.18211584\n",
      "Iteration 140, loss = 0.18165823\n",
      "Iteration 141, loss = 0.18081963\n",
      "Iteration 142, loss = 0.18012052\n",
      "Iteration 143, loss = 0.17943708\n",
      "Iteration 144, loss = 0.17921755\n",
      "Iteration 145, loss = 0.17789166\n",
      "Iteration 146, loss = 0.17724269\n",
      "Iteration 147, loss = 0.17655768\n",
      "Iteration 148, loss = 0.17581194\n",
      "Iteration 149, loss = 0.17494881\n",
      "Iteration 150, loss = 0.17450902\n",
      "Iteration 151, loss = 0.17379553\n",
      "Iteration 152, loss = 0.17335267\n",
      "Iteration 153, loss = 0.17227600\n",
      "Iteration 154, loss = 0.17179301\n",
      "Iteration 155, loss = 0.17126329\n",
      "Iteration 156, loss = 0.17051967\n",
      "Iteration 157, loss = 0.16998011\n",
      "Iteration 158, loss = 0.16935834\n",
      "Iteration 159, loss = 0.16880000\n",
      "Iteration 160, loss = 0.16843452\n",
      "Iteration 161, loss = 0.16771429\n",
      "Iteration 162, loss = 0.16711358\n",
      "Iteration 163, loss = 0.16659880\n",
      "Iteration 164, loss = 0.16589392\n",
      "Iteration 165, loss = 0.16545599\n",
      "Iteration 166, loss = 0.16505911\n",
      "Iteration 167, loss = 0.16434663\n",
      "Iteration 168, loss = 0.16390804\n",
      "Iteration 169, loss = 0.16333120\n",
      "Iteration 170, loss = 0.16259874\n",
      "Iteration 171, loss = 0.16219273\n",
      "Iteration 172, loss = 0.16164993\n",
      "Iteration 173, loss = 0.16121417\n",
      "Iteration 174, loss = 0.16089432\n",
      "Iteration 175, loss = 0.16026016\n",
      "Iteration 176, loss = 0.15967339\n",
      "Iteration 177, loss = 0.15922832\n",
      "Iteration 178, loss = 0.15864253\n",
      "Iteration 179, loss = 0.15825629\n",
      "Iteration 180, loss = 0.15777962\n",
      "Iteration 181, loss = 0.15702579\n",
      "Iteration 182, loss = 0.15652449\n",
      "Iteration 183, loss = 0.15591585\n",
      "Iteration 184, loss = 0.15585268\n",
      "Iteration 185, loss = 0.15509701\n",
      "Iteration 186, loss = 0.15446893\n",
      "Iteration 187, loss = 0.15436188\n",
      "Iteration 188, loss = 0.15365744\n",
      "Iteration 189, loss = 0.15301283\n",
      "Iteration 190, loss = 0.15276534\n",
      "Iteration 191, loss = 0.15231278\n",
      "Iteration 192, loss = 0.15188526\n",
      "Iteration 193, loss = 0.15125947\n",
      "Iteration 194, loss = 0.15064323\n",
      "Iteration 195, loss = 0.15009945\n",
      "Iteration 196, loss = 0.14964391\n",
      "Iteration 197, loss = 0.14946802\n",
      "Iteration 198, loss = 0.14871074\n",
      "Iteration 199, loss = 0.14852044\n",
      "Iteration 200, loss = 0.14783194\n",
      "Iteration 1, loss = 0.55137526\n",
      "Iteration 2, loss = 0.41965986\n",
      "Iteration 3, loss = 0.39008019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.36862674\n",
      "Iteration 5, loss = 0.35100846\n",
      "Iteration 6, loss = 0.33580957\n",
      "Iteration 7, loss = 0.32291240\n",
      "Iteration 8, loss = 0.31208968\n",
      "Iteration 9, loss = 0.30315491\n",
      "Iteration 10, loss = 0.29570469\n",
      "Iteration 11, loss = 0.28960961\n",
      "Iteration 12, loss = 0.28465027\n",
      "Iteration 13, loss = 0.28017703\n",
      "Iteration 14, loss = 0.27654149\n",
      "Iteration 15, loss = 0.27313933\n",
      "Iteration 16, loss = 0.27017541\n",
      "Iteration 17, loss = 0.26789826\n",
      "Iteration 18, loss = 0.26556628\n",
      "Iteration 19, loss = 0.26345731\n",
      "Iteration 20, loss = 0.26142517\n",
      "Iteration 21, loss = 0.25963464\n",
      "Iteration 22, loss = 0.25792653\n",
      "Iteration 23, loss = 0.25653937\n",
      "Iteration 24, loss = 0.25483214\n",
      "Iteration 25, loss = 0.25321645\n",
      "Iteration 26, loss = 0.25178726\n",
      "Iteration 27, loss = 0.25079232\n",
      "Iteration 28, loss = 0.24919703\n",
      "Iteration 29, loss = 0.24786410\n",
      "Iteration 30, loss = 0.24663338\n",
      "Iteration 31, loss = 0.24529943\n",
      "Iteration 32, loss = 0.24408110\n",
      "Iteration 33, loss = 0.24294235\n",
      "Iteration 34, loss = 0.24201055\n",
      "Iteration 35, loss = 0.24091385\n",
      "Iteration 36, loss = 0.24001486\n",
      "Iteration 37, loss = 0.23904919\n",
      "Iteration 38, loss = 0.23814991\n",
      "Iteration 39, loss = 0.23724966\n",
      "Iteration 40, loss = 0.23678873\n",
      "Iteration 41, loss = 0.23595956\n",
      "Iteration 42, loss = 0.23516051\n",
      "Iteration 43, loss = 0.23443686\n",
      "Iteration 44, loss = 0.23378253\n",
      "Iteration 45, loss = 0.23334830\n",
      "Iteration 46, loss = 0.23268676\n",
      "Iteration 47, loss = 0.23241103\n",
      "Iteration 48, loss = 0.23183444\n",
      "Iteration 49, loss = 0.23149434\n",
      "Iteration 50, loss = 0.23120359\n",
      "Iteration 51, loss = 0.23081704\n",
      "Iteration 52, loss = 0.23050359\n",
      "Iteration 53, loss = 0.22985607\n",
      "Iteration 54, loss = 0.22966651\n",
      "Iteration 55, loss = 0.22920583\n",
      "Iteration 56, loss = 0.22927431\n",
      "Iteration 57, loss = 0.22876022\n",
      "Iteration 58, loss = 0.22847329\n",
      "Iteration 59, loss = 0.22814061\n",
      "Iteration 60, loss = 0.22790647\n",
      "Iteration 61, loss = 0.22764711\n",
      "Iteration 62, loss = 0.22733234\n",
      "Iteration 63, loss = 0.22741836\n",
      "Iteration 64, loss = 0.22671707\n",
      "Iteration 65, loss = 0.22660007\n",
      "Iteration 66, loss = 0.22632051\n",
      "Iteration 67, loss = 0.22614992\n",
      "Iteration 68, loss = 0.22587984\n",
      "Iteration 69, loss = 0.22578079\n",
      "Iteration 70, loss = 0.22538562\n",
      "Iteration 71, loss = 0.22550644\n",
      "Iteration 72, loss = 0.22474453\n",
      "Iteration 73, loss = 0.22426541\n",
      "Iteration 74, loss = 0.22382187\n",
      "Iteration 75, loss = 0.22408040\n",
      "Iteration 76, loss = 0.22314294\n",
      "Iteration 77, loss = 0.22288657\n",
      "Iteration 78, loss = 0.22272298\n",
      "Iteration 79, loss = 0.22253714\n",
      "Iteration 80, loss = 0.22169678\n",
      "Iteration 81, loss = 0.22135898\n",
      "Iteration 82, loss = 0.22091478\n",
      "Iteration 83, loss = 0.22058153\n",
      "Iteration 84, loss = 0.22027336\n",
      "Iteration 85, loss = 0.21972278\n",
      "Iteration 86, loss = 0.21937703\n",
      "Iteration 87, loss = 0.21921892\n",
      "Iteration 88, loss = 0.21822011\n",
      "Iteration 89, loss = 0.21800363\n",
      "Iteration 90, loss = 0.21744455\n",
      "Iteration 91, loss = 0.21684151\n",
      "Iteration 92, loss = 0.21618543\n",
      "Iteration 93, loss = 0.21582283\n",
      "Iteration 94, loss = 0.21534872\n",
      "Iteration 95, loss = 0.21450411\n",
      "Iteration 96, loss = 0.21402356\n",
      "Iteration 97, loss = 0.21340765\n",
      "Iteration 98, loss = 0.21310514\n",
      "Iteration 99, loss = 0.21260081\n",
      "Iteration 100, loss = 0.21206837\n",
      "Iteration 101, loss = 0.21115132\n",
      "Iteration 102, loss = 0.21045150\n",
      "Iteration 103, loss = 0.20992272\n",
      "Iteration 104, loss = 0.20929490\n",
      "Iteration 105, loss = 0.20904163\n",
      "Iteration 106, loss = 0.20806654\n",
      "Iteration 107, loss = 0.20751010\n",
      "Iteration 108, loss = 0.20667475\n",
      "Iteration 109, loss = 0.20643343\n",
      "Iteration 110, loss = 0.20509775\n",
      "Iteration 111, loss = 0.20456929\n",
      "Iteration 112, loss = 0.20371259\n",
      "Iteration 113, loss = 0.20319035\n",
      "Iteration 114, loss = 0.20244558\n",
      "Iteration 115, loss = 0.20183903\n",
      "Iteration 116, loss = 0.20130873\n",
      "Iteration 117, loss = 0.20039648\n",
      "Iteration 118, loss = 0.19971947\n",
      "Iteration 119, loss = 0.19897283\n",
      "Iteration 120, loss = 0.19832604\n",
      "Iteration 121, loss = 0.19747146\n",
      "Iteration 122, loss = 0.19704773\n",
      "Iteration 123, loss = 0.19624440\n",
      "Iteration 124, loss = 0.19547206\n",
      "Iteration 125, loss = 0.19450128\n",
      "Iteration 126, loss = 0.19393853\n",
      "Iteration 127, loss = 0.19328796\n",
      "Iteration 128, loss = 0.19249131\n",
      "Iteration 129, loss = 0.19191479\n",
      "Iteration 130, loss = 0.19078690\n",
      "Iteration 131, loss = 0.19025214\n",
      "Iteration 132, loss = 0.18966730\n",
      "Iteration 133, loss = 0.18866792\n",
      "Iteration 134, loss = 0.18837251\n",
      "Iteration 135, loss = 0.18736361\n",
      "Iteration 136, loss = 0.18660904\n",
      "Iteration 137, loss = 0.18606947\n",
      "Iteration 138, loss = 0.18547083\n",
      "Iteration 139, loss = 0.18427303\n",
      "Iteration 140, loss = 0.18379680\n",
      "Iteration 141, loss = 0.18325582\n",
      "Iteration 142, loss = 0.18276262\n",
      "Iteration 143, loss = 0.18184120\n",
      "Iteration 144, loss = 0.18162059\n",
      "Iteration 145, loss = 0.18034952\n",
      "Iteration 146, loss = 0.17946560\n",
      "Iteration 147, loss = 0.17892317\n",
      "Iteration 148, loss = 0.17841832\n",
      "Iteration 149, loss = 0.17772216\n",
      "Iteration 150, loss = 0.17689657\n",
      "Iteration 151, loss = 0.17614528\n",
      "Iteration 152, loss = 0.17565153\n",
      "Iteration 153, loss = 0.17452300\n",
      "Iteration 154, loss = 0.17390551\n",
      "Iteration 155, loss = 0.17330255\n",
      "Iteration 156, loss = 0.17264885\n",
      "Iteration 157, loss = 0.17212094\n",
      "Iteration 158, loss = 0.17149593\n",
      "Iteration 159, loss = 0.17092072\n",
      "Iteration 160, loss = 0.17013416\n",
      "Iteration 161, loss = 0.16967255\n",
      "Iteration 162, loss = 0.16867765\n",
      "Iteration 163, loss = 0.16820276\n",
      "Iteration 164, loss = 0.16786541\n",
      "Iteration 165, loss = 0.16728859\n",
      "Iteration 166, loss = 0.16675112\n",
      "Iteration 167, loss = 0.16613481\n",
      "Iteration 168, loss = 0.16533911\n",
      "Iteration 169, loss = 0.16506331\n",
      "Iteration 170, loss = 0.16392275\n",
      "Iteration 171, loss = 0.16341280\n",
      "Iteration 172, loss = 0.16276056\n",
      "Iteration 173, loss = 0.16220798\n",
      "Iteration 174, loss = 0.16155620\n",
      "Iteration 175, loss = 0.16110930\n",
      "Iteration 176, loss = 0.16043212\n",
      "Iteration 177, loss = 0.15967115\n",
      "Iteration 178, loss = 0.15910973\n",
      "Iteration 179, loss = 0.15876431\n",
      "Iteration 180, loss = 0.15823004\n",
      "Iteration 181, loss = 0.15726817\n",
      "Iteration 182, loss = 0.15672976\n",
      "Iteration 183, loss = 0.15614458\n",
      "Iteration 184, loss = 0.15594917\n",
      "Iteration 185, loss = 0.15513243\n",
      "Iteration 186, loss = 0.15441321\n",
      "Iteration 187, loss = 0.15426292\n",
      "Iteration 188, loss = 0.15379372\n",
      "Iteration 189, loss = 0.15320788\n",
      "Iteration 190, loss = 0.15226658\n",
      "Iteration 191, loss = 0.15186290\n",
      "Iteration 192, loss = 0.15153785\n",
      "Iteration 193, loss = 0.15095795\n",
      "Iteration 194, loss = 0.15025244\n",
      "Iteration 195, loss = 0.14951901\n",
      "Iteration 196, loss = 0.14907160\n",
      "Iteration 197, loss = 0.14867427\n",
      "Iteration 198, loss = 0.14778458\n",
      "Iteration 199, loss = 0.14734752\n",
      "Iteration 200, loss = 0.14678662\n",
      "Iteration 1, loss = 0.55040160\n",
      "Iteration 2, loss = 0.41954209\n",
      "Iteration 3, loss = 0.39029232\n",
      "Iteration 4, loss = 0.36913277\n",
      "Iteration 5, loss = 0.35106183\n",
      "Iteration 6, loss = 0.33571742\n",
      "Iteration 7, loss = 0.32264345\n",
      "Iteration 8, loss = 0.31151933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.30213239\n",
      "Iteration 10, loss = 0.29433675\n",
      "Iteration 11, loss = 0.28778481\n",
      "Iteration 12, loss = 0.28233693\n",
      "Iteration 13, loss = 0.27764365\n",
      "Iteration 14, loss = 0.27379739\n",
      "Iteration 15, loss = 0.26991901\n",
      "Iteration 16, loss = 0.26687784\n",
      "Iteration 17, loss = 0.26426078\n",
      "Iteration 18, loss = 0.26190141\n",
      "Iteration 19, loss = 0.25962050\n",
      "Iteration 20, loss = 0.25739215\n",
      "Iteration 21, loss = 0.25549291\n",
      "Iteration 22, loss = 0.25361480\n",
      "Iteration 23, loss = 0.25220264\n",
      "Iteration 24, loss = 0.25035306\n",
      "Iteration 25, loss = 0.24874222\n",
      "Iteration 26, loss = 0.24738519\n",
      "Iteration 27, loss = 0.24639560\n",
      "Iteration 28, loss = 0.24454367\n",
      "Iteration 29, loss = 0.24340117\n",
      "Iteration 30, loss = 0.24231763\n",
      "Iteration 31, loss = 0.24097651\n",
      "Iteration 32, loss = 0.23958749\n",
      "Iteration 33, loss = 0.23868620\n",
      "Iteration 34, loss = 0.23754205\n",
      "Iteration 35, loss = 0.23672907\n",
      "Iteration 36, loss = 0.23557238\n",
      "Iteration 37, loss = 0.23473716\n",
      "Iteration 38, loss = 0.23399812\n",
      "Iteration 39, loss = 0.23332906\n",
      "Iteration 40, loss = 0.23277320\n",
      "Iteration 41, loss = 0.23171210\n",
      "Iteration 42, loss = 0.23113978\n",
      "Iteration 43, loss = 0.23052316\n",
      "Iteration 44, loss = 0.23002472\n",
      "Iteration 45, loss = 0.22949685\n",
      "Iteration 46, loss = 0.22896487\n",
      "Iteration 47, loss = 0.22852011\n",
      "Iteration 48, loss = 0.22827067\n",
      "Iteration 49, loss = 0.22781524\n",
      "Iteration 50, loss = 0.22744066\n",
      "Iteration 51, loss = 0.22723159\n",
      "Iteration 52, loss = 0.22684393\n",
      "Iteration 53, loss = 0.22641157\n",
      "Iteration 54, loss = 0.22611634\n",
      "Iteration 55, loss = 0.22574184\n",
      "Iteration 56, loss = 0.22565356\n",
      "Iteration 57, loss = 0.22517374\n",
      "Iteration 58, loss = 0.22519109\n",
      "Iteration 59, loss = 0.22474147\n",
      "Iteration 60, loss = 0.22465778\n",
      "Iteration 61, loss = 0.22450391\n",
      "Iteration 62, loss = 0.22412894\n",
      "Iteration 63, loss = 0.22388009\n",
      "Iteration 64, loss = 0.22369148\n",
      "Iteration 65, loss = 0.22363321\n",
      "Iteration 66, loss = 0.22347241\n",
      "Iteration 67, loss = 0.22295916\n",
      "Iteration 68, loss = 0.22277474\n",
      "Iteration 69, loss = 0.22246723\n",
      "Iteration 70, loss = 0.22250611\n",
      "Iteration 71, loss = 0.22210935\n",
      "Iteration 72, loss = 0.22164194\n",
      "Iteration 73, loss = 0.22141953\n",
      "Iteration 74, loss = 0.22092919\n",
      "Iteration 75, loss = 0.22097693\n",
      "Iteration 76, loss = 0.22041817\n",
      "Iteration 77, loss = 0.22017435\n",
      "Iteration 78, loss = 0.21985499\n",
      "Iteration 79, loss = 0.21940558\n",
      "Iteration 80, loss = 0.21916060\n",
      "Iteration 81, loss = 0.21883215\n",
      "Iteration 82, loss = 0.21836823\n",
      "Iteration 83, loss = 0.21767269\n",
      "Iteration 84, loss = 0.21763307\n",
      "Iteration 85, loss = 0.21698022\n",
      "Iteration 86, loss = 0.21686621\n",
      "Iteration 87, loss = 0.21619336\n",
      "Iteration 88, loss = 0.21570919\n",
      "Iteration 89, loss = 0.21557289\n",
      "Iteration 90, loss = 0.21449036\n",
      "Iteration 91, loss = 0.21426350\n",
      "Iteration 92, loss = 0.21393296\n",
      "Iteration 93, loss = 0.21319166\n",
      "Iteration 94, loss = 0.21323255\n",
      "Iteration 95, loss = 0.21231200\n",
      "Iteration 96, loss = 0.21161791\n",
      "Iteration 97, loss = 0.21094343\n",
      "Iteration 98, loss = 0.21036927\n",
      "Iteration 99, loss = 0.20989234\n",
      "Iteration 100, loss = 0.20939900\n",
      "Iteration 101, loss = 0.20864375\n",
      "Iteration 102, loss = 0.20786207\n",
      "Iteration 103, loss = 0.20751474\n",
      "Iteration 104, loss = 0.20692315\n",
      "Iteration 105, loss = 0.20621662\n",
      "Iteration 106, loss = 0.20521944\n",
      "Iteration 107, loss = 0.20484666\n",
      "Iteration 108, loss = 0.20428383\n",
      "Iteration 109, loss = 0.20336293\n",
      "Iteration 110, loss = 0.20251862\n",
      "Iteration 111, loss = 0.20199098\n",
      "Iteration 112, loss = 0.20105489\n",
      "Iteration 113, loss = 0.20009550\n",
      "Iteration 114, loss = 0.19944753\n",
      "Iteration 115, loss = 0.19897202\n",
      "Iteration 116, loss = 0.19833252\n",
      "Iteration 117, loss = 0.19724549\n",
      "Iteration 118, loss = 0.19660400\n",
      "Iteration 119, loss = 0.19591386\n",
      "Iteration 120, loss = 0.19492697\n",
      "Iteration 121, loss = 0.19443863\n",
      "Iteration 122, loss = 0.19344010\n",
      "Iteration 123, loss = 0.19284099\n",
      "Iteration 124, loss = 0.19195823\n",
      "Iteration 125, loss = 0.19104521\n",
      "Iteration 126, loss = 0.19056683\n",
      "Iteration 127, loss = 0.18976113\n",
      "Iteration 128, loss = 0.18885234\n",
      "Iteration 129, loss = 0.18812668\n",
      "Iteration 130, loss = 0.18723756\n",
      "Iteration 131, loss = 0.18682605\n",
      "Iteration 132, loss = 0.18604181\n",
      "Iteration 133, loss = 0.18525459\n",
      "Iteration 134, loss = 0.18460775\n",
      "Iteration 135, loss = 0.18360810\n",
      "Iteration 136, loss = 0.18294045\n",
      "Iteration 137, loss = 0.18177784\n",
      "Iteration 138, loss = 0.18150216\n",
      "Iteration 139, loss = 0.18044413\n",
      "Iteration 140, loss = 0.17990787\n",
      "Iteration 141, loss = 0.17938400\n",
      "Iteration 142, loss = 0.17854008\n",
      "Iteration 143, loss = 0.17792414\n",
      "Iteration 144, loss = 0.17753299\n",
      "Iteration 145, loss = 0.17666468\n",
      "Iteration 146, loss = 0.17585906\n",
      "Iteration 147, loss = 0.17488142\n",
      "Iteration 148, loss = 0.17441404\n",
      "Iteration 149, loss = 0.17394273\n",
      "Iteration 150, loss = 0.17314392\n",
      "Iteration 151, loss = 0.17236459\n",
      "Iteration 152, loss = 0.17195016\n",
      "Iteration 153, loss = 0.17114186\n",
      "Iteration 154, loss = 0.17035894\n",
      "Iteration 155, loss = 0.16984019\n",
      "Iteration 156, loss = 0.16912155\n",
      "Iteration 157, loss = 0.16875475\n",
      "Iteration 158, loss = 0.16822223\n",
      "Iteration 159, loss = 0.16760181\n",
      "Iteration 160, loss = 0.16683349\n",
      "Iteration 161, loss = 0.16637853\n",
      "Iteration 162, loss = 0.16573128\n",
      "Iteration 163, loss = 0.16528688\n",
      "Iteration 164, loss = 0.16506540\n",
      "Iteration 165, loss = 0.16433150\n",
      "Iteration 166, loss = 0.16396368\n",
      "Iteration 167, loss = 0.16364735\n",
      "Iteration 168, loss = 0.16265403\n",
      "Iteration 169, loss = 0.16246677\n",
      "Iteration 170, loss = 0.16174299\n",
      "Iteration 171, loss = 0.16099178\n",
      "Iteration 172, loss = 0.16076334\n",
      "Iteration 173, loss = 0.16020667\n",
      "Iteration 174, loss = 0.15942837\n",
      "Iteration 175, loss = 0.15967345\n",
      "Iteration 176, loss = 0.15876462\n",
      "Iteration 177, loss = 0.15793694\n",
      "Iteration 178, loss = 0.15763623\n",
      "Iteration 179, loss = 0.15735835\n",
      "Iteration 180, loss = 0.15695925\n",
      "Iteration 181, loss = 0.15609810\n",
      "Iteration 182, loss = 0.15541634\n",
      "Iteration 183, loss = 0.15520724\n",
      "Iteration 184, loss = 0.15485317\n",
      "Iteration 185, loss = 0.15435996\n",
      "Iteration 186, loss = 0.15371732\n",
      "Iteration 187, loss = 0.15339918\n",
      "Iteration 188, loss = 0.15315991\n",
      "Iteration 189, loss = 0.15291400\n",
      "Iteration 190, loss = 0.15190872\n",
      "Iteration 191, loss = 0.15146381\n",
      "Iteration 192, loss = 0.15114918\n",
      "Iteration 193, loss = 0.15078414\n",
      "Iteration 194, loss = 0.15018301\n",
      "Iteration 195, loss = 0.14953585\n",
      "Iteration 196, loss = 0.14921587\n",
      "Iteration 197, loss = 0.14873231\n",
      "Iteration 198, loss = 0.14802694\n",
      "Iteration 199, loss = 0.14781182\n",
      "Iteration 200, loss = 0.14756995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58523567\n",
      "Iteration 2, loss = 0.46127749\n",
      "Iteration 3, loss = 0.45171443\n",
      "Iteration 4, loss = 0.45039341\n",
      "Iteration 5, loss = 0.44921194\n",
      "Iteration 6, loss = 0.44808802\n",
      "Iteration 7, loss = 0.44691545\n",
      "Iteration 8, loss = 0.44579091\n",
      "Iteration 9, loss = 0.44467065\n",
      "Iteration 10, loss = 0.44356352\n",
      "Iteration 11, loss = 0.44239826\n",
      "Iteration 12, loss = 0.44136226\n",
      "Iteration 13, loss = 0.44018194\n",
      "Iteration 14, loss = 0.43913207\n",
      "Iteration 15, loss = 0.43800279\n",
      "Iteration 16, loss = 0.43690682\n",
      "Iteration 17, loss = 0.43583948\n",
      "Iteration 18, loss = 0.43470876\n",
      "Iteration 19, loss = 0.43370254\n",
      "Iteration 20, loss = 0.43253598\n",
      "Iteration 21, loss = 0.43144047\n",
      "Iteration 22, loss = 0.43036923\n",
      "Iteration 23, loss = 0.42924995\n",
      "Iteration 24, loss = 0.42814489\n",
      "Iteration 25, loss = 0.42710785\n",
      "Iteration 26, loss = 0.42605620\n",
      "Iteration 27, loss = 0.42486354\n",
      "Iteration 28, loss = 0.42374932\n",
      "Iteration 29, loss = 0.42262193\n",
      "Iteration 30, loss = 0.42153926\n",
      "Iteration 31, loss = 0.42036607\n",
      "Iteration 32, loss = 0.41926031\n",
      "Iteration 33, loss = 0.41821412\n",
      "Iteration 34, loss = 0.41698299\n",
      "Iteration 35, loss = 0.41585315\n",
      "Iteration 36, loss = 0.41469930\n",
      "Iteration 37, loss = 0.41353175\n",
      "Iteration 38, loss = 0.41238878\n",
      "Iteration 39, loss = 0.41125539\n",
      "Iteration 40, loss = 0.41007222\n",
      "Iteration 41, loss = 0.40888429\n",
      "Iteration 42, loss = 0.40769905\n",
      "Iteration 43, loss = 0.40649301\n",
      "Iteration 44, loss = 0.40527762\n",
      "Iteration 45, loss = 0.40408024\n",
      "Iteration 46, loss = 0.40287599\n",
      "Iteration 47, loss = 0.40164690\n",
      "Iteration 48, loss = 0.40042757\n",
      "Iteration 49, loss = 0.39924697\n",
      "Iteration 50, loss = 0.39797053\n",
      "Iteration 51, loss = 0.39671916\n",
      "Iteration 52, loss = 0.39546519\n",
      "Iteration 53, loss = 0.39422463\n",
      "Iteration 54, loss = 0.39293552\n",
      "Iteration 55, loss = 0.39169790\n",
      "Iteration 56, loss = 0.39040532\n",
      "Iteration 57, loss = 0.38918183\n",
      "Iteration 58, loss = 0.38785404\n",
      "Iteration 59, loss = 0.38658292\n",
      "Iteration 60, loss = 0.38528676\n",
      "Iteration 61, loss = 0.38408391\n",
      "Iteration 62, loss = 0.38272369\n",
      "Iteration 63, loss = 0.38142659\n",
      "Iteration 64, loss = 0.38008638\n",
      "Iteration 65, loss = 0.37879923\n",
      "Iteration 66, loss = 0.37750820\n",
      "Iteration 67, loss = 0.37618865\n",
      "Iteration 68, loss = 0.37489898\n",
      "Iteration 69, loss = 0.37355466\n",
      "Iteration 70, loss = 0.37225574\n",
      "Iteration 71, loss = 0.37094772\n",
      "Iteration 72, loss = 0.36963796\n",
      "Iteration 73, loss = 0.36831064\n",
      "Iteration 74, loss = 0.36697732\n",
      "Iteration 75, loss = 0.36570840\n",
      "Iteration 76, loss = 0.36441023\n",
      "Iteration 77, loss = 0.36309920\n",
      "Iteration 78, loss = 0.36179007\n",
      "Iteration 79, loss = 0.36045224\n",
      "Iteration 80, loss = 0.35914495\n",
      "Iteration 81, loss = 0.35788442\n",
      "Iteration 82, loss = 0.35657539\n",
      "Iteration 83, loss = 0.35528684\n",
      "Iteration 84, loss = 0.35403091\n",
      "Iteration 85, loss = 0.35276396\n",
      "Iteration 86, loss = 0.35146993\n",
      "Iteration 87, loss = 0.35026181\n",
      "Iteration 88, loss = 0.34898946\n",
      "Iteration 89, loss = 0.34773410\n",
      "Iteration 90, loss = 0.34653914\n",
      "Iteration 91, loss = 0.34528129\n",
      "Iteration 92, loss = 0.34404481\n",
      "Iteration 93, loss = 0.34284154\n",
      "Iteration 94, loss = 0.34164137\n",
      "Iteration 95, loss = 0.34043447\n",
      "Iteration 96, loss = 0.33930397\n",
      "Iteration 97, loss = 0.33811856\n",
      "Iteration 98, loss = 0.33694176\n",
      "Iteration 99, loss = 0.33581961\n",
      "Iteration 100, loss = 0.33465419\n",
      "Iteration 101, loss = 0.33354724\n",
      "Iteration 102, loss = 0.33244906\n",
      "Iteration 103, loss = 0.33127689\n",
      "Iteration 104, loss = 0.33020147\n",
      "Iteration 105, loss = 0.32914944\n",
      "Iteration 106, loss = 0.32814364\n",
      "Iteration 107, loss = 0.32700393\n",
      "Iteration 108, loss = 0.32599195\n",
      "Iteration 109, loss = 0.32490198\n",
      "Iteration 110, loss = 0.32390211\n",
      "Iteration 111, loss = 0.32289394\n",
      "Iteration 112, loss = 0.32189602\n",
      "Iteration 113, loss = 0.32095062\n",
      "Iteration 114, loss = 0.31993990\n",
      "Iteration 115, loss = 0.31901927\n",
      "Iteration 116, loss = 0.31801185\n",
      "Iteration 117, loss = 0.31711538\n",
      "Iteration 118, loss = 0.31617538\n",
      "Iteration 119, loss = 0.31525338\n",
      "Iteration 120, loss = 0.31438431\n",
      "Iteration 121, loss = 0.31349316\n",
      "Iteration 122, loss = 0.31263384\n",
      "Iteration 123, loss = 0.31177208\n",
      "Iteration 124, loss = 0.31092499\n",
      "Iteration 125, loss = 0.31007785\n",
      "Iteration 126, loss = 0.30927496\n",
      "Iteration 127, loss = 0.30847511\n",
      "Iteration 128, loss = 0.30766696\n",
      "Iteration 129, loss = 0.30689726\n",
      "Iteration 130, loss = 0.30611822\n",
      "Iteration 131, loss = 0.30536258\n",
      "Iteration 132, loss = 0.30460362\n",
      "Iteration 133, loss = 0.30387028\n",
      "Iteration 134, loss = 0.30316985\n",
      "Iteration 135, loss = 0.30243652\n",
      "Iteration 136, loss = 0.30173453\n",
      "Iteration 137, loss = 0.30104661\n",
      "Iteration 138, loss = 0.30036478\n",
      "Iteration 139, loss = 0.29972339\n",
      "Iteration 140, loss = 0.29904886\n",
      "Iteration 141, loss = 0.29838256\n",
      "Iteration 142, loss = 0.29779701\n",
      "Iteration 143, loss = 0.29711176\n",
      "Iteration 144, loss = 0.29651466\n",
      "Iteration 145, loss = 0.29589839\n",
      "Iteration 146, loss = 0.29533467\n",
      "Iteration 147, loss = 0.29469459\n",
      "Iteration 148, loss = 0.29414986\n",
      "Iteration 149, loss = 0.29358031\n",
      "Iteration 150, loss = 0.29300579\n",
      "Iteration 151, loss = 0.29249112\n",
      "Iteration 152, loss = 0.29191251\n",
      "Iteration 153, loss = 0.29141435\n",
      "Iteration 154, loss = 0.29087847\n",
      "Iteration 155, loss = 0.29033963\n",
      "Iteration 156, loss = 0.28983220\n",
      "Iteration 157, loss = 0.28933658\n",
      "Iteration 158, loss = 0.28885412\n",
      "Iteration 159, loss = 0.28836298\n",
      "Iteration 160, loss = 0.28788620\n",
      "Iteration 161, loss = 0.28740293\n",
      "Iteration 162, loss = 0.28695747\n",
      "Iteration 163, loss = 0.28648754\n",
      "Iteration 164, loss = 0.28605281\n",
      "Iteration 165, loss = 0.28561541\n",
      "Iteration 166, loss = 0.28518484\n",
      "Iteration 167, loss = 0.28477451\n",
      "Iteration 168, loss = 0.28435612\n",
      "Iteration 169, loss = 0.28391333\n",
      "Iteration 170, loss = 0.28352687\n",
      "Iteration 171, loss = 0.28312678\n",
      "Iteration 172, loss = 0.28273119\n",
      "Iteration 173, loss = 0.28234466\n",
      "Iteration 174, loss = 0.28194590\n",
      "Iteration 175, loss = 0.28163518\n",
      "Iteration 176, loss = 0.28123744\n",
      "Iteration 177, loss = 0.28083365\n",
      "Iteration 178, loss = 0.28047988\n",
      "Iteration 179, loss = 0.28013856\n",
      "Iteration 180, loss = 0.27978168\n",
      "Iteration 181, loss = 0.27944022\n",
      "Iteration 182, loss = 0.27911644\n",
      "Iteration 183, loss = 0.27878295\n",
      "Iteration 184, loss = 0.27846793\n",
      "Iteration 185, loss = 0.27812769\n",
      "Iteration 186, loss = 0.27781170\n",
      "Iteration 187, loss = 0.27749268\n",
      "Iteration 188, loss = 0.27718242\n",
      "Iteration 189, loss = 0.27689719\n",
      "Iteration 190, loss = 0.27656812\n",
      "Iteration 191, loss = 0.27627326\n",
      "Iteration 192, loss = 0.27602869\n",
      "Iteration 193, loss = 0.27571642\n",
      "Iteration 194, loss = 0.27542496\n",
      "Iteration 195, loss = 0.27514717\n",
      "Iteration 196, loss = 0.27486522\n",
      "Iteration 197, loss = 0.27460450\n",
      "Iteration 198, loss = 0.27433277\n",
      "Iteration 199, loss = 0.27408510\n",
      "Iteration 200, loss = 0.27383249\n",
      "Iteration 1, loss = 0.58528018\n",
      "Iteration 2, loss = 0.46121313\n",
      "Iteration 3, loss = 0.45181791\n",
      "Iteration 4, loss = 0.45054511\n",
      "Iteration 5, loss = 0.44930256\n",
      "Iteration 6, loss = 0.44812612\n",
      "Iteration 7, loss = 0.44693814\n",
      "Iteration 8, loss = 0.44576260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.44468032\n",
      "Iteration 10, loss = 0.44358438\n",
      "Iteration 11, loss = 0.44230392\n",
      "Iteration 12, loss = 0.44126542\n",
      "Iteration 13, loss = 0.44005206\n",
      "Iteration 14, loss = 0.43893150\n",
      "Iteration 15, loss = 0.43778028\n",
      "Iteration 16, loss = 0.43667196\n",
      "Iteration 17, loss = 0.43554194\n",
      "Iteration 18, loss = 0.43441159\n",
      "Iteration 19, loss = 0.43330551\n",
      "Iteration 20, loss = 0.43216524\n",
      "Iteration 21, loss = 0.43106885\n",
      "Iteration 22, loss = 0.42991431\n",
      "Iteration 23, loss = 0.42879246\n",
      "Iteration 24, loss = 0.42766837\n",
      "Iteration 25, loss = 0.42657403\n",
      "Iteration 26, loss = 0.42545289\n",
      "Iteration 27, loss = 0.42430674\n",
      "Iteration 28, loss = 0.42310641\n",
      "Iteration 29, loss = 0.42198039\n",
      "Iteration 30, loss = 0.42084246\n",
      "Iteration 31, loss = 0.41966953\n",
      "Iteration 32, loss = 0.41853179\n",
      "Iteration 33, loss = 0.41741613\n",
      "Iteration 34, loss = 0.41616304\n",
      "Iteration 35, loss = 0.41499362\n",
      "Iteration 36, loss = 0.41381855\n",
      "Iteration 37, loss = 0.41262589\n",
      "Iteration 38, loss = 0.41145635\n",
      "Iteration 39, loss = 0.41026829\n",
      "Iteration 40, loss = 0.40905480\n",
      "Iteration 41, loss = 0.40784199\n",
      "Iteration 42, loss = 0.40669304\n",
      "Iteration 43, loss = 0.40537323\n",
      "Iteration 44, loss = 0.40421745\n",
      "Iteration 45, loss = 0.40292864\n",
      "Iteration 46, loss = 0.40163273\n",
      "Iteration 47, loss = 0.40039313\n",
      "Iteration 48, loss = 0.39911058\n",
      "Iteration 49, loss = 0.39786357\n",
      "Iteration 50, loss = 0.39660603\n",
      "Iteration 51, loss = 0.39531825\n",
      "Iteration 52, loss = 0.39400997\n",
      "Iteration 53, loss = 0.39270536\n",
      "Iteration 54, loss = 0.39140164\n",
      "Iteration 55, loss = 0.39013260\n",
      "Iteration 56, loss = 0.38878655\n",
      "Iteration 57, loss = 0.38756445\n",
      "Iteration 58, loss = 0.38621852\n",
      "Iteration 59, loss = 0.38485780\n",
      "Iteration 60, loss = 0.38351566\n",
      "Iteration 61, loss = 0.38222809\n",
      "Iteration 62, loss = 0.38086426\n",
      "Iteration 63, loss = 0.37954500\n",
      "Iteration 64, loss = 0.37817497\n",
      "Iteration 65, loss = 0.37686078\n",
      "Iteration 66, loss = 0.37547809\n",
      "Iteration 67, loss = 0.37416341\n",
      "Iteration 68, loss = 0.37284453\n",
      "Iteration 69, loss = 0.37145531\n",
      "Iteration 70, loss = 0.37012418\n",
      "Iteration 71, loss = 0.36879155\n",
      "Iteration 72, loss = 0.36742315\n",
      "Iteration 73, loss = 0.36611222\n",
      "Iteration 74, loss = 0.36476132\n",
      "Iteration 75, loss = 0.36340803\n",
      "Iteration 76, loss = 0.36206247\n",
      "Iteration 77, loss = 0.36074968\n",
      "Iteration 78, loss = 0.35939847\n",
      "Iteration 79, loss = 0.35803921\n",
      "Iteration 80, loss = 0.35672322\n",
      "Iteration 81, loss = 0.35540523\n",
      "Iteration 82, loss = 0.35411306\n",
      "Iteration 83, loss = 0.35277351\n",
      "Iteration 84, loss = 0.35148658\n",
      "Iteration 85, loss = 0.35020241\n",
      "Iteration 86, loss = 0.34889507\n",
      "Iteration 87, loss = 0.34768988\n",
      "Iteration 88, loss = 0.34634943\n",
      "Iteration 89, loss = 0.34508117\n",
      "Iteration 90, loss = 0.34385524\n",
      "Iteration 91, loss = 0.34258685\n",
      "Iteration 92, loss = 0.34134205\n",
      "Iteration 93, loss = 0.34012265\n",
      "Iteration 94, loss = 0.33889733\n",
      "Iteration 95, loss = 0.33768486\n",
      "Iteration 96, loss = 0.33653379\n",
      "Iteration 97, loss = 0.33535721\n",
      "Iteration 98, loss = 0.33415864\n",
      "Iteration 99, loss = 0.33303801\n",
      "Iteration 100, loss = 0.33182996\n",
      "Iteration 101, loss = 0.33070741\n",
      "Iteration 102, loss = 0.32958560\n",
      "Iteration 103, loss = 0.32846636\n",
      "Iteration 104, loss = 0.32736029\n",
      "Iteration 105, loss = 0.32630694\n",
      "Iteration 106, loss = 0.32527494\n",
      "Iteration 107, loss = 0.32414344\n",
      "Iteration 108, loss = 0.32310544\n",
      "Iteration 109, loss = 0.32205062\n",
      "Iteration 110, loss = 0.32104106\n",
      "Iteration 111, loss = 0.32003404\n",
      "Iteration 112, loss = 0.31902127\n",
      "Iteration 113, loss = 0.31808106\n",
      "Iteration 114, loss = 0.31709622\n",
      "Iteration 115, loss = 0.31611056\n",
      "Iteration 116, loss = 0.31517248\n",
      "Iteration 117, loss = 0.31421375\n",
      "Iteration 118, loss = 0.31330457\n",
      "Iteration 119, loss = 0.31238494\n",
      "Iteration 120, loss = 0.31151489\n",
      "Iteration 121, loss = 0.31061424\n",
      "Iteration 122, loss = 0.30975120\n",
      "Iteration 123, loss = 0.30890705\n",
      "Iteration 124, loss = 0.30805265\n",
      "Iteration 125, loss = 0.30724091\n",
      "Iteration 126, loss = 0.30645003\n",
      "Iteration 127, loss = 0.30561482\n",
      "Iteration 128, loss = 0.30482700\n",
      "Iteration 129, loss = 0.30406181\n",
      "Iteration 130, loss = 0.30327443\n",
      "Iteration 131, loss = 0.30252613\n",
      "Iteration 132, loss = 0.30178006\n",
      "Iteration 133, loss = 0.30106125\n",
      "Iteration 134, loss = 0.30033940\n",
      "Iteration 135, loss = 0.29962742\n",
      "Iteration 136, loss = 0.29892084\n",
      "Iteration 137, loss = 0.29823971\n",
      "Iteration 138, loss = 0.29760999\n",
      "Iteration 139, loss = 0.29690149\n",
      "Iteration 140, loss = 0.29625775\n",
      "Iteration 141, loss = 0.29560854\n",
      "Iteration 142, loss = 0.29496530\n",
      "Iteration 143, loss = 0.29431720\n",
      "Iteration 144, loss = 0.29372887\n",
      "Iteration 145, loss = 0.29312325\n",
      "Iteration 146, loss = 0.29252404\n",
      "Iteration 147, loss = 0.29194985\n",
      "Iteration 148, loss = 0.29140259\n",
      "Iteration 149, loss = 0.29081941\n",
      "Iteration 150, loss = 0.29024988\n",
      "Iteration 151, loss = 0.28971177\n",
      "Iteration 152, loss = 0.28917550\n",
      "Iteration 153, loss = 0.28866445\n",
      "Iteration 154, loss = 0.28812947\n",
      "Iteration 155, loss = 0.28761497\n",
      "Iteration 156, loss = 0.28710513\n",
      "Iteration 157, loss = 0.28662115\n",
      "Iteration 158, loss = 0.28612518\n",
      "Iteration 159, loss = 0.28565313\n",
      "Iteration 160, loss = 0.28520216\n",
      "Iteration 161, loss = 0.28471614\n",
      "Iteration 162, loss = 0.28424369\n",
      "Iteration 163, loss = 0.28379941\n",
      "Iteration 164, loss = 0.28335900\n",
      "Iteration 165, loss = 0.28293800\n",
      "Iteration 166, loss = 0.28249673\n",
      "Iteration 167, loss = 0.28206554\n",
      "Iteration 168, loss = 0.28165169\n",
      "Iteration 169, loss = 0.28124675\n",
      "Iteration 170, loss = 0.28083469\n",
      "Iteration 171, loss = 0.28043084\n",
      "Iteration 172, loss = 0.28005089\n",
      "Iteration 173, loss = 0.27967739\n",
      "Iteration 174, loss = 0.27927879\n",
      "Iteration 175, loss = 0.27891584\n",
      "Iteration 176, loss = 0.27858834\n",
      "Iteration 177, loss = 0.27817325\n",
      "Iteration 178, loss = 0.27781276\n",
      "Iteration 179, loss = 0.27748046\n",
      "Iteration 180, loss = 0.27712585\n",
      "Iteration 181, loss = 0.27678060\n",
      "Iteration 182, loss = 0.27643835\n",
      "Iteration 183, loss = 0.27610851\n",
      "Iteration 184, loss = 0.27578908\n",
      "Iteration 185, loss = 0.27545983\n",
      "Iteration 186, loss = 0.27516135\n",
      "Iteration 187, loss = 0.27483796\n",
      "Iteration 188, loss = 0.27452373\n",
      "Iteration 189, loss = 0.27426024\n",
      "Iteration 190, loss = 0.27391257\n",
      "Iteration 191, loss = 0.27362131\n",
      "Iteration 192, loss = 0.27333409\n",
      "Iteration 193, loss = 0.27304557\n",
      "Iteration 194, loss = 0.27276413\n",
      "Iteration 195, loss = 0.27247125\n",
      "Iteration 196, loss = 0.27219987\n",
      "Iteration 197, loss = 0.27193338\n",
      "Iteration 198, loss = 0.27166971\n",
      "Iteration 199, loss = 0.27139589\n",
      "Iteration 200, loss = 0.27114205\n",
      "Iteration 1, loss = 0.58586111\n",
      "Iteration 2, loss = 0.46113489\n",
      "Iteration 3, loss = 0.45227479\n",
      "Iteration 4, loss = 0.45107143\n",
      "Iteration 5, loss = 0.44983544\n",
      "Iteration 6, loss = 0.44868253\n",
      "Iteration 7, loss = 0.44753273\n",
      "Iteration 8, loss = 0.44637702\n",
      "Iteration 9, loss = 0.44527894\n",
      "Iteration 10, loss = 0.44416994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.44304191\n",
      "Iteration 12, loss = 0.44199592\n",
      "Iteration 13, loss = 0.44083446\n",
      "Iteration 14, loss = 0.43973805\n",
      "Iteration 15, loss = 0.43865971\n",
      "Iteration 16, loss = 0.43755073\n",
      "Iteration 17, loss = 0.43646254\n",
      "Iteration 18, loss = 0.43538052\n",
      "Iteration 19, loss = 0.43428091\n",
      "Iteration 20, loss = 0.43322083\n",
      "Iteration 21, loss = 0.43211851\n",
      "Iteration 22, loss = 0.43108922\n",
      "Iteration 23, loss = 0.42993203\n",
      "Iteration 24, loss = 0.42885693\n",
      "Iteration 25, loss = 0.42779830\n",
      "Iteration 26, loss = 0.42665043\n",
      "Iteration 27, loss = 0.42565081\n",
      "Iteration 28, loss = 0.42443879\n",
      "Iteration 29, loss = 0.42337200\n",
      "Iteration 30, loss = 0.42226672\n",
      "Iteration 31, loss = 0.42114629\n",
      "Iteration 32, loss = 0.42003090\n",
      "Iteration 33, loss = 0.41894929\n",
      "Iteration 34, loss = 0.41774802\n",
      "Iteration 35, loss = 0.41664075\n",
      "Iteration 36, loss = 0.41546381\n",
      "Iteration 37, loss = 0.41435639\n",
      "Iteration 38, loss = 0.41322333\n",
      "Iteration 39, loss = 0.41203518\n",
      "Iteration 40, loss = 0.41089587\n",
      "Iteration 41, loss = 0.40980437\n",
      "Iteration 42, loss = 0.40856252\n",
      "Iteration 43, loss = 0.40734457\n",
      "Iteration 44, loss = 0.40616745\n",
      "Iteration 45, loss = 0.40498782\n",
      "Iteration 46, loss = 0.40379814\n",
      "Iteration 47, loss = 0.40256998\n",
      "Iteration 48, loss = 0.40136571\n",
      "Iteration 49, loss = 0.40013454\n",
      "Iteration 50, loss = 0.39890695\n",
      "Iteration 51, loss = 0.39773658\n",
      "Iteration 52, loss = 0.39645723\n",
      "Iteration 53, loss = 0.39515820\n",
      "Iteration 54, loss = 0.39396542\n",
      "Iteration 55, loss = 0.39265303\n",
      "Iteration 56, loss = 0.39141167\n",
      "Iteration 57, loss = 0.39016056\n",
      "Iteration 58, loss = 0.38893674\n",
      "Iteration 59, loss = 0.38761481\n",
      "Iteration 60, loss = 0.38633359\n",
      "Iteration 61, loss = 0.38506617\n",
      "Iteration 62, loss = 0.38378413\n",
      "Iteration 63, loss = 0.38246714\n",
      "Iteration 64, loss = 0.38118234\n",
      "Iteration 65, loss = 0.37988434\n",
      "Iteration 66, loss = 0.37859157\n",
      "Iteration 67, loss = 0.37728849\n",
      "Iteration 68, loss = 0.37598693\n",
      "Iteration 69, loss = 0.37467868\n",
      "Iteration 70, loss = 0.37337790\n",
      "Iteration 71, loss = 0.37211077\n",
      "Iteration 72, loss = 0.37079647\n",
      "Iteration 73, loss = 0.36948297\n",
      "Iteration 74, loss = 0.36818163\n",
      "Iteration 75, loss = 0.36686806\n",
      "Iteration 76, loss = 0.36560186\n",
      "Iteration 77, loss = 0.36429340\n",
      "Iteration 78, loss = 0.36297918\n",
      "Iteration 79, loss = 0.36169882\n",
      "Iteration 80, loss = 0.36040824\n",
      "Iteration 81, loss = 0.35912531\n",
      "Iteration 82, loss = 0.35784817\n",
      "Iteration 83, loss = 0.35655550\n",
      "Iteration 84, loss = 0.35531166\n",
      "Iteration 85, loss = 0.35405025\n",
      "Iteration 86, loss = 0.35278415\n",
      "Iteration 87, loss = 0.35155132\n",
      "Iteration 88, loss = 0.35029675\n",
      "Iteration 89, loss = 0.34906168\n",
      "Iteration 90, loss = 0.34786428\n",
      "Iteration 91, loss = 0.34666576\n",
      "Iteration 92, loss = 0.34541168\n",
      "Iteration 93, loss = 0.34422177\n",
      "Iteration 94, loss = 0.34301132\n",
      "Iteration 95, loss = 0.34182460\n",
      "Iteration 96, loss = 0.34074113\n",
      "Iteration 97, loss = 0.33954807\n",
      "Iteration 98, loss = 0.33837469\n",
      "Iteration 99, loss = 0.33726412\n",
      "Iteration 100, loss = 0.33610039\n",
      "Iteration 101, loss = 0.33498783\n",
      "Iteration 102, loss = 0.33387904\n",
      "Iteration 103, loss = 0.33280159\n",
      "Iteration 104, loss = 0.33170096\n",
      "Iteration 105, loss = 0.33067389\n",
      "Iteration 106, loss = 0.32961287\n",
      "Iteration 107, loss = 0.32853958\n",
      "Iteration 108, loss = 0.32747977\n",
      "Iteration 109, loss = 0.32647576\n",
      "Iteration 110, loss = 0.32547893\n",
      "Iteration 111, loss = 0.32447747\n",
      "Iteration 112, loss = 0.32346138\n",
      "Iteration 113, loss = 0.32253451\n",
      "Iteration 114, loss = 0.32154415\n",
      "Iteration 115, loss = 0.32061089\n",
      "Iteration 116, loss = 0.31962910\n",
      "Iteration 117, loss = 0.31871680\n",
      "Iteration 118, loss = 0.31781107\n",
      "Iteration 119, loss = 0.31689281\n",
      "Iteration 120, loss = 0.31602601\n",
      "Iteration 121, loss = 0.31513865\n",
      "Iteration 122, loss = 0.31430998\n",
      "Iteration 123, loss = 0.31343729\n",
      "Iteration 124, loss = 0.31258586\n",
      "Iteration 125, loss = 0.31179135\n",
      "Iteration 126, loss = 0.31106114\n",
      "Iteration 127, loss = 0.31018579\n",
      "Iteration 128, loss = 0.30938646\n",
      "Iteration 129, loss = 0.30860460\n",
      "Iteration 130, loss = 0.30784750\n",
      "Iteration 131, loss = 0.30709131\n",
      "Iteration 132, loss = 0.30636413\n",
      "Iteration 133, loss = 0.30563670\n",
      "Iteration 134, loss = 0.30490969\n",
      "Iteration 135, loss = 0.30420267\n",
      "Iteration 136, loss = 0.30350451\n",
      "Iteration 137, loss = 0.30281089\n",
      "Iteration 138, loss = 0.30219631\n",
      "Iteration 139, loss = 0.30149085\n",
      "Iteration 140, loss = 0.30083687\n",
      "Iteration 141, loss = 0.30018647\n",
      "Iteration 142, loss = 0.29955602\n",
      "Iteration 143, loss = 0.29893054\n",
      "Iteration 144, loss = 0.29833994\n",
      "Iteration 145, loss = 0.29774686\n",
      "Iteration 146, loss = 0.29713506\n",
      "Iteration 147, loss = 0.29659387\n",
      "Iteration 148, loss = 0.29600657\n",
      "Iteration 149, loss = 0.29544640\n",
      "Iteration 150, loss = 0.29488482\n",
      "Iteration 151, loss = 0.29433944\n",
      "Iteration 152, loss = 0.29381815\n",
      "Iteration 153, loss = 0.29329705\n",
      "Iteration 154, loss = 0.29277482\n",
      "Iteration 155, loss = 0.29227421\n",
      "Iteration 156, loss = 0.29174984\n",
      "Iteration 157, loss = 0.29126349\n",
      "Iteration 158, loss = 0.29076926\n",
      "Iteration 159, loss = 0.29030508\n",
      "Iteration 160, loss = 0.28983492\n",
      "Iteration 161, loss = 0.28937290\n",
      "Iteration 162, loss = 0.28890907\n",
      "Iteration 163, loss = 0.28846356\n",
      "Iteration 164, loss = 0.28803382\n",
      "Iteration 165, loss = 0.28762411\n",
      "Iteration 166, loss = 0.28720122\n",
      "Iteration 167, loss = 0.28673607\n",
      "Iteration 168, loss = 0.28636132\n",
      "Iteration 169, loss = 0.28596565\n",
      "Iteration 170, loss = 0.28553442\n",
      "Iteration 171, loss = 0.28512799\n",
      "Iteration 172, loss = 0.28475710\n",
      "Iteration 173, loss = 0.28438981\n",
      "Iteration 174, loss = 0.28400802\n",
      "Iteration 175, loss = 0.28362476\n",
      "Iteration 176, loss = 0.28326892\n",
      "Iteration 177, loss = 0.28289330\n",
      "Iteration 178, loss = 0.28256239\n",
      "Iteration 179, loss = 0.28223218\n",
      "Iteration 180, loss = 0.28187073\n",
      "Iteration 181, loss = 0.28152074\n",
      "Iteration 182, loss = 0.28118573\n",
      "Iteration 183, loss = 0.28085627\n",
      "Iteration 184, loss = 0.28056941\n",
      "Iteration 185, loss = 0.28022267\n",
      "Iteration 186, loss = 0.27991942\n",
      "Iteration 187, loss = 0.27963807\n",
      "Iteration 188, loss = 0.27929411\n",
      "Iteration 189, loss = 0.27906103\n",
      "Iteration 190, loss = 0.27870788\n",
      "Iteration 191, loss = 0.27850040\n",
      "Iteration 192, loss = 0.27816299\n",
      "Iteration 193, loss = 0.27785289\n",
      "Iteration 194, loss = 0.27755898\n",
      "Iteration 195, loss = 0.27729216\n",
      "Iteration 196, loss = 0.27701481\n",
      "Iteration 197, loss = 0.27676646\n",
      "Iteration 198, loss = 0.27649823\n",
      "Iteration 199, loss = 0.27623588\n",
      "Iteration 200, loss = 0.27599328\n",
      "Iteration 1, loss = 0.58444695\n",
      "Iteration 2, loss = 0.46103485\n",
      "Iteration 3, loss = 0.45183497\n",
      "Iteration 4, loss = 0.45055889\n",
      "Iteration 5, loss = 0.44934938\n",
      "Iteration 6, loss = 0.44818902\n",
      "Iteration 7, loss = 0.44699667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44583497\n",
      "Iteration 9, loss = 0.44469881\n",
      "Iteration 10, loss = 0.44359255\n",
      "Iteration 11, loss = 0.44243379\n",
      "Iteration 12, loss = 0.44135626\n",
      "Iteration 13, loss = 0.44020636\n",
      "Iteration 14, loss = 0.43909576\n",
      "Iteration 15, loss = 0.43797092\n",
      "Iteration 16, loss = 0.43682867\n",
      "Iteration 17, loss = 0.43571693\n",
      "Iteration 18, loss = 0.43462462\n",
      "Iteration 19, loss = 0.43349505\n",
      "Iteration 20, loss = 0.43240769\n",
      "Iteration 21, loss = 0.43127824\n",
      "Iteration 22, loss = 0.43018385\n",
      "Iteration 23, loss = 0.42904946\n",
      "Iteration 24, loss = 0.42794250\n",
      "Iteration 25, loss = 0.42680975\n",
      "Iteration 26, loss = 0.42567571\n",
      "Iteration 27, loss = 0.42465197\n",
      "Iteration 28, loss = 0.42343004\n",
      "Iteration 29, loss = 0.42233808\n",
      "Iteration 30, loss = 0.42117012\n",
      "Iteration 31, loss = 0.42004030\n",
      "Iteration 32, loss = 0.41893209\n",
      "Iteration 33, loss = 0.41776957\n",
      "Iteration 34, loss = 0.41660497\n",
      "Iteration 35, loss = 0.41544589\n",
      "Iteration 36, loss = 0.41429010\n",
      "Iteration 37, loss = 0.41311444\n",
      "Iteration 38, loss = 0.41193509\n",
      "Iteration 39, loss = 0.41072203\n",
      "Iteration 40, loss = 0.40955763\n",
      "Iteration 41, loss = 0.40837868\n",
      "Iteration 42, loss = 0.40712462\n",
      "Iteration 43, loss = 0.40591425\n",
      "Iteration 44, loss = 0.40468926\n",
      "Iteration 45, loss = 0.40349255\n",
      "Iteration 46, loss = 0.40223633\n",
      "Iteration 47, loss = 0.40104244\n",
      "Iteration 48, loss = 0.39975238\n",
      "Iteration 49, loss = 0.39858401\n",
      "Iteration 50, loss = 0.39727255\n",
      "Iteration 51, loss = 0.39597930\n",
      "Iteration 52, loss = 0.39469910\n",
      "Iteration 53, loss = 0.39340625\n",
      "Iteration 54, loss = 0.39217421\n",
      "Iteration 55, loss = 0.39080755\n",
      "Iteration 56, loss = 0.38954580\n",
      "Iteration 57, loss = 0.38826599\n",
      "Iteration 58, loss = 0.38695613\n",
      "Iteration 59, loss = 0.38565664\n",
      "Iteration 60, loss = 0.38435440\n",
      "Iteration 61, loss = 0.38306146\n",
      "Iteration 62, loss = 0.38166243\n",
      "Iteration 63, loss = 0.38035980\n",
      "Iteration 64, loss = 0.37905759\n",
      "Iteration 65, loss = 0.37772384\n",
      "Iteration 66, loss = 0.37638955\n",
      "Iteration 67, loss = 0.37504818\n",
      "Iteration 68, loss = 0.37371083\n",
      "Iteration 69, loss = 0.37238421\n",
      "Iteration 70, loss = 0.37104080\n",
      "Iteration 71, loss = 0.36973469\n",
      "Iteration 72, loss = 0.36838854\n",
      "Iteration 73, loss = 0.36704984\n",
      "Iteration 74, loss = 0.36570599\n",
      "Iteration 75, loss = 0.36438563\n",
      "Iteration 76, loss = 0.36304419\n",
      "Iteration 77, loss = 0.36169745\n",
      "Iteration 78, loss = 0.36038024\n",
      "Iteration 79, loss = 0.35906623\n",
      "Iteration 80, loss = 0.35771584\n",
      "Iteration 81, loss = 0.35641066\n",
      "Iteration 82, loss = 0.35511986\n",
      "Iteration 83, loss = 0.35380928\n",
      "Iteration 84, loss = 0.35254582\n",
      "Iteration 85, loss = 0.35123205\n",
      "Iteration 86, loss = 0.34996426\n",
      "Iteration 87, loss = 0.34869995\n",
      "Iteration 88, loss = 0.34740335\n",
      "Iteration 89, loss = 0.34616050\n",
      "Iteration 90, loss = 0.34492767\n",
      "Iteration 91, loss = 0.34372253\n",
      "Iteration 92, loss = 0.34242184\n",
      "Iteration 93, loss = 0.34119585\n",
      "Iteration 94, loss = 0.33996629\n",
      "Iteration 95, loss = 0.33876600\n",
      "Iteration 96, loss = 0.33760720\n",
      "Iteration 97, loss = 0.33642263\n",
      "Iteration 98, loss = 0.33525096\n",
      "Iteration 99, loss = 0.33408369\n",
      "Iteration 100, loss = 0.33292817\n",
      "Iteration 101, loss = 0.33179468\n",
      "Iteration 102, loss = 0.33064944\n",
      "Iteration 103, loss = 0.32956100\n",
      "Iteration 104, loss = 0.32845974\n",
      "Iteration 105, loss = 0.32742775\n",
      "Iteration 106, loss = 0.32628713\n",
      "Iteration 107, loss = 0.32521081\n",
      "Iteration 108, loss = 0.32418648\n",
      "Iteration 109, loss = 0.32313658\n",
      "Iteration 110, loss = 0.32210088\n",
      "Iteration 111, loss = 0.32109035\n",
      "Iteration 112, loss = 0.32005773\n",
      "Iteration 113, loss = 0.31906409\n",
      "Iteration 114, loss = 0.31808761\n",
      "Iteration 115, loss = 0.31716020\n",
      "Iteration 116, loss = 0.31622480\n",
      "Iteration 117, loss = 0.31524715\n",
      "Iteration 118, loss = 0.31434671\n",
      "Iteration 119, loss = 0.31342276\n",
      "Iteration 120, loss = 0.31253626\n",
      "Iteration 121, loss = 0.31161857\n",
      "Iteration 122, loss = 0.31078907\n",
      "Iteration 123, loss = 0.30989478\n",
      "Iteration 124, loss = 0.30904830\n",
      "Iteration 125, loss = 0.30822016\n",
      "Iteration 126, loss = 0.30741884\n",
      "Iteration 127, loss = 0.30660696\n",
      "Iteration 128, loss = 0.30579841\n",
      "Iteration 129, loss = 0.30500124\n",
      "Iteration 130, loss = 0.30423635\n",
      "Iteration 131, loss = 0.30349392\n",
      "Iteration 132, loss = 0.30274210\n",
      "Iteration 133, loss = 0.30198607\n",
      "Iteration 134, loss = 0.30127733\n",
      "Iteration 135, loss = 0.30055173\n",
      "Iteration 136, loss = 0.29985810\n",
      "Iteration 137, loss = 0.29916116\n",
      "Iteration 138, loss = 0.29852939\n",
      "Iteration 139, loss = 0.29778846\n",
      "Iteration 140, loss = 0.29717451\n",
      "Iteration 141, loss = 0.29650826\n",
      "Iteration 142, loss = 0.29587384\n",
      "Iteration 143, loss = 0.29524134\n",
      "Iteration 144, loss = 0.29463751\n",
      "Iteration 145, loss = 0.29401080\n",
      "Iteration 146, loss = 0.29341329\n",
      "Iteration 147, loss = 0.29290107\n",
      "Iteration 148, loss = 0.29231437\n",
      "Iteration 149, loss = 0.29168682\n",
      "Iteration 150, loss = 0.29112445\n",
      "Iteration 151, loss = 0.29058304\n",
      "Iteration 152, loss = 0.29003999\n",
      "Iteration 153, loss = 0.28951270\n",
      "Iteration 154, loss = 0.28900067\n",
      "Iteration 155, loss = 0.28846839\n",
      "Iteration 156, loss = 0.28796888\n",
      "Iteration 157, loss = 0.28746339\n",
      "Iteration 158, loss = 0.28697945\n",
      "Iteration 159, loss = 0.28650173\n",
      "Iteration 160, loss = 0.28601853\n",
      "Iteration 161, loss = 0.28555786\n",
      "Iteration 162, loss = 0.28508975\n",
      "Iteration 163, loss = 0.28463406\n",
      "Iteration 164, loss = 0.28421025\n",
      "Iteration 165, loss = 0.28376527\n",
      "Iteration 166, loss = 0.28337053\n",
      "Iteration 167, loss = 0.28288953\n",
      "Iteration 168, loss = 0.28251730\n",
      "Iteration 169, loss = 0.28214781\n",
      "Iteration 170, loss = 0.28166182\n",
      "Iteration 171, loss = 0.28125212\n",
      "Iteration 172, loss = 0.28087286\n",
      "Iteration 173, loss = 0.28047938\n",
      "Iteration 174, loss = 0.28011706\n",
      "Iteration 175, loss = 0.27972533\n",
      "Iteration 176, loss = 0.27937866\n",
      "Iteration 177, loss = 0.27899801\n",
      "Iteration 178, loss = 0.27864940\n",
      "Iteration 179, loss = 0.27833697\n",
      "Iteration 180, loss = 0.27795814\n",
      "Iteration 181, loss = 0.27761301\n",
      "Iteration 182, loss = 0.27727722\n",
      "Iteration 183, loss = 0.27694263\n",
      "Iteration 184, loss = 0.27665489\n",
      "Iteration 185, loss = 0.27630301\n",
      "Iteration 186, loss = 0.27599898\n",
      "Iteration 187, loss = 0.27569906\n",
      "Iteration 188, loss = 0.27539912\n",
      "Iteration 189, loss = 0.27511388\n",
      "Iteration 190, loss = 0.27476709\n",
      "Iteration 191, loss = 0.27456425\n",
      "Iteration 192, loss = 0.27426610\n",
      "Iteration 193, loss = 0.27390070\n",
      "Iteration 194, loss = 0.27363201\n",
      "Iteration 195, loss = 0.27334084\n",
      "Iteration 196, loss = 0.27308348\n",
      "Iteration 197, loss = 0.27279909\n",
      "Iteration 198, loss = 0.27254674\n",
      "Iteration 199, loss = 0.27226135\n",
      "Iteration 200, loss = 0.27201006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58466182\n",
      "Iteration 2, loss = 0.46112550\n",
      "Iteration 3, loss = 0.45219909\n",
      "Iteration 4, loss = 0.45081139\n",
      "Iteration 5, loss = 0.44960447\n",
      "Iteration 6, loss = 0.44844300\n",
      "Iteration 7, loss = 0.44730270\n",
      "Iteration 8, loss = 0.44616322\n",
      "Iteration 9, loss = 0.44501902\n",
      "Iteration 10, loss = 0.44387821\n",
      "Iteration 11, loss = 0.44276072\n",
      "Iteration 12, loss = 0.44163116\n",
      "Iteration 13, loss = 0.44053272\n",
      "Iteration 14, loss = 0.43949072\n",
      "Iteration 15, loss = 0.43828924\n",
      "Iteration 16, loss = 0.43722307\n",
      "Iteration 17, loss = 0.43607247\n",
      "Iteration 18, loss = 0.43496678\n",
      "Iteration 19, loss = 0.43388504\n",
      "Iteration 20, loss = 0.43275913\n",
      "Iteration 21, loss = 0.43165205\n",
      "Iteration 22, loss = 0.43053578\n",
      "Iteration 23, loss = 0.42946383\n",
      "Iteration 24, loss = 0.42835080\n",
      "Iteration 25, loss = 0.42721804\n",
      "Iteration 26, loss = 0.42612164\n",
      "Iteration 27, loss = 0.42511641\n",
      "Iteration 28, loss = 0.42393579\n",
      "Iteration 29, loss = 0.42288102\n",
      "Iteration 30, loss = 0.42167925\n",
      "Iteration 31, loss = 0.42053462\n",
      "Iteration 32, loss = 0.41941187\n",
      "Iteration 33, loss = 0.41824650\n",
      "Iteration 34, loss = 0.41712691\n",
      "Iteration 35, loss = 0.41599709\n",
      "Iteration 36, loss = 0.41478857\n",
      "Iteration 37, loss = 0.41361054\n",
      "Iteration 38, loss = 0.41254669\n",
      "Iteration 39, loss = 0.41130955\n",
      "Iteration 40, loss = 0.41010335\n",
      "Iteration 41, loss = 0.40889114\n",
      "Iteration 42, loss = 0.40769497\n",
      "Iteration 43, loss = 0.40651708\n",
      "Iteration 44, loss = 0.40531532\n",
      "Iteration 45, loss = 0.40408099\n",
      "Iteration 46, loss = 0.40287850\n",
      "Iteration 47, loss = 0.40164271\n",
      "Iteration 48, loss = 0.40036058\n",
      "Iteration 49, loss = 0.39923393\n",
      "Iteration 50, loss = 0.39786877\n",
      "Iteration 51, loss = 0.39663257\n",
      "Iteration 52, loss = 0.39534015\n",
      "Iteration 53, loss = 0.39408110\n",
      "Iteration 54, loss = 0.39282394\n",
      "Iteration 55, loss = 0.39147587\n",
      "Iteration 56, loss = 0.39021710\n",
      "Iteration 57, loss = 0.38892185\n",
      "Iteration 58, loss = 0.38762145\n",
      "Iteration 59, loss = 0.38634306\n",
      "Iteration 60, loss = 0.38500697\n",
      "Iteration 61, loss = 0.38369341\n",
      "Iteration 62, loss = 0.38235616\n",
      "Iteration 63, loss = 0.38102838\n",
      "Iteration 64, loss = 0.37970886\n",
      "Iteration 65, loss = 0.37840371\n",
      "Iteration 66, loss = 0.37713234\n",
      "Iteration 67, loss = 0.37570015\n",
      "Iteration 68, loss = 0.37437568\n",
      "Iteration 69, loss = 0.37303863\n",
      "Iteration 70, loss = 0.37169946\n",
      "Iteration 71, loss = 0.37036849\n",
      "Iteration 72, loss = 0.36901700\n",
      "Iteration 73, loss = 0.36768602\n",
      "Iteration 74, loss = 0.36633675\n",
      "Iteration 75, loss = 0.36502639\n",
      "Iteration 76, loss = 0.36365751\n",
      "Iteration 77, loss = 0.36230073\n",
      "Iteration 78, loss = 0.36099999\n",
      "Iteration 79, loss = 0.35964818\n",
      "Iteration 80, loss = 0.35835263\n",
      "Iteration 81, loss = 0.35698443\n",
      "Iteration 82, loss = 0.35568173\n",
      "Iteration 83, loss = 0.35440651\n",
      "Iteration 84, loss = 0.35306784\n",
      "Iteration 85, loss = 0.35175092\n",
      "Iteration 86, loss = 0.35047487\n",
      "Iteration 87, loss = 0.34913333\n",
      "Iteration 88, loss = 0.34786251\n",
      "Iteration 89, loss = 0.34662547\n",
      "Iteration 90, loss = 0.34531167\n",
      "Iteration 91, loss = 0.34409061\n",
      "Iteration 92, loss = 0.34282424\n",
      "Iteration 93, loss = 0.34157083\n",
      "Iteration 94, loss = 0.34034353\n",
      "Iteration 95, loss = 0.33910283\n",
      "Iteration 96, loss = 0.33790046\n",
      "Iteration 97, loss = 0.33667552\n",
      "Iteration 98, loss = 0.33553296\n",
      "Iteration 99, loss = 0.33427658\n",
      "Iteration 100, loss = 0.33312430\n",
      "Iteration 101, loss = 0.33197039\n",
      "Iteration 102, loss = 0.33079359\n",
      "Iteration 103, loss = 0.32967045\n",
      "Iteration 104, loss = 0.32858070\n",
      "Iteration 105, loss = 0.32745588\n",
      "Iteration 106, loss = 0.32628898\n",
      "Iteration 107, loss = 0.32519944\n",
      "Iteration 108, loss = 0.32416208\n",
      "Iteration 109, loss = 0.32304483\n",
      "Iteration 110, loss = 0.32203403\n",
      "Iteration 111, loss = 0.32095021\n",
      "Iteration 112, loss = 0.31988045\n",
      "Iteration 113, loss = 0.31885443\n",
      "Iteration 114, loss = 0.31785171\n",
      "Iteration 115, loss = 0.31685472\n",
      "Iteration 116, loss = 0.31594388\n",
      "Iteration 117, loss = 0.31489420\n",
      "Iteration 118, loss = 0.31397556\n",
      "Iteration 119, loss = 0.31300325\n",
      "Iteration 120, loss = 0.31205834\n",
      "Iteration 121, loss = 0.31114140\n",
      "Iteration 122, loss = 0.31026889\n",
      "Iteration 123, loss = 0.30934037\n",
      "Iteration 124, loss = 0.30846236\n",
      "Iteration 125, loss = 0.30758727\n",
      "Iteration 126, loss = 0.30674490\n",
      "Iteration 127, loss = 0.30590650\n",
      "Iteration 128, loss = 0.30508714\n",
      "Iteration 129, loss = 0.30422350\n",
      "Iteration 130, loss = 0.30344775\n",
      "Iteration 131, loss = 0.30265108\n",
      "Iteration 132, loss = 0.30190689\n",
      "Iteration 133, loss = 0.30105207\n",
      "Iteration 134, loss = 0.30032900\n",
      "Iteration 135, loss = 0.29956614\n",
      "Iteration 136, loss = 0.29883740\n",
      "Iteration 137, loss = 0.29808722\n",
      "Iteration 138, loss = 0.29745195\n",
      "Iteration 139, loss = 0.29666149\n",
      "Iteration 140, loss = 0.29600324\n",
      "Iteration 141, loss = 0.29534260\n",
      "Iteration 142, loss = 0.29466946\n",
      "Iteration 143, loss = 0.29399818\n",
      "Iteration 144, loss = 0.29334328\n",
      "Iteration 145, loss = 0.29271438\n",
      "Iteration 146, loss = 0.29207174\n",
      "Iteration 147, loss = 0.29144133\n",
      "Iteration 148, loss = 0.29085439\n",
      "Iteration 149, loss = 0.29024411\n",
      "Iteration 150, loss = 0.28965300\n",
      "Iteration 151, loss = 0.28907411\n",
      "Iteration 152, loss = 0.28848596\n",
      "Iteration 153, loss = 0.28791274\n",
      "Iteration 154, loss = 0.28737434\n",
      "Iteration 155, loss = 0.28681673\n",
      "Iteration 156, loss = 0.28628402\n",
      "Iteration 157, loss = 0.28577407\n",
      "Iteration 158, loss = 0.28522476\n",
      "Iteration 159, loss = 0.28473129\n",
      "Iteration 160, loss = 0.28421430\n",
      "Iteration 161, loss = 0.28375101\n",
      "Iteration 162, loss = 0.28323782\n",
      "Iteration 163, loss = 0.28274995\n",
      "Iteration 164, loss = 0.28230289\n",
      "Iteration 165, loss = 0.28181746\n",
      "Iteration 166, loss = 0.28135935\n",
      "Iteration 167, loss = 0.28092955\n",
      "Iteration 168, loss = 0.28047524\n",
      "Iteration 169, loss = 0.28005021\n",
      "Iteration 170, loss = 0.27960686\n",
      "Iteration 171, loss = 0.27915661\n",
      "Iteration 172, loss = 0.27876187\n",
      "Iteration 173, loss = 0.27834606\n",
      "Iteration 174, loss = 0.27792289\n",
      "Iteration 175, loss = 0.27756939\n",
      "Iteration 176, loss = 0.27712486\n",
      "Iteration 177, loss = 0.27674554\n",
      "Iteration 178, loss = 0.27637978\n",
      "Iteration 179, loss = 0.27601361\n",
      "Iteration 180, loss = 0.27562363\n",
      "Iteration 181, loss = 0.27525944\n",
      "Iteration 182, loss = 0.27490507\n",
      "Iteration 183, loss = 0.27455628\n",
      "Iteration 184, loss = 0.27423441\n",
      "Iteration 185, loss = 0.27385047\n",
      "Iteration 186, loss = 0.27352778\n",
      "Iteration 187, loss = 0.27319650\n",
      "Iteration 188, loss = 0.27288600\n",
      "Iteration 189, loss = 0.27257439\n",
      "Iteration 190, loss = 0.27222138\n",
      "Iteration 191, loss = 0.27196466\n",
      "Iteration 192, loss = 0.27163241\n",
      "Iteration 193, loss = 0.27129321\n",
      "Iteration 194, loss = 0.27098273\n",
      "Iteration 195, loss = 0.27068996\n",
      "Iteration 196, loss = 0.27039636\n",
      "Iteration 197, loss = 0.27011798\n",
      "Iteration 198, loss = 0.26982401\n",
      "Iteration 199, loss = 0.26953982\n",
      "Iteration 200, loss = 0.26927854\n",
      "Iteration 1, loss = 0.55079558\n",
      "Iteration 2, loss = 0.41982747\n",
      "Iteration 3, loss = 0.39031789\n",
      "Iteration 4, loss = 0.36929628\n",
      "Iteration 5, loss = 0.35164028\n",
      "Iteration 6, loss = 0.33677530\n",
      "Iteration 7, loss = 0.32375707\n",
      "Iteration 8, loss = 0.31305296\n",
      "Iteration 9, loss = 0.30420859\n",
      "Iteration 10, loss = 0.29702052\n",
      "Iteration 11, loss = 0.29047278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.28569246\n",
      "Iteration 13, loss = 0.28094136\n",
      "Iteration 14, loss = 0.27734456\n",
      "Iteration 15, loss = 0.27383438\n",
      "Iteration 16, loss = 0.27106516\n",
      "Iteration 17, loss = 0.26879086\n",
      "Iteration 18, loss = 0.26593749\n",
      "Iteration 19, loss = 0.26372580\n",
      "Iteration 20, loss = 0.26179554\n",
      "Iteration 21, loss = 0.25991315\n",
      "Iteration 22, loss = 0.25816602\n",
      "Iteration 23, loss = 0.25660096\n",
      "Iteration 24, loss = 0.25499173\n",
      "Iteration 25, loss = 0.25364281\n",
      "Iteration 26, loss = 0.25226616\n",
      "Iteration 27, loss = 0.25054580\n",
      "Iteration 28, loss = 0.24937886\n",
      "Iteration 29, loss = 0.24814420\n",
      "Iteration 30, loss = 0.24743198\n",
      "Iteration 31, loss = 0.24579138\n",
      "Iteration 32, loss = 0.24471167\n",
      "Iteration 33, loss = 0.24416869\n",
      "Iteration 34, loss = 0.24281894\n",
      "Iteration 35, loss = 0.24164966\n",
      "Iteration 36, loss = 0.24093685\n",
      "Iteration 37, loss = 0.24015413\n",
      "Iteration 38, loss = 0.23964860\n",
      "Iteration 39, loss = 0.23870074\n",
      "Iteration 40, loss = 0.23811068\n",
      "Iteration 41, loss = 0.23735612\n",
      "Iteration 42, loss = 0.23676754\n",
      "Iteration 43, loss = 0.23626662\n",
      "Iteration 44, loss = 0.23562140\n",
      "Iteration 45, loss = 0.23534647\n",
      "Iteration 46, loss = 0.23504116\n",
      "Iteration 47, loss = 0.23450915\n",
      "Iteration 48, loss = 0.23440415\n",
      "Iteration 49, loss = 0.23419637\n",
      "Iteration 50, loss = 0.23376842\n",
      "Iteration 51, loss = 0.23319975\n",
      "Iteration 52, loss = 0.23313817\n",
      "Iteration 53, loss = 0.23299451\n",
      "Iteration 54, loss = 0.23259377\n",
      "Iteration 55, loss = 0.23209515\n",
      "Iteration 56, loss = 0.23203363\n",
      "Iteration 57, loss = 0.23169149\n",
      "Iteration 58, loss = 0.23163221\n",
      "Iteration 59, loss = 0.23132252\n",
      "Iteration 60, loss = 0.23099255\n",
      "Iteration 61, loss = 0.23113616\n",
      "Iteration 62, loss = 0.23072894\n",
      "Iteration 63, loss = 0.23092597\n",
      "Iteration 64, loss = 0.23040615\n",
      "Iteration 65, loss = 0.23010242\n",
      "Iteration 66, loss = 0.23031834\n",
      "Iteration 67, loss = 0.22969847\n",
      "Iteration 68, loss = 0.22971769\n",
      "Iteration 69, loss = 0.22895296\n",
      "Iteration 70, loss = 0.22914829\n",
      "Iteration 71, loss = 0.22907936\n",
      "Iteration 72, loss = 0.22856659\n",
      "Iteration 73, loss = 0.22807643\n",
      "Iteration 74, loss = 0.22799615\n",
      "Iteration 75, loss = 0.22774728\n",
      "Iteration 76, loss = 0.22797764\n",
      "Iteration 77, loss = 0.22703861\n",
      "Iteration 78, loss = 0.22675371\n",
      "Iteration 79, loss = 0.22675612\n",
      "Iteration 80, loss = 0.22592679\n",
      "Iteration 81, loss = 0.22589478\n",
      "Iteration 82, loss = 0.22547457\n",
      "Iteration 83, loss = 0.22493892\n",
      "Iteration 84, loss = 0.22489924\n",
      "Iteration 85, loss = 0.22456983\n",
      "Iteration 86, loss = 0.22377185\n",
      "Iteration 87, loss = 0.22354858\n",
      "Iteration 88, loss = 0.22297859\n",
      "Iteration 89, loss = 0.22286884\n",
      "Iteration 90, loss = 0.22208250\n",
      "Iteration 91, loss = 0.22187616\n",
      "Iteration 92, loss = 0.22096827\n",
      "Iteration 93, loss = 0.22059492\n",
      "Iteration 94, loss = 0.22012859\n",
      "Iteration 95, loss = 0.21960339\n",
      "Iteration 96, loss = 0.21902868\n",
      "Iteration 97, loss = 0.21854502\n",
      "Iteration 98, loss = 0.21812528\n",
      "Iteration 99, loss = 0.21782525\n",
      "Iteration 100, loss = 0.21701032\n",
      "Iteration 101, loss = 0.21643616\n",
      "Iteration 102, loss = 0.21648271\n",
      "Iteration 103, loss = 0.21524586\n",
      "Iteration 104, loss = 0.21451202\n",
      "Iteration 105, loss = 0.21399733\n",
      "Iteration 106, loss = 0.21440766\n",
      "Iteration 107, loss = 0.21258975\n",
      "Iteration 108, loss = 0.21214280\n",
      "Iteration 109, loss = 0.21137376\n",
      "Iteration 110, loss = 0.21080373\n",
      "Iteration 111, loss = 0.20995780\n",
      "Iteration 112, loss = 0.20938270\n",
      "Iteration 113, loss = 0.20898336\n",
      "Iteration 114, loss = 0.20819189\n",
      "Iteration 115, loss = 0.20740535\n",
      "Iteration 116, loss = 0.20658098\n",
      "Iteration 117, loss = 0.20586082\n",
      "Iteration 118, loss = 0.20499196\n",
      "Iteration 119, loss = 0.20431805\n",
      "Iteration 120, loss = 0.20374613\n",
      "Iteration 121, loss = 0.20315356\n",
      "Iteration 122, loss = 0.20243933\n",
      "Iteration 123, loss = 0.20145335\n",
      "Iteration 124, loss = 0.20094540\n",
      "Iteration 125, loss = 0.20002816\n",
      "Iteration 126, loss = 0.19915675\n",
      "Iteration 127, loss = 0.19875535\n",
      "Iteration 128, loss = 0.19781533\n",
      "Iteration 129, loss = 0.19760582\n",
      "Iteration 130, loss = 0.19634146\n",
      "Iteration 131, loss = 0.19568923\n",
      "Iteration 132, loss = 0.19483533\n",
      "Iteration 133, loss = 0.19381238\n",
      "Iteration 134, loss = 0.19361013\n",
      "Iteration 135, loss = 0.19254476\n",
      "Iteration 136, loss = 0.19198379\n",
      "Iteration 137, loss = 0.19092371\n",
      "Iteration 138, loss = 0.19021733\n",
      "Iteration 139, loss = 0.18966375\n",
      "Iteration 140, loss = 0.18882997\n",
      "Iteration 141, loss = 0.18809688\n",
      "Iteration 142, loss = 0.18777428\n",
      "Iteration 143, loss = 0.18660684\n",
      "Iteration 144, loss = 0.18615501\n",
      "Iteration 145, loss = 0.18526519\n",
      "Iteration 146, loss = 0.18444998\n",
      "Iteration 147, loss = 0.18364373\n",
      "Iteration 148, loss = 0.18294788\n",
      "Iteration 149, loss = 0.18224151\n",
      "Iteration 150, loss = 0.18152889\n",
      "Iteration 151, loss = 0.18118620\n",
      "Iteration 152, loss = 0.18036017\n",
      "Iteration 153, loss = 0.17975805\n",
      "Iteration 154, loss = 0.17969639\n",
      "Iteration 155, loss = 0.17844151\n",
      "Iteration 156, loss = 0.17777911\n",
      "Iteration 157, loss = 0.17705151\n",
      "Iteration 158, loss = 0.17628608\n",
      "Iteration 159, loss = 0.17587513\n",
      "Iteration 160, loss = 0.17531436\n",
      "Iteration 161, loss = 0.17462369\n",
      "Iteration 162, loss = 0.17388840\n",
      "Iteration 163, loss = 0.17318544\n",
      "Iteration 164, loss = 0.17258294\n",
      "Iteration 165, loss = 0.17214648\n",
      "Iteration 166, loss = 0.17149210\n",
      "Iteration 167, loss = 0.17138577\n",
      "Iteration 168, loss = 0.17133785\n",
      "Iteration 169, loss = 0.16978497\n",
      "Iteration 170, loss = 0.16973916\n",
      "Iteration 171, loss = 0.16888399\n",
      "Iteration 172, loss = 0.16819373\n",
      "Iteration 173, loss = 0.16759283\n",
      "Iteration 174, loss = 0.16692667\n",
      "Iteration 175, loss = 0.16679571\n",
      "Iteration 176, loss = 0.16644658\n",
      "Iteration 177, loss = 0.16533579\n",
      "Iteration 178, loss = 0.16489943\n",
      "Iteration 179, loss = 0.16438920\n",
      "Iteration 180, loss = 0.16360527\n",
      "Iteration 181, loss = 0.16306449\n",
      "Iteration 182, loss = 0.16291360\n",
      "Iteration 183, loss = 0.16240065\n",
      "Iteration 184, loss = 0.16189083\n",
      "Iteration 185, loss = 0.16110072\n",
      "Iteration 186, loss = 0.16066804\n",
      "Iteration 187, loss = 0.16016151\n",
      "Iteration 188, loss = 0.15976325\n",
      "Iteration 189, loss = 0.15898237\n",
      "Iteration 190, loss = 0.15862876\n",
      "Iteration 191, loss = 0.15811423\n",
      "Iteration 192, loss = 0.15779538\n",
      "Iteration 193, loss = 0.15733146\n",
      "Iteration 194, loss = 0.15650609\n",
      "Iteration 195, loss = 0.15607102\n",
      "Iteration 196, loss = 0.15571397\n",
      "Iteration 197, loss = 0.15555685\n",
      "Iteration 198, loss = 0.15453763\n",
      "Iteration 199, loss = 0.15459540\n",
      "Iteration 200, loss = 0.15440699\n",
      "Iteration 1, loss = 0.55197845\n",
      "Iteration 2, loss = 0.41972768\n",
      "Iteration 3, loss = 0.38994650\n",
      "Iteration 4, loss = 0.36869049\n",
      "Iteration 5, loss = 0.35097130\n",
      "Iteration 6, loss = 0.33592083\n",
      "Iteration 7, loss = 0.32289487\n",
      "Iteration 8, loss = 0.31218614\n",
      "Iteration 9, loss = 0.30322555\n",
      "Iteration 10, loss = 0.29593045\n",
      "Iteration 11, loss = 0.28925784\n",
      "Iteration 12, loss = 0.28471594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.27980390\n",
      "Iteration 14, loss = 0.27569199\n",
      "Iteration 15, loss = 0.27236249\n",
      "Iteration 16, loss = 0.26931185\n",
      "Iteration 17, loss = 0.26669405\n",
      "Iteration 18, loss = 0.26396632\n",
      "Iteration 19, loss = 0.26179766\n",
      "Iteration 20, loss = 0.25966993\n",
      "Iteration 21, loss = 0.25772896\n",
      "Iteration 22, loss = 0.25570793\n",
      "Iteration 23, loss = 0.25407058\n",
      "Iteration 24, loss = 0.25247332\n",
      "Iteration 25, loss = 0.25087995\n",
      "Iteration 26, loss = 0.24924412\n",
      "Iteration 27, loss = 0.24762140\n",
      "Iteration 28, loss = 0.24636811\n",
      "Iteration 29, loss = 0.24501002\n",
      "Iteration 30, loss = 0.24391174\n",
      "Iteration 31, loss = 0.24231192\n",
      "Iteration 32, loss = 0.24112096\n",
      "Iteration 33, loss = 0.24019535\n",
      "Iteration 34, loss = 0.23909177\n",
      "Iteration 35, loss = 0.23781057\n",
      "Iteration 36, loss = 0.23708117\n",
      "Iteration 37, loss = 0.23601049\n",
      "Iteration 38, loss = 0.23524460\n",
      "Iteration 39, loss = 0.23457552\n",
      "Iteration 40, loss = 0.23368416\n",
      "Iteration 41, loss = 0.23299641\n",
      "Iteration 42, loss = 0.23254329\n",
      "Iteration 43, loss = 0.23170209\n",
      "Iteration 44, loss = 0.23124027\n",
      "Iteration 45, loss = 0.23068345\n",
      "Iteration 46, loss = 0.23005722\n",
      "Iteration 47, loss = 0.22979065\n",
      "Iteration 48, loss = 0.22931537\n",
      "Iteration 49, loss = 0.22920631\n",
      "Iteration 50, loss = 0.22864810\n",
      "Iteration 51, loss = 0.22843742\n",
      "Iteration 52, loss = 0.22812308\n",
      "Iteration 53, loss = 0.22761658\n",
      "Iteration 54, loss = 0.22722215\n",
      "Iteration 55, loss = 0.22689684\n",
      "Iteration 56, loss = 0.22672085\n",
      "Iteration 57, loss = 0.22642402\n",
      "Iteration 58, loss = 0.22642117\n",
      "Iteration 59, loss = 0.22600957\n",
      "Iteration 60, loss = 0.22555853\n",
      "Iteration 61, loss = 0.22551230\n",
      "Iteration 62, loss = 0.22523052\n",
      "Iteration 63, loss = 0.22536531\n",
      "Iteration 64, loss = 0.22471193\n",
      "Iteration 65, loss = 0.22474005\n",
      "Iteration 66, loss = 0.22452905\n",
      "Iteration 67, loss = 0.22390108\n",
      "Iteration 68, loss = 0.22407661\n",
      "Iteration 69, loss = 0.22351975\n",
      "Iteration 70, loss = 0.22314030\n",
      "Iteration 71, loss = 0.22355775\n",
      "Iteration 72, loss = 0.22284861\n",
      "Iteration 73, loss = 0.22221254\n",
      "Iteration 74, loss = 0.22193699\n",
      "Iteration 75, loss = 0.22199012\n",
      "Iteration 76, loss = 0.22129325\n",
      "Iteration 77, loss = 0.22089234\n",
      "Iteration 78, loss = 0.22072519\n",
      "Iteration 79, loss = 0.22042702\n",
      "Iteration 80, loss = 0.21961903\n",
      "Iteration 81, loss = 0.21934909\n",
      "Iteration 82, loss = 0.21908670\n",
      "Iteration 83, loss = 0.21847894\n",
      "Iteration 84, loss = 0.21816051\n",
      "Iteration 85, loss = 0.21788059\n",
      "Iteration 86, loss = 0.21719333\n",
      "Iteration 87, loss = 0.21723656\n",
      "Iteration 88, loss = 0.21618753\n",
      "Iteration 89, loss = 0.21604823\n",
      "Iteration 90, loss = 0.21533341\n",
      "Iteration 91, loss = 0.21509764\n",
      "Iteration 92, loss = 0.21412303\n",
      "Iteration 93, loss = 0.21372298\n",
      "Iteration 94, loss = 0.21326733\n",
      "Iteration 95, loss = 0.21268832\n",
      "Iteration 96, loss = 0.21196415\n",
      "Iteration 97, loss = 0.21138861\n",
      "Iteration 98, loss = 0.21092769\n",
      "Iteration 99, loss = 0.21068388\n",
      "Iteration 100, loss = 0.20983252\n",
      "Iteration 101, loss = 0.20912368\n",
      "Iteration 102, loss = 0.20906828\n",
      "Iteration 103, loss = 0.20802038\n",
      "Iteration 104, loss = 0.20722492\n",
      "Iteration 105, loss = 0.20685238\n",
      "Iteration 106, loss = 0.20668545\n",
      "Iteration 107, loss = 0.20537316\n",
      "Iteration 108, loss = 0.20460973\n",
      "Iteration 109, loss = 0.20395963\n",
      "Iteration 110, loss = 0.20314793\n",
      "Iteration 111, loss = 0.20252802\n",
      "Iteration 112, loss = 0.20177343\n",
      "Iteration 113, loss = 0.20122996\n",
      "Iteration 114, loss = 0.20098215\n",
      "Iteration 115, loss = 0.19966660\n",
      "Iteration 116, loss = 0.19903486\n",
      "Iteration 117, loss = 0.19808804\n",
      "Iteration 118, loss = 0.19740864\n",
      "Iteration 119, loss = 0.19665924\n",
      "Iteration 120, loss = 0.19613871\n",
      "Iteration 121, loss = 0.19530555\n",
      "Iteration 122, loss = 0.19487546\n",
      "Iteration 123, loss = 0.19378853\n",
      "Iteration 124, loss = 0.19320731\n",
      "Iteration 125, loss = 0.19220771\n",
      "Iteration 126, loss = 0.19162077\n",
      "Iteration 127, loss = 0.19099864\n",
      "Iteration 128, loss = 0.19036062\n",
      "Iteration 129, loss = 0.18993707\n",
      "Iteration 130, loss = 0.18848758\n",
      "Iteration 131, loss = 0.18777061\n",
      "Iteration 132, loss = 0.18725624\n",
      "Iteration 133, loss = 0.18644960\n",
      "Iteration 134, loss = 0.18595649\n",
      "Iteration 135, loss = 0.18511009\n",
      "Iteration 136, loss = 0.18413677\n",
      "Iteration 137, loss = 0.18336684\n",
      "Iteration 138, loss = 0.18283417\n",
      "Iteration 139, loss = 0.18206992\n",
      "Iteration 140, loss = 0.18130191\n",
      "Iteration 141, loss = 0.18066083\n",
      "Iteration 142, loss = 0.18017795\n",
      "Iteration 143, loss = 0.17922989\n",
      "Iteration 144, loss = 0.17877185\n",
      "Iteration 145, loss = 0.17782437\n",
      "Iteration 146, loss = 0.17718010\n",
      "Iteration 147, loss = 0.17648916\n",
      "Iteration 148, loss = 0.17593550\n",
      "Iteration 149, loss = 0.17502433\n",
      "Iteration 150, loss = 0.17443869\n",
      "Iteration 151, loss = 0.17383027\n",
      "Iteration 152, loss = 0.17324838\n",
      "Iteration 153, loss = 0.17238934\n",
      "Iteration 154, loss = 0.17192632\n",
      "Iteration 155, loss = 0.17129478\n",
      "Iteration 156, loss = 0.17072194\n",
      "Iteration 157, loss = 0.16991760\n",
      "Iteration 158, loss = 0.16932008\n",
      "Iteration 159, loss = 0.16888340\n",
      "Iteration 160, loss = 0.16857023\n",
      "Iteration 161, loss = 0.16788402\n",
      "Iteration 162, loss = 0.16705984\n",
      "Iteration 163, loss = 0.16659523\n",
      "Iteration 164, loss = 0.16573096\n",
      "Iteration 165, loss = 0.16522207\n",
      "Iteration 166, loss = 0.16458236\n",
      "Iteration 167, loss = 0.16424344\n",
      "Iteration 168, loss = 0.16373229\n",
      "Iteration 169, loss = 0.16298538\n",
      "Iteration 170, loss = 0.16260165\n",
      "Iteration 171, loss = 0.16197727\n",
      "Iteration 172, loss = 0.16127184\n",
      "Iteration 173, loss = 0.16084923\n",
      "Iteration 174, loss = 0.16025489\n",
      "Iteration 175, loss = 0.15968679\n",
      "Iteration 176, loss = 0.15982826\n",
      "Iteration 177, loss = 0.15882887\n",
      "Iteration 178, loss = 0.15794156\n",
      "Iteration 179, loss = 0.15757047\n",
      "Iteration 180, loss = 0.15703400\n",
      "Iteration 181, loss = 0.15637548\n",
      "Iteration 182, loss = 0.15608647\n",
      "Iteration 183, loss = 0.15562200\n",
      "Iteration 184, loss = 0.15501500\n",
      "Iteration 185, loss = 0.15437595\n",
      "Iteration 186, loss = 0.15386878\n",
      "Iteration 187, loss = 0.15340110\n",
      "Iteration 188, loss = 0.15293332\n",
      "Iteration 189, loss = 0.15221571\n",
      "Iteration 190, loss = 0.15210342\n",
      "Iteration 191, loss = 0.15133824\n",
      "Iteration 192, loss = 0.15100686\n",
      "Iteration 193, loss = 0.15030908\n",
      "Iteration 194, loss = 0.14996109\n",
      "Iteration 195, loss = 0.14934010\n",
      "Iteration 196, loss = 0.14892156\n",
      "Iteration 197, loss = 0.14866904\n",
      "Iteration 198, loss = 0.14780898\n",
      "Iteration 199, loss = 0.14772799\n",
      "Iteration 200, loss = 0.14702298\n",
      "Iteration 1, loss = 0.55287046\n",
      "Iteration 2, loss = 0.41960997\n",
      "Iteration 3, loss = 0.39034349\n",
      "Iteration 4, loss = 0.36925972\n",
      "Iteration 5, loss = 0.35200990\n",
      "Iteration 6, loss = 0.33723050\n",
      "Iteration 7, loss = 0.32471665\n",
      "Iteration 8, loss = 0.31430682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.30572241\n",
      "Iteration 10, loss = 0.29862143\n",
      "Iteration 11, loss = 0.29262893\n",
      "Iteration 12, loss = 0.28814043\n",
      "Iteration 13, loss = 0.28359736\n",
      "Iteration 14, loss = 0.27966287\n",
      "Iteration 15, loss = 0.27667978\n",
      "Iteration 16, loss = 0.27366179\n",
      "Iteration 17, loss = 0.27123884\n",
      "Iteration 18, loss = 0.26873440\n",
      "Iteration 19, loss = 0.26656715\n",
      "Iteration 20, loss = 0.26439708\n",
      "Iteration 21, loss = 0.26247989\n",
      "Iteration 22, loss = 0.26078861\n",
      "Iteration 23, loss = 0.25901443\n",
      "Iteration 24, loss = 0.25738347\n",
      "Iteration 25, loss = 0.25571538\n",
      "Iteration 26, loss = 0.25408124\n",
      "Iteration 27, loss = 0.25269743\n",
      "Iteration 28, loss = 0.25122296\n",
      "Iteration 29, loss = 0.25005280\n",
      "Iteration 30, loss = 0.24887285\n",
      "Iteration 31, loss = 0.24734175\n",
      "Iteration 32, loss = 0.24605033\n",
      "Iteration 33, loss = 0.24495932\n",
      "Iteration 34, loss = 0.24390989\n",
      "Iteration 35, loss = 0.24287888\n",
      "Iteration 36, loss = 0.24195949\n",
      "Iteration 37, loss = 0.24103685\n",
      "Iteration 38, loss = 0.24044637\n",
      "Iteration 39, loss = 0.23947438\n",
      "Iteration 40, loss = 0.23899412\n",
      "Iteration 41, loss = 0.23823539\n",
      "Iteration 42, loss = 0.23781740\n",
      "Iteration 43, loss = 0.23701856\n",
      "Iteration 44, loss = 0.23643719\n",
      "Iteration 45, loss = 0.23619214\n",
      "Iteration 46, loss = 0.23564763\n",
      "Iteration 47, loss = 0.23518373\n",
      "Iteration 48, loss = 0.23469205\n",
      "Iteration 49, loss = 0.23442207\n",
      "Iteration 50, loss = 0.23389259\n",
      "Iteration 51, loss = 0.23407193\n",
      "Iteration 52, loss = 0.23364326\n",
      "Iteration 53, loss = 0.23303991\n",
      "Iteration 54, loss = 0.23286219\n",
      "Iteration 55, loss = 0.23250615\n",
      "Iteration 56, loss = 0.23254277\n",
      "Iteration 57, loss = 0.23198699\n",
      "Iteration 58, loss = 0.23210913\n",
      "Iteration 59, loss = 0.23179615\n",
      "Iteration 60, loss = 0.23138610\n",
      "Iteration 61, loss = 0.23125125\n",
      "Iteration 62, loss = 0.23119887\n",
      "Iteration 63, loss = 0.23104084\n",
      "Iteration 64, loss = 0.23024418\n",
      "Iteration 65, loss = 0.23016136\n",
      "Iteration 66, loss = 0.22986864\n",
      "Iteration 67, loss = 0.22970788\n",
      "Iteration 68, loss = 0.22953817\n",
      "Iteration 69, loss = 0.22912487\n",
      "Iteration 70, loss = 0.22895252\n",
      "Iteration 71, loss = 0.22909971\n",
      "Iteration 72, loss = 0.22818859\n",
      "Iteration 73, loss = 0.22779718\n",
      "Iteration 74, loss = 0.22735712\n",
      "Iteration 75, loss = 0.22743835\n",
      "Iteration 76, loss = 0.22680834\n",
      "Iteration 77, loss = 0.22637446\n",
      "Iteration 78, loss = 0.22606314\n",
      "Iteration 79, loss = 0.22594074\n",
      "Iteration 80, loss = 0.22523810\n",
      "Iteration 81, loss = 0.22468867\n",
      "Iteration 82, loss = 0.22456105\n",
      "Iteration 83, loss = 0.22399552\n",
      "Iteration 84, loss = 0.22352959\n",
      "Iteration 85, loss = 0.22307543\n",
      "Iteration 86, loss = 0.22249812\n",
      "Iteration 87, loss = 0.22210201\n",
      "Iteration 88, loss = 0.22151286\n",
      "Iteration 89, loss = 0.22091054\n",
      "Iteration 90, loss = 0.22039323\n",
      "Iteration 91, loss = 0.21995878\n",
      "Iteration 92, loss = 0.21903809\n",
      "Iteration 93, loss = 0.21867831\n",
      "Iteration 94, loss = 0.21801686\n",
      "Iteration 95, loss = 0.21742757\n",
      "Iteration 96, loss = 0.21674308\n",
      "Iteration 97, loss = 0.21580586\n",
      "Iteration 98, loss = 0.21554721\n",
      "Iteration 99, loss = 0.21474020\n",
      "Iteration 100, loss = 0.21437786\n",
      "Iteration 101, loss = 0.21322594\n",
      "Iteration 102, loss = 0.21288254\n",
      "Iteration 103, loss = 0.21181664\n",
      "Iteration 104, loss = 0.21090463\n",
      "Iteration 105, loss = 0.21040438\n",
      "Iteration 106, loss = 0.20985893\n",
      "Iteration 107, loss = 0.20858607\n",
      "Iteration 108, loss = 0.20781116\n",
      "Iteration 109, loss = 0.20751656\n",
      "Iteration 110, loss = 0.20605456\n",
      "Iteration 111, loss = 0.20536441\n",
      "Iteration 112, loss = 0.20458598\n",
      "Iteration 113, loss = 0.20414966\n",
      "Iteration 114, loss = 0.20308096\n",
      "Iteration 115, loss = 0.20220574\n",
      "Iteration 116, loss = 0.20125409\n",
      "Iteration 117, loss = 0.20038296\n",
      "Iteration 118, loss = 0.19935270\n",
      "Iteration 119, loss = 0.19870807\n",
      "Iteration 120, loss = 0.19787619\n",
      "Iteration 121, loss = 0.19690529\n",
      "Iteration 122, loss = 0.19629145\n",
      "Iteration 123, loss = 0.19560146\n",
      "Iteration 124, loss = 0.19458297\n",
      "Iteration 125, loss = 0.19342841\n",
      "Iteration 126, loss = 0.19293161\n",
      "Iteration 127, loss = 0.19191957\n",
      "Iteration 128, loss = 0.19140481\n",
      "Iteration 129, loss = 0.19051349\n",
      "Iteration 130, loss = 0.18928498\n",
      "Iteration 131, loss = 0.18841576\n",
      "Iteration 132, loss = 0.18790672\n",
      "Iteration 133, loss = 0.18699665\n",
      "Iteration 134, loss = 0.18638138\n",
      "Iteration 135, loss = 0.18534600\n",
      "Iteration 136, loss = 0.18455742\n",
      "Iteration 137, loss = 0.18384919\n",
      "Iteration 138, loss = 0.18319236\n",
      "Iteration 139, loss = 0.18211584\n",
      "Iteration 140, loss = 0.18165823\n",
      "Iteration 141, loss = 0.18081963\n",
      "Iteration 142, loss = 0.18012052\n",
      "Iteration 143, loss = 0.17943708\n",
      "Iteration 144, loss = 0.17921755\n",
      "Iteration 145, loss = 0.17789166\n",
      "Iteration 146, loss = 0.17724269\n",
      "Iteration 147, loss = 0.17655768\n",
      "Iteration 148, loss = 0.17581194\n",
      "Iteration 149, loss = 0.17494881\n",
      "Iteration 150, loss = 0.17450902\n",
      "Iteration 151, loss = 0.17379553\n",
      "Iteration 152, loss = 0.17335267\n",
      "Iteration 153, loss = 0.17227600\n",
      "Iteration 154, loss = 0.17179301\n",
      "Iteration 155, loss = 0.17126329\n",
      "Iteration 156, loss = 0.17051967\n",
      "Iteration 157, loss = 0.16998011\n",
      "Iteration 158, loss = 0.16935834\n",
      "Iteration 159, loss = 0.16880000\n",
      "Iteration 160, loss = 0.16843452\n",
      "Iteration 161, loss = 0.16771429\n",
      "Iteration 162, loss = 0.16711358\n",
      "Iteration 163, loss = 0.16659880\n",
      "Iteration 164, loss = 0.16589392\n",
      "Iteration 165, loss = 0.16545599\n",
      "Iteration 166, loss = 0.16505911\n",
      "Iteration 167, loss = 0.16434663\n",
      "Iteration 168, loss = 0.16390804\n",
      "Iteration 169, loss = 0.16333120\n",
      "Iteration 170, loss = 0.16259874\n",
      "Iteration 171, loss = 0.16219273\n",
      "Iteration 172, loss = 0.16164993\n",
      "Iteration 173, loss = 0.16121417\n",
      "Iteration 174, loss = 0.16089432\n",
      "Iteration 175, loss = 0.16026016\n",
      "Iteration 176, loss = 0.15967339\n",
      "Iteration 177, loss = 0.15922832\n",
      "Iteration 178, loss = 0.15864253\n",
      "Iteration 179, loss = 0.15825629\n",
      "Iteration 180, loss = 0.15777962\n",
      "Iteration 181, loss = 0.15702579\n",
      "Iteration 182, loss = 0.15652449\n",
      "Iteration 183, loss = 0.15591585\n",
      "Iteration 184, loss = 0.15585268\n",
      "Iteration 185, loss = 0.15509701\n",
      "Iteration 186, loss = 0.15446893\n",
      "Iteration 187, loss = 0.15436188\n",
      "Iteration 188, loss = 0.15365744\n",
      "Iteration 189, loss = 0.15301283\n",
      "Iteration 190, loss = 0.15276534\n",
      "Iteration 191, loss = 0.15231278\n",
      "Iteration 192, loss = 0.15188526\n",
      "Iteration 193, loss = 0.15125947\n",
      "Iteration 194, loss = 0.15064323\n",
      "Iteration 195, loss = 0.15009945\n",
      "Iteration 196, loss = 0.14964391\n",
      "Iteration 197, loss = 0.14946802\n",
      "Iteration 198, loss = 0.14871074\n",
      "Iteration 199, loss = 0.14852044\n",
      "Iteration 200, loss = 0.14783194\n",
      "Iteration 1, loss = 0.55137526\n",
      "Iteration 2, loss = 0.41965986\n",
      "Iteration 3, loss = 0.39008019\n",
      "Iteration 4, loss = 0.36862674\n",
      "Iteration 5, loss = 0.35100846\n",
      "Iteration 6, loss = 0.33580957\n",
      "Iteration 7, loss = 0.32291240\n",
      "Iteration 8, loss = 0.31208968\n",
      "Iteration 9, loss = 0.30315491\n",
      "Iteration 10, loss = 0.29570469\n",
      "Iteration 11, loss = 0.28960961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.28465027\n",
      "Iteration 13, loss = 0.28017703\n",
      "Iteration 14, loss = 0.27654149\n",
      "Iteration 15, loss = 0.27313933\n",
      "Iteration 16, loss = 0.27017541\n",
      "Iteration 17, loss = 0.26789826\n",
      "Iteration 18, loss = 0.26556628\n",
      "Iteration 19, loss = 0.26345731\n",
      "Iteration 20, loss = 0.26142517\n",
      "Iteration 21, loss = 0.25963464\n",
      "Iteration 22, loss = 0.25792653\n",
      "Iteration 23, loss = 0.25653937\n",
      "Iteration 24, loss = 0.25483214\n",
      "Iteration 25, loss = 0.25321645\n",
      "Iteration 26, loss = 0.25178726\n",
      "Iteration 27, loss = 0.25079232\n",
      "Iteration 28, loss = 0.24919703\n",
      "Iteration 29, loss = 0.24786410\n",
      "Iteration 30, loss = 0.24663338\n",
      "Iteration 31, loss = 0.24529943\n",
      "Iteration 32, loss = 0.24408110\n",
      "Iteration 33, loss = 0.24294235\n",
      "Iteration 34, loss = 0.24201055\n",
      "Iteration 35, loss = 0.24091385\n",
      "Iteration 36, loss = 0.24001486\n",
      "Iteration 37, loss = 0.23904919\n",
      "Iteration 38, loss = 0.23814991\n",
      "Iteration 39, loss = 0.23724966\n",
      "Iteration 40, loss = 0.23678873\n",
      "Iteration 41, loss = 0.23595956\n",
      "Iteration 42, loss = 0.23516051\n",
      "Iteration 43, loss = 0.23443686\n",
      "Iteration 44, loss = 0.23378253\n",
      "Iteration 45, loss = 0.23334830\n",
      "Iteration 46, loss = 0.23268676\n",
      "Iteration 47, loss = 0.23241103\n",
      "Iteration 48, loss = 0.23183444\n",
      "Iteration 49, loss = 0.23149434\n",
      "Iteration 50, loss = 0.23120359\n",
      "Iteration 51, loss = 0.23081704\n",
      "Iteration 52, loss = 0.23050359\n",
      "Iteration 53, loss = 0.22985607\n",
      "Iteration 54, loss = 0.22966651\n",
      "Iteration 55, loss = 0.22920583\n",
      "Iteration 56, loss = 0.22927431\n",
      "Iteration 57, loss = 0.22876022\n",
      "Iteration 58, loss = 0.22847329\n",
      "Iteration 59, loss = 0.22814061\n",
      "Iteration 60, loss = 0.22790647\n",
      "Iteration 61, loss = 0.22764711\n",
      "Iteration 62, loss = 0.22733234\n",
      "Iteration 63, loss = 0.22741836\n",
      "Iteration 64, loss = 0.22671707\n",
      "Iteration 65, loss = 0.22660007\n",
      "Iteration 66, loss = 0.22632051\n",
      "Iteration 67, loss = 0.22614992\n",
      "Iteration 68, loss = 0.22587984\n",
      "Iteration 69, loss = 0.22578079\n",
      "Iteration 70, loss = 0.22538562\n",
      "Iteration 71, loss = 0.22550644\n",
      "Iteration 72, loss = 0.22474453\n",
      "Iteration 73, loss = 0.22426541\n",
      "Iteration 74, loss = 0.22382187\n",
      "Iteration 75, loss = 0.22408040\n",
      "Iteration 76, loss = 0.22314294\n",
      "Iteration 77, loss = 0.22288657\n",
      "Iteration 78, loss = 0.22272298\n",
      "Iteration 79, loss = 0.22253714\n",
      "Iteration 80, loss = 0.22169678\n",
      "Iteration 81, loss = 0.22135898\n",
      "Iteration 82, loss = 0.22091478\n",
      "Iteration 83, loss = 0.22058153\n",
      "Iteration 84, loss = 0.22027336\n",
      "Iteration 85, loss = 0.21972278\n",
      "Iteration 86, loss = 0.21937703\n",
      "Iteration 87, loss = 0.21921892\n",
      "Iteration 88, loss = 0.21822011\n",
      "Iteration 89, loss = 0.21800363\n",
      "Iteration 90, loss = 0.21744455\n",
      "Iteration 91, loss = 0.21684151\n",
      "Iteration 92, loss = 0.21618543\n",
      "Iteration 93, loss = 0.21582283\n",
      "Iteration 94, loss = 0.21534872\n",
      "Iteration 95, loss = 0.21450411\n",
      "Iteration 96, loss = 0.21402356\n",
      "Iteration 97, loss = 0.21340765\n",
      "Iteration 98, loss = 0.21310514\n",
      "Iteration 99, loss = 0.21260081\n",
      "Iteration 100, loss = 0.21206837\n",
      "Iteration 101, loss = 0.21115132\n",
      "Iteration 102, loss = 0.21045150\n",
      "Iteration 103, loss = 0.20992272\n",
      "Iteration 104, loss = 0.20929490\n",
      "Iteration 105, loss = 0.20904163\n",
      "Iteration 106, loss = 0.20806654\n",
      "Iteration 107, loss = 0.20751010\n",
      "Iteration 108, loss = 0.20667475\n",
      "Iteration 109, loss = 0.20643343\n",
      "Iteration 110, loss = 0.20509775\n",
      "Iteration 111, loss = 0.20456929\n",
      "Iteration 112, loss = 0.20371259\n",
      "Iteration 113, loss = 0.20319035\n",
      "Iteration 114, loss = 0.20244558\n",
      "Iteration 115, loss = 0.20183903\n",
      "Iteration 116, loss = 0.20130873\n",
      "Iteration 117, loss = 0.20039648\n",
      "Iteration 118, loss = 0.19971947\n",
      "Iteration 119, loss = 0.19897283\n",
      "Iteration 120, loss = 0.19832604\n",
      "Iteration 121, loss = 0.19747146\n",
      "Iteration 122, loss = 0.19704773\n",
      "Iteration 123, loss = 0.19624440\n",
      "Iteration 124, loss = 0.19547206\n",
      "Iteration 125, loss = 0.19450128\n",
      "Iteration 126, loss = 0.19393853\n",
      "Iteration 127, loss = 0.19328796\n",
      "Iteration 128, loss = 0.19249131\n",
      "Iteration 129, loss = 0.19191479\n",
      "Iteration 130, loss = 0.19078690\n",
      "Iteration 131, loss = 0.19025214\n",
      "Iteration 132, loss = 0.18966730\n",
      "Iteration 133, loss = 0.18866792\n",
      "Iteration 134, loss = 0.18837251\n",
      "Iteration 135, loss = 0.18736361\n",
      "Iteration 136, loss = 0.18660904\n",
      "Iteration 137, loss = 0.18606947\n",
      "Iteration 138, loss = 0.18547083\n",
      "Iteration 139, loss = 0.18427303\n",
      "Iteration 140, loss = 0.18379680\n",
      "Iteration 141, loss = 0.18325582\n",
      "Iteration 142, loss = 0.18276262\n",
      "Iteration 143, loss = 0.18184120\n",
      "Iteration 144, loss = 0.18162059\n",
      "Iteration 145, loss = 0.18034952\n",
      "Iteration 146, loss = 0.17946560\n",
      "Iteration 147, loss = 0.17892317\n",
      "Iteration 148, loss = 0.17841832\n",
      "Iteration 149, loss = 0.17772216\n",
      "Iteration 150, loss = 0.17689657\n",
      "Iteration 151, loss = 0.17614528\n",
      "Iteration 152, loss = 0.17565153\n",
      "Iteration 153, loss = 0.17452300\n",
      "Iteration 154, loss = 0.17390551\n",
      "Iteration 155, loss = 0.17330255\n",
      "Iteration 156, loss = 0.17264885\n",
      "Iteration 157, loss = 0.17212094\n",
      "Iteration 158, loss = 0.17149593\n",
      "Iteration 159, loss = 0.17092072\n",
      "Iteration 160, loss = 0.17013416\n",
      "Iteration 161, loss = 0.16967255\n",
      "Iteration 162, loss = 0.16867765\n",
      "Iteration 163, loss = 0.16820276\n",
      "Iteration 164, loss = 0.16786541\n",
      "Iteration 165, loss = 0.16728859\n",
      "Iteration 166, loss = 0.16675112\n",
      "Iteration 167, loss = 0.16613481\n",
      "Iteration 168, loss = 0.16533911\n",
      "Iteration 169, loss = 0.16506331\n",
      "Iteration 170, loss = 0.16392275\n",
      "Iteration 171, loss = 0.16341280\n",
      "Iteration 172, loss = 0.16276056\n",
      "Iteration 173, loss = 0.16220798\n",
      "Iteration 174, loss = 0.16155620\n",
      "Iteration 175, loss = 0.16110930\n",
      "Iteration 176, loss = 0.16043212\n",
      "Iteration 177, loss = 0.15967115\n",
      "Iteration 178, loss = 0.15910973\n",
      "Iteration 179, loss = 0.15876431\n",
      "Iteration 180, loss = 0.15823004\n",
      "Iteration 181, loss = 0.15726817\n",
      "Iteration 182, loss = 0.15672976\n",
      "Iteration 183, loss = 0.15614458\n",
      "Iteration 184, loss = 0.15594917\n",
      "Iteration 185, loss = 0.15513243\n",
      "Iteration 186, loss = 0.15441321\n",
      "Iteration 187, loss = 0.15426292\n",
      "Iteration 188, loss = 0.15379372\n",
      "Iteration 189, loss = 0.15320788\n",
      "Iteration 190, loss = 0.15226658\n",
      "Iteration 191, loss = 0.15186290\n",
      "Iteration 192, loss = 0.15153785\n",
      "Iteration 193, loss = 0.15095795\n",
      "Iteration 194, loss = 0.15025244\n",
      "Iteration 195, loss = 0.14951901\n",
      "Iteration 196, loss = 0.14907160\n",
      "Iteration 197, loss = 0.14867427\n",
      "Iteration 198, loss = 0.14778458\n",
      "Iteration 199, loss = 0.14734752\n",
      "Iteration 200, loss = 0.14678662\n",
      "Iteration 1, loss = 0.55040160\n",
      "Iteration 2, loss = 0.41954209\n",
      "Iteration 3, loss = 0.39029232\n",
      "Iteration 4, loss = 0.36913277\n",
      "Iteration 5, loss = 0.35106183\n",
      "Iteration 6, loss = 0.33571742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.32264345\n",
      "Iteration 8, loss = 0.31151933\n",
      "Iteration 9, loss = 0.30213239\n",
      "Iteration 10, loss = 0.29433675\n",
      "Iteration 11, loss = 0.28778481\n",
      "Iteration 12, loss = 0.28233693\n",
      "Iteration 13, loss = 0.27764365\n",
      "Iteration 14, loss = 0.27379739\n",
      "Iteration 15, loss = 0.26991901\n",
      "Iteration 16, loss = 0.26687784\n",
      "Iteration 17, loss = 0.26426078\n",
      "Iteration 18, loss = 0.26190141\n",
      "Iteration 19, loss = 0.25962050\n",
      "Iteration 20, loss = 0.25739215\n",
      "Iteration 21, loss = 0.25549291\n",
      "Iteration 22, loss = 0.25361480\n",
      "Iteration 23, loss = 0.25220264\n",
      "Iteration 24, loss = 0.25035306\n",
      "Iteration 25, loss = 0.24874222\n",
      "Iteration 26, loss = 0.24738519\n",
      "Iteration 27, loss = 0.24639560\n",
      "Iteration 28, loss = 0.24454367\n",
      "Iteration 29, loss = 0.24340117\n",
      "Iteration 30, loss = 0.24231763\n",
      "Iteration 31, loss = 0.24097651\n",
      "Iteration 32, loss = 0.23958749\n",
      "Iteration 33, loss = 0.23868620\n",
      "Iteration 34, loss = 0.23754205\n",
      "Iteration 35, loss = 0.23672907\n",
      "Iteration 36, loss = 0.23557238\n",
      "Iteration 37, loss = 0.23473716\n",
      "Iteration 38, loss = 0.23399812\n",
      "Iteration 39, loss = 0.23332906\n",
      "Iteration 40, loss = 0.23277320\n",
      "Iteration 41, loss = 0.23171210\n",
      "Iteration 42, loss = 0.23113978\n",
      "Iteration 43, loss = 0.23052316\n",
      "Iteration 44, loss = 0.23002472\n",
      "Iteration 45, loss = 0.22949685\n",
      "Iteration 46, loss = 0.22896487\n",
      "Iteration 47, loss = 0.22852011\n",
      "Iteration 48, loss = 0.22827067\n",
      "Iteration 49, loss = 0.22781524\n",
      "Iteration 50, loss = 0.22744066\n",
      "Iteration 51, loss = 0.22723159\n",
      "Iteration 52, loss = 0.22684393\n",
      "Iteration 53, loss = 0.22641157\n",
      "Iteration 54, loss = 0.22611634\n",
      "Iteration 55, loss = 0.22574184\n",
      "Iteration 56, loss = 0.22565356\n",
      "Iteration 57, loss = 0.22517374\n",
      "Iteration 58, loss = 0.22519109\n",
      "Iteration 59, loss = 0.22474147\n",
      "Iteration 60, loss = 0.22465778\n",
      "Iteration 61, loss = 0.22450391\n",
      "Iteration 62, loss = 0.22412894\n",
      "Iteration 63, loss = 0.22388009\n",
      "Iteration 64, loss = 0.22369148\n",
      "Iteration 65, loss = 0.22363321\n",
      "Iteration 66, loss = 0.22347241\n",
      "Iteration 67, loss = 0.22295916\n",
      "Iteration 68, loss = 0.22277474\n",
      "Iteration 69, loss = 0.22246723\n",
      "Iteration 70, loss = 0.22250611\n",
      "Iteration 71, loss = 0.22210935\n",
      "Iteration 72, loss = 0.22164194\n",
      "Iteration 73, loss = 0.22141953\n",
      "Iteration 74, loss = 0.22092919\n",
      "Iteration 75, loss = 0.22097693\n",
      "Iteration 76, loss = 0.22041817\n",
      "Iteration 77, loss = 0.22017435\n",
      "Iteration 78, loss = 0.21985499\n",
      "Iteration 79, loss = 0.21940558\n",
      "Iteration 80, loss = 0.21916060\n",
      "Iteration 81, loss = 0.21883215\n",
      "Iteration 82, loss = 0.21836823\n",
      "Iteration 83, loss = 0.21767269\n",
      "Iteration 84, loss = 0.21763307\n",
      "Iteration 85, loss = 0.21698022\n",
      "Iteration 86, loss = 0.21686621\n",
      "Iteration 87, loss = 0.21619336\n",
      "Iteration 88, loss = 0.21570919\n",
      "Iteration 89, loss = 0.21557289\n",
      "Iteration 90, loss = 0.21449036\n",
      "Iteration 91, loss = 0.21426350\n",
      "Iteration 92, loss = 0.21393296\n",
      "Iteration 93, loss = 0.21319166\n",
      "Iteration 94, loss = 0.21323255\n",
      "Iteration 95, loss = 0.21231200\n",
      "Iteration 96, loss = 0.21161791\n",
      "Iteration 97, loss = 0.21094343\n",
      "Iteration 98, loss = 0.21036927\n",
      "Iteration 99, loss = 0.20989234\n",
      "Iteration 100, loss = 0.20939900\n",
      "Iteration 101, loss = 0.20864375\n",
      "Iteration 102, loss = 0.20786207\n",
      "Iteration 103, loss = 0.20751474\n",
      "Iteration 104, loss = 0.20692315\n",
      "Iteration 105, loss = 0.20621662\n",
      "Iteration 106, loss = 0.20521944\n",
      "Iteration 107, loss = 0.20484666\n",
      "Iteration 108, loss = 0.20428383\n",
      "Iteration 109, loss = 0.20336293\n",
      "Iteration 110, loss = 0.20251862\n",
      "Iteration 111, loss = 0.20199098\n",
      "Iteration 112, loss = 0.20105489\n",
      "Iteration 113, loss = 0.20009550\n",
      "Iteration 114, loss = 0.19944753\n",
      "Iteration 115, loss = 0.19897202\n",
      "Iteration 116, loss = 0.19833252\n",
      "Iteration 117, loss = 0.19724549\n",
      "Iteration 118, loss = 0.19660400\n",
      "Iteration 119, loss = 0.19591386\n",
      "Iteration 120, loss = 0.19492697\n",
      "Iteration 121, loss = 0.19443863\n",
      "Iteration 122, loss = 0.19344010\n",
      "Iteration 123, loss = 0.19284099\n",
      "Iteration 124, loss = 0.19195823\n",
      "Iteration 125, loss = 0.19104521\n",
      "Iteration 126, loss = 0.19056683\n",
      "Iteration 127, loss = 0.18976113\n",
      "Iteration 128, loss = 0.18885234\n",
      "Iteration 129, loss = 0.18812668\n",
      "Iteration 130, loss = 0.18723756\n",
      "Iteration 131, loss = 0.18682605\n",
      "Iteration 132, loss = 0.18604181\n",
      "Iteration 133, loss = 0.18525459\n",
      "Iteration 134, loss = 0.18460775\n",
      "Iteration 135, loss = 0.18360810\n",
      "Iteration 136, loss = 0.18294045\n",
      "Iteration 137, loss = 0.18177784\n",
      "Iteration 138, loss = 0.18150216\n",
      "Iteration 139, loss = 0.18044413\n",
      "Iteration 140, loss = 0.17990787\n",
      "Iteration 141, loss = 0.17938400\n",
      "Iteration 142, loss = 0.17854008\n",
      "Iteration 143, loss = 0.17792414\n",
      "Iteration 144, loss = 0.17753299\n",
      "Iteration 145, loss = 0.17666468\n",
      "Iteration 146, loss = 0.17585906\n",
      "Iteration 147, loss = 0.17488142\n",
      "Iteration 148, loss = 0.17441404\n",
      "Iteration 149, loss = 0.17394273\n",
      "Iteration 150, loss = 0.17314392\n",
      "Iteration 151, loss = 0.17236459\n",
      "Iteration 152, loss = 0.17195016\n",
      "Iteration 153, loss = 0.17114186\n",
      "Iteration 154, loss = 0.17035894\n",
      "Iteration 155, loss = 0.16984019\n",
      "Iteration 156, loss = 0.16912155\n",
      "Iteration 157, loss = 0.16875475\n",
      "Iteration 158, loss = 0.16822223\n",
      "Iteration 159, loss = 0.16760181\n",
      "Iteration 160, loss = 0.16683349\n",
      "Iteration 161, loss = 0.16637853\n",
      "Iteration 162, loss = 0.16573128\n",
      "Iteration 163, loss = 0.16528688\n",
      "Iteration 164, loss = 0.16506540\n",
      "Iteration 165, loss = 0.16433150\n",
      "Iteration 166, loss = 0.16396368\n",
      "Iteration 167, loss = 0.16364735\n",
      "Iteration 168, loss = 0.16265403\n",
      "Iteration 169, loss = 0.16246677\n",
      "Iteration 170, loss = 0.16174299\n",
      "Iteration 171, loss = 0.16099178\n",
      "Iteration 172, loss = 0.16076334\n",
      "Iteration 173, loss = 0.16020667\n",
      "Iteration 174, loss = 0.15942837\n",
      "Iteration 175, loss = 0.15967345\n",
      "Iteration 176, loss = 0.15876462\n",
      "Iteration 177, loss = 0.15793694\n",
      "Iteration 178, loss = 0.15763623\n",
      "Iteration 179, loss = 0.15735835\n",
      "Iteration 180, loss = 0.15695925\n",
      "Iteration 181, loss = 0.15609810\n",
      "Iteration 182, loss = 0.15541634\n",
      "Iteration 183, loss = 0.15520724\n",
      "Iteration 184, loss = 0.15485317\n",
      "Iteration 185, loss = 0.15435996\n",
      "Iteration 186, loss = 0.15371732\n",
      "Iteration 187, loss = 0.15339918\n",
      "Iteration 188, loss = 0.15315991\n",
      "Iteration 189, loss = 0.15291400\n",
      "Iteration 190, loss = 0.15190872\n",
      "Iteration 191, loss = 0.15146381\n",
      "Iteration 192, loss = 0.15114918\n",
      "Iteration 193, loss = 0.15078414\n",
      "Iteration 194, loss = 0.15018301\n",
      "Iteration 195, loss = 0.14953585\n",
      "Iteration 196, loss = 0.14921587\n",
      "Iteration 197, loss = 0.14873231\n",
      "Iteration 198, loss = 0.14802694\n",
      "Iteration 199, loss = 0.14781182\n",
      "Iteration 200, loss = 0.14756995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.58523567\n",
      "Iteration 2, loss = 0.46127749\n",
      "Iteration 3, loss = 0.45171443\n",
      "Iteration 4, loss = 0.45039341\n",
      "Iteration 5, loss = 0.44921194\n",
      "Iteration 6, loss = 0.44808802\n",
      "Iteration 7, loss = 0.44691545\n",
      "Iteration 8, loss = 0.44579091\n",
      "Iteration 9, loss = 0.44467065\n",
      "Iteration 10, loss = 0.44356352\n",
      "Iteration 11, loss = 0.44239826\n",
      "Iteration 12, loss = 0.44136226\n",
      "Iteration 13, loss = 0.44018194\n",
      "Iteration 14, loss = 0.43913207\n",
      "Iteration 15, loss = 0.43800279\n",
      "Iteration 16, loss = 0.43690682\n",
      "Iteration 17, loss = 0.43583948\n",
      "Iteration 18, loss = 0.43470876\n",
      "Iteration 19, loss = 0.43370254\n",
      "Iteration 20, loss = 0.43253598\n",
      "Iteration 21, loss = 0.43144047\n",
      "Iteration 22, loss = 0.43036923\n",
      "Iteration 23, loss = 0.42924995\n",
      "Iteration 24, loss = 0.42814489\n",
      "Iteration 25, loss = 0.42710785\n",
      "Iteration 26, loss = 0.42605620\n",
      "Iteration 27, loss = 0.42486354\n",
      "Iteration 28, loss = 0.42374932\n",
      "Iteration 29, loss = 0.42262193\n",
      "Iteration 30, loss = 0.42153926\n",
      "Iteration 31, loss = 0.42036607\n",
      "Iteration 32, loss = 0.41926031\n",
      "Iteration 33, loss = 0.41821412\n",
      "Iteration 34, loss = 0.41698299\n",
      "Iteration 35, loss = 0.41585315\n",
      "Iteration 36, loss = 0.41469930\n",
      "Iteration 37, loss = 0.41353175\n",
      "Iteration 38, loss = 0.41238878\n",
      "Iteration 39, loss = 0.41125539\n",
      "Iteration 40, loss = 0.41007222\n",
      "Iteration 41, loss = 0.40888429\n",
      "Iteration 42, loss = 0.40769905\n",
      "Iteration 43, loss = 0.40649301\n",
      "Iteration 44, loss = 0.40527762\n",
      "Iteration 45, loss = 0.40408024\n",
      "Iteration 46, loss = 0.40287599\n",
      "Iteration 47, loss = 0.40164690\n",
      "Iteration 48, loss = 0.40042757\n",
      "Iteration 49, loss = 0.39924697\n",
      "Iteration 50, loss = 0.39797053\n",
      "Iteration 51, loss = 0.39671916\n",
      "Iteration 52, loss = 0.39546519\n",
      "Iteration 53, loss = 0.39422463\n",
      "Iteration 54, loss = 0.39293552\n",
      "Iteration 55, loss = 0.39169790\n",
      "Iteration 56, loss = 0.39040532\n",
      "Iteration 57, loss = 0.38918183\n",
      "Iteration 58, loss = 0.38785404\n",
      "Iteration 59, loss = 0.38658292\n",
      "Iteration 60, loss = 0.38528676\n",
      "Iteration 61, loss = 0.38408391\n",
      "Iteration 62, loss = 0.38272369\n",
      "Iteration 63, loss = 0.38142659\n",
      "Iteration 64, loss = 0.38008638\n",
      "Iteration 65, loss = 0.37879923\n",
      "Iteration 66, loss = 0.37750820\n",
      "Iteration 67, loss = 0.37618865\n",
      "Iteration 68, loss = 0.37489898\n",
      "Iteration 69, loss = 0.37355466\n",
      "Iteration 70, loss = 0.37225574\n",
      "Iteration 71, loss = 0.37094772\n",
      "Iteration 72, loss = 0.36963796\n",
      "Iteration 73, loss = 0.36831064\n",
      "Iteration 74, loss = 0.36697732\n",
      "Iteration 75, loss = 0.36570840\n",
      "Iteration 76, loss = 0.36441023\n",
      "Iteration 77, loss = 0.36309920\n",
      "Iteration 78, loss = 0.36179007\n",
      "Iteration 79, loss = 0.36045224\n",
      "Iteration 80, loss = 0.35914495\n",
      "Iteration 81, loss = 0.35788442\n",
      "Iteration 82, loss = 0.35657539\n",
      "Iteration 83, loss = 0.35528684\n",
      "Iteration 84, loss = 0.35403091\n",
      "Iteration 85, loss = 0.35276396\n",
      "Iteration 86, loss = 0.35146993\n",
      "Iteration 87, loss = 0.35026181\n",
      "Iteration 88, loss = 0.34898946\n",
      "Iteration 89, loss = 0.34773410\n",
      "Iteration 90, loss = 0.34653914\n",
      "Iteration 91, loss = 0.34528129\n",
      "Iteration 92, loss = 0.34404481\n",
      "Iteration 93, loss = 0.34284154\n",
      "Iteration 94, loss = 0.34164137\n",
      "Iteration 95, loss = 0.34043447\n",
      "Iteration 96, loss = 0.33930397\n",
      "Iteration 97, loss = 0.33811856\n",
      "Iteration 98, loss = 0.33694176\n",
      "Iteration 99, loss = 0.33581961\n",
      "Iteration 100, loss = 0.33465419\n",
      "Iteration 101, loss = 0.33354724\n",
      "Iteration 102, loss = 0.33244906\n",
      "Iteration 103, loss = 0.33127689\n",
      "Iteration 104, loss = 0.33020147\n",
      "Iteration 105, loss = 0.32914944\n",
      "Iteration 106, loss = 0.32814364\n",
      "Iteration 107, loss = 0.32700393\n",
      "Iteration 108, loss = 0.32599195\n",
      "Iteration 109, loss = 0.32490198\n",
      "Iteration 110, loss = 0.32390211\n",
      "Iteration 111, loss = 0.32289394\n",
      "Iteration 112, loss = 0.32189602\n",
      "Iteration 113, loss = 0.32095062\n",
      "Iteration 114, loss = 0.31993990\n",
      "Iteration 115, loss = 0.31901927\n",
      "Iteration 116, loss = 0.31801185\n",
      "Iteration 117, loss = 0.31711538\n",
      "Iteration 118, loss = 0.31617538\n",
      "Iteration 119, loss = 0.31525338\n",
      "Iteration 120, loss = 0.31438431\n",
      "Iteration 121, loss = 0.31349316\n",
      "Iteration 122, loss = 0.31263384\n",
      "Iteration 123, loss = 0.31177208\n",
      "Iteration 124, loss = 0.31092499\n",
      "Iteration 125, loss = 0.31007785\n",
      "Iteration 126, loss = 0.30927496\n",
      "Iteration 127, loss = 0.30847511\n",
      "Iteration 128, loss = 0.30766696\n",
      "Iteration 129, loss = 0.30689726\n",
      "Iteration 130, loss = 0.30611822\n",
      "Iteration 131, loss = 0.30536258\n",
      "Iteration 132, loss = 0.30460362\n",
      "Iteration 133, loss = 0.30387028\n",
      "Iteration 134, loss = 0.30316985\n",
      "Iteration 135, loss = 0.30243652\n",
      "Iteration 136, loss = 0.30173453\n",
      "Iteration 137, loss = 0.30104661\n",
      "Iteration 138, loss = 0.30036478\n",
      "Iteration 139, loss = 0.29972339\n",
      "Iteration 140, loss = 0.29904886\n",
      "Iteration 141, loss = 0.29838256\n",
      "Iteration 142, loss = 0.29779701\n",
      "Iteration 143, loss = 0.29711176\n",
      "Iteration 144, loss = 0.29651466\n",
      "Iteration 145, loss = 0.29589839\n",
      "Iteration 146, loss = 0.29533467\n",
      "Iteration 147, loss = 0.29469459\n",
      "Iteration 148, loss = 0.29414986\n",
      "Iteration 149, loss = 0.29358031\n",
      "Iteration 150, loss = 0.29300579\n",
      "Iteration 151, loss = 0.29249112\n",
      "Iteration 152, loss = 0.29191251\n",
      "Iteration 153, loss = 0.29141435\n",
      "Iteration 154, loss = 0.29087847\n",
      "Iteration 155, loss = 0.29033963\n",
      "Iteration 156, loss = 0.28983220\n",
      "Iteration 157, loss = 0.28933658\n",
      "Iteration 158, loss = 0.28885412\n",
      "Iteration 159, loss = 0.28836298\n",
      "Iteration 160, loss = 0.28788620\n",
      "Iteration 161, loss = 0.28740293\n",
      "Iteration 162, loss = 0.28695747\n",
      "Iteration 163, loss = 0.28648754\n",
      "Iteration 164, loss = 0.28605281\n",
      "Iteration 165, loss = 0.28561541\n",
      "Iteration 166, loss = 0.28518484\n",
      "Iteration 167, loss = 0.28477451\n",
      "Iteration 168, loss = 0.28435612\n",
      "Iteration 169, loss = 0.28391333\n",
      "Iteration 170, loss = 0.28352687\n",
      "Iteration 171, loss = 0.28312678\n",
      "Iteration 172, loss = 0.28273119\n",
      "Iteration 173, loss = 0.28234466\n",
      "Iteration 174, loss = 0.28194590\n",
      "Iteration 175, loss = 0.28163518\n",
      "Iteration 176, loss = 0.28123744\n",
      "Iteration 177, loss = 0.28083365\n",
      "Iteration 178, loss = 0.28047988\n",
      "Iteration 179, loss = 0.28013856\n",
      "Iteration 180, loss = 0.27978168\n",
      "Iteration 181, loss = 0.27944022\n",
      "Iteration 182, loss = 0.27911644\n",
      "Iteration 183, loss = 0.27878295\n",
      "Iteration 184, loss = 0.27846793\n",
      "Iteration 185, loss = 0.27812769\n",
      "Iteration 186, loss = 0.27781170\n",
      "Iteration 187, loss = 0.27749268\n",
      "Iteration 188, loss = 0.27718242\n",
      "Iteration 189, loss = 0.27689719\n",
      "Iteration 190, loss = 0.27656812\n",
      "Iteration 191, loss = 0.27627326\n",
      "Iteration 192, loss = 0.27602869\n",
      "Iteration 193, loss = 0.27571642\n",
      "Iteration 194, loss = 0.27542496\n",
      "Iteration 195, loss = 0.27514717\n",
      "Iteration 196, loss = 0.27486522\n",
      "Iteration 197, loss = 0.27460450\n",
      "Iteration 198, loss = 0.27433277\n",
      "Iteration 199, loss = 0.27408510\n",
      "Iteration 200, loss = 0.27383249\n",
      "Iteration 1, loss = 0.58528018\n",
      "Iteration 2, loss = 0.46121313\n",
      "Iteration 3, loss = 0.45181791\n",
      "Iteration 4, loss = 0.45054511\n",
      "Iteration 5, loss = 0.44930256\n",
      "Iteration 6, loss = 0.44812612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.44693814\n",
      "Iteration 8, loss = 0.44576260\n",
      "Iteration 9, loss = 0.44468032\n",
      "Iteration 10, loss = 0.44358438\n",
      "Iteration 11, loss = 0.44230392\n",
      "Iteration 12, loss = 0.44126542\n",
      "Iteration 13, loss = 0.44005206\n",
      "Iteration 14, loss = 0.43893150\n",
      "Iteration 15, loss = 0.43778028\n",
      "Iteration 16, loss = 0.43667196\n",
      "Iteration 17, loss = 0.43554194\n",
      "Iteration 18, loss = 0.43441159\n",
      "Iteration 19, loss = 0.43330551\n",
      "Iteration 20, loss = 0.43216524\n",
      "Iteration 21, loss = 0.43106885\n",
      "Iteration 22, loss = 0.42991431\n",
      "Iteration 23, loss = 0.42879246\n",
      "Iteration 24, loss = 0.42766837\n",
      "Iteration 25, loss = 0.42657403\n",
      "Iteration 26, loss = 0.42545289\n",
      "Iteration 27, loss = 0.42430674\n",
      "Iteration 28, loss = 0.42310641\n",
      "Iteration 29, loss = 0.42198039\n",
      "Iteration 30, loss = 0.42084246\n",
      "Iteration 31, loss = 0.41966953\n",
      "Iteration 32, loss = 0.41853179\n",
      "Iteration 33, loss = 0.41741613\n",
      "Iteration 34, loss = 0.41616304\n",
      "Iteration 35, loss = 0.41499362\n",
      "Iteration 36, loss = 0.41381855\n",
      "Iteration 37, loss = 0.41262589\n",
      "Iteration 38, loss = 0.41145635\n",
      "Iteration 39, loss = 0.41026829\n",
      "Iteration 40, loss = 0.40905480\n",
      "Iteration 41, loss = 0.40784199\n",
      "Iteration 42, loss = 0.40669304\n",
      "Iteration 43, loss = 0.40537323\n",
      "Iteration 44, loss = 0.40421745\n",
      "Iteration 45, loss = 0.40292864\n",
      "Iteration 46, loss = 0.40163273\n",
      "Iteration 47, loss = 0.40039313\n",
      "Iteration 48, loss = 0.39911058\n",
      "Iteration 49, loss = 0.39786357\n",
      "Iteration 50, loss = 0.39660603\n",
      "Iteration 51, loss = 0.39531825\n",
      "Iteration 52, loss = 0.39400997\n",
      "Iteration 53, loss = 0.39270536\n",
      "Iteration 54, loss = 0.39140164\n",
      "Iteration 55, loss = 0.39013260\n",
      "Iteration 56, loss = 0.38878655\n",
      "Iteration 57, loss = 0.38756445\n",
      "Iteration 58, loss = 0.38621852\n",
      "Iteration 59, loss = 0.38485780\n",
      "Iteration 60, loss = 0.38351566\n",
      "Iteration 61, loss = 0.38222809\n",
      "Iteration 62, loss = 0.38086426\n",
      "Iteration 63, loss = 0.37954500\n",
      "Iteration 64, loss = 0.37817497\n",
      "Iteration 65, loss = 0.37686078\n",
      "Iteration 66, loss = 0.37547809\n",
      "Iteration 67, loss = 0.37416341\n",
      "Iteration 68, loss = 0.37284453\n",
      "Iteration 69, loss = 0.37145531\n",
      "Iteration 70, loss = 0.37012418\n",
      "Iteration 71, loss = 0.36879155\n",
      "Iteration 72, loss = 0.36742315\n",
      "Iteration 73, loss = 0.36611222\n",
      "Iteration 74, loss = 0.36476132\n",
      "Iteration 75, loss = 0.36340803\n",
      "Iteration 76, loss = 0.36206247\n",
      "Iteration 77, loss = 0.36074968\n",
      "Iteration 78, loss = 0.35939847\n",
      "Iteration 79, loss = 0.35803921\n",
      "Iteration 80, loss = 0.35672322\n",
      "Iteration 81, loss = 0.35540523\n",
      "Iteration 82, loss = 0.35411306\n",
      "Iteration 83, loss = 0.35277351\n",
      "Iteration 84, loss = 0.35148658\n",
      "Iteration 85, loss = 0.35020241\n",
      "Iteration 86, loss = 0.34889507\n",
      "Iteration 87, loss = 0.34768988\n",
      "Iteration 88, loss = 0.34634943\n",
      "Iteration 89, loss = 0.34508117\n",
      "Iteration 90, loss = 0.34385524\n",
      "Iteration 91, loss = 0.34258685\n",
      "Iteration 92, loss = 0.34134205\n",
      "Iteration 93, loss = 0.34012265\n",
      "Iteration 94, loss = 0.33889733\n",
      "Iteration 95, loss = 0.33768486\n",
      "Iteration 96, loss = 0.33653379\n",
      "Iteration 97, loss = 0.33535721\n",
      "Iteration 98, loss = 0.33415864\n",
      "Iteration 99, loss = 0.33303801\n",
      "Iteration 100, loss = 0.33182996\n",
      "Iteration 101, loss = 0.33070741\n",
      "Iteration 102, loss = 0.32958560\n",
      "Iteration 103, loss = 0.32846636\n",
      "Iteration 104, loss = 0.32736029\n",
      "Iteration 105, loss = 0.32630694\n",
      "Iteration 106, loss = 0.32527494\n",
      "Iteration 107, loss = 0.32414344\n",
      "Iteration 108, loss = 0.32310544\n",
      "Iteration 109, loss = 0.32205062\n",
      "Iteration 110, loss = 0.32104106\n",
      "Iteration 111, loss = 0.32003404\n",
      "Iteration 112, loss = 0.31902127\n",
      "Iteration 113, loss = 0.31808106\n",
      "Iteration 114, loss = 0.31709622\n",
      "Iteration 115, loss = 0.31611056\n",
      "Iteration 116, loss = 0.31517248\n",
      "Iteration 117, loss = 0.31421375\n",
      "Iteration 118, loss = 0.31330457\n",
      "Iteration 119, loss = 0.31238494\n",
      "Iteration 120, loss = 0.31151489\n",
      "Iteration 121, loss = 0.31061424\n",
      "Iteration 122, loss = 0.30975120\n",
      "Iteration 123, loss = 0.30890705\n",
      "Iteration 124, loss = 0.30805265\n",
      "Iteration 125, loss = 0.30724091\n",
      "Iteration 126, loss = 0.30645003\n",
      "Iteration 127, loss = 0.30561482\n",
      "Iteration 128, loss = 0.30482700\n",
      "Iteration 129, loss = 0.30406181\n",
      "Iteration 130, loss = 0.30327443\n",
      "Iteration 131, loss = 0.30252613\n",
      "Iteration 132, loss = 0.30178006\n",
      "Iteration 133, loss = 0.30106125\n",
      "Iteration 134, loss = 0.30033940\n",
      "Iteration 135, loss = 0.29962742\n",
      "Iteration 136, loss = 0.29892084\n",
      "Iteration 137, loss = 0.29823971\n",
      "Iteration 138, loss = 0.29760999\n",
      "Iteration 139, loss = 0.29690149\n",
      "Iteration 140, loss = 0.29625775\n",
      "Iteration 141, loss = 0.29560854\n",
      "Iteration 142, loss = 0.29496530\n",
      "Iteration 143, loss = 0.29431720\n",
      "Iteration 144, loss = 0.29372887\n",
      "Iteration 145, loss = 0.29312325\n",
      "Iteration 146, loss = 0.29252404\n",
      "Iteration 147, loss = 0.29194985\n",
      "Iteration 148, loss = 0.29140259\n",
      "Iteration 149, loss = 0.29081941\n",
      "Iteration 150, loss = 0.29024988\n",
      "Iteration 151, loss = 0.28971177\n",
      "Iteration 152, loss = 0.28917550\n",
      "Iteration 153, loss = 0.28866445\n",
      "Iteration 154, loss = 0.28812947\n",
      "Iteration 155, loss = 0.28761497\n",
      "Iteration 156, loss = 0.28710513\n",
      "Iteration 157, loss = 0.28662115\n",
      "Iteration 158, loss = 0.28612518\n",
      "Iteration 159, loss = 0.28565313\n",
      "Iteration 160, loss = 0.28520216\n",
      "Iteration 161, loss = 0.28471614\n",
      "Iteration 162, loss = 0.28424369\n",
      "Iteration 163, loss = 0.28379941\n",
      "Iteration 164, loss = 0.28335900\n",
      "Iteration 165, loss = 0.28293800\n",
      "Iteration 166, loss = 0.28249673\n",
      "Iteration 167, loss = 0.28206554\n",
      "Iteration 168, loss = 0.28165169\n",
      "Iteration 169, loss = 0.28124675\n",
      "Iteration 170, loss = 0.28083469\n",
      "Iteration 171, loss = 0.28043084\n",
      "Iteration 172, loss = 0.28005089\n",
      "Iteration 173, loss = 0.27967739\n",
      "Iteration 174, loss = 0.27927879\n",
      "Iteration 175, loss = 0.27891584\n",
      "Iteration 176, loss = 0.27858834\n",
      "Iteration 177, loss = 0.27817325\n",
      "Iteration 178, loss = 0.27781276\n",
      "Iteration 179, loss = 0.27748046\n",
      "Iteration 180, loss = 0.27712585\n",
      "Iteration 181, loss = 0.27678060\n",
      "Iteration 182, loss = 0.27643835\n",
      "Iteration 183, loss = 0.27610851\n",
      "Iteration 184, loss = 0.27578908\n",
      "Iteration 185, loss = 0.27545983\n",
      "Iteration 186, loss = 0.27516135\n",
      "Iteration 187, loss = 0.27483796\n",
      "Iteration 188, loss = 0.27452373\n",
      "Iteration 189, loss = 0.27426024\n",
      "Iteration 190, loss = 0.27391257\n",
      "Iteration 191, loss = 0.27362131\n",
      "Iteration 192, loss = 0.27333409\n",
      "Iteration 193, loss = 0.27304557\n",
      "Iteration 194, loss = 0.27276413\n",
      "Iteration 195, loss = 0.27247125\n",
      "Iteration 196, loss = 0.27219987\n",
      "Iteration 197, loss = 0.27193338\n",
      "Iteration 198, loss = 0.27166971\n",
      "Iteration 199, loss = 0.27139589\n",
      "Iteration 200, loss = 0.27114205\n",
      "Iteration 1, loss = 0.58586111\n",
      "Iteration 2, loss = 0.46113489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.45227479\n",
      "Iteration 4, loss = 0.45107143\n",
      "Iteration 5, loss = 0.44983544\n",
      "Iteration 6, loss = 0.44868253\n",
      "Iteration 7, loss = 0.44753273\n",
      "Iteration 8, loss = 0.44637702\n",
      "Iteration 9, loss = 0.44527894\n",
      "Iteration 10, loss = 0.44416994\n",
      "Iteration 11, loss = 0.44304191\n",
      "Iteration 12, loss = 0.44199592\n",
      "Iteration 13, loss = 0.44083446\n",
      "Iteration 14, loss = 0.43973805\n",
      "Iteration 15, loss = 0.43865971\n",
      "Iteration 16, loss = 0.43755073\n",
      "Iteration 17, loss = 0.43646254\n",
      "Iteration 18, loss = 0.43538052\n",
      "Iteration 19, loss = 0.43428091\n",
      "Iteration 20, loss = 0.43322083\n",
      "Iteration 21, loss = 0.43211851\n",
      "Iteration 22, loss = 0.43108922\n",
      "Iteration 23, loss = 0.42993203\n",
      "Iteration 24, loss = 0.42885693\n",
      "Iteration 25, loss = 0.42779830\n",
      "Iteration 26, loss = 0.42665043\n",
      "Iteration 27, loss = 0.42565081\n",
      "Iteration 28, loss = 0.42443879\n",
      "Iteration 29, loss = 0.42337200\n",
      "Iteration 30, loss = 0.42226672\n",
      "Iteration 31, loss = 0.42114629\n",
      "Iteration 32, loss = 0.42003090\n",
      "Iteration 33, loss = 0.41894929\n",
      "Iteration 34, loss = 0.41774802\n",
      "Iteration 35, loss = 0.41664075\n",
      "Iteration 36, loss = 0.41546381\n",
      "Iteration 37, loss = 0.41435639\n",
      "Iteration 38, loss = 0.41322333\n",
      "Iteration 39, loss = 0.41203518\n",
      "Iteration 40, loss = 0.41089587\n",
      "Iteration 41, loss = 0.40980437\n",
      "Iteration 42, loss = 0.40856252\n",
      "Iteration 43, loss = 0.40734457\n",
      "Iteration 44, loss = 0.40616745\n",
      "Iteration 45, loss = 0.40498782\n",
      "Iteration 46, loss = 0.40379814\n",
      "Iteration 47, loss = 0.40256998\n",
      "Iteration 48, loss = 0.40136571\n",
      "Iteration 49, loss = 0.40013454\n",
      "Iteration 50, loss = 0.39890695\n",
      "Iteration 51, loss = 0.39773658\n",
      "Iteration 52, loss = 0.39645723\n",
      "Iteration 53, loss = 0.39515820\n",
      "Iteration 54, loss = 0.39396542\n",
      "Iteration 55, loss = 0.39265303\n",
      "Iteration 56, loss = 0.39141167\n",
      "Iteration 57, loss = 0.39016056\n",
      "Iteration 58, loss = 0.38893674\n",
      "Iteration 59, loss = 0.38761481\n",
      "Iteration 60, loss = 0.38633359\n",
      "Iteration 61, loss = 0.38506617\n",
      "Iteration 62, loss = 0.38378413\n",
      "Iteration 63, loss = 0.38246714\n",
      "Iteration 64, loss = 0.38118234\n",
      "Iteration 65, loss = 0.37988434\n",
      "Iteration 66, loss = 0.37859157\n",
      "Iteration 67, loss = 0.37728849\n",
      "Iteration 68, loss = 0.37598693\n",
      "Iteration 69, loss = 0.37467868\n",
      "Iteration 70, loss = 0.37337790\n",
      "Iteration 71, loss = 0.37211077\n",
      "Iteration 72, loss = 0.37079647\n",
      "Iteration 73, loss = 0.36948297\n",
      "Iteration 74, loss = 0.36818163\n",
      "Iteration 75, loss = 0.36686806\n",
      "Iteration 76, loss = 0.36560186\n",
      "Iteration 77, loss = 0.36429340\n",
      "Iteration 78, loss = 0.36297918\n",
      "Iteration 79, loss = 0.36169882\n",
      "Iteration 80, loss = 0.36040824\n",
      "Iteration 81, loss = 0.35912531\n",
      "Iteration 82, loss = 0.35784817\n",
      "Iteration 83, loss = 0.35655550\n",
      "Iteration 84, loss = 0.35531166\n",
      "Iteration 85, loss = 0.35405025\n",
      "Iteration 86, loss = 0.35278415\n",
      "Iteration 87, loss = 0.35155132\n",
      "Iteration 88, loss = 0.35029675\n",
      "Iteration 89, loss = 0.34906168\n",
      "Iteration 90, loss = 0.34786428\n",
      "Iteration 91, loss = 0.34666576\n",
      "Iteration 92, loss = 0.34541168\n",
      "Iteration 93, loss = 0.34422177\n",
      "Iteration 94, loss = 0.34301132\n",
      "Iteration 95, loss = 0.34182460\n",
      "Iteration 96, loss = 0.34074113\n",
      "Iteration 97, loss = 0.33954807\n",
      "Iteration 98, loss = 0.33837469\n",
      "Iteration 99, loss = 0.33726412\n",
      "Iteration 100, loss = 0.33610039\n",
      "Iteration 101, loss = 0.33498783\n",
      "Iteration 102, loss = 0.33387904\n",
      "Iteration 103, loss = 0.33280159\n",
      "Iteration 104, loss = 0.33170096\n",
      "Iteration 105, loss = 0.33067389\n",
      "Iteration 106, loss = 0.32961287\n",
      "Iteration 107, loss = 0.32853958\n",
      "Iteration 108, loss = 0.32747977\n",
      "Iteration 109, loss = 0.32647576\n",
      "Iteration 110, loss = 0.32547893\n",
      "Iteration 111, loss = 0.32447747\n",
      "Iteration 112, loss = 0.32346138\n",
      "Iteration 113, loss = 0.32253451\n",
      "Iteration 114, loss = 0.32154415\n",
      "Iteration 115, loss = 0.32061089\n",
      "Iteration 116, loss = 0.31962910\n",
      "Iteration 117, loss = 0.31871680\n",
      "Iteration 118, loss = 0.31781107\n",
      "Iteration 119, loss = 0.31689281\n",
      "Iteration 120, loss = 0.31602601\n",
      "Iteration 121, loss = 0.31513865\n",
      "Iteration 122, loss = 0.31430998\n",
      "Iteration 123, loss = 0.31343729\n",
      "Iteration 124, loss = 0.31258586\n",
      "Iteration 125, loss = 0.31179135\n",
      "Iteration 126, loss = 0.31106114\n",
      "Iteration 127, loss = 0.31018579\n",
      "Iteration 128, loss = 0.30938646\n",
      "Iteration 129, loss = 0.30860460\n",
      "Iteration 130, loss = 0.30784750\n",
      "Iteration 131, loss = 0.30709131\n",
      "Iteration 132, loss = 0.30636413\n",
      "Iteration 133, loss = 0.30563670\n",
      "Iteration 134, loss = 0.30490969\n",
      "Iteration 135, loss = 0.30420267\n",
      "Iteration 136, loss = 0.30350451\n",
      "Iteration 137, loss = 0.30281089\n",
      "Iteration 138, loss = 0.30219631\n",
      "Iteration 139, loss = 0.30149085\n",
      "Iteration 140, loss = 0.30083687\n",
      "Iteration 141, loss = 0.30018647\n",
      "Iteration 142, loss = 0.29955602\n",
      "Iteration 143, loss = 0.29893054\n",
      "Iteration 144, loss = 0.29833994\n",
      "Iteration 145, loss = 0.29774686\n",
      "Iteration 146, loss = 0.29713506\n",
      "Iteration 147, loss = 0.29659387\n",
      "Iteration 148, loss = 0.29600657\n",
      "Iteration 149, loss = 0.29544640\n",
      "Iteration 150, loss = 0.29488482\n",
      "Iteration 151, loss = 0.29433944\n",
      "Iteration 152, loss = 0.29381815\n",
      "Iteration 153, loss = 0.29329705\n",
      "Iteration 154, loss = 0.29277482\n",
      "Iteration 155, loss = 0.29227421\n",
      "Iteration 156, loss = 0.29174984\n",
      "Iteration 157, loss = 0.29126349\n",
      "Iteration 158, loss = 0.29076926\n",
      "Iteration 159, loss = 0.29030508\n",
      "Iteration 160, loss = 0.28983492\n",
      "Iteration 161, loss = 0.28937290\n",
      "Iteration 162, loss = 0.28890907\n",
      "Iteration 163, loss = 0.28846356\n",
      "Iteration 164, loss = 0.28803382\n",
      "Iteration 165, loss = 0.28762411\n",
      "Iteration 166, loss = 0.28720122\n",
      "Iteration 167, loss = 0.28673607\n",
      "Iteration 168, loss = 0.28636132\n",
      "Iteration 169, loss = 0.28596565\n",
      "Iteration 170, loss = 0.28553442\n",
      "Iteration 171, loss = 0.28512799\n",
      "Iteration 172, loss = 0.28475710\n",
      "Iteration 173, loss = 0.28438981\n",
      "Iteration 174, loss = 0.28400802\n",
      "Iteration 175, loss = 0.28362476\n",
      "Iteration 176, loss = 0.28326892\n",
      "Iteration 177, loss = 0.28289330\n",
      "Iteration 178, loss = 0.28256239\n",
      "Iteration 179, loss = 0.28223218\n",
      "Iteration 180, loss = 0.28187073\n",
      "Iteration 181, loss = 0.28152074\n",
      "Iteration 182, loss = 0.28118573\n",
      "Iteration 183, loss = 0.28085627\n",
      "Iteration 184, loss = 0.28056941\n",
      "Iteration 185, loss = 0.28022267\n",
      "Iteration 186, loss = 0.27991942\n",
      "Iteration 187, loss = 0.27963807\n",
      "Iteration 188, loss = 0.27929411\n",
      "Iteration 189, loss = 0.27906103\n",
      "Iteration 190, loss = 0.27870788\n",
      "Iteration 191, loss = 0.27850040\n",
      "Iteration 192, loss = 0.27816299\n",
      "Iteration 193, loss = 0.27785289\n",
      "Iteration 194, loss = 0.27755898\n",
      "Iteration 195, loss = 0.27729216\n",
      "Iteration 196, loss = 0.27701481\n",
      "Iteration 197, loss = 0.27676646\n",
      "Iteration 198, loss = 0.27649823\n",
      "Iteration 199, loss = 0.27623588\n",
      "Iteration 200, loss = 0.27599328\n",
      "Iteration 1, loss = 0.58444695\n",
      "Iteration 2, loss = 0.46103485\n",
      "Iteration 3, loss = 0.45183497\n",
      "Iteration 4, loss = 0.45055889\n",
      "Iteration 5, loss = 0.44934938\n",
      "Iteration 6, loss = 0.44818902\n",
      "Iteration 7, loss = 0.44699667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44583497\n",
      "Iteration 9, loss = 0.44469881\n",
      "Iteration 10, loss = 0.44359255\n",
      "Iteration 11, loss = 0.44243379\n",
      "Iteration 12, loss = 0.44135626\n",
      "Iteration 13, loss = 0.44020636\n",
      "Iteration 14, loss = 0.43909576\n",
      "Iteration 15, loss = 0.43797092\n",
      "Iteration 16, loss = 0.43682867\n",
      "Iteration 17, loss = 0.43571693\n",
      "Iteration 18, loss = 0.43462462\n",
      "Iteration 19, loss = 0.43349505\n",
      "Iteration 20, loss = 0.43240769\n",
      "Iteration 21, loss = 0.43127824\n",
      "Iteration 22, loss = 0.43018385\n",
      "Iteration 23, loss = 0.42904946\n",
      "Iteration 24, loss = 0.42794250\n",
      "Iteration 25, loss = 0.42680975\n",
      "Iteration 26, loss = 0.42567571\n",
      "Iteration 27, loss = 0.42465197\n",
      "Iteration 28, loss = 0.42343004\n",
      "Iteration 29, loss = 0.42233808\n",
      "Iteration 30, loss = 0.42117012\n",
      "Iteration 31, loss = 0.42004030\n",
      "Iteration 32, loss = 0.41893209\n",
      "Iteration 33, loss = 0.41776957\n",
      "Iteration 34, loss = 0.41660497\n",
      "Iteration 35, loss = 0.41544589\n",
      "Iteration 36, loss = 0.41429010\n",
      "Iteration 37, loss = 0.41311444\n",
      "Iteration 38, loss = 0.41193509\n",
      "Iteration 39, loss = 0.41072203\n",
      "Iteration 40, loss = 0.40955763\n",
      "Iteration 41, loss = 0.40837868\n",
      "Iteration 42, loss = 0.40712462\n",
      "Iteration 43, loss = 0.40591425\n",
      "Iteration 44, loss = 0.40468926\n",
      "Iteration 45, loss = 0.40349255\n",
      "Iteration 46, loss = 0.40223633\n",
      "Iteration 47, loss = 0.40104244\n",
      "Iteration 48, loss = 0.39975238\n",
      "Iteration 49, loss = 0.39858401\n",
      "Iteration 50, loss = 0.39727255\n",
      "Iteration 51, loss = 0.39597930\n",
      "Iteration 52, loss = 0.39469910\n",
      "Iteration 53, loss = 0.39340625\n",
      "Iteration 54, loss = 0.39217421\n",
      "Iteration 55, loss = 0.39080755\n",
      "Iteration 56, loss = 0.38954580\n",
      "Iteration 57, loss = 0.38826599\n",
      "Iteration 58, loss = 0.38695613\n",
      "Iteration 59, loss = 0.38565664\n",
      "Iteration 60, loss = 0.38435440\n",
      "Iteration 61, loss = 0.38306146\n",
      "Iteration 62, loss = 0.38166243\n",
      "Iteration 63, loss = 0.38035980\n",
      "Iteration 64, loss = 0.37905759\n",
      "Iteration 65, loss = 0.37772384\n",
      "Iteration 66, loss = 0.37638955\n",
      "Iteration 67, loss = 0.37504818\n",
      "Iteration 68, loss = 0.37371083\n",
      "Iteration 69, loss = 0.37238421\n",
      "Iteration 70, loss = 0.37104080\n",
      "Iteration 71, loss = 0.36973469\n",
      "Iteration 72, loss = 0.36838854\n",
      "Iteration 73, loss = 0.36704984\n",
      "Iteration 74, loss = 0.36570599\n",
      "Iteration 75, loss = 0.36438563\n",
      "Iteration 76, loss = 0.36304419\n",
      "Iteration 77, loss = 0.36169745\n",
      "Iteration 78, loss = 0.36038024\n",
      "Iteration 79, loss = 0.35906623\n",
      "Iteration 80, loss = 0.35771584\n",
      "Iteration 81, loss = 0.35641066\n",
      "Iteration 82, loss = 0.35511986\n",
      "Iteration 83, loss = 0.35380928\n",
      "Iteration 84, loss = 0.35254582\n",
      "Iteration 85, loss = 0.35123205\n",
      "Iteration 86, loss = 0.34996426\n",
      "Iteration 87, loss = 0.34869995\n",
      "Iteration 88, loss = 0.34740335\n",
      "Iteration 89, loss = 0.34616050\n",
      "Iteration 90, loss = 0.34492767\n",
      "Iteration 91, loss = 0.34372253\n",
      "Iteration 92, loss = 0.34242184\n",
      "Iteration 93, loss = 0.34119585\n",
      "Iteration 94, loss = 0.33996629\n",
      "Iteration 95, loss = 0.33876600\n",
      "Iteration 96, loss = 0.33760720\n",
      "Iteration 97, loss = 0.33642263\n",
      "Iteration 98, loss = 0.33525096\n",
      "Iteration 99, loss = 0.33408369\n",
      "Iteration 100, loss = 0.33292817\n",
      "Iteration 101, loss = 0.33179468\n",
      "Iteration 102, loss = 0.33064944\n",
      "Iteration 103, loss = 0.32956100\n",
      "Iteration 104, loss = 0.32845974\n",
      "Iteration 105, loss = 0.32742775\n",
      "Iteration 106, loss = 0.32628713\n",
      "Iteration 107, loss = 0.32521081\n",
      "Iteration 108, loss = 0.32418648\n",
      "Iteration 109, loss = 0.32313658\n",
      "Iteration 110, loss = 0.32210088\n",
      "Iteration 111, loss = 0.32109035\n",
      "Iteration 112, loss = 0.32005773\n",
      "Iteration 113, loss = 0.31906409\n",
      "Iteration 114, loss = 0.31808761\n",
      "Iteration 115, loss = 0.31716020\n",
      "Iteration 116, loss = 0.31622480\n",
      "Iteration 117, loss = 0.31524715\n",
      "Iteration 118, loss = 0.31434671\n",
      "Iteration 119, loss = 0.31342276\n",
      "Iteration 120, loss = 0.31253626\n",
      "Iteration 121, loss = 0.31161857\n",
      "Iteration 122, loss = 0.31078907\n",
      "Iteration 123, loss = 0.30989478\n",
      "Iteration 124, loss = 0.30904830\n",
      "Iteration 125, loss = 0.30822016\n",
      "Iteration 126, loss = 0.30741884\n",
      "Iteration 127, loss = 0.30660696\n",
      "Iteration 128, loss = 0.30579841\n",
      "Iteration 129, loss = 0.30500124\n",
      "Iteration 130, loss = 0.30423635\n",
      "Iteration 131, loss = 0.30349392\n",
      "Iteration 132, loss = 0.30274210\n",
      "Iteration 133, loss = 0.30198607\n",
      "Iteration 134, loss = 0.30127733\n",
      "Iteration 135, loss = 0.30055173\n",
      "Iteration 136, loss = 0.29985810\n",
      "Iteration 137, loss = 0.29916116\n",
      "Iteration 138, loss = 0.29852939\n",
      "Iteration 139, loss = 0.29778846\n",
      "Iteration 140, loss = 0.29717451\n",
      "Iteration 141, loss = 0.29650826\n",
      "Iteration 142, loss = 0.29587384\n",
      "Iteration 143, loss = 0.29524134\n",
      "Iteration 144, loss = 0.29463751\n",
      "Iteration 145, loss = 0.29401080\n",
      "Iteration 146, loss = 0.29341329\n",
      "Iteration 147, loss = 0.29290107\n",
      "Iteration 148, loss = 0.29231437\n",
      "Iteration 149, loss = 0.29168682\n",
      "Iteration 150, loss = 0.29112445\n",
      "Iteration 151, loss = 0.29058304\n",
      "Iteration 152, loss = 0.29003999\n",
      "Iteration 153, loss = 0.28951270\n",
      "Iteration 154, loss = 0.28900067\n",
      "Iteration 155, loss = 0.28846839\n",
      "Iteration 156, loss = 0.28796888\n",
      "Iteration 157, loss = 0.28746339\n",
      "Iteration 158, loss = 0.28697945\n",
      "Iteration 159, loss = 0.28650173\n",
      "Iteration 160, loss = 0.28601853\n",
      "Iteration 161, loss = 0.28555786\n",
      "Iteration 162, loss = 0.28508975\n",
      "Iteration 163, loss = 0.28463406\n",
      "Iteration 164, loss = 0.28421025\n",
      "Iteration 165, loss = 0.28376527\n",
      "Iteration 166, loss = 0.28337053\n",
      "Iteration 167, loss = 0.28288953\n",
      "Iteration 168, loss = 0.28251730\n",
      "Iteration 169, loss = 0.28214781\n",
      "Iteration 170, loss = 0.28166182\n",
      "Iteration 171, loss = 0.28125212\n",
      "Iteration 172, loss = 0.28087286\n",
      "Iteration 173, loss = 0.28047938\n",
      "Iteration 174, loss = 0.28011706\n",
      "Iteration 175, loss = 0.27972533\n",
      "Iteration 176, loss = 0.27937866\n",
      "Iteration 177, loss = 0.27899801\n",
      "Iteration 178, loss = 0.27864940\n",
      "Iteration 179, loss = 0.27833697\n",
      "Iteration 180, loss = 0.27795814\n",
      "Iteration 181, loss = 0.27761301\n",
      "Iteration 182, loss = 0.27727722\n",
      "Iteration 183, loss = 0.27694263\n",
      "Iteration 184, loss = 0.27665489\n",
      "Iteration 185, loss = 0.27630301\n",
      "Iteration 186, loss = 0.27599898\n",
      "Iteration 187, loss = 0.27569906\n",
      "Iteration 188, loss = 0.27539912\n",
      "Iteration 189, loss = 0.27511388\n",
      "Iteration 190, loss = 0.27476709\n",
      "Iteration 191, loss = 0.27456425\n",
      "Iteration 192, loss = 0.27426610\n",
      "Iteration 193, loss = 0.27390070\n",
      "Iteration 194, loss = 0.27363201\n",
      "Iteration 195, loss = 0.27334084\n",
      "Iteration 196, loss = 0.27308348\n",
      "Iteration 197, loss = 0.27279909\n",
      "Iteration 198, loss = 0.27254674\n",
      "Iteration 199, loss = 0.27226135\n",
      "Iteration 200, loss = 0.27201006\n",
      "Iteration 1, loss = 0.58466182\n",
      "Iteration 2, loss = 0.46112550\n",
      "Iteration 3, loss = 0.45219909\n",
      "Iteration 4, loss = 0.45081139\n",
      "Iteration 5, loss = 0.44960447\n",
      "Iteration 6, loss = 0.44844300\n",
      "Iteration 7, loss = 0.44730270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44616322\n",
      "Iteration 9, loss = 0.44501902\n",
      "Iteration 10, loss = 0.44387821\n",
      "Iteration 11, loss = 0.44276072\n",
      "Iteration 12, loss = 0.44163116\n",
      "Iteration 13, loss = 0.44053272\n",
      "Iteration 14, loss = 0.43949072\n",
      "Iteration 15, loss = 0.43828924\n",
      "Iteration 16, loss = 0.43722307\n",
      "Iteration 17, loss = 0.43607247\n",
      "Iteration 18, loss = 0.43496678\n",
      "Iteration 19, loss = 0.43388504\n",
      "Iteration 20, loss = 0.43275913\n",
      "Iteration 21, loss = 0.43165205\n",
      "Iteration 22, loss = 0.43053578\n",
      "Iteration 23, loss = 0.42946383\n",
      "Iteration 24, loss = 0.42835080\n",
      "Iteration 25, loss = 0.42721804\n",
      "Iteration 26, loss = 0.42612164\n",
      "Iteration 27, loss = 0.42511641\n",
      "Iteration 28, loss = 0.42393579\n",
      "Iteration 29, loss = 0.42288102\n",
      "Iteration 30, loss = 0.42167925\n",
      "Iteration 31, loss = 0.42053462\n",
      "Iteration 32, loss = 0.41941187\n",
      "Iteration 33, loss = 0.41824650\n",
      "Iteration 34, loss = 0.41712691\n",
      "Iteration 35, loss = 0.41599709\n",
      "Iteration 36, loss = 0.41478857\n",
      "Iteration 37, loss = 0.41361054\n",
      "Iteration 38, loss = 0.41254669\n",
      "Iteration 39, loss = 0.41130955\n",
      "Iteration 40, loss = 0.41010335\n",
      "Iteration 41, loss = 0.40889114\n",
      "Iteration 42, loss = 0.40769497\n",
      "Iteration 43, loss = 0.40651708\n",
      "Iteration 44, loss = 0.40531532\n",
      "Iteration 45, loss = 0.40408099\n",
      "Iteration 46, loss = 0.40287850\n",
      "Iteration 47, loss = 0.40164271\n",
      "Iteration 48, loss = 0.40036058\n",
      "Iteration 49, loss = 0.39923393\n",
      "Iteration 50, loss = 0.39786877\n",
      "Iteration 51, loss = 0.39663257\n",
      "Iteration 52, loss = 0.39534015\n",
      "Iteration 53, loss = 0.39408110\n",
      "Iteration 54, loss = 0.39282394\n",
      "Iteration 55, loss = 0.39147587\n",
      "Iteration 56, loss = 0.39021710\n",
      "Iteration 57, loss = 0.38892185\n",
      "Iteration 58, loss = 0.38762145\n",
      "Iteration 59, loss = 0.38634306\n",
      "Iteration 60, loss = 0.38500697\n",
      "Iteration 61, loss = 0.38369341\n",
      "Iteration 62, loss = 0.38235616\n",
      "Iteration 63, loss = 0.38102838\n",
      "Iteration 64, loss = 0.37970886\n",
      "Iteration 65, loss = 0.37840371\n",
      "Iteration 66, loss = 0.37713234\n",
      "Iteration 67, loss = 0.37570015\n",
      "Iteration 68, loss = 0.37437568\n",
      "Iteration 69, loss = 0.37303863\n",
      "Iteration 70, loss = 0.37169946\n",
      "Iteration 71, loss = 0.37036849\n",
      "Iteration 72, loss = 0.36901700\n",
      "Iteration 73, loss = 0.36768602\n",
      "Iteration 74, loss = 0.36633675\n",
      "Iteration 75, loss = 0.36502639\n",
      "Iteration 76, loss = 0.36365751\n",
      "Iteration 77, loss = 0.36230073\n",
      "Iteration 78, loss = 0.36099999\n",
      "Iteration 79, loss = 0.35964818\n",
      "Iteration 80, loss = 0.35835263\n",
      "Iteration 81, loss = 0.35698443\n",
      "Iteration 82, loss = 0.35568173\n",
      "Iteration 83, loss = 0.35440651\n",
      "Iteration 84, loss = 0.35306784\n",
      "Iteration 85, loss = 0.35175092\n",
      "Iteration 86, loss = 0.35047487\n",
      "Iteration 87, loss = 0.34913333\n",
      "Iteration 88, loss = 0.34786251\n",
      "Iteration 89, loss = 0.34662547\n",
      "Iteration 90, loss = 0.34531167\n",
      "Iteration 91, loss = 0.34409061\n",
      "Iteration 92, loss = 0.34282424\n",
      "Iteration 93, loss = 0.34157083\n",
      "Iteration 94, loss = 0.34034353\n",
      "Iteration 95, loss = 0.33910283\n",
      "Iteration 96, loss = 0.33790046\n",
      "Iteration 97, loss = 0.33667552\n",
      "Iteration 98, loss = 0.33553296\n",
      "Iteration 99, loss = 0.33427658\n",
      "Iteration 100, loss = 0.33312430\n",
      "Iteration 101, loss = 0.33197039\n",
      "Iteration 102, loss = 0.33079359\n",
      "Iteration 103, loss = 0.32967045\n",
      "Iteration 104, loss = 0.32858070\n",
      "Iteration 105, loss = 0.32745588\n",
      "Iteration 106, loss = 0.32628898\n",
      "Iteration 107, loss = 0.32519944\n",
      "Iteration 108, loss = 0.32416208\n",
      "Iteration 109, loss = 0.32304483\n",
      "Iteration 110, loss = 0.32203403\n",
      "Iteration 111, loss = 0.32095021\n",
      "Iteration 112, loss = 0.31988045\n",
      "Iteration 113, loss = 0.31885443\n",
      "Iteration 114, loss = 0.31785171\n",
      "Iteration 115, loss = 0.31685472\n",
      "Iteration 116, loss = 0.31594388\n",
      "Iteration 117, loss = 0.31489420\n",
      "Iteration 118, loss = 0.31397556\n",
      "Iteration 119, loss = 0.31300325\n",
      "Iteration 120, loss = 0.31205834\n",
      "Iteration 121, loss = 0.31114140\n",
      "Iteration 122, loss = 0.31026889\n",
      "Iteration 123, loss = 0.30934037\n",
      "Iteration 124, loss = 0.30846236\n",
      "Iteration 125, loss = 0.30758727\n",
      "Iteration 126, loss = 0.30674490\n",
      "Iteration 127, loss = 0.30590650\n",
      "Iteration 128, loss = 0.30508714\n",
      "Iteration 129, loss = 0.30422350\n",
      "Iteration 130, loss = 0.30344775\n",
      "Iteration 131, loss = 0.30265108\n",
      "Iteration 132, loss = 0.30190689\n",
      "Iteration 133, loss = 0.30105207\n",
      "Iteration 134, loss = 0.30032900\n",
      "Iteration 135, loss = 0.29956614\n",
      "Iteration 136, loss = 0.29883740\n",
      "Iteration 137, loss = 0.29808722\n",
      "Iteration 138, loss = 0.29745195\n",
      "Iteration 139, loss = 0.29666149\n",
      "Iteration 140, loss = 0.29600324\n",
      "Iteration 141, loss = 0.29534260\n",
      "Iteration 142, loss = 0.29466946\n",
      "Iteration 143, loss = 0.29399818\n",
      "Iteration 144, loss = 0.29334328\n",
      "Iteration 145, loss = 0.29271438\n",
      "Iteration 146, loss = 0.29207174\n",
      "Iteration 147, loss = 0.29144133\n",
      "Iteration 148, loss = 0.29085439\n",
      "Iteration 149, loss = 0.29024411\n",
      "Iteration 150, loss = 0.28965300\n",
      "Iteration 151, loss = 0.28907411\n",
      "Iteration 152, loss = 0.28848596\n",
      "Iteration 153, loss = 0.28791274\n",
      "Iteration 154, loss = 0.28737434\n",
      "Iteration 155, loss = 0.28681673\n",
      "Iteration 156, loss = 0.28628402\n",
      "Iteration 157, loss = 0.28577407\n",
      "Iteration 158, loss = 0.28522476\n",
      "Iteration 159, loss = 0.28473129\n",
      "Iteration 160, loss = 0.28421430\n",
      "Iteration 161, loss = 0.28375101\n",
      "Iteration 162, loss = 0.28323782\n",
      "Iteration 163, loss = 0.28274995\n",
      "Iteration 164, loss = 0.28230289\n",
      "Iteration 165, loss = 0.28181746\n",
      "Iteration 166, loss = 0.28135935\n",
      "Iteration 167, loss = 0.28092955\n",
      "Iteration 168, loss = 0.28047524\n",
      "Iteration 169, loss = 0.28005021\n",
      "Iteration 170, loss = 0.27960686\n",
      "Iteration 171, loss = 0.27915661\n",
      "Iteration 172, loss = 0.27876187\n",
      "Iteration 173, loss = 0.27834606\n",
      "Iteration 174, loss = 0.27792289\n",
      "Iteration 175, loss = 0.27756939\n",
      "Iteration 176, loss = 0.27712486\n",
      "Iteration 177, loss = 0.27674554\n",
      "Iteration 178, loss = 0.27637978\n",
      "Iteration 179, loss = 0.27601361\n",
      "Iteration 180, loss = 0.27562363\n",
      "Iteration 181, loss = 0.27525944\n",
      "Iteration 182, loss = 0.27490507\n",
      "Iteration 183, loss = 0.27455628\n",
      "Iteration 184, loss = 0.27423441\n",
      "Iteration 185, loss = 0.27385047\n",
      "Iteration 186, loss = 0.27352778\n",
      "Iteration 187, loss = 0.27319650\n",
      "Iteration 188, loss = 0.27288600\n",
      "Iteration 189, loss = 0.27257439\n",
      "Iteration 190, loss = 0.27222138\n",
      "Iteration 191, loss = 0.27196466\n",
      "Iteration 192, loss = 0.27163241\n",
      "Iteration 193, loss = 0.27129321\n",
      "Iteration 194, loss = 0.27098273\n",
      "Iteration 195, loss = 0.27068996\n",
      "Iteration 196, loss = 0.27039636\n",
      "Iteration 197, loss = 0.27011798\n",
      "Iteration 198, loss = 0.26982401\n",
      "Iteration 199, loss = 0.26953982\n",
      "Iteration 200, loss = 0.26927854\n",
      "Iteration 1, loss = 0.55079558\n",
      "Iteration 2, loss = 0.41982747\n",
      "Iteration 3, loss = 0.39031789\n",
      "Iteration 4, loss = 0.36929628\n",
      "Iteration 5, loss = 0.35164028\n",
      "Iteration 6, loss = 0.33677530\n",
      "Iteration 7, loss = 0.32375707\n",
      "Iteration 8, loss = 0.31305296\n",
      "Iteration 9, loss = 0.30420859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.29702052\n",
      "Iteration 11, loss = 0.29047278\n",
      "Iteration 12, loss = 0.28569246\n",
      "Iteration 13, loss = 0.28094136\n",
      "Iteration 14, loss = 0.27734456\n",
      "Iteration 15, loss = 0.27383438\n",
      "Iteration 16, loss = 0.27106516\n",
      "Iteration 17, loss = 0.26879086\n",
      "Iteration 18, loss = 0.26593749\n",
      "Iteration 19, loss = 0.26372580\n",
      "Iteration 20, loss = 0.26179554\n",
      "Iteration 21, loss = 0.25991315\n",
      "Iteration 22, loss = 0.25816602\n",
      "Iteration 23, loss = 0.25660096\n",
      "Iteration 24, loss = 0.25499173\n",
      "Iteration 25, loss = 0.25364281\n",
      "Iteration 26, loss = 0.25226616\n",
      "Iteration 27, loss = 0.25054580\n",
      "Iteration 28, loss = 0.24937886\n",
      "Iteration 29, loss = 0.24814420\n",
      "Iteration 30, loss = 0.24743198\n",
      "Iteration 31, loss = 0.24579138\n",
      "Iteration 32, loss = 0.24471167\n",
      "Iteration 33, loss = 0.24416869\n",
      "Iteration 34, loss = 0.24281894\n",
      "Iteration 35, loss = 0.24164966\n",
      "Iteration 36, loss = 0.24093685\n",
      "Iteration 37, loss = 0.24015413\n",
      "Iteration 38, loss = 0.23964860\n",
      "Iteration 39, loss = 0.23870074\n",
      "Iteration 40, loss = 0.23811068\n",
      "Iteration 41, loss = 0.23735612\n",
      "Iteration 42, loss = 0.23676754\n",
      "Iteration 43, loss = 0.23626662\n",
      "Iteration 44, loss = 0.23562140\n",
      "Iteration 45, loss = 0.23534647\n",
      "Iteration 46, loss = 0.23504116\n",
      "Iteration 47, loss = 0.23450915\n",
      "Iteration 48, loss = 0.23440415\n",
      "Iteration 49, loss = 0.23419637\n",
      "Iteration 50, loss = 0.23376842\n",
      "Iteration 51, loss = 0.23319975\n",
      "Iteration 52, loss = 0.23313817\n",
      "Iteration 53, loss = 0.23299451\n",
      "Iteration 54, loss = 0.23259377\n",
      "Iteration 55, loss = 0.23209515\n",
      "Iteration 56, loss = 0.23203363\n",
      "Iteration 57, loss = 0.23169149\n",
      "Iteration 58, loss = 0.23163221\n",
      "Iteration 59, loss = 0.23132252\n",
      "Iteration 60, loss = 0.23099255\n",
      "Iteration 61, loss = 0.23113616\n",
      "Iteration 62, loss = 0.23072894\n",
      "Iteration 63, loss = 0.23092597\n",
      "Iteration 64, loss = 0.23040615\n",
      "Iteration 65, loss = 0.23010242\n",
      "Iteration 66, loss = 0.23031834\n",
      "Iteration 67, loss = 0.22969847\n",
      "Iteration 68, loss = 0.22971769\n",
      "Iteration 69, loss = 0.22895296\n",
      "Iteration 70, loss = 0.22914829\n",
      "Iteration 71, loss = 0.22907936\n",
      "Iteration 72, loss = 0.22856659\n",
      "Iteration 73, loss = 0.22807643\n",
      "Iteration 74, loss = 0.22799615\n",
      "Iteration 75, loss = 0.22774728\n",
      "Iteration 76, loss = 0.22797764\n",
      "Iteration 77, loss = 0.22703861\n",
      "Iteration 78, loss = 0.22675371\n",
      "Iteration 79, loss = 0.22675612\n",
      "Iteration 80, loss = 0.22592679\n",
      "Iteration 81, loss = 0.22589478\n",
      "Iteration 82, loss = 0.22547457\n",
      "Iteration 83, loss = 0.22493892\n",
      "Iteration 84, loss = 0.22489924\n",
      "Iteration 85, loss = 0.22456983\n",
      "Iteration 86, loss = 0.22377185\n",
      "Iteration 87, loss = 0.22354858\n",
      "Iteration 88, loss = 0.22297859\n",
      "Iteration 89, loss = 0.22286884\n",
      "Iteration 90, loss = 0.22208250\n",
      "Iteration 91, loss = 0.22187616\n",
      "Iteration 92, loss = 0.22096827\n",
      "Iteration 93, loss = 0.22059492\n",
      "Iteration 94, loss = 0.22012859\n",
      "Iteration 95, loss = 0.21960339\n",
      "Iteration 96, loss = 0.21902868\n",
      "Iteration 97, loss = 0.21854502\n",
      "Iteration 98, loss = 0.21812528\n",
      "Iteration 99, loss = 0.21782525\n",
      "Iteration 100, loss = 0.21701032\n",
      "Iteration 101, loss = 0.21643616\n",
      "Iteration 102, loss = 0.21648271\n",
      "Iteration 103, loss = 0.21524586\n",
      "Iteration 104, loss = 0.21451202\n",
      "Iteration 105, loss = 0.21399733\n",
      "Iteration 106, loss = 0.21440766\n",
      "Iteration 107, loss = 0.21258975\n",
      "Iteration 108, loss = 0.21214280\n",
      "Iteration 109, loss = 0.21137376\n",
      "Iteration 110, loss = 0.21080373\n",
      "Iteration 111, loss = 0.20995780\n",
      "Iteration 112, loss = 0.20938270\n",
      "Iteration 113, loss = 0.20898336\n",
      "Iteration 114, loss = 0.20819189\n",
      "Iteration 115, loss = 0.20740535\n",
      "Iteration 116, loss = 0.20658098\n",
      "Iteration 117, loss = 0.20586082\n",
      "Iteration 118, loss = 0.20499196\n",
      "Iteration 119, loss = 0.20431805\n",
      "Iteration 120, loss = 0.20374613\n",
      "Iteration 121, loss = 0.20315356\n",
      "Iteration 122, loss = 0.20243933\n",
      "Iteration 123, loss = 0.20145335\n",
      "Iteration 124, loss = 0.20094540\n",
      "Iteration 125, loss = 0.20002816\n",
      "Iteration 126, loss = 0.19915675\n",
      "Iteration 127, loss = 0.19875535\n",
      "Iteration 128, loss = 0.19781533\n",
      "Iteration 129, loss = 0.19760582\n",
      "Iteration 130, loss = 0.19634146\n",
      "Iteration 131, loss = 0.19568923\n",
      "Iteration 132, loss = 0.19483533\n",
      "Iteration 133, loss = 0.19381238\n",
      "Iteration 134, loss = 0.19361013\n",
      "Iteration 135, loss = 0.19254476\n",
      "Iteration 136, loss = 0.19198379\n",
      "Iteration 137, loss = 0.19092371\n",
      "Iteration 138, loss = 0.19021733\n",
      "Iteration 139, loss = 0.18966375\n",
      "Iteration 140, loss = 0.18882997\n",
      "Iteration 141, loss = 0.18809688\n",
      "Iteration 142, loss = 0.18777428\n",
      "Iteration 143, loss = 0.18660684\n",
      "Iteration 144, loss = 0.18615501\n",
      "Iteration 145, loss = 0.18526519\n",
      "Iteration 146, loss = 0.18444998\n",
      "Iteration 147, loss = 0.18364373\n",
      "Iteration 148, loss = 0.18294788\n",
      "Iteration 149, loss = 0.18224151\n",
      "Iteration 150, loss = 0.18152889\n",
      "Iteration 151, loss = 0.18118620\n",
      "Iteration 152, loss = 0.18036017\n",
      "Iteration 153, loss = 0.17975805\n",
      "Iteration 154, loss = 0.17969639\n",
      "Iteration 155, loss = 0.17844151\n",
      "Iteration 156, loss = 0.17777911\n",
      "Iteration 157, loss = 0.17705151\n",
      "Iteration 158, loss = 0.17628608\n",
      "Iteration 159, loss = 0.17587513\n",
      "Iteration 160, loss = 0.17531436\n",
      "Iteration 161, loss = 0.17462369\n",
      "Iteration 162, loss = 0.17388840\n",
      "Iteration 163, loss = 0.17318544\n",
      "Iteration 164, loss = 0.17258294\n",
      "Iteration 165, loss = 0.17214648\n",
      "Iteration 166, loss = 0.17149210\n",
      "Iteration 167, loss = 0.17138577\n",
      "Iteration 168, loss = 0.17133785\n",
      "Iteration 169, loss = 0.16978497\n",
      "Iteration 170, loss = 0.16973916\n",
      "Iteration 171, loss = 0.16888399\n",
      "Iteration 172, loss = 0.16819373\n",
      "Iteration 173, loss = 0.16759283\n",
      "Iteration 174, loss = 0.16692667\n",
      "Iteration 175, loss = 0.16679571\n",
      "Iteration 176, loss = 0.16644658\n",
      "Iteration 177, loss = 0.16533579\n",
      "Iteration 178, loss = 0.16489943\n",
      "Iteration 179, loss = 0.16438920\n",
      "Iteration 180, loss = 0.16360527\n",
      "Iteration 181, loss = 0.16306449\n",
      "Iteration 182, loss = 0.16291360\n",
      "Iteration 183, loss = 0.16240065\n",
      "Iteration 184, loss = 0.16189083\n",
      "Iteration 185, loss = 0.16110072\n",
      "Iteration 186, loss = 0.16066804\n",
      "Iteration 187, loss = 0.16016151\n",
      "Iteration 188, loss = 0.15976325\n",
      "Iteration 189, loss = 0.15898237\n",
      "Iteration 190, loss = 0.15862876\n",
      "Iteration 191, loss = 0.15811423\n",
      "Iteration 192, loss = 0.15779538\n",
      "Iteration 193, loss = 0.15733146\n",
      "Iteration 194, loss = 0.15650609\n",
      "Iteration 195, loss = 0.15607102\n",
      "Iteration 196, loss = 0.15571397\n",
      "Iteration 197, loss = 0.15555685\n",
      "Iteration 198, loss = 0.15453763\n",
      "Iteration 199, loss = 0.15459540\n",
      "Iteration 200, loss = 0.15440699\n",
      "Iteration 1, loss = 0.55197845\n",
      "Iteration 2, loss = 0.41972768\n",
      "Iteration 3, loss = 0.38994650\n",
      "Iteration 4, loss = 0.36869049\n",
      "Iteration 5, loss = 0.35097130\n",
      "Iteration 6, loss = 0.33592083\n",
      "Iteration 7, loss = 0.32289487\n",
      "Iteration 8, loss = 0.31218614\n",
      "Iteration 9, loss = 0.30322555\n",
      "Iteration 10, loss = 0.29593045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.28925784\n",
      "Iteration 12, loss = 0.28471594\n",
      "Iteration 13, loss = 0.27980390\n",
      "Iteration 14, loss = 0.27569199\n",
      "Iteration 15, loss = 0.27236249\n",
      "Iteration 16, loss = 0.26931185\n",
      "Iteration 17, loss = 0.26669405\n",
      "Iteration 18, loss = 0.26396632\n",
      "Iteration 19, loss = 0.26179766\n",
      "Iteration 20, loss = 0.25966993\n",
      "Iteration 21, loss = 0.25772896\n",
      "Iteration 22, loss = 0.25570793\n",
      "Iteration 23, loss = 0.25407058\n",
      "Iteration 24, loss = 0.25247332\n",
      "Iteration 25, loss = 0.25087995\n",
      "Iteration 26, loss = 0.24924412\n",
      "Iteration 27, loss = 0.24762140\n",
      "Iteration 28, loss = 0.24636811\n",
      "Iteration 29, loss = 0.24501002\n",
      "Iteration 30, loss = 0.24391174\n",
      "Iteration 31, loss = 0.24231192\n",
      "Iteration 32, loss = 0.24112096\n",
      "Iteration 33, loss = 0.24019535\n",
      "Iteration 34, loss = 0.23909177\n",
      "Iteration 35, loss = 0.23781057\n",
      "Iteration 36, loss = 0.23708117\n",
      "Iteration 37, loss = 0.23601049\n",
      "Iteration 38, loss = 0.23524460\n",
      "Iteration 39, loss = 0.23457552\n",
      "Iteration 40, loss = 0.23368416\n",
      "Iteration 41, loss = 0.23299641\n",
      "Iteration 42, loss = 0.23254329\n",
      "Iteration 43, loss = 0.23170209\n",
      "Iteration 44, loss = 0.23124027\n",
      "Iteration 45, loss = 0.23068345\n",
      "Iteration 46, loss = 0.23005722\n",
      "Iteration 47, loss = 0.22979065\n",
      "Iteration 48, loss = 0.22931537\n",
      "Iteration 49, loss = 0.22920631\n",
      "Iteration 50, loss = 0.22864810\n",
      "Iteration 51, loss = 0.22843742\n",
      "Iteration 52, loss = 0.22812308\n",
      "Iteration 53, loss = 0.22761658\n",
      "Iteration 54, loss = 0.22722215\n",
      "Iteration 55, loss = 0.22689684\n",
      "Iteration 56, loss = 0.22672085\n",
      "Iteration 57, loss = 0.22642402\n",
      "Iteration 58, loss = 0.22642117\n",
      "Iteration 59, loss = 0.22600957\n",
      "Iteration 60, loss = 0.22555853\n",
      "Iteration 61, loss = 0.22551230\n",
      "Iteration 62, loss = 0.22523052\n",
      "Iteration 63, loss = 0.22536531\n",
      "Iteration 64, loss = 0.22471193\n",
      "Iteration 65, loss = 0.22474005\n",
      "Iteration 66, loss = 0.22452905\n",
      "Iteration 67, loss = 0.22390108\n",
      "Iteration 68, loss = 0.22407661\n",
      "Iteration 69, loss = 0.22351975\n",
      "Iteration 70, loss = 0.22314030\n",
      "Iteration 71, loss = 0.22355775\n",
      "Iteration 72, loss = 0.22284861\n",
      "Iteration 73, loss = 0.22221254\n",
      "Iteration 74, loss = 0.22193699\n",
      "Iteration 75, loss = 0.22199012\n",
      "Iteration 76, loss = 0.22129325\n",
      "Iteration 77, loss = 0.22089234\n",
      "Iteration 78, loss = 0.22072519\n",
      "Iteration 79, loss = 0.22042702\n",
      "Iteration 80, loss = 0.21961903\n",
      "Iteration 81, loss = 0.21934909\n",
      "Iteration 82, loss = 0.21908670\n",
      "Iteration 83, loss = 0.21847894\n",
      "Iteration 84, loss = 0.21816051\n",
      "Iteration 85, loss = 0.21788059\n",
      "Iteration 86, loss = 0.21719333\n",
      "Iteration 87, loss = 0.21723656\n",
      "Iteration 88, loss = 0.21618753\n",
      "Iteration 89, loss = 0.21604823\n",
      "Iteration 90, loss = 0.21533341\n",
      "Iteration 91, loss = 0.21509764\n",
      "Iteration 92, loss = 0.21412303\n",
      "Iteration 93, loss = 0.21372298\n",
      "Iteration 94, loss = 0.21326733\n",
      "Iteration 95, loss = 0.21268832\n",
      "Iteration 96, loss = 0.21196415\n",
      "Iteration 97, loss = 0.21138861\n",
      "Iteration 98, loss = 0.21092769\n",
      "Iteration 99, loss = 0.21068388\n",
      "Iteration 100, loss = 0.20983252\n",
      "Iteration 101, loss = 0.20912368\n",
      "Iteration 102, loss = 0.20906828\n",
      "Iteration 103, loss = 0.20802038\n",
      "Iteration 104, loss = 0.20722492\n",
      "Iteration 105, loss = 0.20685238\n",
      "Iteration 106, loss = 0.20668545\n",
      "Iteration 107, loss = 0.20537316\n",
      "Iteration 108, loss = 0.20460973\n",
      "Iteration 109, loss = 0.20395963\n",
      "Iteration 110, loss = 0.20314793\n",
      "Iteration 111, loss = 0.20252802\n",
      "Iteration 112, loss = 0.20177343\n",
      "Iteration 113, loss = 0.20122996\n",
      "Iteration 114, loss = 0.20098215\n",
      "Iteration 115, loss = 0.19966660\n",
      "Iteration 116, loss = 0.19903486\n",
      "Iteration 117, loss = 0.19808804\n",
      "Iteration 118, loss = 0.19740864\n",
      "Iteration 119, loss = 0.19665924\n",
      "Iteration 120, loss = 0.19613871\n",
      "Iteration 121, loss = 0.19530555\n",
      "Iteration 122, loss = 0.19487546\n",
      "Iteration 123, loss = 0.19378853\n",
      "Iteration 124, loss = 0.19320731\n",
      "Iteration 125, loss = 0.19220771\n",
      "Iteration 126, loss = 0.19162077\n",
      "Iteration 127, loss = 0.19099864\n",
      "Iteration 128, loss = 0.19036062\n",
      "Iteration 129, loss = 0.18993707\n",
      "Iteration 130, loss = 0.18848758\n",
      "Iteration 131, loss = 0.18777061\n",
      "Iteration 132, loss = 0.18725624\n",
      "Iteration 133, loss = 0.18644960\n",
      "Iteration 134, loss = 0.18595649\n",
      "Iteration 135, loss = 0.18511009\n",
      "Iteration 136, loss = 0.18413677\n",
      "Iteration 137, loss = 0.18336684\n",
      "Iteration 138, loss = 0.18283417\n",
      "Iteration 139, loss = 0.18206992\n",
      "Iteration 140, loss = 0.18130191\n",
      "Iteration 141, loss = 0.18066083\n",
      "Iteration 142, loss = 0.18017795\n",
      "Iteration 143, loss = 0.17922989\n",
      "Iteration 144, loss = 0.17877185\n",
      "Iteration 145, loss = 0.17782437\n",
      "Iteration 146, loss = 0.17718010\n",
      "Iteration 147, loss = 0.17648916\n",
      "Iteration 148, loss = 0.17593550\n",
      "Iteration 149, loss = 0.17502433\n",
      "Iteration 150, loss = 0.17443869\n",
      "Iteration 151, loss = 0.17383027\n",
      "Iteration 152, loss = 0.17324838\n",
      "Iteration 153, loss = 0.17238934\n",
      "Iteration 154, loss = 0.17192632\n",
      "Iteration 155, loss = 0.17129478\n",
      "Iteration 156, loss = 0.17072194\n",
      "Iteration 157, loss = 0.16991760\n",
      "Iteration 158, loss = 0.16932008\n",
      "Iteration 159, loss = 0.16888340\n",
      "Iteration 160, loss = 0.16857023\n",
      "Iteration 161, loss = 0.16788402\n",
      "Iteration 162, loss = 0.16705984\n",
      "Iteration 163, loss = 0.16659523\n",
      "Iteration 164, loss = 0.16573096\n",
      "Iteration 165, loss = 0.16522207\n",
      "Iteration 166, loss = 0.16458236\n",
      "Iteration 167, loss = 0.16424344\n",
      "Iteration 168, loss = 0.16373229\n",
      "Iteration 169, loss = 0.16298538\n",
      "Iteration 170, loss = 0.16260165\n",
      "Iteration 171, loss = 0.16197727\n",
      "Iteration 172, loss = 0.16127184\n",
      "Iteration 173, loss = 0.16084923\n",
      "Iteration 174, loss = 0.16025489\n",
      "Iteration 175, loss = 0.15968679\n",
      "Iteration 176, loss = 0.15982826\n",
      "Iteration 177, loss = 0.15882887\n",
      "Iteration 178, loss = 0.15794156\n",
      "Iteration 179, loss = 0.15757047\n",
      "Iteration 180, loss = 0.15703400\n",
      "Iteration 181, loss = 0.15637548\n",
      "Iteration 182, loss = 0.15608647\n",
      "Iteration 183, loss = 0.15562200\n",
      "Iteration 184, loss = 0.15501500\n",
      "Iteration 185, loss = 0.15437595\n",
      "Iteration 186, loss = 0.15386878\n",
      "Iteration 187, loss = 0.15340110\n",
      "Iteration 188, loss = 0.15293332\n",
      "Iteration 189, loss = 0.15221571\n",
      "Iteration 190, loss = 0.15210342\n",
      "Iteration 191, loss = 0.15133824\n",
      "Iteration 192, loss = 0.15100686\n",
      "Iteration 193, loss = 0.15030908\n",
      "Iteration 194, loss = 0.14996109\n",
      "Iteration 195, loss = 0.14934010\n",
      "Iteration 196, loss = 0.14892156\n",
      "Iteration 197, loss = 0.14866904\n",
      "Iteration 198, loss = 0.14780898\n",
      "Iteration 199, loss = 0.14772799\n",
      "Iteration 200, loss = 0.14702298\n",
      "Iteration 1, loss = 0.55287046\n",
      "Iteration 2, loss = 0.41960997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.39034349\n",
      "Iteration 4, loss = 0.36925972\n",
      "Iteration 5, loss = 0.35200990\n",
      "Iteration 6, loss = 0.33723050\n",
      "Iteration 7, loss = 0.32471665\n",
      "Iteration 8, loss = 0.31430682\n",
      "Iteration 9, loss = 0.30572241\n",
      "Iteration 10, loss = 0.29862143\n",
      "Iteration 11, loss = 0.29262893\n",
      "Iteration 12, loss = 0.28814043\n",
      "Iteration 13, loss = 0.28359736\n",
      "Iteration 14, loss = 0.27966287\n",
      "Iteration 15, loss = 0.27667978\n",
      "Iteration 16, loss = 0.27366179\n",
      "Iteration 17, loss = 0.27123884\n",
      "Iteration 18, loss = 0.26873440\n",
      "Iteration 19, loss = 0.26656715\n",
      "Iteration 20, loss = 0.26439708\n",
      "Iteration 21, loss = 0.26247989\n",
      "Iteration 22, loss = 0.26078861\n",
      "Iteration 23, loss = 0.25901443\n",
      "Iteration 24, loss = 0.25738347\n",
      "Iteration 25, loss = 0.25571538\n",
      "Iteration 26, loss = 0.25408124\n",
      "Iteration 27, loss = 0.25269743\n",
      "Iteration 28, loss = 0.25122296\n",
      "Iteration 29, loss = 0.25005280\n",
      "Iteration 30, loss = 0.24887285\n",
      "Iteration 31, loss = 0.24734175\n",
      "Iteration 32, loss = 0.24605033\n",
      "Iteration 33, loss = 0.24495932\n",
      "Iteration 34, loss = 0.24390989\n",
      "Iteration 35, loss = 0.24287888\n",
      "Iteration 36, loss = 0.24195949\n",
      "Iteration 37, loss = 0.24103685\n",
      "Iteration 38, loss = 0.24044637\n",
      "Iteration 39, loss = 0.23947438\n",
      "Iteration 40, loss = 0.23899412\n",
      "Iteration 41, loss = 0.23823539\n",
      "Iteration 42, loss = 0.23781740\n",
      "Iteration 43, loss = 0.23701856\n",
      "Iteration 44, loss = 0.23643719\n",
      "Iteration 45, loss = 0.23619214\n",
      "Iteration 46, loss = 0.23564763\n",
      "Iteration 47, loss = 0.23518373\n",
      "Iteration 48, loss = 0.23469205\n",
      "Iteration 49, loss = 0.23442207\n",
      "Iteration 50, loss = 0.23389259\n",
      "Iteration 51, loss = 0.23407193\n",
      "Iteration 52, loss = 0.23364326\n",
      "Iteration 53, loss = 0.23303991\n",
      "Iteration 54, loss = 0.23286219\n",
      "Iteration 55, loss = 0.23250615\n",
      "Iteration 56, loss = 0.23254277\n",
      "Iteration 57, loss = 0.23198699\n",
      "Iteration 58, loss = 0.23210913\n",
      "Iteration 59, loss = 0.23179615\n",
      "Iteration 60, loss = 0.23138610\n",
      "Iteration 61, loss = 0.23125125\n",
      "Iteration 62, loss = 0.23119887\n",
      "Iteration 63, loss = 0.23104084\n",
      "Iteration 64, loss = 0.23024418\n",
      "Iteration 65, loss = 0.23016136\n",
      "Iteration 66, loss = 0.22986864\n",
      "Iteration 67, loss = 0.22970788\n",
      "Iteration 68, loss = 0.22953817\n",
      "Iteration 69, loss = 0.22912487\n",
      "Iteration 70, loss = 0.22895252\n",
      "Iteration 71, loss = 0.22909971\n",
      "Iteration 72, loss = 0.22818859\n",
      "Iteration 73, loss = 0.22779718\n",
      "Iteration 74, loss = 0.22735712\n",
      "Iteration 75, loss = 0.22743835\n",
      "Iteration 76, loss = 0.22680834\n",
      "Iteration 77, loss = 0.22637446\n",
      "Iteration 78, loss = 0.22606314\n",
      "Iteration 79, loss = 0.22594074\n",
      "Iteration 80, loss = 0.22523810\n",
      "Iteration 81, loss = 0.22468867\n",
      "Iteration 82, loss = 0.22456105\n",
      "Iteration 83, loss = 0.22399552\n",
      "Iteration 84, loss = 0.22352959\n",
      "Iteration 85, loss = 0.22307543\n",
      "Iteration 86, loss = 0.22249812\n",
      "Iteration 87, loss = 0.22210201\n",
      "Iteration 88, loss = 0.22151286\n",
      "Iteration 89, loss = 0.22091054\n",
      "Iteration 90, loss = 0.22039323\n",
      "Iteration 91, loss = 0.21995878\n",
      "Iteration 92, loss = 0.21903809\n",
      "Iteration 93, loss = 0.21867831\n",
      "Iteration 94, loss = 0.21801686\n",
      "Iteration 95, loss = 0.21742757\n",
      "Iteration 96, loss = 0.21674308\n",
      "Iteration 97, loss = 0.21580586\n",
      "Iteration 98, loss = 0.21554721\n",
      "Iteration 99, loss = 0.21474020\n",
      "Iteration 100, loss = 0.21437786\n",
      "Iteration 101, loss = 0.21322594\n",
      "Iteration 102, loss = 0.21288254\n",
      "Iteration 103, loss = 0.21181664\n",
      "Iteration 104, loss = 0.21090463\n",
      "Iteration 105, loss = 0.21040438\n",
      "Iteration 106, loss = 0.20985893\n",
      "Iteration 107, loss = 0.20858607\n",
      "Iteration 108, loss = 0.20781116\n",
      "Iteration 109, loss = 0.20751656\n",
      "Iteration 110, loss = 0.20605456\n",
      "Iteration 111, loss = 0.20536441\n",
      "Iteration 112, loss = 0.20458598\n",
      "Iteration 113, loss = 0.20414966\n",
      "Iteration 114, loss = 0.20308096\n",
      "Iteration 115, loss = 0.20220574\n",
      "Iteration 116, loss = 0.20125409\n",
      "Iteration 117, loss = 0.20038296\n",
      "Iteration 118, loss = 0.19935270\n",
      "Iteration 119, loss = 0.19870807\n",
      "Iteration 120, loss = 0.19787619\n",
      "Iteration 121, loss = 0.19690529\n",
      "Iteration 122, loss = 0.19629145\n",
      "Iteration 123, loss = 0.19560146\n",
      "Iteration 124, loss = 0.19458297\n",
      "Iteration 125, loss = 0.19342841\n",
      "Iteration 126, loss = 0.19293161\n",
      "Iteration 127, loss = 0.19191957\n",
      "Iteration 128, loss = 0.19140481\n",
      "Iteration 129, loss = 0.19051349\n",
      "Iteration 130, loss = 0.18928498\n",
      "Iteration 131, loss = 0.18841576\n",
      "Iteration 132, loss = 0.18790672\n",
      "Iteration 133, loss = 0.18699665\n",
      "Iteration 134, loss = 0.18638138\n",
      "Iteration 135, loss = 0.18534600\n",
      "Iteration 136, loss = 0.18455742\n",
      "Iteration 137, loss = 0.18384919\n",
      "Iteration 138, loss = 0.18319236\n",
      "Iteration 139, loss = 0.18211584\n",
      "Iteration 140, loss = 0.18165823\n",
      "Iteration 141, loss = 0.18081963\n",
      "Iteration 142, loss = 0.18012052\n",
      "Iteration 143, loss = 0.17943708\n",
      "Iteration 144, loss = 0.17921755\n",
      "Iteration 145, loss = 0.17789166\n",
      "Iteration 146, loss = 0.17724269\n",
      "Iteration 147, loss = 0.17655768\n",
      "Iteration 148, loss = 0.17581194\n",
      "Iteration 149, loss = 0.17494881\n",
      "Iteration 150, loss = 0.17450902\n",
      "Iteration 151, loss = 0.17379553\n",
      "Iteration 152, loss = 0.17335267\n",
      "Iteration 153, loss = 0.17227600\n",
      "Iteration 154, loss = 0.17179301\n",
      "Iteration 155, loss = 0.17126329\n",
      "Iteration 156, loss = 0.17051967\n",
      "Iteration 157, loss = 0.16998011\n",
      "Iteration 158, loss = 0.16935834\n",
      "Iteration 159, loss = 0.16880000\n",
      "Iteration 160, loss = 0.16843452\n",
      "Iteration 161, loss = 0.16771429\n",
      "Iteration 162, loss = 0.16711358\n",
      "Iteration 163, loss = 0.16659880\n",
      "Iteration 164, loss = 0.16589392\n",
      "Iteration 165, loss = 0.16545599\n",
      "Iteration 166, loss = 0.16505911\n",
      "Iteration 167, loss = 0.16434663\n",
      "Iteration 168, loss = 0.16390804\n",
      "Iteration 169, loss = 0.16333120\n",
      "Iteration 170, loss = 0.16259874\n",
      "Iteration 171, loss = 0.16219273\n",
      "Iteration 172, loss = 0.16164993\n",
      "Iteration 173, loss = 0.16121417\n",
      "Iteration 174, loss = 0.16089432\n",
      "Iteration 175, loss = 0.16026016\n",
      "Iteration 176, loss = 0.15967339\n",
      "Iteration 177, loss = 0.15922832\n",
      "Iteration 178, loss = 0.15864253\n",
      "Iteration 179, loss = 0.15825629\n",
      "Iteration 180, loss = 0.15777962\n",
      "Iteration 181, loss = 0.15702579\n",
      "Iteration 182, loss = 0.15652449\n",
      "Iteration 183, loss = 0.15591585\n",
      "Iteration 184, loss = 0.15585268\n",
      "Iteration 185, loss = 0.15509701\n",
      "Iteration 186, loss = 0.15446893\n",
      "Iteration 187, loss = 0.15436188\n",
      "Iteration 188, loss = 0.15365744\n",
      "Iteration 189, loss = 0.15301283\n",
      "Iteration 190, loss = 0.15276534\n",
      "Iteration 191, loss = 0.15231278\n",
      "Iteration 192, loss = 0.15188526\n",
      "Iteration 193, loss = 0.15125947\n",
      "Iteration 194, loss = 0.15064323\n",
      "Iteration 195, loss = 0.15009945\n",
      "Iteration 196, loss = 0.14964391\n",
      "Iteration 197, loss = 0.14946802\n",
      "Iteration 198, loss = 0.14871074\n",
      "Iteration 199, loss = 0.14852044\n",
      "Iteration 200, loss = 0.14783194\n",
      "Iteration 1, loss = 0.55137526\n",
      "Iteration 2, loss = 0.41965986\n",
      "Iteration 3, loss = 0.39008019\n",
      "Iteration 4, loss = 0.36862674\n",
      "Iteration 5, loss = 0.35100846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.33580957\n",
      "Iteration 7, loss = 0.32291240\n",
      "Iteration 8, loss = 0.31208968\n",
      "Iteration 9, loss = 0.30315491\n",
      "Iteration 10, loss = 0.29570469\n",
      "Iteration 11, loss = 0.28960961\n",
      "Iteration 12, loss = 0.28465027\n",
      "Iteration 13, loss = 0.28017703\n",
      "Iteration 14, loss = 0.27654149\n",
      "Iteration 15, loss = 0.27313933\n",
      "Iteration 16, loss = 0.27017541\n",
      "Iteration 17, loss = 0.26789826\n",
      "Iteration 18, loss = 0.26556628\n",
      "Iteration 19, loss = 0.26345731\n",
      "Iteration 20, loss = 0.26142517\n",
      "Iteration 21, loss = 0.25963464\n",
      "Iteration 22, loss = 0.25792653\n",
      "Iteration 23, loss = 0.25653937\n",
      "Iteration 24, loss = 0.25483214\n",
      "Iteration 25, loss = 0.25321645\n",
      "Iteration 26, loss = 0.25178726\n",
      "Iteration 27, loss = 0.25079232\n",
      "Iteration 28, loss = 0.24919703\n",
      "Iteration 29, loss = 0.24786410\n",
      "Iteration 30, loss = 0.24663338\n",
      "Iteration 31, loss = 0.24529943\n",
      "Iteration 32, loss = 0.24408110\n",
      "Iteration 33, loss = 0.24294235\n",
      "Iteration 34, loss = 0.24201055\n",
      "Iteration 35, loss = 0.24091385\n",
      "Iteration 36, loss = 0.24001486\n",
      "Iteration 37, loss = 0.23904919\n",
      "Iteration 38, loss = 0.23814991\n",
      "Iteration 39, loss = 0.23724966\n",
      "Iteration 40, loss = 0.23678873\n",
      "Iteration 41, loss = 0.23595956\n",
      "Iteration 42, loss = 0.23516051\n",
      "Iteration 43, loss = 0.23443686\n",
      "Iteration 44, loss = 0.23378253\n",
      "Iteration 45, loss = 0.23334830\n",
      "Iteration 46, loss = 0.23268676\n",
      "Iteration 47, loss = 0.23241103\n",
      "Iteration 48, loss = 0.23183444\n",
      "Iteration 49, loss = 0.23149434\n",
      "Iteration 50, loss = 0.23120359\n",
      "Iteration 51, loss = 0.23081704\n",
      "Iteration 52, loss = 0.23050359\n",
      "Iteration 53, loss = 0.22985607\n",
      "Iteration 54, loss = 0.22966651\n",
      "Iteration 55, loss = 0.22920583\n",
      "Iteration 56, loss = 0.22927431\n",
      "Iteration 57, loss = 0.22876022\n",
      "Iteration 58, loss = 0.22847329\n",
      "Iteration 59, loss = 0.22814061\n",
      "Iteration 60, loss = 0.22790647\n",
      "Iteration 61, loss = 0.22764711\n",
      "Iteration 62, loss = 0.22733234\n",
      "Iteration 63, loss = 0.22741836\n",
      "Iteration 64, loss = 0.22671707\n",
      "Iteration 65, loss = 0.22660007\n",
      "Iteration 66, loss = 0.22632051\n",
      "Iteration 67, loss = 0.22614992\n",
      "Iteration 68, loss = 0.22587984\n",
      "Iteration 69, loss = 0.22578079\n",
      "Iteration 70, loss = 0.22538562\n",
      "Iteration 71, loss = 0.22550644\n",
      "Iteration 72, loss = 0.22474453\n",
      "Iteration 73, loss = 0.22426541\n",
      "Iteration 74, loss = 0.22382187\n",
      "Iteration 75, loss = 0.22408040\n",
      "Iteration 76, loss = 0.22314294\n",
      "Iteration 77, loss = 0.22288657\n",
      "Iteration 78, loss = 0.22272298\n",
      "Iteration 79, loss = 0.22253714\n",
      "Iteration 80, loss = 0.22169678\n",
      "Iteration 81, loss = 0.22135898\n",
      "Iteration 82, loss = 0.22091478\n",
      "Iteration 83, loss = 0.22058153\n",
      "Iteration 84, loss = 0.22027336\n",
      "Iteration 85, loss = 0.21972278\n",
      "Iteration 86, loss = 0.21937703\n",
      "Iteration 87, loss = 0.21921892\n",
      "Iteration 88, loss = 0.21822011\n",
      "Iteration 89, loss = 0.21800363\n",
      "Iteration 90, loss = 0.21744455\n",
      "Iteration 91, loss = 0.21684151\n",
      "Iteration 92, loss = 0.21618543\n",
      "Iteration 93, loss = 0.21582283\n",
      "Iteration 94, loss = 0.21534872\n",
      "Iteration 95, loss = 0.21450411\n",
      "Iteration 96, loss = 0.21402356\n",
      "Iteration 97, loss = 0.21340765\n",
      "Iteration 98, loss = 0.21310514\n",
      "Iteration 99, loss = 0.21260081\n",
      "Iteration 100, loss = 0.21206837\n",
      "Iteration 101, loss = 0.21115132\n",
      "Iteration 102, loss = 0.21045150\n",
      "Iteration 103, loss = 0.20992272\n",
      "Iteration 104, loss = 0.20929490\n",
      "Iteration 105, loss = 0.20904163\n",
      "Iteration 106, loss = 0.20806654\n",
      "Iteration 107, loss = 0.20751010\n",
      "Iteration 108, loss = 0.20667475\n",
      "Iteration 109, loss = 0.20643343\n",
      "Iteration 110, loss = 0.20509775\n",
      "Iteration 111, loss = 0.20456929\n",
      "Iteration 112, loss = 0.20371259\n",
      "Iteration 113, loss = 0.20319035\n",
      "Iteration 114, loss = 0.20244558\n",
      "Iteration 115, loss = 0.20183903\n",
      "Iteration 116, loss = 0.20130873\n",
      "Iteration 117, loss = 0.20039648\n",
      "Iteration 118, loss = 0.19971947\n",
      "Iteration 119, loss = 0.19897283\n",
      "Iteration 120, loss = 0.19832604\n",
      "Iteration 121, loss = 0.19747146\n",
      "Iteration 122, loss = 0.19704773\n",
      "Iteration 123, loss = 0.19624440\n",
      "Iteration 124, loss = 0.19547206\n",
      "Iteration 125, loss = 0.19450128\n",
      "Iteration 126, loss = 0.19393853\n",
      "Iteration 127, loss = 0.19328796\n",
      "Iteration 128, loss = 0.19249131\n",
      "Iteration 129, loss = 0.19191479\n",
      "Iteration 130, loss = 0.19078690\n",
      "Iteration 131, loss = 0.19025214\n",
      "Iteration 132, loss = 0.18966730\n",
      "Iteration 133, loss = 0.18866792\n",
      "Iteration 134, loss = 0.18837251\n",
      "Iteration 135, loss = 0.18736361\n",
      "Iteration 136, loss = 0.18660904\n",
      "Iteration 137, loss = 0.18606947\n",
      "Iteration 138, loss = 0.18547083\n",
      "Iteration 139, loss = 0.18427303\n",
      "Iteration 140, loss = 0.18379680\n",
      "Iteration 141, loss = 0.18325582\n",
      "Iteration 142, loss = 0.18276262\n",
      "Iteration 143, loss = 0.18184120\n",
      "Iteration 144, loss = 0.18162059\n",
      "Iteration 145, loss = 0.18034952\n",
      "Iteration 146, loss = 0.17946560\n",
      "Iteration 147, loss = 0.17892317\n",
      "Iteration 148, loss = 0.17841832\n",
      "Iteration 149, loss = 0.17772216\n",
      "Iteration 150, loss = 0.17689657\n",
      "Iteration 151, loss = 0.17614528\n",
      "Iteration 152, loss = 0.17565153\n",
      "Iteration 153, loss = 0.17452300\n",
      "Iteration 154, loss = 0.17390551\n",
      "Iteration 155, loss = 0.17330255\n",
      "Iteration 156, loss = 0.17264885\n",
      "Iteration 157, loss = 0.17212094\n",
      "Iteration 158, loss = 0.17149593\n",
      "Iteration 159, loss = 0.17092072\n",
      "Iteration 160, loss = 0.17013416\n",
      "Iteration 161, loss = 0.16967255\n",
      "Iteration 162, loss = 0.16867765\n",
      "Iteration 163, loss = 0.16820276\n",
      "Iteration 164, loss = 0.16786541\n",
      "Iteration 165, loss = 0.16728859\n",
      "Iteration 166, loss = 0.16675112\n",
      "Iteration 167, loss = 0.16613481\n",
      "Iteration 168, loss = 0.16533911\n",
      "Iteration 169, loss = 0.16506331\n",
      "Iteration 170, loss = 0.16392275\n",
      "Iteration 171, loss = 0.16341280\n",
      "Iteration 172, loss = 0.16276056\n",
      "Iteration 173, loss = 0.16220798\n",
      "Iteration 174, loss = 0.16155620\n",
      "Iteration 175, loss = 0.16110930\n",
      "Iteration 176, loss = 0.16043212\n",
      "Iteration 177, loss = 0.15967115\n",
      "Iteration 178, loss = 0.15910973\n",
      "Iteration 179, loss = 0.15876431\n",
      "Iteration 180, loss = 0.15823004\n",
      "Iteration 181, loss = 0.15726817\n",
      "Iteration 182, loss = 0.15672976\n",
      "Iteration 183, loss = 0.15614458\n",
      "Iteration 184, loss = 0.15594917\n",
      "Iteration 185, loss = 0.15513243\n",
      "Iteration 186, loss = 0.15441321\n",
      "Iteration 187, loss = 0.15426292\n",
      "Iteration 188, loss = 0.15379372\n",
      "Iteration 189, loss = 0.15320788\n",
      "Iteration 190, loss = 0.15226658\n",
      "Iteration 191, loss = 0.15186290\n",
      "Iteration 192, loss = 0.15153785\n",
      "Iteration 193, loss = 0.15095795\n",
      "Iteration 194, loss = 0.15025244\n",
      "Iteration 195, loss = 0.14951901\n",
      "Iteration 196, loss = 0.14907160\n",
      "Iteration 197, loss = 0.14867427\n",
      "Iteration 198, loss = 0.14778458\n",
      "Iteration 199, loss = 0.14734752\n",
      "Iteration 200, loss = 0.14678662\n",
      "Iteration 1, loss = 0.55040160\n",
      "Iteration 2, loss = 0.41954209\n",
      "Iteration 3, loss = 0.39029232\n",
      "Iteration 4, loss = 0.36913277\n",
      "Iteration 5, loss = 0.35106183\n",
      "Iteration 6, loss = 0.33571742\n",
      "Iteration 7, loss = 0.32264345\n",
      "Iteration 8, loss = 0.31151933\n",
      "Iteration 9, loss = 0.30213239\n",
      "Iteration 10, loss = 0.29433675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.28778481\n",
      "Iteration 12, loss = 0.28233693\n",
      "Iteration 13, loss = 0.27764365\n",
      "Iteration 14, loss = 0.27379739\n",
      "Iteration 15, loss = 0.26991901\n",
      "Iteration 16, loss = 0.26687784\n",
      "Iteration 17, loss = 0.26426078\n",
      "Iteration 18, loss = 0.26190141\n",
      "Iteration 19, loss = 0.25962050\n",
      "Iteration 20, loss = 0.25739215\n",
      "Iteration 21, loss = 0.25549291\n",
      "Iteration 22, loss = 0.25361480\n",
      "Iteration 23, loss = 0.25220264\n",
      "Iteration 24, loss = 0.25035306\n",
      "Iteration 25, loss = 0.24874222\n",
      "Iteration 26, loss = 0.24738519\n",
      "Iteration 27, loss = 0.24639560\n",
      "Iteration 28, loss = 0.24454367\n",
      "Iteration 29, loss = 0.24340117\n",
      "Iteration 30, loss = 0.24231763\n",
      "Iteration 31, loss = 0.24097651\n",
      "Iteration 32, loss = 0.23958749\n",
      "Iteration 33, loss = 0.23868620\n",
      "Iteration 34, loss = 0.23754205\n",
      "Iteration 35, loss = 0.23672907\n",
      "Iteration 36, loss = 0.23557238\n",
      "Iteration 37, loss = 0.23473716\n",
      "Iteration 38, loss = 0.23399812\n",
      "Iteration 39, loss = 0.23332906\n",
      "Iteration 40, loss = 0.23277320\n",
      "Iteration 41, loss = 0.23171210\n",
      "Iteration 42, loss = 0.23113978\n",
      "Iteration 43, loss = 0.23052316\n",
      "Iteration 44, loss = 0.23002472\n",
      "Iteration 45, loss = 0.22949685\n",
      "Iteration 46, loss = 0.22896487\n",
      "Iteration 47, loss = 0.22852011\n",
      "Iteration 48, loss = 0.22827067\n",
      "Iteration 49, loss = 0.22781524\n",
      "Iteration 50, loss = 0.22744066\n",
      "Iteration 51, loss = 0.22723159\n",
      "Iteration 52, loss = 0.22684393\n",
      "Iteration 53, loss = 0.22641157\n",
      "Iteration 54, loss = 0.22611634\n",
      "Iteration 55, loss = 0.22574184\n",
      "Iteration 56, loss = 0.22565356\n",
      "Iteration 57, loss = 0.22517374\n",
      "Iteration 58, loss = 0.22519109\n",
      "Iteration 59, loss = 0.22474147\n",
      "Iteration 60, loss = 0.22465778\n",
      "Iteration 61, loss = 0.22450391\n",
      "Iteration 62, loss = 0.22412894\n",
      "Iteration 63, loss = 0.22388009\n",
      "Iteration 64, loss = 0.22369148\n",
      "Iteration 65, loss = 0.22363321\n",
      "Iteration 66, loss = 0.22347241\n",
      "Iteration 67, loss = 0.22295916\n",
      "Iteration 68, loss = 0.22277474\n",
      "Iteration 69, loss = 0.22246723\n",
      "Iteration 70, loss = 0.22250611\n",
      "Iteration 71, loss = 0.22210935\n",
      "Iteration 72, loss = 0.22164194\n",
      "Iteration 73, loss = 0.22141953\n",
      "Iteration 74, loss = 0.22092919\n",
      "Iteration 75, loss = 0.22097693\n",
      "Iteration 76, loss = 0.22041817\n",
      "Iteration 77, loss = 0.22017435\n",
      "Iteration 78, loss = 0.21985499\n",
      "Iteration 79, loss = 0.21940558\n",
      "Iteration 80, loss = 0.21916060\n",
      "Iteration 81, loss = 0.21883215\n",
      "Iteration 82, loss = 0.21836823\n",
      "Iteration 83, loss = 0.21767269\n",
      "Iteration 84, loss = 0.21763307\n",
      "Iteration 85, loss = 0.21698022\n",
      "Iteration 86, loss = 0.21686621\n",
      "Iteration 87, loss = 0.21619336\n",
      "Iteration 88, loss = 0.21570919\n",
      "Iteration 89, loss = 0.21557289\n",
      "Iteration 90, loss = 0.21449036\n",
      "Iteration 91, loss = 0.21426350\n",
      "Iteration 92, loss = 0.21393296\n",
      "Iteration 93, loss = 0.21319166\n",
      "Iteration 94, loss = 0.21323255\n",
      "Iteration 95, loss = 0.21231200\n",
      "Iteration 96, loss = 0.21161791\n",
      "Iteration 97, loss = 0.21094343\n",
      "Iteration 98, loss = 0.21036927\n",
      "Iteration 99, loss = 0.20989234\n",
      "Iteration 100, loss = 0.20939900\n",
      "Iteration 101, loss = 0.20864375\n",
      "Iteration 102, loss = 0.20786207\n",
      "Iteration 103, loss = 0.20751474\n",
      "Iteration 104, loss = 0.20692315\n",
      "Iteration 105, loss = 0.20621662\n",
      "Iteration 106, loss = 0.20521944\n",
      "Iteration 107, loss = 0.20484666\n",
      "Iteration 108, loss = 0.20428383\n",
      "Iteration 109, loss = 0.20336293\n",
      "Iteration 110, loss = 0.20251862\n",
      "Iteration 111, loss = 0.20199098\n",
      "Iteration 112, loss = 0.20105489\n",
      "Iteration 113, loss = 0.20009550\n",
      "Iteration 114, loss = 0.19944753\n",
      "Iteration 115, loss = 0.19897202\n",
      "Iteration 116, loss = 0.19833252\n",
      "Iteration 117, loss = 0.19724549\n",
      "Iteration 118, loss = 0.19660400\n",
      "Iteration 119, loss = 0.19591386\n",
      "Iteration 120, loss = 0.19492697\n",
      "Iteration 121, loss = 0.19443863\n",
      "Iteration 122, loss = 0.19344010\n",
      "Iteration 123, loss = 0.19284099\n",
      "Iteration 124, loss = 0.19195823\n",
      "Iteration 125, loss = 0.19104521\n",
      "Iteration 126, loss = 0.19056683\n",
      "Iteration 127, loss = 0.18976113\n",
      "Iteration 128, loss = 0.18885234\n",
      "Iteration 129, loss = 0.18812668\n",
      "Iteration 130, loss = 0.18723756\n",
      "Iteration 131, loss = 0.18682605\n",
      "Iteration 132, loss = 0.18604181\n",
      "Iteration 133, loss = 0.18525459\n",
      "Iteration 134, loss = 0.18460775\n",
      "Iteration 135, loss = 0.18360810\n",
      "Iteration 136, loss = 0.18294045\n",
      "Iteration 137, loss = 0.18177784\n",
      "Iteration 138, loss = 0.18150216\n",
      "Iteration 139, loss = 0.18044413\n",
      "Iteration 140, loss = 0.17990787\n",
      "Iteration 141, loss = 0.17938400\n",
      "Iteration 142, loss = 0.17854008\n",
      "Iteration 143, loss = 0.17792414\n",
      "Iteration 144, loss = 0.17753299\n",
      "Iteration 145, loss = 0.17666468\n",
      "Iteration 146, loss = 0.17585906\n",
      "Iteration 147, loss = 0.17488142\n",
      "Iteration 148, loss = 0.17441404\n",
      "Iteration 149, loss = 0.17394273\n",
      "Iteration 150, loss = 0.17314392\n",
      "Iteration 151, loss = 0.17236459\n",
      "Iteration 152, loss = 0.17195016\n",
      "Iteration 153, loss = 0.17114186\n",
      "Iteration 154, loss = 0.17035894\n",
      "Iteration 155, loss = 0.16984019\n",
      "Iteration 156, loss = 0.16912155\n",
      "Iteration 157, loss = 0.16875475\n",
      "Iteration 158, loss = 0.16822223\n",
      "Iteration 159, loss = 0.16760181\n",
      "Iteration 160, loss = 0.16683349\n",
      "Iteration 161, loss = 0.16637853\n",
      "Iteration 162, loss = 0.16573128\n",
      "Iteration 163, loss = 0.16528688\n",
      "Iteration 164, loss = 0.16506540\n",
      "Iteration 165, loss = 0.16433150\n",
      "Iteration 166, loss = 0.16396368\n",
      "Iteration 167, loss = 0.16364735\n",
      "Iteration 168, loss = 0.16265403\n",
      "Iteration 169, loss = 0.16246677\n",
      "Iteration 170, loss = 0.16174299\n",
      "Iteration 171, loss = 0.16099178\n",
      "Iteration 172, loss = 0.16076334\n",
      "Iteration 173, loss = 0.16020667\n",
      "Iteration 174, loss = 0.15942837\n",
      "Iteration 175, loss = 0.15967345\n",
      "Iteration 176, loss = 0.15876462\n",
      "Iteration 177, loss = 0.15793694\n",
      "Iteration 178, loss = 0.15763623\n",
      "Iteration 179, loss = 0.15735835\n",
      "Iteration 180, loss = 0.15695925\n",
      "Iteration 181, loss = 0.15609810\n",
      "Iteration 182, loss = 0.15541634\n",
      "Iteration 183, loss = 0.15520724\n",
      "Iteration 184, loss = 0.15485317\n",
      "Iteration 185, loss = 0.15435996\n",
      "Iteration 186, loss = 0.15371732\n",
      "Iteration 187, loss = 0.15339918\n",
      "Iteration 188, loss = 0.15315991\n",
      "Iteration 189, loss = 0.15291400\n",
      "Iteration 190, loss = 0.15190872\n",
      "Iteration 191, loss = 0.15146381\n",
      "Iteration 192, loss = 0.15114918\n",
      "Iteration 193, loss = 0.15078414\n",
      "Iteration 194, loss = 0.15018301\n",
      "Iteration 195, loss = 0.14953585\n",
      "Iteration 196, loss = 0.14921587\n",
      "Iteration 197, loss = 0.14873231\n",
      "Iteration 198, loss = 0.14802694\n",
      "Iteration 199, loss = 0.14781182\n",
      "Iteration 200, loss = 0.14756995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84417395\n",
      "Iteration 2, loss = 0.73242823\n",
      "Iteration 3, loss = 0.63636766\n",
      "Iteration 4, loss = 0.56459624\n",
      "Iteration 5, loss = 0.51175604\n",
      "Iteration 6, loss = 0.47145104\n",
      "Iteration 7, loss = 0.44015397\n",
      "Iteration 8, loss = 0.41536734\n",
      "Iteration 9, loss = 0.39533999\n",
      "Iteration 10, loss = 0.37877388\n",
      "Iteration 11, loss = 0.36497606\n",
      "Iteration 12, loss = 0.35342595\n",
      "Iteration 13, loss = 0.34353629\n",
      "Iteration 14, loss = 0.33504673\n",
      "Iteration 15, loss = 0.32760649\n",
      "Iteration 16, loss = 0.32114925\n",
      "Iteration 17, loss = 0.31551001\n",
      "Iteration 18, loss = 0.31038220\n",
      "Iteration 19, loss = 0.30585915\n",
      "Iteration 20, loss = 0.30180838\n",
      "Iteration 21, loss = 0.29819645\n",
      "Iteration 22, loss = 0.29489893\n",
      "Iteration 23, loss = 0.29194803\n",
      "Iteration 24, loss = 0.28919191\n",
      "Iteration 25, loss = 0.28671788\n",
      "Iteration 26, loss = 0.28445137\n",
      "Iteration 27, loss = 0.28233976\n",
      "Iteration 28, loss = 0.28042054\n",
      "Iteration 29, loss = 0.27860096\n",
      "Iteration 30, loss = 0.27696101\n",
      "Iteration 31, loss = 0.27542631\n",
      "Iteration 32, loss = 0.27397758\n",
      "Iteration 33, loss = 0.27264442\n",
      "Iteration 34, loss = 0.27137220\n",
      "Iteration 35, loss = 0.27016347\n",
      "Iteration 36, loss = 0.26905484\n",
      "Iteration 37, loss = 0.26799887\n",
      "Iteration 38, loss = 0.26705309\n",
      "Iteration 39, loss = 0.26606953\n",
      "Iteration 40, loss = 0.26520123\n",
      "Iteration 41, loss = 0.26433138\n",
      "Iteration 42, loss = 0.26353074\n",
      "Iteration 43, loss = 0.26274815\n",
      "Iteration 44, loss = 0.26199923\n",
      "Iteration 45, loss = 0.26130850\n",
      "Iteration 46, loss = 0.26064408\n",
      "Iteration 47, loss = 0.25999151\n",
      "Iteration 48, loss = 0.25941357\n",
      "Iteration 49, loss = 0.25879427\n",
      "Iteration 50, loss = 0.25822447\n",
      "Iteration 51, loss = 0.25766285\n",
      "Iteration 52, loss = 0.25713588\n",
      "Iteration 53, loss = 0.25664028\n",
      "Iteration 54, loss = 0.25613837\n",
      "Iteration 55, loss = 0.25564222\n",
      "Iteration 56, loss = 0.25517371\n",
      "Iteration 57, loss = 0.25470852\n",
      "Iteration 58, loss = 0.25429721\n",
      "Iteration 59, loss = 0.25385492\n",
      "Iteration 60, loss = 0.25342431\n",
      "Iteration 61, loss = 0.25304559\n",
      "Iteration 62, loss = 0.25264073\n",
      "Iteration 63, loss = 0.25228448\n",
      "Iteration 64, loss = 0.25189022\n",
      "Iteration 65, loss = 0.25152300\n",
      "Iteration 66, loss = 0.25118924\n",
      "Iteration 67, loss = 0.25080877\n",
      "Iteration 68, loss = 0.25050425\n",
      "Iteration 69, loss = 0.25013324\n",
      "Iteration 70, loss = 0.24980888\n",
      "Iteration 71, loss = 0.24951640\n",
      "Iteration 72, loss = 0.24919954\n",
      "Iteration 73, loss = 0.24887538\n",
      "Iteration 74, loss = 0.24859069\n",
      "Iteration 75, loss = 0.24828738\n",
      "Iteration 76, loss = 0.24799986\n",
      "Iteration 77, loss = 0.24770985\n",
      "Iteration 78, loss = 0.24742839\n",
      "Iteration 79, loss = 0.24718297\n",
      "Iteration 80, loss = 0.24687029\n",
      "Iteration 81, loss = 0.24662008\n",
      "Iteration 82, loss = 0.24634614\n",
      "Iteration 83, loss = 0.24610288\n",
      "Iteration 84, loss = 0.24586388\n",
      "Iteration 85, loss = 0.24563313\n",
      "Iteration 86, loss = 0.24536487\n",
      "Iteration 87, loss = 0.24512714\n",
      "Iteration 88, loss = 0.24487369\n",
      "Iteration 89, loss = 0.24465737\n",
      "Iteration 90, loss = 0.24441528\n",
      "Iteration 91, loss = 0.24419434\n",
      "Iteration 92, loss = 0.24395085\n",
      "Iteration 93, loss = 0.24373321\n",
      "Iteration 94, loss = 0.24352746\n",
      "Iteration 95, loss = 0.24329881\n",
      "Iteration 96, loss = 0.24308814\n",
      "Iteration 97, loss = 0.24287958\n",
      "Iteration 98, loss = 0.24268071\n",
      "Iteration 99, loss = 0.24247739\n",
      "Iteration 100, loss = 0.24225497\n",
      "Iteration 101, loss = 0.24206176\n",
      "Iteration 102, loss = 0.24189208\n",
      "Iteration 103, loss = 0.24166632\n",
      "Iteration 104, loss = 0.24147932\n",
      "Iteration 105, loss = 0.24130323\n",
      "Iteration 106, loss = 0.24113544\n",
      "Iteration 107, loss = 0.24089740\n",
      "Iteration 108, loss = 0.24073582\n",
      "Iteration 109, loss = 0.24055875\n",
      "Iteration 110, loss = 0.24037510\n",
      "Iteration 111, loss = 0.24018592\n",
      "Iteration 112, loss = 0.24001870\n",
      "Iteration 113, loss = 0.23985825\n",
      "Iteration 114, loss = 0.23967468\n",
      "Iteration 115, loss = 0.23947899\n",
      "Iteration 116, loss = 0.23932461\n",
      "Iteration 117, loss = 0.23916466\n",
      "Iteration 118, loss = 0.23896920\n",
      "Iteration 119, loss = 0.23881677\n",
      "Iteration 120, loss = 0.23865757\n",
      "Iteration 121, loss = 0.23851726\n",
      "Iteration 122, loss = 0.23834644\n",
      "Iteration 123, loss = 0.23817092\n",
      "Iteration 124, loss = 0.23802108\n",
      "Iteration 125, loss = 0.23787821\n",
      "Iteration 126, loss = 0.23771091\n",
      "Iteration 127, loss = 0.23756960\n",
      "Iteration 128, loss = 0.23744137\n",
      "Iteration 129, loss = 0.23729792\n",
      "Iteration 130, loss = 0.23711884\n",
      "Iteration 131, loss = 0.23700551\n",
      "Iteration 132, loss = 0.23684468\n",
      "Iteration 133, loss = 0.23667162\n",
      "Iteration 134, loss = 0.23657164\n",
      "Iteration 135, loss = 0.23641263\n",
      "Iteration 136, loss = 0.23630511\n",
      "Iteration 137, loss = 0.23613334\n",
      "Iteration 138, loss = 0.23599987\n",
      "Iteration 139, loss = 0.23587098\n",
      "Iteration 140, loss = 0.23572998\n",
      "Iteration 141, loss = 0.23560184\n",
      "Iteration 142, loss = 0.23548297\n",
      "Iteration 143, loss = 0.23533547\n",
      "Iteration 144, loss = 0.23521946\n",
      "Iteration 145, loss = 0.23509139\n",
      "Iteration 146, loss = 0.23496317\n",
      "Iteration 147, loss = 0.23482674\n",
      "Iteration 148, loss = 0.23469235\n",
      "Iteration 149, loss = 0.23456958\n",
      "Iteration 150, loss = 0.23445607\n",
      "Iteration 151, loss = 0.23433939\n",
      "Iteration 152, loss = 0.23423274\n",
      "Iteration 153, loss = 0.23410794\n",
      "Iteration 154, loss = 0.23399872\n",
      "Iteration 155, loss = 0.23387160\n",
      "Iteration 156, loss = 0.23379172\n",
      "Iteration 157, loss = 0.23364316\n",
      "Iteration 158, loss = 0.23352963\n",
      "Iteration 159, loss = 0.23342228\n",
      "Iteration 160, loss = 0.23331688\n",
      "Iteration 161, loss = 0.23318331\n",
      "Iteration 162, loss = 0.23307494\n",
      "Iteration 163, loss = 0.23296697\n",
      "Iteration 164, loss = 0.23285767\n",
      "Iteration 165, loss = 0.23277987\n",
      "Iteration 166, loss = 0.23265706\n",
      "Iteration 167, loss = 0.23259957\n",
      "Iteration 168, loss = 0.23245538\n",
      "Iteration 169, loss = 0.23234669\n",
      "Iteration 170, loss = 0.23226116\n",
      "Iteration 171, loss = 0.23213597\n",
      "Iteration 172, loss = 0.23203620\n",
      "Iteration 173, loss = 0.23193265\n",
      "Iteration 174, loss = 0.23183660\n",
      "Iteration 175, loss = 0.23175756\n",
      "Iteration 176, loss = 0.23165684\n",
      "Iteration 177, loss = 0.23152996\n",
      "Iteration 178, loss = 0.23145009\n",
      "Iteration 179, loss = 0.23133889\n",
      "Iteration 180, loss = 0.23123854\n",
      "Iteration 181, loss = 0.23113512\n",
      "Iteration 182, loss = 0.23108522\n",
      "Iteration 183, loss = 0.23096377\n",
      "Iteration 184, loss = 0.23088546\n",
      "Iteration 185, loss = 0.23077696\n",
      "Iteration 186, loss = 0.23068970\n",
      "Iteration 187, loss = 0.23060729\n",
      "Iteration 188, loss = 0.23052363\n",
      "Iteration 189, loss = 0.23041626\n",
      "Iteration 190, loss = 0.23033038\n",
      "Iteration 191, loss = 0.23024444\n",
      "Iteration 192, loss = 0.23016533\n",
      "Iteration 193, loss = 0.23007833\n",
      "Iteration 194, loss = 0.22997673\n",
      "Iteration 195, loss = 0.22989719\n",
      "Iteration 196, loss = 0.22981186\n",
      "Iteration 197, loss = 0.22975032\n",
      "Iteration 198, loss = 0.22962541\n",
      "Iteration 199, loss = 0.22958940\n",
      "Iteration 200, loss = 0.22951284\n",
      "Iteration 1, loss = 0.84286491\n",
      "Iteration 2, loss = 0.73108972\n",
      "Iteration 3, loss = 0.63494203\n",
      "Iteration 4, loss = 0.56308067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.50990778\n",
      "Iteration 6, loss = 0.46956048\n",
      "Iteration 7, loss = 0.43795871\n",
      "Iteration 8, loss = 0.41299647\n",
      "Iteration 9, loss = 0.39276283\n",
      "Iteration 10, loss = 0.37603595\n",
      "Iteration 11, loss = 0.36210913\n",
      "Iteration 12, loss = 0.35044839\n",
      "Iteration 13, loss = 0.34047583\n",
      "Iteration 14, loss = 0.33181593\n",
      "Iteration 15, loss = 0.32432736\n",
      "Iteration 16, loss = 0.31778532\n",
      "Iteration 17, loss = 0.31206163\n",
      "Iteration 18, loss = 0.30687894\n",
      "Iteration 19, loss = 0.30230911\n",
      "Iteration 20, loss = 0.29818560\n",
      "Iteration 21, loss = 0.29453259\n",
      "Iteration 22, loss = 0.29117758\n",
      "Iteration 23, loss = 0.28817580\n",
      "Iteration 24, loss = 0.28540454\n",
      "Iteration 25, loss = 0.28286100\n",
      "Iteration 26, loss = 0.28056352\n",
      "Iteration 27, loss = 0.27842849\n",
      "Iteration 28, loss = 0.27646650\n",
      "Iteration 29, loss = 0.27462347\n",
      "Iteration 30, loss = 0.27293879\n",
      "Iteration 31, loss = 0.27135840\n",
      "Iteration 32, loss = 0.26988155\n",
      "Iteration 33, loss = 0.26848892\n",
      "Iteration 34, loss = 0.26721506\n",
      "Iteration 35, loss = 0.26595682\n",
      "Iteration 36, loss = 0.26483691\n",
      "Iteration 37, loss = 0.26373495\n",
      "Iteration 38, loss = 0.26272925\n",
      "Iteration 39, loss = 0.26174404\n",
      "Iteration 40, loss = 0.26082780\n",
      "Iteration 41, loss = 0.25994839\n",
      "Iteration 42, loss = 0.25913245\n",
      "Iteration 43, loss = 0.25831633\n",
      "Iteration 44, loss = 0.25757016\n",
      "Iteration 45, loss = 0.25681875\n",
      "Iteration 46, loss = 0.25610545\n",
      "Iteration 47, loss = 0.25544980\n",
      "Iteration 48, loss = 0.25481471\n",
      "Iteration 49, loss = 0.25419731\n",
      "Iteration 50, loss = 0.25357880\n",
      "Iteration 51, loss = 0.25302635\n",
      "Iteration 52, loss = 0.25246083\n",
      "Iteration 53, loss = 0.25191539\n",
      "Iteration 54, loss = 0.25139964\n",
      "Iteration 55, loss = 0.25088870\n",
      "Iteration 56, loss = 0.25040765\n",
      "Iteration 57, loss = 0.24992404\n",
      "Iteration 58, loss = 0.24947498\n",
      "Iteration 59, loss = 0.24901656\n",
      "Iteration 60, loss = 0.24857068\n",
      "Iteration 61, loss = 0.24815853\n",
      "Iteration 62, loss = 0.24772988\n",
      "Iteration 63, loss = 0.24736912\n",
      "Iteration 64, loss = 0.24696227\n",
      "Iteration 65, loss = 0.24659691\n",
      "Iteration 66, loss = 0.24622230\n",
      "Iteration 67, loss = 0.24582154\n",
      "Iteration 68, loss = 0.24550545\n",
      "Iteration 69, loss = 0.24510624\n",
      "Iteration 70, loss = 0.24476349\n",
      "Iteration 71, loss = 0.24445990\n",
      "Iteration 72, loss = 0.24411518\n",
      "Iteration 73, loss = 0.24378269\n",
      "Iteration 74, loss = 0.24347217\n",
      "Iteration 75, loss = 0.24315795\n",
      "Iteration 76, loss = 0.24283750\n",
      "Iteration 77, loss = 0.24253959\n",
      "Iteration 78, loss = 0.24225464\n",
      "Iteration 79, loss = 0.24197253\n",
      "Iteration 80, loss = 0.24163792\n",
      "Iteration 81, loss = 0.24138240\n",
      "Iteration 82, loss = 0.24110277\n",
      "Iteration 83, loss = 0.24083732\n",
      "Iteration 84, loss = 0.24056289\n",
      "Iteration 85, loss = 0.24033545\n",
      "Iteration 86, loss = 0.24004591\n",
      "Iteration 87, loss = 0.23981167\n",
      "Iteration 88, loss = 0.23952336\n",
      "Iteration 89, loss = 0.23931221\n",
      "Iteration 90, loss = 0.23905505\n",
      "Iteration 91, loss = 0.23880845\n",
      "Iteration 92, loss = 0.23855201\n",
      "Iteration 93, loss = 0.23833394\n",
      "Iteration 94, loss = 0.23810429\n",
      "Iteration 95, loss = 0.23787327\n",
      "Iteration 96, loss = 0.23763796\n",
      "Iteration 97, loss = 0.23743695\n",
      "Iteration 98, loss = 0.23719978\n",
      "Iteration 99, loss = 0.23699614\n",
      "Iteration 100, loss = 0.23676729\n",
      "Iteration 101, loss = 0.23655712\n",
      "Iteration 102, loss = 0.23635508\n",
      "Iteration 103, loss = 0.23616725\n",
      "Iteration 104, loss = 0.23592842\n",
      "Iteration 105, loss = 0.23573613\n",
      "Iteration 106, loss = 0.23557413\n",
      "Iteration 107, loss = 0.23534379\n",
      "Iteration 108, loss = 0.23514998\n",
      "Iteration 109, loss = 0.23497093\n",
      "Iteration 110, loss = 0.23476610\n",
      "Iteration 111, loss = 0.23458189\n",
      "Iteration 112, loss = 0.23439057\n",
      "Iteration 113, loss = 0.23420772\n",
      "Iteration 114, loss = 0.23406364\n",
      "Iteration 115, loss = 0.23381160\n",
      "Iteration 116, loss = 0.23366563\n",
      "Iteration 117, loss = 0.23347787\n",
      "Iteration 118, loss = 0.23330445\n",
      "Iteration 119, loss = 0.23313484\n",
      "Iteration 120, loss = 0.23297994\n",
      "Iteration 121, loss = 0.23278945\n",
      "Iteration 122, loss = 0.23264369\n",
      "Iteration 123, loss = 0.23244596\n",
      "Iteration 124, loss = 0.23229319\n",
      "Iteration 125, loss = 0.23211656\n",
      "Iteration 126, loss = 0.23197879\n",
      "Iteration 127, loss = 0.23180727\n",
      "Iteration 128, loss = 0.23168978\n",
      "Iteration 129, loss = 0.23150642\n",
      "Iteration 130, loss = 0.23133297\n",
      "Iteration 131, loss = 0.23118106\n",
      "Iteration 132, loss = 0.23105416\n",
      "Iteration 133, loss = 0.23088752\n",
      "Iteration 134, loss = 0.23075984\n",
      "Iteration 135, loss = 0.23061128\n",
      "Iteration 136, loss = 0.23045515\n",
      "Iteration 137, loss = 0.23028805\n",
      "Iteration 138, loss = 0.23016946\n",
      "Iteration 139, loss = 0.23001242\n",
      "Iteration 140, loss = 0.22989333\n",
      "Iteration 141, loss = 0.22974370\n",
      "Iteration 142, loss = 0.22961760\n",
      "Iteration 143, loss = 0.22945493\n",
      "Iteration 144, loss = 0.22936917\n",
      "Iteration 145, loss = 0.22919668\n",
      "Iteration 146, loss = 0.22906501\n",
      "Iteration 147, loss = 0.22893149\n",
      "Iteration 148, loss = 0.22879527\n",
      "Iteration 149, loss = 0.22865759\n",
      "Iteration 150, loss = 0.22855098\n",
      "Iteration 151, loss = 0.22840992\n",
      "Iteration 152, loss = 0.22830743\n",
      "Iteration 153, loss = 0.22816069\n",
      "Iteration 154, loss = 0.22803815\n",
      "Iteration 155, loss = 0.22792267\n",
      "Iteration 156, loss = 0.22784552\n",
      "Iteration 157, loss = 0.22768350\n",
      "Iteration 158, loss = 0.22755654\n",
      "Iteration 159, loss = 0.22746103\n",
      "Iteration 160, loss = 0.22735146\n",
      "Iteration 161, loss = 0.22722805\n",
      "Iteration 162, loss = 0.22708945\n",
      "Iteration 163, loss = 0.22698756\n",
      "Iteration 164, loss = 0.22687127\n",
      "Iteration 165, loss = 0.22677169\n",
      "Iteration 166, loss = 0.22663271\n",
      "Iteration 167, loss = 0.22657487\n",
      "Iteration 168, loss = 0.22643084\n",
      "Iteration 169, loss = 0.22632740\n",
      "Iteration 170, loss = 0.22622205\n",
      "Iteration 171, loss = 0.22609551\n",
      "Iteration 172, loss = 0.22599906\n",
      "Iteration 173, loss = 0.22590160\n",
      "Iteration 174, loss = 0.22578986\n",
      "Iteration 175, loss = 0.22570499\n",
      "Iteration 176, loss = 0.22561314\n",
      "Iteration 177, loss = 0.22549650\n",
      "Iteration 178, loss = 0.22535795\n",
      "Iteration 179, loss = 0.22527765\n",
      "Iteration 180, loss = 0.22517326\n",
      "Iteration 181, loss = 0.22506502\n",
      "Iteration 182, loss = 0.22499744\n",
      "Iteration 183, loss = 0.22487458\n",
      "Iteration 184, loss = 0.22479470\n",
      "Iteration 185, loss = 0.22469085\n",
      "Iteration 186, loss = 0.22459895\n",
      "Iteration 187, loss = 0.22451167\n",
      "Iteration 188, loss = 0.22442862\n",
      "Iteration 189, loss = 0.22431927\n",
      "Iteration 190, loss = 0.22422869\n",
      "Iteration 191, loss = 0.22413875\n",
      "Iteration 192, loss = 0.22405110\n",
      "Iteration 193, loss = 0.22395308\n",
      "Iteration 194, loss = 0.22386563\n",
      "Iteration 195, loss = 0.22377350\n",
      "Iteration 196, loss = 0.22369366\n",
      "Iteration 197, loss = 0.22360321\n",
      "Iteration 198, loss = 0.22350237\n",
      "Iteration 199, loss = 0.22345751\n",
      "Iteration 200, loss = 0.22334466\n",
      "Iteration 1, loss = 0.84927725\n",
      "Iteration 2, loss = 0.73512048\n",
      "Iteration 3, loss = 0.63761253\n",
      "Iteration 4, loss = 0.56535933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.51188548\n",
      "Iteration 6, loss = 0.47130704\n",
      "Iteration 7, loss = 0.43989253\n",
      "Iteration 8, loss = 0.41513332\n",
      "Iteration 9, loss = 0.39499253\n",
      "Iteration 10, loss = 0.37846590\n",
      "Iteration 11, loss = 0.36472533\n",
      "Iteration 12, loss = 0.35322389\n",
      "Iteration 13, loss = 0.34340586\n",
      "Iteration 14, loss = 0.33487384\n",
      "Iteration 15, loss = 0.32754514\n",
      "Iteration 16, loss = 0.32109959\n",
      "Iteration 17, loss = 0.31546892\n",
      "Iteration 18, loss = 0.31041485\n",
      "Iteration 19, loss = 0.30595731\n",
      "Iteration 20, loss = 0.30190528\n",
      "Iteration 21, loss = 0.29832955\n",
      "Iteration 22, loss = 0.29508895\n",
      "Iteration 23, loss = 0.29212649\n",
      "Iteration 24, loss = 0.28941246\n",
      "Iteration 25, loss = 0.28693573\n",
      "Iteration 26, loss = 0.28467411\n",
      "Iteration 27, loss = 0.28260932\n",
      "Iteration 28, loss = 0.28065621\n",
      "Iteration 29, loss = 0.27889196\n",
      "Iteration 30, loss = 0.27723826\n",
      "Iteration 31, loss = 0.27572101\n",
      "Iteration 32, loss = 0.27424776\n",
      "Iteration 33, loss = 0.27289082\n",
      "Iteration 34, loss = 0.27163248\n",
      "Iteration 35, loss = 0.27045455\n",
      "Iteration 36, loss = 0.26930851\n",
      "Iteration 37, loss = 0.26826407\n",
      "Iteration 38, loss = 0.26727816\n",
      "Iteration 39, loss = 0.26631664\n",
      "Iteration 40, loss = 0.26544525\n",
      "Iteration 41, loss = 0.26456662\n",
      "Iteration 42, loss = 0.26377247\n",
      "Iteration 43, loss = 0.26298651\n",
      "Iteration 44, loss = 0.26225286\n",
      "Iteration 45, loss = 0.26154395\n",
      "Iteration 46, loss = 0.26084722\n",
      "Iteration 47, loss = 0.26020092\n",
      "Iteration 48, loss = 0.25957362\n",
      "Iteration 49, loss = 0.25898475\n",
      "Iteration 50, loss = 0.25835493\n",
      "Iteration 51, loss = 0.25783251\n",
      "Iteration 52, loss = 0.25727675\n",
      "Iteration 53, loss = 0.25673608\n",
      "Iteration 54, loss = 0.25624397\n",
      "Iteration 55, loss = 0.25573969\n",
      "Iteration 56, loss = 0.25527566\n",
      "Iteration 57, loss = 0.25480333\n",
      "Iteration 58, loss = 0.25436826\n",
      "Iteration 59, loss = 0.25391096\n",
      "Iteration 60, loss = 0.25349015\n",
      "Iteration 61, loss = 0.25307609\n",
      "Iteration 62, loss = 0.25266363\n",
      "Iteration 63, loss = 0.25231321\n",
      "Iteration 64, loss = 0.25188911\n",
      "Iteration 65, loss = 0.25152472\n",
      "Iteration 66, loss = 0.25115016\n",
      "Iteration 67, loss = 0.25080210\n",
      "Iteration 68, loss = 0.25045771\n",
      "Iteration 69, loss = 0.25008528\n",
      "Iteration 70, loss = 0.24976243\n",
      "Iteration 71, loss = 0.24942890\n",
      "Iteration 72, loss = 0.24909295\n",
      "Iteration 73, loss = 0.24877080\n",
      "Iteration 74, loss = 0.24846280\n",
      "Iteration 75, loss = 0.24817354\n",
      "Iteration 76, loss = 0.24786266\n",
      "Iteration 77, loss = 0.24754455\n",
      "Iteration 78, loss = 0.24726343\n",
      "Iteration 79, loss = 0.24699625\n",
      "Iteration 80, loss = 0.24669228\n",
      "Iteration 81, loss = 0.24640670\n",
      "Iteration 82, loss = 0.24614641\n",
      "Iteration 83, loss = 0.24588299\n",
      "Iteration 84, loss = 0.24562096\n",
      "Iteration 85, loss = 0.24536109\n",
      "Iteration 86, loss = 0.24508911\n",
      "Iteration 87, loss = 0.24484913\n",
      "Iteration 88, loss = 0.24457917\n",
      "Iteration 89, loss = 0.24433933\n",
      "Iteration 90, loss = 0.24411023\n",
      "Iteration 91, loss = 0.24386546\n",
      "Iteration 92, loss = 0.24360450\n",
      "Iteration 93, loss = 0.24340035\n",
      "Iteration 94, loss = 0.24315698\n",
      "Iteration 95, loss = 0.24293092\n",
      "Iteration 96, loss = 0.24271721\n",
      "Iteration 97, loss = 0.24247121\n",
      "Iteration 98, loss = 0.24227194\n",
      "Iteration 99, loss = 0.24205748\n",
      "Iteration 100, loss = 0.24184594\n",
      "Iteration 101, loss = 0.24162337\n",
      "Iteration 102, loss = 0.24141005\n",
      "Iteration 103, loss = 0.24123524\n",
      "Iteration 104, loss = 0.24100124\n",
      "Iteration 105, loss = 0.24081046\n",
      "Iteration 106, loss = 0.24061588\n",
      "Iteration 107, loss = 0.24040989\n",
      "Iteration 108, loss = 0.24020712\n",
      "Iteration 109, loss = 0.24007369\n",
      "Iteration 110, loss = 0.23982825\n",
      "Iteration 111, loss = 0.23963902\n",
      "Iteration 112, loss = 0.23945544\n",
      "Iteration 113, loss = 0.23928258\n",
      "Iteration 114, loss = 0.23909877\n",
      "Iteration 115, loss = 0.23890560\n",
      "Iteration 116, loss = 0.23872994\n",
      "Iteration 117, loss = 0.23855928\n",
      "Iteration 118, loss = 0.23836083\n",
      "Iteration 119, loss = 0.23820604\n",
      "Iteration 120, loss = 0.23803762\n",
      "Iteration 121, loss = 0.23785321\n",
      "Iteration 122, loss = 0.23769007\n",
      "Iteration 123, loss = 0.23753902\n",
      "Iteration 124, loss = 0.23737267\n",
      "Iteration 125, loss = 0.23718723\n",
      "Iteration 126, loss = 0.23706735\n",
      "Iteration 127, loss = 0.23686915\n",
      "Iteration 128, loss = 0.23678703\n",
      "Iteration 129, loss = 0.23658098\n",
      "Iteration 130, loss = 0.23639117\n",
      "Iteration 131, loss = 0.23624301\n",
      "Iteration 132, loss = 0.23611694\n",
      "Iteration 133, loss = 0.23593724\n",
      "Iteration 134, loss = 0.23583311\n",
      "Iteration 135, loss = 0.23566963\n",
      "Iteration 136, loss = 0.23550971\n",
      "Iteration 137, loss = 0.23538528\n",
      "Iteration 138, loss = 0.23524808\n",
      "Iteration 139, loss = 0.23507180\n",
      "Iteration 140, loss = 0.23495528\n",
      "Iteration 141, loss = 0.23481805\n",
      "Iteration 142, loss = 0.23466029\n",
      "Iteration 143, loss = 0.23452407\n",
      "Iteration 144, loss = 0.23443369\n",
      "Iteration 145, loss = 0.23425304\n",
      "Iteration 146, loss = 0.23412427\n",
      "Iteration 147, loss = 0.23398198\n",
      "Iteration 148, loss = 0.23384789\n",
      "Iteration 149, loss = 0.23370447\n",
      "Iteration 150, loss = 0.23362059\n",
      "Iteration 151, loss = 0.23346491\n",
      "Iteration 152, loss = 0.23335964\n",
      "Iteration 153, loss = 0.23320474\n",
      "Iteration 154, loss = 0.23308269\n",
      "Iteration 155, loss = 0.23296117\n",
      "Iteration 156, loss = 0.23286277\n",
      "Iteration 157, loss = 0.23272536\n",
      "Iteration 158, loss = 0.23260074\n",
      "Iteration 159, loss = 0.23248939\n",
      "Iteration 160, loss = 0.23239636\n",
      "Iteration 161, loss = 0.23225201\n",
      "Iteration 162, loss = 0.23213758\n",
      "Iteration 163, loss = 0.23201058\n",
      "Iteration 164, loss = 0.23189899\n",
      "Iteration 165, loss = 0.23181779\n",
      "Iteration 166, loss = 0.23168832\n",
      "Iteration 167, loss = 0.23159231\n",
      "Iteration 168, loss = 0.23145694\n",
      "Iteration 169, loss = 0.23135227\n",
      "Iteration 170, loss = 0.23124481\n",
      "Iteration 171, loss = 0.23113157\n",
      "Iteration 172, loss = 0.23101770\n",
      "Iteration 173, loss = 0.23092548\n",
      "Iteration 174, loss = 0.23080881\n",
      "Iteration 175, loss = 0.23071240\n",
      "Iteration 176, loss = 0.23061685\n",
      "Iteration 177, loss = 0.23051693\n",
      "Iteration 178, loss = 0.23039220\n",
      "Iteration 179, loss = 0.23029556\n",
      "Iteration 180, loss = 0.23020425\n",
      "Iteration 181, loss = 0.23007707\n",
      "Iteration 182, loss = 0.23001143\n",
      "Iteration 183, loss = 0.22988101\n",
      "Iteration 184, loss = 0.22982353\n",
      "Iteration 185, loss = 0.22972099\n",
      "Iteration 186, loss = 0.22961205\n",
      "Iteration 187, loss = 0.22953336\n",
      "Iteration 188, loss = 0.22944138\n",
      "Iteration 189, loss = 0.22933914\n",
      "Iteration 190, loss = 0.22922023\n",
      "Iteration 191, loss = 0.22917807\n",
      "Iteration 192, loss = 0.22905015\n",
      "Iteration 193, loss = 0.22897541\n",
      "Iteration 194, loss = 0.22885742\n",
      "Iteration 195, loss = 0.22876325\n",
      "Iteration 196, loss = 0.22868352\n",
      "Iteration 197, loss = 0.22859684\n",
      "Iteration 198, loss = 0.22851852\n",
      "Iteration 199, loss = 0.22844966\n",
      "Iteration 200, loss = 0.22832379\n",
      "Iteration 1, loss = 0.84476495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.73211981\n",
      "Iteration 3, loss = 0.63559504\n",
      "Iteration 4, loss = 0.56361933\n",
      "Iteration 5, loss = 0.51041556\n",
      "Iteration 6, loss = 0.46977917\n",
      "Iteration 7, loss = 0.43834969\n",
      "Iteration 8, loss = 0.41340333\n",
      "Iteration 9, loss = 0.39310191\n",
      "Iteration 10, loss = 0.37642365\n",
      "Iteration 11, loss = 0.36252292\n",
      "Iteration 12, loss = 0.35087689\n",
      "Iteration 13, loss = 0.34092224\n",
      "Iteration 14, loss = 0.33226023\n",
      "Iteration 15, loss = 0.32484748\n",
      "Iteration 16, loss = 0.31824652\n",
      "Iteration 17, loss = 0.31257151\n",
      "Iteration 18, loss = 0.30741962\n",
      "Iteration 19, loss = 0.30287508\n",
      "Iteration 20, loss = 0.29877990\n",
      "Iteration 21, loss = 0.29512758\n",
      "Iteration 22, loss = 0.29183641\n",
      "Iteration 23, loss = 0.28882871\n",
      "Iteration 24, loss = 0.28606009\n",
      "Iteration 25, loss = 0.28355856\n",
      "Iteration 26, loss = 0.28125317\n",
      "Iteration 27, loss = 0.27918004\n",
      "Iteration 28, loss = 0.27719551\n",
      "Iteration 29, loss = 0.27537244\n",
      "Iteration 30, loss = 0.27368783\n",
      "Iteration 31, loss = 0.27214350\n",
      "Iteration 32, loss = 0.27065968\n",
      "Iteration 33, loss = 0.26929752\n",
      "Iteration 34, loss = 0.26802468\n",
      "Iteration 35, loss = 0.26683602\n",
      "Iteration 36, loss = 0.26570015\n",
      "Iteration 37, loss = 0.26462287\n",
      "Iteration 38, loss = 0.26361134\n",
      "Iteration 39, loss = 0.26267114\n",
      "Iteration 40, loss = 0.26177730\n",
      "Iteration 41, loss = 0.26091135\n",
      "Iteration 42, loss = 0.26009356\n",
      "Iteration 43, loss = 0.25931311\n",
      "Iteration 44, loss = 0.25856229\n",
      "Iteration 45, loss = 0.25786349\n",
      "Iteration 46, loss = 0.25717156\n",
      "Iteration 47, loss = 0.25653286\n",
      "Iteration 48, loss = 0.25589621\n",
      "Iteration 49, loss = 0.25532034\n",
      "Iteration 50, loss = 0.25471355\n",
      "Iteration 51, loss = 0.25416736\n",
      "Iteration 52, loss = 0.25362995\n",
      "Iteration 53, loss = 0.25308955\n",
      "Iteration 54, loss = 0.25259733\n",
      "Iteration 55, loss = 0.25210893\n",
      "Iteration 56, loss = 0.25165203\n",
      "Iteration 57, loss = 0.25118607\n",
      "Iteration 58, loss = 0.25073970\n",
      "Iteration 59, loss = 0.25029807\n",
      "Iteration 60, loss = 0.24989313\n",
      "Iteration 61, loss = 0.24947422\n",
      "Iteration 62, loss = 0.24908248\n",
      "Iteration 63, loss = 0.24871607\n",
      "Iteration 64, loss = 0.24832185\n",
      "Iteration 65, loss = 0.24794899\n",
      "Iteration 66, loss = 0.24759644\n",
      "Iteration 67, loss = 0.24726284\n",
      "Iteration 68, loss = 0.24689829\n",
      "Iteration 69, loss = 0.24656841\n",
      "Iteration 70, loss = 0.24624029\n",
      "Iteration 71, loss = 0.24590461\n",
      "Iteration 72, loss = 0.24560061\n",
      "Iteration 73, loss = 0.24527223\n",
      "Iteration 74, loss = 0.24496960\n",
      "Iteration 75, loss = 0.24470097\n",
      "Iteration 76, loss = 0.24437088\n",
      "Iteration 77, loss = 0.24407101\n",
      "Iteration 78, loss = 0.24380704\n",
      "Iteration 79, loss = 0.24354329\n",
      "Iteration 80, loss = 0.24324262\n",
      "Iteration 81, loss = 0.24295887\n",
      "Iteration 82, loss = 0.24269376\n",
      "Iteration 83, loss = 0.24243455\n",
      "Iteration 84, loss = 0.24218527\n",
      "Iteration 85, loss = 0.24191154\n",
      "Iteration 86, loss = 0.24169229\n",
      "Iteration 87, loss = 0.24143913\n",
      "Iteration 88, loss = 0.24116617\n",
      "Iteration 89, loss = 0.24094976\n",
      "Iteration 90, loss = 0.24071600\n",
      "Iteration 91, loss = 0.24046831\n",
      "Iteration 92, loss = 0.24022233\n",
      "Iteration 93, loss = 0.24000377\n",
      "Iteration 94, loss = 0.23977675\n",
      "Iteration 95, loss = 0.23953521\n",
      "Iteration 96, loss = 0.23932882\n",
      "Iteration 97, loss = 0.23910143\n",
      "Iteration 98, loss = 0.23891340\n",
      "Iteration 99, loss = 0.23871316\n",
      "Iteration 100, loss = 0.23848176\n",
      "Iteration 101, loss = 0.23827565\n",
      "Iteration 102, loss = 0.23805009\n",
      "Iteration 103, loss = 0.23787660\n",
      "Iteration 104, loss = 0.23766576\n",
      "Iteration 105, loss = 0.23747272\n",
      "Iteration 106, loss = 0.23725058\n",
      "Iteration 107, loss = 0.23707664\n",
      "Iteration 108, loss = 0.23688125\n",
      "Iteration 109, loss = 0.23670774\n",
      "Iteration 110, loss = 0.23649687\n",
      "Iteration 111, loss = 0.23630214\n",
      "Iteration 112, loss = 0.23611681\n",
      "Iteration 113, loss = 0.23593930\n",
      "Iteration 114, loss = 0.23574044\n",
      "Iteration 115, loss = 0.23557054\n",
      "Iteration 116, loss = 0.23540861\n",
      "Iteration 117, loss = 0.23521841\n",
      "Iteration 118, loss = 0.23503934\n",
      "Iteration 119, loss = 0.23486819\n",
      "Iteration 120, loss = 0.23470259\n",
      "Iteration 121, loss = 0.23452357\n",
      "Iteration 122, loss = 0.23436686\n",
      "Iteration 123, loss = 0.23419648\n",
      "Iteration 124, loss = 0.23404322\n",
      "Iteration 125, loss = 0.23385468\n",
      "Iteration 126, loss = 0.23372088\n",
      "Iteration 127, loss = 0.23354460\n",
      "Iteration 128, loss = 0.23340042\n",
      "Iteration 129, loss = 0.23322624\n",
      "Iteration 130, loss = 0.23304840\n",
      "Iteration 131, loss = 0.23291600\n",
      "Iteration 132, loss = 0.23275991\n",
      "Iteration 133, loss = 0.23259267\n",
      "Iteration 134, loss = 0.23246859\n",
      "Iteration 135, loss = 0.23230741\n",
      "Iteration 136, loss = 0.23215565\n",
      "Iteration 137, loss = 0.23202251\n",
      "Iteration 138, loss = 0.23188960\n",
      "Iteration 139, loss = 0.23171050\n",
      "Iteration 140, loss = 0.23158460\n",
      "Iteration 141, loss = 0.23146759\n",
      "Iteration 142, loss = 0.23132813\n",
      "Iteration 143, loss = 0.23116160\n",
      "Iteration 144, loss = 0.23105911\n",
      "Iteration 145, loss = 0.23087878\n",
      "Iteration 146, loss = 0.23072929\n",
      "Iteration 147, loss = 0.23060058\n",
      "Iteration 148, loss = 0.23047133\n",
      "Iteration 149, loss = 0.23032146\n",
      "Iteration 150, loss = 0.23021326\n",
      "Iteration 151, loss = 0.23007702\n",
      "Iteration 152, loss = 0.22995990\n",
      "Iteration 153, loss = 0.22978883\n",
      "Iteration 154, loss = 0.22965403\n",
      "Iteration 155, loss = 0.22953681\n",
      "Iteration 156, loss = 0.22943731\n",
      "Iteration 157, loss = 0.22931147\n",
      "Iteration 158, loss = 0.22918101\n",
      "Iteration 159, loss = 0.22907389\n",
      "Iteration 160, loss = 0.22892744\n",
      "Iteration 161, loss = 0.22880994\n",
      "Iteration 162, loss = 0.22868851\n",
      "Iteration 163, loss = 0.22856684\n",
      "Iteration 164, loss = 0.22845267\n",
      "Iteration 165, loss = 0.22834329\n",
      "Iteration 166, loss = 0.22824386\n",
      "Iteration 167, loss = 0.22810818\n",
      "Iteration 168, loss = 0.22799562\n",
      "Iteration 169, loss = 0.22789498\n",
      "Iteration 170, loss = 0.22777759\n",
      "Iteration 171, loss = 0.22767093\n",
      "Iteration 172, loss = 0.22753450\n",
      "Iteration 173, loss = 0.22742982\n",
      "Iteration 174, loss = 0.22731379\n",
      "Iteration 175, loss = 0.22720671\n",
      "Iteration 176, loss = 0.22710027\n",
      "Iteration 177, loss = 0.22698166\n",
      "Iteration 178, loss = 0.22685960\n",
      "Iteration 179, loss = 0.22677751\n",
      "Iteration 180, loss = 0.22666595\n",
      "Iteration 181, loss = 0.22653114\n",
      "Iteration 182, loss = 0.22644431\n",
      "Iteration 183, loss = 0.22634255\n",
      "Iteration 184, loss = 0.22626216\n",
      "Iteration 185, loss = 0.22614888\n",
      "Iteration 186, loss = 0.22604709\n",
      "Iteration 187, loss = 0.22596682\n",
      "Iteration 188, loss = 0.22588274\n",
      "Iteration 189, loss = 0.22576912\n",
      "Iteration 190, loss = 0.22562655\n",
      "Iteration 191, loss = 0.22556182\n",
      "Iteration 192, loss = 0.22545364\n",
      "Iteration 193, loss = 0.22537511\n",
      "Iteration 194, loss = 0.22526777\n",
      "Iteration 195, loss = 0.22514766\n",
      "Iteration 196, loss = 0.22506077\n",
      "Iteration 197, loss = 0.22496409\n",
      "Iteration 198, loss = 0.22488546\n",
      "Iteration 199, loss = 0.22478056\n",
      "Iteration 200, loss = 0.22467558\n",
      "Iteration 1, loss = 0.84906207\n",
      "Iteration 2, loss = 0.73617711\n",
      "Iteration 3, loss = 0.63867597\n",
      "Iteration 4, loss = 0.56661873\n",
      "Iteration 5, loss = 0.51271002\n",
      "Iteration 6, loss = 0.47170489\n",
      "Iteration 7, loss = 0.43991359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.41477269\n",
      "Iteration 9, loss = 0.39404729\n",
      "Iteration 10, loss = 0.37723421\n",
      "Iteration 11, loss = 0.36305463\n",
      "Iteration 12, loss = 0.35114667\n",
      "Iteration 13, loss = 0.34098824\n",
      "Iteration 14, loss = 0.33215535\n",
      "Iteration 15, loss = 0.32455590\n",
      "Iteration 16, loss = 0.31778800\n",
      "Iteration 17, loss = 0.31191612\n",
      "Iteration 18, loss = 0.30663915\n",
      "Iteration 19, loss = 0.30189639\n",
      "Iteration 20, loss = 0.29768526\n",
      "Iteration 21, loss = 0.29388773\n",
      "Iteration 22, loss = 0.29043711\n",
      "Iteration 23, loss = 0.28733054\n",
      "Iteration 24, loss = 0.28443396\n",
      "Iteration 25, loss = 0.28184080\n",
      "Iteration 26, loss = 0.27941243\n",
      "Iteration 27, loss = 0.27723924\n",
      "Iteration 28, loss = 0.27517317\n",
      "Iteration 29, loss = 0.27327828\n",
      "Iteration 30, loss = 0.27150972\n",
      "Iteration 31, loss = 0.26988853\n",
      "Iteration 32, loss = 0.26830613\n",
      "Iteration 33, loss = 0.26689316\n",
      "Iteration 34, loss = 0.26553261\n",
      "Iteration 35, loss = 0.26429984\n",
      "Iteration 36, loss = 0.26307381\n",
      "Iteration 37, loss = 0.26197079\n",
      "Iteration 38, loss = 0.26090175\n",
      "Iteration 39, loss = 0.25991942\n",
      "Iteration 40, loss = 0.25899080\n",
      "Iteration 41, loss = 0.25805605\n",
      "Iteration 42, loss = 0.25719807\n",
      "Iteration 43, loss = 0.25638776\n",
      "Iteration 44, loss = 0.25558963\n",
      "Iteration 45, loss = 0.25486639\n",
      "Iteration 46, loss = 0.25414264\n",
      "Iteration 47, loss = 0.25346873\n",
      "Iteration 48, loss = 0.25282558\n",
      "Iteration 49, loss = 0.25219337\n",
      "Iteration 50, loss = 0.25155897\n",
      "Iteration 51, loss = 0.25099841\n",
      "Iteration 52, loss = 0.25044165\n",
      "Iteration 53, loss = 0.24988026\n",
      "Iteration 54, loss = 0.24935419\n",
      "Iteration 55, loss = 0.24886841\n",
      "Iteration 56, loss = 0.24836273\n",
      "Iteration 57, loss = 0.24788670\n",
      "Iteration 58, loss = 0.24744227\n",
      "Iteration 59, loss = 0.24697471\n",
      "Iteration 60, loss = 0.24655404\n",
      "Iteration 61, loss = 0.24613942\n",
      "Iteration 62, loss = 0.24573574\n",
      "Iteration 63, loss = 0.24531234\n",
      "Iteration 64, loss = 0.24496897\n",
      "Iteration 65, loss = 0.24457950\n",
      "Iteration 66, loss = 0.24420064\n",
      "Iteration 67, loss = 0.24384994\n",
      "Iteration 68, loss = 0.24348582\n",
      "Iteration 69, loss = 0.24315671\n",
      "Iteration 70, loss = 0.24284774\n",
      "Iteration 71, loss = 0.24246888\n",
      "Iteration 72, loss = 0.24216963\n",
      "Iteration 73, loss = 0.24185340\n",
      "Iteration 74, loss = 0.24153579\n",
      "Iteration 75, loss = 0.24125771\n",
      "Iteration 76, loss = 0.24094785\n",
      "Iteration 77, loss = 0.24064854\n",
      "Iteration 78, loss = 0.24038207\n",
      "Iteration 79, loss = 0.24009787\n",
      "Iteration 80, loss = 0.23982422\n",
      "Iteration 81, loss = 0.23954243\n",
      "Iteration 82, loss = 0.23928393\n",
      "Iteration 83, loss = 0.23899992\n",
      "Iteration 84, loss = 0.23875313\n",
      "Iteration 85, loss = 0.23848080\n",
      "Iteration 86, loss = 0.23827311\n",
      "Iteration 87, loss = 0.23799484\n",
      "Iteration 88, loss = 0.23776219\n",
      "Iteration 89, loss = 0.23754022\n",
      "Iteration 90, loss = 0.23727240\n",
      "Iteration 91, loss = 0.23706983\n",
      "Iteration 92, loss = 0.23682820\n",
      "Iteration 93, loss = 0.23658734\n",
      "Iteration 94, loss = 0.23638551\n",
      "Iteration 95, loss = 0.23616071\n",
      "Iteration 96, loss = 0.23594095\n",
      "Iteration 97, loss = 0.23571407\n",
      "Iteration 98, loss = 0.23551358\n",
      "Iteration 99, loss = 0.23531293\n",
      "Iteration 100, loss = 0.23509830\n",
      "Iteration 101, loss = 0.23490434\n",
      "Iteration 102, loss = 0.23469061\n",
      "Iteration 103, loss = 0.23450237\n",
      "Iteration 104, loss = 0.23432842\n",
      "Iteration 105, loss = 0.23409833\n",
      "Iteration 106, loss = 0.23391224\n",
      "Iteration 107, loss = 0.23373621\n",
      "Iteration 108, loss = 0.23355620\n",
      "Iteration 109, loss = 0.23334337\n",
      "Iteration 110, loss = 0.23317867\n",
      "Iteration 111, loss = 0.23300309\n",
      "Iteration 112, loss = 0.23281122\n",
      "Iteration 113, loss = 0.23263895\n",
      "Iteration 114, loss = 0.23243356\n",
      "Iteration 115, loss = 0.23229101\n",
      "Iteration 116, loss = 0.23211618\n",
      "Iteration 117, loss = 0.23192955\n",
      "Iteration 118, loss = 0.23175876\n",
      "Iteration 119, loss = 0.23161405\n",
      "Iteration 120, loss = 0.23143189\n",
      "Iteration 121, loss = 0.23128537\n",
      "Iteration 122, loss = 0.23110633\n",
      "Iteration 123, loss = 0.23095822\n",
      "Iteration 124, loss = 0.23080486\n",
      "Iteration 125, loss = 0.23061795\n",
      "Iteration 126, loss = 0.23048967\n",
      "Iteration 127, loss = 0.23032287\n",
      "Iteration 128, loss = 0.23017557\n",
      "Iteration 129, loss = 0.23001473\n",
      "Iteration 130, loss = 0.22986206\n",
      "Iteration 131, loss = 0.22973351\n",
      "Iteration 132, loss = 0.22960299\n",
      "Iteration 133, loss = 0.22942411\n",
      "Iteration 134, loss = 0.22930582\n",
      "Iteration 135, loss = 0.22914174\n",
      "Iteration 136, loss = 0.22902188\n",
      "Iteration 137, loss = 0.22885047\n",
      "Iteration 138, loss = 0.22875169\n",
      "Iteration 139, loss = 0.22858646\n",
      "Iteration 140, loss = 0.22845504\n",
      "Iteration 141, loss = 0.22835757\n",
      "Iteration 142, loss = 0.22820122\n",
      "Iteration 143, loss = 0.22807114\n",
      "Iteration 144, loss = 0.22795038\n",
      "Iteration 145, loss = 0.22779950\n",
      "Iteration 146, loss = 0.22767255\n",
      "Iteration 147, loss = 0.22751812\n",
      "Iteration 148, loss = 0.22741311\n",
      "Iteration 149, loss = 0.22727449\n",
      "Iteration 150, loss = 0.22716378\n",
      "Iteration 151, loss = 0.22703421\n",
      "Iteration 152, loss = 0.22692372\n",
      "Iteration 153, loss = 0.22675652\n",
      "Iteration 154, loss = 0.22664322\n",
      "Iteration 155, loss = 0.22653221\n",
      "Iteration 156, loss = 0.22640815\n",
      "Iteration 157, loss = 0.22631497\n",
      "Iteration 158, loss = 0.22619877\n",
      "Iteration 159, loss = 0.22608757\n",
      "Iteration 160, loss = 0.22593567\n",
      "Iteration 161, loss = 0.22583302\n",
      "Iteration 162, loss = 0.22571608\n",
      "Iteration 163, loss = 0.22560767\n",
      "Iteration 164, loss = 0.22550706\n",
      "Iteration 165, loss = 0.22538482\n",
      "Iteration 166, loss = 0.22530995\n",
      "Iteration 167, loss = 0.22517636\n",
      "Iteration 168, loss = 0.22505490\n",
      "Iteration 169, loss = 0.22497185\n",
      "Iteration 170, loss = 0.22486921\n",
      "Iteration 171, loss = 0.22474756\n",
      "Iteration 172, loss = 0.22466654\n",
      "Iteration 173, loss = 0.22454255\n",
      "Iteration 174, loss = 0.22443053\n",
      "Iteration 175, loss = 0.22434655\n",
      "Iteration 176, loss = 0.22423205\n",
      "Iteration 177, loss = 0.22409598\n",
      "Iteration 178, loss = 0.22401196\n",
      "Iteration 179, loss = 0.22393008\n",
      "Iteration 180, loss = 0.22382473\n",
      "Iteration 181, loss = 0.22371841\n",
      "Iteration 182, loss = 0.22360091\n",
      "Iteration 183, loss = 0.22352500\n",
      "Iteration 184, loss = 0.22344209\n",
      "Iteration 185, loss = 0.22334451\n",
      "Iteration 186, loss = 0.22325577\n",
      "Iteration 187, loss = 0.22316024\n",
      "Iteration 188, loss = 0.22306806\n",
      "Iteration 189, loss = 0.22301237\n",
      "Iteration 190, loss = 0.22284228\n",
      "Iteration 191, loss = 0.22278576\n",
      "Iteration 192, loss = 0.22269674\n",
      "Iteration 193, loss = 0.22259741\n",
      "Iteration 194, loss = 0.22251565\n",
      "Iteration 195, loss = 0.22241072\n",
      "Iteration 196, loss = 0.22231934\n",
      "Iteration 197, loss = 0.22224470\n",
      "Iteration 198, loss = 0.22213776\n",
      "Iteration 199, loss = 0.22207986\n",
      "Iteration 200, loss = 0.22200060\n",
      "Iteration 1, loss = 0.72251243\n",
      "Iteration 2, loss = 0.51809279\n",
      "Iteration 3, loss = 0.41508819\n",
      "Iteration 4, loss = 0.35041637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.30970397\n",
      "Iteration 6, loss = 0.28402331\n",
      "Iteration 7, loss = 0.26678385\n",
      "Iteration 8, loss = 0.25667932\n",
      "Iteration 9, loss = 0.24915447\n",
      "Iteration 10, loss = 0.24401101\n",
      "Iteration 11, loss = 0.23884372\n",
      "Iteration 12, loss = 0.23620538\n",
      "Iteration 13, loss = 0.23328709\n",
      "Iteration 14, loss = 0.23148566\n",
      "Iteration 15, loss = 0.22964324\n",
      "Iteration 16, loss = 0.22825496\n",
      "Iteration 17, loss = 0.22684591\n",
      "Iteration 18, loss = 0.22540990\n",
      "Iteration 19, loss = 0.22370201\n",
      "Iteration 20, loss = 0.22226432\n",
      "Iteration 21, loss = 0.22105904\n",
      "Iteration 22, loss = 0.21945470\n",
      "Iteration 23, loss = 0.21887571\n",
      "Iteration 24, loss = 0.21747312\n",
      "Iteration 25, loss = 0.21610937\n",
      "Iteration 26, loss = 0.21509018\n",
      "Iteration 27, loss = 0.21349173\n",
      "Iteration 28, loss = 0.21201784\n",
      "Iteration 29, loss = 0.21065445\n",
      "Iteration 30, loss = 0.20998014\n",
      "Iteration 31, loss = 0.20767614\n",
      "Iteration 32, loss = 0.20637307\n",
      "Iteration 33, loss = 0.20539291\n",
      "Iteration 34, loss = 0.20362411\n",
      "Iteration 35, loss = 0.20125687\n",
      "Iteration 36, loss = 0.19981683\n",
      "Iteration 37, loss = 0.19849350\n",
      "Iteration 38, loss = 0.19765853\n",
      "Iteration 39, loss = 0.19522267\n",
      "Iteration 40, loss = 0.19361427\n",
      "Iteration 41, loss = 0.19175209\n",
      "Iteration 42, loss = 0.18997930\n",
      "Iteration 43, loss = 0.18786253\n",
      "Iteration 44, loss = 0.18601389\n",
      "Iteration 45, loss = 0.18431803\n",
      "Iteration 46, loss = 0.18286394\n",
      "Iteration 47, loss = 0.18080150\n",
      "Iteration 48, loss = 0.17985233\n",
      "Iteration 49, loss = 0.17738779\n",
      "Iteration 50, loss = 0.17628316\n",
      "Iteration 51, loss = 0.17400752\n",
      "Iteration 52, loss = 0.17220310\n",
      "Iteration 53, loss = 0.17053163\n",
      "Iteration 54, loss = 0.16919396\n",
      "Iteration 55, loss = 0.16667316\n",
      "Iteration 56, loss = 0.16525833\n",
      "Iteration 57, loss = 0.16293720\n",
      "Iteration 58, loss = 0.16142170\n",
      "Iteration 59, loss = 0.15982695\n",
      "Iteration 60, loss = 0.15807180\n",
      "Iteration 61, loss = 0.15624613\n",
      "Iteration 62, loss = 0.15428192\n",
      "Iteration 63, loss = 0.15329990\n",
      "Iteration 64, loss = 0.15153586\n",
      "Iteration 65, loss = 0.14894809\n",
      "Iteration 66, loss = 0.14773298\n",
      "Iteration 67, loss = 0.14669039\n",
      "Iteration 68, loss = 0.14489209\n",
      "Iteration 69, loss = 0.14261455\n",
      "Iteration 70, loss = 0.14150004\n",
      "Iteration 71, loss = 0.14028000\n",
      "Iteration 72, loss = 0.13856208\n",
      "Iteration 73, loss = 0.13664948\n",
      "Iteration 74, loss = 0.13518496\n",
      "Iteration 75, loss = 0.13401935\n",
      "Iteration 76, loss = 0.13274478\n",
      "Iteration 77, loss = 0.13099020\n",
      "Iteration 78, loss = 0.12939618\n",
      "Iteration 79, loss = 0.12829696\n",
      "Iteration 80, loss = 0.12665433\n",
      "Iteration 81, loss = 0.12550262\n",
      "Iteration 82, loss = 0.12367891\n",
      "Iteration 83, loss = 0.12206708\n",
      "Iteration 84, loss = 0.12129527\n",
      "Iteration 85, loss = 0.12057686\n",
      "Iteration 86, loss = 0.11856608\n",
      "Iteration 87, loss = 0.11746320\n",
      "Iteration 88, loss = 0.11613895\n",
      "Iteration 89, loss = 0.11531520\n",
      "Iteration 90, loss = 0.11319549\n",
      "Iteration 91, loss = 0.11272456\n",
      "Iteration 92, loss = 0.11114850\n",
      "Iteration 93, loss = 0.10978071\n",
      "Iteration 94, loss = 0.10865479\n",
      "Iteration 95, loss = 0.10772843\n",
      "Iteration 96, loss = 0.10638197\n",
      "Iteration 97, loss = 0.10495265\n",
      "Iteration 98, loss = 0.10412959\n",
      "Iteration 99, loss = 0.10300741\n",
      "Iteration 100, loss = 0.10211683\n",
      "Iteration 101, loss = 0.10038177\n",
      "Iteration 102, loss = 0.09954865\n",
      "Iteration 103, loss = 0.09832974\n",
      "Iteration 104, loss = 0.09731652\n",
      "Iteration 105, loss = 0.09615714\n",
      "Iteration 106, loss = 0.09576102\n",
      "Iteration 107, loss = 0.09387379\n",
      "Iteration 108, loss = 0.09307314\n",
      "Iteration 109, loss = 0.09236198\n",
      "Iteration 110, loss = 0.09147707\n",
      "Iteration 111, loss = 0.09028639\n",
      "Iteration 112, loss = 0.08950976\n",
      "Iteration 113, loss = 0.08830962\n",
      "Iteration 114, loss = 0.08733062\n",
      "Iteration 115, loss = 0.08677685\n",
      "Iteration 116, loss = 0.08543550\n",
      "Iteration 117, loss = 0.08431926\n",
      "Iteration 118, loss = 0.08297758\n",
      "Iteration 119, loss = 0.08233101\n",
      "Iteration 120, loss = 0.08151042\n",
      "Iteration 121, loss = 0.08057152\n",
      "Iteration 122, loss = 0.07990832\n",
      "Iteration 123, loss = 0.07848802\n",
      "Iteration 124, loss = 0.07820607\n",
      "Iteration 125, loss = 0.07675662\n",
      "Iteration 126, loss = 0.07609294\n",
      "Iteration 127, loss = 0.07550685\n",
      "Iteration 128, loss = 0.07415761\n",
      "Iteration 129, loss = 0.07357527\n",
      "Iteration 130, loss = 0.07229489\n",
      "Iteration 131, loss = 0.07208653\n",
      "Iteration 132, loss = 0.07069337\n",
      "Iteration 133, loss = 0.06970239\n",
      "Iteration 134, loss = 0.06920122\n",
      "Iteration 135, loss = 0.06866834\n",
      "Iteration 136, loss = 0.06764128\n",
      "Iteration 137, loss = 0.06679072\n",
      "Iteration 138, loss = 0.06576883\n",
      "Iteration 139, loss = 0.06482315\n",
      "Iteration 140, loss = 0.06421141\n",
      "Iteration 141, loss = 0.06346737\n",
      "Iteration 142, loss = 0.06281161\n",
      "Iteration 143, loss = 0.06171079\n",
      "Iteration 144, loss = 0.06155415\n",
      "Iteration 145, loss = 0.06082708\n",
      "Iteration 146, loss = 0.06001571\n",
      "Iteration 147, loss = 0.05900805\n",
      "Iteration 148, loss = 0.05810438\n",
      "Iteration 149, loss = 0.05746354\n",
      "Iteration 150, loss = 0.05656174\n",
      "Iteration 151, loss = 0.05628745\n",
      "Iteration 152, loss = 0.05576134\n",
      "Iteration 153, loss = 0.05464707\n",
      "Iteration 154, loss = 0.05418229\n",
      "Iteration 155, loss = 0.05382149\n",
      "Iteration 156, loss = 0.05289045\n",
      "Iteration 157, loss = 0.05237724\n",
      "Iteration 158, loss = 0.05161456\n",
      "Iteration 159, loss = 0.05108790\n",
      "Iteration 160, loss = 0.05025601\n",
      "Iteration 161, loss = 0.04975852\n",
      "Iteration 162, loss = 0.04908677\n",
      "Iteration 163, loss = 0.04837554\n",
      "Iteration 164, loss = 0.04783374\n",
      "Iteration 165, loss = 0.04780430\n",
      "Iteration 166, loss = 0.04638641\n",
      "Iteration 167, loss = 0.04606528\n",
      "Iteration 168, loss = 0.04599869\n",
      "Iteration 169, loss = 0.04495113\n",
      "Iteration 170, loss = 0.04438356\n",
      "Iteration 171, loss = 0.04406636\n",
      "Iteration 172, loss = 0.04307215\n",
      "Iteration 173, loss = 0.04254552\n",
      "Iteration 174, loss = 0.04204227\n",
      "Iteration 175, loss = 0.04166907\n",
      "Iteration 176, loss = 0.04106620\n",
      "Iteration 177, loss = 0.04037005\n",
      "Iteration 178, loss = 0.03999752\n",
      "Iteration 179, loss = 0.03947018\n",
      "Iteration 180, loss = 0.03897059\n",
      "Iteration 181, loss = 0.03802481\n",
      "Iteration 182, loss = 0.03813677\n",
      "Iteration 183, loss = 0.03754505\n",
      "Iteration 184, loss = 0.03705655\n",
      "Iteration 185, loss = 0.03639550\n",
      "Iteration 186, loss = 0.03559760\n",
      "Iteration 187, loss = 0.03529285\n",
      "Iteration 188, loss = 0.03495264\n",
      "Iteration 189, loss = 0.03424573\n",
      "Iteration 190, loss = 0.03393636\n",
      "Iteration 191, loss = 0.03380580\n",
      "Iteration 192, loss = 0.03315037\n",
      "Iteration 193, loss = 0.03263234\n",
      "Iteration 194, loss = 0.03202860\n",
      "Iteration 195, loss = 0.03170703\n",
      "Iteration 196, loss = 0.03127537\n",
      "Iteration 197, loss = 0.03147984\n",
      "Iteration 198, loss = 0.03060676\n",
      "Iteration 199, loss = 0.03010475\n",
      "Iteration 200, loss = 0.02987628\n",
      "Iteration 1, loss = 0.72213913\n",
      "Iteration 2, loss = 0.51612710\n",
      "Iteration 3, loss = 0.41248234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.34830762\n",
      "Iteration 5, loss = 0.30702170\n",
      "Iteration 6, loss = 0.28141908\n",
      "Iteration 7, loss = 0.26415908\n",
      "Iteration 8, loss = 0.25347827\n",
      "Iteration 9, loss = 0.24572356\n",
      "Iteration 10, loss = 0.23963731\n",
      "Iteration 11, loss = 0.23470298\n",
      "Iteration 12, loss = 0.23235974\n",
      "Iteration 13, loss = 0.22923163\n",
      "Iteration 14, loss = 0.22645817\n",
      "Iteration 15, loss = 0.22460456\n",
      "Iteration 16, loss = 0.22295542\n",
      "Iteration 17, loss = 0.22127401\n",
      "Iteration 18, loss = 0.22000822\n",
      "Iteration 19, loss = 0.21866820\n",
      "Iteration 20, loss = 0.21746826\n",
      "Iteration 21, loss = 0.21609062\n",
      "Iteration 22, loss = 0.21446745\n",
      "Iteration 23, loss = 0.21355174\n",
      "Iteration 24, loss = 0.21271832\n",
      "Iteration 25, loss = 0.21119885\n",
      "Iteration 26, loss = 0.20953585\n",
      "Iteration 27, loss = 0.20832818\n",
      "Iteration 28, loss = 0.20728462\n",
      "Iteration 29, loss = 0.20590208\n",
      "Iteration 30, loss = 0.20483095\n",
      "Iteration 31, loss = 0.20268439\n",
      "Iteration 32, loss = 0.20130260\n",
      "Iteration 33, loss = 0.19976285\n",
      "Iteration 34, loss = 0.19887515\n",
      "Iteration 35, loss = 0.19629180\n",
      "Iteration 36, loss = 0.19520778\n",
      "Iteration 37, loss = 0.19359029\n",
      "Iteration 38, loss = 0.19194513\n",
      "Iteration 39, loss = 0.18995565\n",
      "Iteration 40, loss = 0.18817137\n",
      "Iteration 41, loss = 0.18666538\n",
      "Iteration 42, loss = 0.18516316\n",
      "Iteration 43, loss = 0.18238718\n",
      "Iteration 44, loss = 0.18093498\n",
      "Iteration 45, loss = 0.17906657\n",
      "Iteration 46, loss = 0.17703588\n",
      "Iteration 47, loss = 0.17533956\n",
      "Iteration 48, loss = 0.17371175\n",
      "Iteration 49, loss = 0.17133123\n",
      "Iteration 50, loss = 0.16976372\n",
      "Iteration 51, loss = 0.16801880\n",
      "Iteration 52, loss = 0.16565924\n",
      "Iteration 53, loss = 0.16394728\n",
      "Iteration 54, loss = 0.16218271\n",
      "Iteration 55, loss = 0.16000589\n",
      "Iteration 56, loss = 0.15877624\n",
      "Iteration 57, loss = 0.15617878\n",
      "Iteration 58, loss = 0.15492841\n",
      "Iteration 59, loss = 0.15300020\n",
      "Iteration 60, loss = 0.15103328\n",
      "Iteration 61, loss = 0.14912467\n",
      "Iteration 62, loss = 0.14729823\n",
      "Iteration 63, loss = 0.14644311\n",
      "Iteration 64, loss = 0.14434291\n",
      "Iteration 65, loss = 0.14245786\n",
      "Iteration 66, loss = 0.14122011\n",
      "Iteration 67, loss = 0.13936506\n",
      "Iteration 68, loss = 0.13810569\n",
      "Iteration 69, loss = 0.13593252\n",
      "Iteration 70, loss = 0.13410559\n",
      "Iteration 71, loss = 0.13331488\n",
      "Iteration 72, loss = 0.13218173\n",
      "Iteration 73, loss = 0.12954493\n",
      "Iteration 74, loss = 0.12782023\n",
      "Iteration 75, loss = 0.12706351\n",
      "Iteration 76, loss = 0.12535369\n",
      "Iteration 77, loss = 0.12385415\n",
      "Iteration 78, loss = 0.12256617\n",
      "Iteration 79, loss = 0.12098861\n",
      "Iteration 80, loss = 0.11955616\n",
      "Iteration 81, loss = 0.11822168\n",
      "Iteration 82, loss = 0.11694913\n",
      "Iteration 83, loss = 0.11478682\n",
      "Iteration 84, loss = 0.11390422\n",
      "Iteration 85, loss = 0.11329767\n",
      "Iteration 86, loss = 0.11108871\n",
      "Iteration 87, loss = 0.11008296\n",
      "Iteration 88, loss = 0.10881807\n",
      "Iteration 89, loss = 0.10761479\n",
      "Iteration 90, loss = 0.10586014\n",
      "Iteration 91, loss = 0.10494491\n",
      "Iteration 92, loss = 0.10346102\n",
      "Iteration 93, loss = 0.10222773\n",
      "Iteration 94, loss = 0.10099940\n",
      "Iteration 95, loss = 0.10044302\n",
      "Iteration 96, loss = 0.09869675\n",
      "Iteration 97, loss = 0.09746403\n",
      "Iteration 98, loss = 0.09636838\n",
      "Iteration 99, loss = 0.09542013\n",
      "Iteration 100, loss = 0.09449004\n",
      "Iteration 101, loss = 0.09302604\n",
      "Iteration 102, loss = 0.09229087\n",
      "Iteration 103, loss = 0.09130767\n",
      "Iteration 104, loss = 0.08992282\n",
      "Iteration 105, loss = 0.08856284\n",
      "Iteration 106, loss = 0.08816669\n",
      "Iteration 107, loss = 0.08658959\n",
      "Iteration 108, loss = 0.08552012\n",
      "Iteration 109, loss = 0.08460789\n",
      "Iteration 110, loss = 0.08372888\n",
      "Iteration 111, loss = 0.08244440\n",
      "Iteration 112, loss = 0.08157245\n",
      "Iteration 113, loss = 0.08078217\n",
      "Iteration 114, loss = 0.07984079\n",
      "Iteration 115, loss = 0.07897696\n",
      "Iteration 116, loss = 0.07745843\n",
      "Iteration 117, loss = 0.07650858\n",
      "Iteration 118, loss = 0.07556396\n",
      "Iteration 119, loss = 0.07515814\n",
      "Iteration 120, loss = 0.07415787\n",
      "Iteration 121, loss = 0.07317254\n",
      "Iteration 122, loss = 0.07227236\n",
      "Iteration 123, loss = 0.07149281\n",
      "Iteration 124, loss = 0.07062890\n",
      "Iteration 125, loss = 0.06937959\n",
      "Iteration 126, loss = 0.06867822\n",
      "Iteration 127, loss = 0.06779255\n",
      "Iteration 128, loss = 0.06711137\n",
      "Iteration 129, loss = 0.06630134\n",
      "Iteration 130, loss = 0.06511198\n",
      "Iteration 131, loss = 0.06425878\n",
      "Iteration 132, loss = 0.06363170\n",
      "Iteration 133, loss = 0.06271893\n",
      "Iteration 134, loss = 0.06199551\n",
      "Iteration 135, loss = 0.06152527\n",
      "Iteration 136, loss = 0.06035304\n",
      "Iteration 137, loss = 0.05953519\n",
      "Iteration 138, loss = 0.05885354\n",
      "Iteration 139, loss = 0.05809713\n",
      "Iteration 140, loss = 0.05750561\n",
      "Iteration 141, loss = 0.05660256\n",
      "Iteration 142, loss = 0.05627257\n",
      "Iteration 143, loss = 0.05496532\n",
      "Iteration 144, loss = 0.05510722\n",
      "Iteration 145, loss = 0.05372375\n",
      "Iteration 146, loss = 0.05310367\n",
      "Iteration 147, loss = 0.05235836\n",
      "Iteration 148, loss = 0.05153071\n",
      "Iteration 149, loss = 0.05070274\n",
      "Iteration 150, loss = 0.05005522\n",
      "Iteration 151, loss = 0.04990898\n",
      "Iteration 152, loss = 0.04938472\n",
      "Iteration 153, loss = 0.04811771\n",
      "Iteration 154, loss = 0.04742056\n",
      "Iteration 155, loss = 0.04694260\n",
      "Iteration 156, loss = 0.04660291\n",
      "Iteration 157, loss = 0.04576323\n",
      "Iteration 158, loss = 0.04517577\n",
      "Iteration 159, loss = 0.04449766\n",
      "Iteration 160, loss = 0.04420307\n",
      "Iteration 161, loss = 0.04342671\n",
      "Iteration 162, loss = 0.04262262\n",
      "Iteration 163, loss = 0.04225462\n",
      "Iteration 164, loss = 0.04151766\n",
      "Iteration 165, loss = 0.04097884\n",
      "Iteration 166, loss = 0.04014270\n",
      "Iteration 167, loss = 0.03965822\n",
      "Iteration 168, loss = 0.03924081\n",
      "Iteration 169, loss = 0.03873826\n",
      "Iteration 170, loss = 0.03822179\n",
      "Iteration 171, loss = 0.03804608\n",
      "Iteration 172, loss = 0.03697203\n",
      "Iteration 173, loss = 0.03660574\n",
      "Iteration 174, loss = 0.03607877\n",
      "Iteration 175, loss = 0.03562325\n",
      "Iteration 176, loss = 0.03503873\n",
      "Iteration 177, loss = 0.03444446\n",
      "Iteration 178, loss = 0.03409670\n",
      "Iteration 179, loss = 0.03358219\n",
      "Iteration 180, loss = 0.03305444\n",
      "Iteration 181, loss = 0.03244499\n",
      "Iteration 182, loss = 0.03197313\n",
      "Iteration 183, loss = 0.03180757\n",
      "Iteration 184, loss = 0.03104661\n",
      "Iteration 185, loss = 0.03084601\n",
      "Iteration 186, loss = 0.03008960\n",
      "Iteration 187, loss = 0.02987944\n",
      "Iteration 188, loss = 0.02960711\n",
      "Iteration 189, loss = 0.02885198\n",
      "Iteration 190, loss = 0.02853812\n",
      "Iteration 191, loss = 0.02818131\n",
      "Iteration 192, loss = 0.02777855\n",
      "Iteration 193, loss = 0.02738200\n",
      "Iteration 194, loss = 0.02676240\n",
      "Iteration 195, loss = 0.02650069\n",
      "Iteration 196, loss = 0.02602971\n",
      "Iteration 197, loss = 0.02594425\n",
      "Iteration 198, loss = 0.02535219\n",
      "Iteration 199, loss = 0.02505616\n",
      "Iteration 200, loss = 0.02469175\n",
      "Iteration 1, loss = 0.72472582\n",
      "Iteration 2, loss = 0.51719898\n",
      "Iteration 3, loss = 0.41386519\n",
      "Iteration 4, loss = 0.35005371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.30974994\n",
      "Iteration 6, loss = 0.28404133\n",
      "Iteration 7, loss = 0.26787153\n",
      "Iteration 8, loss = 0.25716951\n",
      "Iteration 9, loss = 0.24883406\n",
      "Iteration 10, loss = 0.24327859\n",
      "Iteration 11, loss = 0.23901340\n",
      "Iteration 12, loss = 0.23653891\n",
      "Iteration 13, loss = 0.23283420\n",
      "Iteration 14, loss = 0.23031185\n",
      "Iteration 15, loss = 0.22901070\n",
      "Iteration 16, loss = 0.22701373\n",
      "Iteration 17, loss = 0.22544378\n",
      "Iteration 18, loss = 0.22442087\n",
      "Iteration 19, loss = 0.22294685\n",
      "Iteration 20, loss = 0.22120517\n",
      "Iteration 21, loss = 0.21974312\n",
      "Iteration 22, loss = 0.21863178\n",
      "Iteration 23, loss = 0.21749344\n",
      "Iteration 24, loss = 0.21648464\n",
      "Iteration 25, loss = 0.21498108\n",
      "Iteration 26, loss = 0.21328311\n",
      "Iteration 27, loss = 0.21206451\n",
      "Iteration 28, loss = 0.21072967\n",
      "Iteration 29, loss = 0.20965126\n",
      "Iteration 30, loss = 0.20830673\n",
      "Iteration 31, loss = 0.20635013\n",
      "Iteration 32, loss = 0.20464409\n",
      "Iteration 33, loss = 0.20309288\n",
      "Iteration 34, loss = 0.20144368\n",
      "Iteration 35, loss = 0.19990264\n",
      "Iteration 36, loss = 0.19796668\n",
      "Iteration 37, loss = 0.19666329\n",
      "Iteration 38, loss = 0.19503634\n",
      "Iteration 39, loss = 0.19278952\n",
      "Iteration 40, loss = 0.19127466\n",
      "Iteration 41, loss = 0.18956255\n",
      "Iteration 42, loss = 0.18798799\n",
      "Iteration 43, loss = 0.18556702\n",
      "Iteration 44, loss = 0.18365394\n",
      "Iteration 45, loss = 0.18222529\n",
      "Iteration 46, loss = 0.18032926\n",
      "Iteration 47, loss = 0.17797168\n",
      "Iteration 48, loss = 0.17621694\n",
      "Iteration 49, loss = 0.17432117\n",
      "Iteration 50, loss = 0.17204331\n",
      "Iteration 51, loss = 0.17063089\n",
      "Iteration 52, loss = 0.16831845\n",
      "Iteration 53, loss = 0.16638090\n",
      "Iteration 54, loss = 0.16453212\n",
      "Iteration 55, loss = 0.16234241\n",
      "Iteration 56, loss = 0.16092075\n",
      "Iteration 57, loss = 0.15840103\n",
      "Iteration 58, loss = 0.15678199\n",
      "Iteration 59, loss = 0.15526817\n",
      "Iteration 60, loss = 0.15318683\n",
      "Iteration 61, loss = 0.15127685\n",
      "Iteration 62, loss = 0.14938787\n",
      "Iteration 63, loss = 0.14812750\n",
      "Iteration 64, loss = 0.14588388\n",
      "Iteration 65, loss = 0.14365644\n",
      "Iteration 66, loss = 0.14271048\n",
      "Iteration 67, loss = 0.14070579\n",
      "Iteration 68, loss = 0.13926772\n",
      "Iteration 69, loss = 0.13716671\n",
      "Iteration 70, loss = 0.13562041\n",
      "Iteration 71, loss = 0.13439276\n",
      "Iteration 72, loss = 0.13220567\n",
      "Iteration 73, loss = 0.13013628\n",
      "Iteration 74, loss = 0.12846378\n",
      "Iteration 75, loss = 0.12787373\n",
      "Iteration 76, loss = 0.12630382\n",
      "Iteration 77, loss = 0.12408131\n",
      "Iteration 78, loss = 0.12332694\n",
      "Iteration 79, loss = 0.12109516\n",
      "Iteration 80, loss = 0.11973904\n",
      "Iteration 81, loss = 0.11774075\n",
      "Iteration 82, loss = 0.11722001\n",
      "Iteration 83, loss = 0.11521478\n",
      "Iteration 84, loss = 0.11342915\n",
      "Iteration 85, loss = 0.11283877\n",
      "Iteration 86, loss = 0.11130390\n",
      "Iteration 87, loss = 0.10943886\n",
      "Iteration 88, loss = 0.10852101\n",
      "Iteration 89, loss = 0.10691614\n",
      "Iteration 90, loss = 0.10578075\n",
      "Iteration 91, loss = 0.10433764\n",
      "Iteration 92, loss = 0.10270608\n",
      "Iteration 93, loss = 0.10185664\n",
      "Iteration 94, loss = 0.10047802\n",
      "Iteration 95, loss = 0.09962424\n",
      "Iteration 96, loss = 0.09776536\n",
      "Iteration 97, loss = 0.09650408\n",
      "Iteration 98, loss = 0.09563899\n",
      "Iteration 99, loss = 0.09468422\n",
      "Iteration 100, loss = 0.09361371\n",
      "Iteration 101, loss = 0.09224624\n",
      "Iteration 102, loss = 0.09105254\n",
      "Iteration 103, loss = 0.09023056\n",
      "Iteration 104, loss = 0.08883401\n",
      "Iteration 105, loss = 0.08783007\n",
      "Iteration 106, loss = 0.08659052\n",
      "Iteration 107, loss = 0.08507736\n",
      "Iteration 108, loss = 0.08406673\n",
      "Iteration 109, loss = 0.08371473\n",
      "Iteration 110, loss = 0.08249352\n",
      "Iteration 111, loss = 0.08102861\n",
      "Iteration 112, loss = 0.08023535\n",
      "Iteration 113, loss = 0.07984085\n",
      "Iteration 114, loss = 0.07823274\n",
      "Iteration 115, loss = 0.07719299\n",
      "Iteration 116, loss = 0.07637199\n",
      "Iteration 117, loss = 0.07500054\n",
      "Iteration 118, loss = 0.07402570\n",
      "Iteration 119, loss = 0.07346454\n",
      "Iteration 120, loss = 0.07247876\n",
      "Iteration 121, loss = 0.07140635\n",
      "Iteration 122, loss = 0.07059673\n",
      "Iteration 123, loss = 0.06983860\n",
      "Iteration 124, loss = 0.06937934\n",
      "Iteration 125, loss = 0.06744731\n",
      "Iteration 126, loss = 0.06688251\n",
      "Iteration 127, loss = 0.06587269\n",
      "Iteration 128, loss = 0.06532561\n",
      "Iteration 129, loss = 0.06467371\n",
      "Iteration 130, loss = 0.06360859\n",
      "Iteration 131, loss = 0.06257094\n",
      "Iteration 132, loss = 0.06212131\n",
      "Iteration 133, loss = 0.06071056\n",
      "Iteration 134, loss = 0.06049942\n",
      "Iteration 135, loss = 0.05951440\n",
      "Iteration 136, loss = 0.05853709\n",
      "Iteration 137, loss = 0.05798322\n",
      "Iteration 138, loss = 0.05735191\n",
      "Iteration 139, loss = 0.05608779\n",
      "Iteration 140, loss = 0.05549285\n",
      "Iteration 141, loss = 0.05494618\n",
      "Iteration 142, loss = 0.05397057\n",
      "Iteration 143, loss = 0.05317433\n",
      "Iteration 144, loss = 0.05300093\n",
      "Iteration 145, loss = 0.05179924\n",
      "Iteration 146, loss = 0.05147605\n",
      "Iteration 147, loss = 0.05039962\n",
      "Iteration 148, loss = 0.04982670\n",
      "Iteration 149, loss = 0.04894895\n",
      "Iteration 150, loss = 0.04844980\n",
      "Iteration 151, loss = 0.04794983\n",
      "Iteration 152, loss = 0.04777668\n",
      "Iteration 153, loss = 0.04646037\n",
      "Iteration 154, loss = 0.04574953\n",
      "Iteration 155, loss = 0.04529281\n",
      "Iteration 156, loss = 0.04457666\n",
      "Iteration 157, loss = 0.04382851\n",
      "Iteration 158, loss = 0.04330744\n",
      "Iteration 159, loss = 0.04265979\n",
      "Iteration 160, loss = 0.04233842\n",
      "Iteration 161, loss = 0.04163910\n",
      "Iteration 162, loss = 0.04092038\n",
      "Iteration 163, loss = 0.04046158\n",
      "Iteration 164, loss = 0.03979725\n",
      "Iteration 165, loss = 0.03942476\n",
      "Iteration 166, loss = 0.03879825\n",
      "Iteration 167, loss = 0.03805701\n",
      "Iteration 168, loss = 0.03789977\n",
      "Iteration 169, loss = 0.03704744\n",
      "Iteration 170, loss = 0.03641811\n",
      "Iteration 171, loss = 0.03633504\n",
      "Iteration 172, loss = 0.03555517\n",
      "Iteration 173, loss = 0.03500316\n",
      "Iteration 174, loss = 0.03461498\n",
      "Iteration 175, loss = 0.03437599\n",
      "Iteration 176, loss = 0.03363748\n",
      "Iteration 177, loss = 0.03361048\n",
      "Iteration 178, loss = 0.03296227\n",
      "Iteration 179, loss = 0.03234411\n",
      "Iteration 180, loss = 0.03179581\n",
      "Iteration 181, loss = 0.03114234\n",
      "Iteration 182, loss = 0.03081868\n",
      "Iteration 183, loss = 0.03026749\n",
      "Iteration 184, loss = 0.03010627\n",
      "Iteration 185, loss = 0.02935921\n",
      "Iteration 186, loss = 0.02888290\n",
      "Iteration 187, loss = 0.02887736\n",
      "Iteration 188, loss = 0.02828344\n",
      "Iteration 189, loss = 0.02767970\n",
      "Iteration 190, loss = 0.02743772\n",
      "Iteration 191, loss = 0.02729730\n",
      "Iteration 192, loss = 0.02678868\n",
      "Iteration 193, loss = 0.02657804\n",
      "Iteration 194, loss = 0.02584012\n",
      "Iteration 195, loss = 0.02548537\n",
      "Iteration 196, loss = 0.02515282\n",
      "Iteration 197, loss = 0.02493745\n",
      "Iteration 198, loss = 0.02452793\n",
      "Iteration 199, loss = 0.02414946\n",
      "Iteration 200, loss = 0.02392390\n",
      "Iteration 1, loss = 0.71906259\n",
      "Iteration 2, loss = 0.51368735\n",
      "Iteration 3, loss = 0.41234431\n",
      "Iteration 4, loss = 0.34804736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.30779447\n",
      "Iteration 6, loss = 0.28158248\n",
      "Iteration 7, loss = 0.26536926\n",
      "Iteration 8, loss = 0.25421010\n",
      "Iteration 9, loss = 0.24616072\n",
      "Iteration 10, loss = 0.24006016\n",
      "Iteration 11, loss = 0.23644336\n",
      "Iteration 12, loss = 0.23322218\n",
      "Iteration 13, loss = 0.22952956\n",
      "Iteration 14, loss = 0.22746940\n",
      "Iteration 15, loss = 0.22535161\n",
      "Iteration 16, loss = 0.22340274\n",
      "Iteration 17, loss = 0.22190997\n",
      "Iteration 18, loss = 0.22095135\n",
      "Iteration 19, loss = 0.21929920\n",
      "Iteration 20, loss = 0.21764276\n",
      "Iteration 21, loss = 0.21623462\n",
      "Iteration 22, loss = 0.21490759\n",
      "Iteration 23, loss = 0.21378628\n",
      "Iteration 24, loss = 0.21237582\n",
      "Iteration 25, loss = 0.21087248\n",
      "Iteration 26, loss = 0.20935747\n",
      "Iteration 27, loss = 0.20835595\n",
      "Iteration 28, loss = 0.20720063\n",
      "Iteration 29, loss = 0.20566211\n",
      "Iteration 30, loss = 0.20363238\n",
      "Iteration 31, loss = 0.20218908\n",
      "Iteration 32, loss = 0.20005740\n",
      "Iteration 33, loss = 0.19875094\n",
      "Iteration 34, loss = 0.19714911\n",
      "Iteration 35, loss = 0.19525588\n",
      "Iteration 36, loss = 0.19380132\n",
      "Iteration 37, loss = 0.19200313\n",
      "Iteration 38, loss = 0.18980805\n",
      "Iteration 39, loss = 0.18812811\n",
      "Iteration 40, loss = 0.18669034\n",
      "Iteration 41, loss = 0.18479968\n",
      "Iteration 42, loss = 0.18302402\n",
      "Iteration 43, loss = 0.18067716\n",
      "Iteration 44, loss = 0.17895935\n",
      "Iteration 45, loss = 0.17714846\n",
      "Iteration 46, loss = 0.17502668\n",
      "Iteration 47, loss = 0.17326122\n",
      "Iteration 48, loss = 0.17152347\n",
      "Iteration 49, loss = 0.16937920\n",
      "Iteration 50, loss = 0.16832628\n",
      "Iteration 51, loss = 0.16606759\n",
      "Iteration 52, loss = 0.16439327\n",
      "Iteration 53, loss = 0.16198951\n",
      "Iteration 54, loss = 0.16004255\n",
      "Iteration 55, loss = 0.15840086\n",
      "Iteration 56, loss = 0.15689881\n",
      "Iteration 57, loss = 0.15480649\n",
      "Iteration 58, loss = 0.15305921\n",
      "Iteration 59, loss = 0.15125211\n",
      "Iteration 60, loss = 0.14929503\n",
      "Iteration 61, loss = 0.14745237\n",
      "Iteration 62, loss = 0.14586676\n",
      "Iteration 63, loss = 0.14436907\n",
      "Iteration 64, loss = 0.14267561\n",
      "Iteration 65, loss = 0.14061032\n",
      "Iteration 66, loss = 0.13957997\n",
      "Iteration 67, loss = 0.13766076\n",
      "Iteration 68, loss = 0.13616348\n",
      "Iteration 69, loss = 0.13442011\n",
      "Iteration 70, loss = 0.13320431\n",
      "Iteration 71, loss = 0.13145696\n",
      "Iteration 72, loss = 0.12972040\n",
      "Iteration 73, loss = 0.12810821\n",
      "Iteration 74, loss = 0.12625726\n",
      "Iteration 75, loss = 0.12541263\n",
      "Iteration 76, loss = 0.12355377\n",
      "Iteration 77, loss = 0.12179282\n",
      "Iteration 78, loss = 0.12103845\n",
      "Iteration 79, loss = 0.11947972\n",
      "Iteration 80, loss = 0.11815735\n",
      "Iteration 81, loss = 0.11608116\n",
      "Iteration 82, loss = 0.11547406\n",
      "Iteration 83, loss = 0.11358259\n",
      "Iteration 84, loss = 0.11226933\n",
      "Iteration 85, loss = 0.11091076\n",
      "Iteration 86, loss = 0.11029861\n",
      "Iteration 87, loss = 0.10865298\n",
      "Iteration 88, loss = 0.10761705\n",
      "Iteration 89, loss = 0.10613538\n",
      "Iteration 90, loss = 0.10454499\n",
      "Iteration 91, loss = 0.10326396\n",
      "Iteration 92, loss = 0.10225328\n",
      "Iteration 93, loss = 0.10085197\n",
      "Iteration 94, loss = 0.09977405\n",
      "Iteration 95, loss = 0.09836516\n",
      "Iteration 96, loss = 0.09753948\n",
      "Iteration 97, loss = 0.09620374\n",
      "Iteration 98, loss = 0.09532725\n",
      "Iteration 99, loss = 0.09428135\n",
      "Iteration 100, loss = 0.09346186\n",
      "Iteration 101, loss = 0.09216857\n",
      "Iteration 102, loss = 0.09079958\n",
      "Iteration 103, loss = 0.09015239\n",
      "Iteration 104, loss = 0.08891079\n",
      "Iteration 105, loss = 0.08781633\n",
      "Iteration 106, loss = 0.08646072\n",
      "Iteration 107, loss = 0.08621953\n",
      "Iteration 108, loss = 0.08519572\n",
      "Iteration 109, loss = 0.08418524\n",
      "Iteration 110, loss = 0.08278867\n",
      "Iteration 111, loss = 0.08166869\n",
      "Iteration 112, loss = 0.08063520\n",
      "Iteration 113, loss = 0.07963592\n",
      "Iteration 114, loss = 0.07893392\n",
      "Iteration 115, loss = 0.07754763\n",
      "Iteration 116, loss = 0.07754965\n",
      "Iteration 117, loss = 0.07590807\n",
      "Iteration 118, loss = 0.07507419\n",
      "Iteration 119, loss = 0.07425975\n",
      "Iteration 120, loss = 0.07318646\n",
      "Iteration 121, loss = 0.07268669\n",
      "Iteration 122, loss = 0.07172198\n",
      "Iteration 123, loss = 0.07080654\n",
      "Iteration 124, loss = 0.07033284\n",
      "Iteration 125, loss = 0.06864506\n",
      "Iteration 126, loss = 0.06835023\n",
      "Iteration 127, loss = 0.06758264\n",
      "Iteration 128, loss = 0.06625657\n",
      "Iteration 129, loss = 0.06595267\n",
      "Iteration 130, loss = 0.06476829\n",
      "Iteration 131, loss = 0.06396170\n",
      "Iteration 132, loss = 0.06274056\n",
      "Iteration 133, loss = 0.06222143\n",
      "Iteration 134, loss = 0.06163277\n",
      "Iteration 135, loss = 0.06084159\n",
      "Iteration 136, loss = 0.05995288\n",
      "Iteration 137, loss = 0.05966957\n",
      "Iteration 138, loss = 0.05906019\n",
      "Iteration 139, loss = 0.05762599\n",
      "Iteration 140, loss = 0.05732879\n",
      "Iteration 141, loss = 0.05653605\n",
      "Iteration 142, loss = 0.05579849\n",
      "Iteration 143, loss = 0.05494393\n",
      "Iteration 144, loss = 0.05466299\n",
      "Iteration 145, loss = 0.05342571\n",
      "Iteration 146, loss = 0.05282246\n",
      "Iteration 147, loss = 0.05209380\n",
      "Iteration 148, loss = 0.05182312\n",
      "Iteration 149, loss = 0.05093713\n",
      "Iteration 150, loss = 0.05020667\n",
      "Iteration 151, loss = 0.04958575\n",
      "Iteration 152, loss = 0.04921142\n",
      "Iteration 153, loss = 0.04853139\n",
      "Iteration 154, loss = 0.04763789\n",
      "Iteration 155, loss = 0.04678728\n",
      "Iteration 156, loss = 0.04618624\n",
      "Iteration 157, loss = 0.04553161\n",
      "Iteration 158, loss = 0.04495135\n",
      "Iteration 159, loss = 0.04461628\n",
      "Iteration 160, loss = 0.04377445\n",
      "Iteration 161, loss = 0.04340482\n",
      "Iteration 162, loss = 0.04239007\n",
      "Iteration 163, loss = 0.04216893\n",
      "Iteration 164, loss = 0.04152914\n",
      "Iteration 165, loss = 0.04133426\n",
      "Iteration 166, loss = 0.04046734\n",
      "Iteration 167, loss = 0.04076317\n",
      "Iteration 168, loss = 0.03942813\n",
      "Iteration 169, loss = 0.03916327\n",
      "Iteration 170, loss = 0.03821976\n",
      "Iteration 171, loss = 0.03792154\n",
      "Iteration 172, loss = 0.03711562\n",
      "Iteration 173, loss = 0.03673763\n",
      "Iteration 174, loss = 0.03599043\n",
      "Iteration 175, loss = 0.03600715\n",
      "Iteration 176, loss = 0.03522619\n",
      "Iteration 177, loss = 0.03498039\n",
      "Iteration 178, loss = 0.03420690\n",
      "Iteration 179, loss = 0.03377460\n",
      "Iteration 180, loss = 0.03318701\n",
      "Iteration 181, loss = 0.03271673\n",
      "Iteration 182, loss = 0.03208577\n",
      "Iteration 183, loss = 0.03161615\n",
      "Iteration 184, loss = 0.03152195\n",
      "Iteration 185, loss = 0.03097846\n",
      "Iteration 186, loss = 0.03030444\n",
      "Iteration 187, loss = 0.03017017\n",
      "Iteration 188, loss = 0.02957146\n",
      "Iteration 189, loss = 0.02932253\n",
      "Iteration 190, loss = 0.02872904\n",
      "Iteration 191, loss = 0.02837664\n",
      "Iteration 192, loss = 0.02800841\n",
      "Iteration 193, loss = 0.02778940\n",
      "Iteration 194, loss = 0.02710144\n",
      "Iteration 195, loss = 0.02663399\n",
      "Iteration 196, loss = 0.02638720\n",
      "Iteration 197, loss = 0.02620711\n",
      "Iteration 198, loss = 0.02549507\n",
      "Iteration 199, loss = 0.02520184\n",
      "Iteration 200, loss = 0.02494781\n",
      "Iteration 1, loss = 0.72215615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.51622045\n",
      "Iteration 3, loss = 0.41247219\n",
      "Iteration 4, loss = 0.34678340\n",
      "Iteration 5, loss = 0.30538762\n",
      "Iteration 6, loss = 0.27874681\n",
      "Iteration 7, loss = 0.26178222\n",
      "Iteration 8, loss = 0.25066948\n",
      "Iteration 9, loss = 0.24259375\n",
      "Iteration 10, loss = 0.23696005\n",
      "Iteration 11, loss = 0.23274848\n",
      "Iteration 12, loss = 0.22911572\n",
      "Iteration 13, loss = 0.22649935\n",
      "Iteration 14, loss = 0.22385875\n",
      "Iteration 15, loss = 0.22201645\n",
      "Iteration 16, loss = 0.22076930\n",
      "Iteration 17, loss = 0.21920433\n",
      "Iteration 18, loss = 0.21851229\n",
      "Iteration 19, loss = 0.21682594\n",
      "Iteration 20, loss = 0.21533587\n",
      "Iteration 21, loss = 0.21400349\n",
      "Iteration 22, loss = 0.21256754\n",
      "Iteration 23, loss = 0.21194152\n",
      "Iteration 24, loss = 0.21041443\n",
      "Iteration 25, loss = 0.20889891\n",
      "Iteration 26, loss = 0.20741090\n",
      "Iteration 27, loss = 0.20671650\n",
      "Iteration 28, loss = 0.20522810\n",
      "Iteration 29, loss = 0.20445778\n",
      "Iteration 30, loss = 0.20256779\n",
      "Iteration 31, loss = 0.20130436\n",
      "Iteration 32, loss = 0.19924842\n",
      "Iteration 33, loss = 0.19794397\n",
      "Iteration 34, loss = 0.19626995\n",
      "Iteration 35, loss = 0.19482083\n",
      "Iteration 36, loss = 0.19290347\n",
      "Iteration 37, loss = 0.19171370\n",
      "Iteration 38, loss = 0.18946595\n",
      "Iteration 39, loss = 0.18835383\n",
      "Iteration 40, loss = 0.18751400\n",
      "Iteration 41, loss = 0.18519046\n",
      "Iteration 42, loss = 0.18313650\n",
      "Iteration 43, loss = 0.18077436\n",
      "Iteration 44, loss = 0.17890197\n",
      "Iteration 45, loss = 0.17787837\n",
      "Iteration 46, loss = 0.17546783\n",
      "Iteration 47, loss = 0.17379011\n",
      "Iteration 48, loss = 0.17245817\n",
      "Iteration 49, loss = 0.17018853\n",
      "Iteration 50, loss = 0.16846463\n",
      "Iteration 51, loss = 0.16687887\n",
      "Iteration 52, loss = 0.16511041\n",
      "Iteration 53, loss = 0.16264481\n",
      "Iteration 54, loss = 0.16083711\n",
      "Iteration 55, loss = 0.15903545\n",
      "Iteration 56, loss = 0.15717450\n",
      "Iteration 57, loss = 0.15542235\n",
      "Iteration 58, loss = 0.15376747\n",
      "Iteration 59, loss = 0.15214298\n",
      "Iteration 60, loss = 0.15008978\n",
      "Iteration 61, loss = 0.14847634\n",
      "Iteration 62, loss = 0.14703106\n",
      "Iteration 63, loss = 0.14485504\n",
      "Iteration 64, loss = 0.14381692\n",
      "Iteration 65, loss = 0.14191725\n",
      "Iteration 66, loss = 0.13998870\n",
      "Iteration 67, loss = 0.13807507\n",
      "Iteration 68, loss = 0.13683840\n",
      "Iteration 69, loss = 0.13509515\n",
      "Iteration 70, loss = 0.13384532\n",
      "Iteration 71, loss = 0.13188636\n",
      "Iteration 72, loss = 0.13036436\n",
      "Iteration 73, loss = 0.12891184\n",
      "Iteration 74, loss = 0.12688898\n",
      "Iteration 75, loss = 0.12620392\n",
      "Iteration 76, loss = 0.12457571\n",
      "Iteration 77, loss = 0.12306126\n",
      "Iteration 78, loss = 0.12175567\n",
      "Iteration 79, loss = 0.11986907\n",
      "Iteration 80, loss = 0.11853001\n",
      "Iteration 81, loss = 0.11695023\n",
      "Iteration 82, loss = 0.11550693\n",
      "Iteration 83, loss = 0.11398449\n",
      "Iteration 84, loss = 0.11248053\n",
      "Iteration 85, loss = 0.11100957\n",
      "Iteration 86, loss = 0.11039457\n",
      "Iteration 87, loss = 0.10860244\n",
      "Iteration 88, loss = 0.10768864\n",
      "Iteration 89, loss = 0.10624417\n",
      "Iteration 90, loss = 0.10465458\n",
      "Iteration 91, loss = 0.10339407\n",
      "Iteration 92, loss = 0.10219927\n",
      "Iteration 93, loss = 0.10050095\n",
      "Iteration 94, loss = 0.09968396\n",
      "Iteration 95, loss = 0.09813297\n",
      "Iteration 96, loss = 0.09744058\n",
      "Iteration 97, loss = 0.09563593\n",
      "Iteration 98, loss = 0.09444611\n",
      "Iteration 99, loss = 0.09324422\n",
      "Iteration 100, loss = 0.09246507\n",
      "Iteration 101, loss = 0.09117300\n",
      "Iteration 102, loss = 0.08983352\n",
      "Iteration 103, loss = 0.08872435\n",
      "Iteration 104, loss = 0.08753285\n",
      "Iteration 105, loss = 0.08630176\n",
      "Iteration 106, loss = 0.08525531\n",
      "Iteration 107, loss = 0.08498820\n",
      "Iteration 108, loss = 0.08423295\n",
      "Iteration 109, loss = 0.08245374\n",
      "Iteration 110, loss = 0.08086011\n",
      "Iteration 111, loss = 0.08026785\n",
      "Iteration 112, loss = 0.07922973\n",
      "Iteration 113, loss = 0.07778101\n",
      "Iteration 114, loss = 0.07690295\n",
      "Iteration 115, loss = 0.07580017\n",
      "Iteration 116, loss = 0.07536196\n",
      "Iteration 117, loss = 0.07387336\n",
      "Iteration 118, loss = 0.07301715\n",
      "Iteration 119, loss = 0.07218032\n",
      "Iteration 120, loss = 0.07117761\n",
      "Iteration 121, loss = 0.07109949\n",
      "Iteration 122, loss = 0.06933295\n",
      "Iteration 123, loss = 0.06872120\n",
      "Iteration 124, loss = 0.06775744\n",
      "Iteration 125, loss = 0.06641900\n",
      "Iteration 126, loss = 0.06577927\n",
      "Iteration 127, loss = 0.06506380\n",
      "Iteration 128, loss = 0.06374376\n",
      "Iteration 129, loss = 0.06352985\n",
      "Iteration 130, loss = 0.06232688\n",
      "Iteration 131, loss = 0.06147895\n",
      "Iteration 132, loss = 0.06069228\n",
      "Iteration 133, loss = 0.05998697\n",
      "Iteration 134, loss = 0.05925690\n",
      "Iteration 135, loss = 0.05875135\n",
      "Iteration 136, loss = 0.05784493\n",
      "Iteration 137, loss = 0.05689770\n",
      "Iteration 138, loss = 0.05628601\n",
      "Iteration 139, loss = 0.05531048\n",
      "Iteration 140, loss = 0.05468862\n",
      "Iteration 141, loss = 0.05396484\n",
      "Iteration 142, loss = 0.05298573\n",
      "Iteration 143, loss = 0.05233092\n",
      "Iteration 144, loss = 0.05214808\n",
      "Iteration 145, loss = 0.05117740\n",
      "Iteration 146, loss = 0.05051438\n",
      "Iteration 147, loss = 0.04976729\n",
      "Iteration 148, loss = 0.04921005\n",
      "Iteration 149, loss = 0.04891199\n",
      "Iteration 150, loss = 0.04798565\n",
      "Iteration 151, loss = 0.04771009\n",
      "Iteration 152, loss = 0.04671350\n",
      "Iteration 153, loss = 0.04660571\n",
      "Iteration 154, loss = 0.04570693\n",
      "Iteration 155, loss = 0.04466849\n",
      "Iteration 156, loss = 0.04406639\n",
      "Iteration 157, loss = 0.04316522\n",
      "Iteration 158, loss = 0.04287803\n",
      "Iteration 159, loss = 0.04229605\n",
      "Iteration 160, loss = 0.04144416\n",
      "Iteration 161, loss = 0.04104558\n",
      "Iteration 162, loss = 0.04055031\n",
      "Iteration 163, loss = 0.03983310\n",
      "Iteration 164, loss = 0.03959601\n",
      "Iteration 165, loss = 0.03910076\n",
      "Iteration 166, loss = 0.03836050\n",
      "Iteration 167, loss = 0.03851566\n",
      "Iteration 168, loss = 0.03737482\n",
      "Iteration 169, loss = 0.03716057\n",
      "Iteration 170, loss = 0.03642685\n",
      "Iteration 171, loss = 0.03592651\n",
      "Iteration 172, loss = 0.03522896\n",
      "Iteration 173, loss = 0.03529441\n",
      "Iteration 174, loss = 0.03433670\n",
      "Iteration 175, loss = 0.03401344\n",
      "Iteration 176, loss = 0.03367198\n",
      "Iteration 177, loss = 0.03305631\n",
      "Iteration 178, loss = 0.03243622\n",
      "Iteration 179, loss = 0.03197187\n",
      "Iteration 180, loss = 0.03152365\n",
      "Iteration 181, loss = 0.03108253\n",
      "Iteration 182, loss = 0.03059887\n",
      "Iteration 183, loss = 0.03017767\n",
      "Iteration 184, loss = 0.02989526\n",
      "Iteration 185, loss = 0.02956208\n",
      "Iteration 186, loss = 0.02903636\n",
      "Iteration 187, loss = 0.02862687\n",
      "Iteration 188, loss = 0.02817958\n",
      "Iteration 189, loss = 0.02786518\n",
      "Iteration 190, loss = 0.02742394\n",
      "Iteration 191, loss = 0.02693596\n",
      "Iteration 192, loss = 0.02662994\n",
      "Iteration 193, loss = 0.02643315\n",
      "Iteration 194, loss = 0.02593263\n",
      "Iteration 195, loss = 0.02552099\n",
      "Iteration 196, loss = 0.02529741\n",
      "Iteration 197, loss = 0.02493527\n",
      "Iteration 198, loss = 0.02434774\n",
      "Iteration 199, loss = 0.02406572\n",
      "Iteration 200, loss = 0.02394237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84417395\n",
      "Iteration 2, loss = 0.73242823\n",
      "Iteration 3, loss = 0.63636766\n",
      "Iteration 4, loss = 0.56459624\n",
      "Iteration 5, loss = 0.51175604\n",
      "Iteration 6, loss = 0.47145104\n",
      "Iteration 7, loss = 0.44015397\n",
      "Iteration 8, loss = 0.41536734\n",
      "Iteration 9, loss = 0.39533999\n",
      "Iteration 10, loss = 0.37877388\n",
      "Iteration 11, loss = 0.36497606\n",
      "Iteration 12, loss = 0.35342595\n",
      "Iteration 13, loss = 0.34353629\n",
      "Iteration 14, loss = 0.33504673\n",
      "Iteration 15, loss = 0.32760649\n",
      "Iteration 16, loss = 0.32114925\n",
      "Iteration 17, loss = 0.31551001\n",
      "Iteration 18, loss = 0.31038220\n",
      "Iteration 19, loss = 0.30585915\n",
      "Iteration 20, loss = 0.30180838\n",
      "Iteration 21, loss = 0.29819645\n",
      "Iteration 22, loss = 0.29489893\n",
      "Iteration 23, loss = 0.29194803\n",
      "Iteration 24, loss = 0.28919191\n",
      "Iteration 25, loss = 0.28671788\n",
      "Iteration 26, loss = 0.28445137\n",
      "Iteration 27, loss = 0.28233976\n",
      "Iteration 28, loss = 0.28042054\n",
      "Iteration 29, loss = 0.27860096\n",
      "Iteration 30, loss = 0.27696101\n",
      "Iteration 31, loss = 0.27542631\n",
      "Iteration 32, loss = 0.27397758\n",
      "Iteration 33, loss = 0.27264442\n",
      "Iteration 34, loss = 0.27137220\n",
      "Iteration 35, loss = 0.27016347\n",
      "Iteration 36, loss = 0.26905484\n",
      "Iteration 37, loss = 0.26799887\n",
      "Iteration 38, loss = 0.26705309\n",
      "Iteration 39, loss = 0.26606953\n",
      "Iteration 40, loss = 0.26520123\n",
      "Iteration 41, loss = 0.26433138\n",
      "Iteration 42, loss = 0.26353074\n",
      "Iteration 43, loss = 0.26274815\n",
      "Iteration 44, loss = 0.26199923\n",
      "Iteration 45, loss = 0.26130850\n",
      "Iteration 46, loss = 0.26064408\n",
      "Iteration 47, loss = 0.25999151\n",
      "Iteration 48, loss = 0.25941357\n",
      "Iteration 49, loss = 0.25879427\n",
      "Iteration 50, loss = 0.25822447\n",
      "Iteration 51, loss = 0.25766285\n",
      "Iteration 52, loss = 0.25713588\n",
      "Iteration 53, loss = 0.25664028\n",
      "Iteration 54, loss = 0.25613837\n",
      "Iteration 55, loss = 0.25564222\n",
      "Iteration 56, loss = 0.25517371\n",
      "Iteration 57, loss = 0.25470852\n",
      "Iteration 58, loss = 0.25429721\n",
      "Iteration 59, loss = 0.25385492\n",
      "Iteration 60, loss = 0.25342431\n",
      "Iteration 61, loss = 0.25304559\n",
      "Iteration 62, loss = 0.25264073\n",
      "Iteration 63, loss = 0.25228448\n",
      "Iteration 64, loss = 0.25189022\n",
      "Iteration 65, loss = 0.25152300\n",
      "Iteration 66, loss = 0.25118924\n",
      "Iteration 67, loss = 0.25080877\n",
      "Iteration 68, loss = 0.25050425\n",
      "Iteration 69, loss = 0.25013324\n",
      "Iteration 70, loss = 0.24980888\n",
      "Iteration 71, loss = 0.24951640\n",
      "Iteration 72, loss = 0.24919954\n",
      "Iteration 73, loss = 0.24887538\n",
      "Iteration 74, loss = 0.24859069\n",
      "Iteration 75, loss = 0.24828738\n",
      "Iteration 76, loss = 0.24799986\n",
      "Iteration 77, loss = 0.24770985\n",
      "Iteration 78, loss = 0.24742839\n",
      "Iteration 79, loss = 0.24718297\n",
      "Iteration 80, loss = 0.24687029\n",
      "Iteration 81, loss = 0.24662008\n",
      "Iteration 82, loss = 0.24634614\n",
      "Iteration 83, loss = 0.24610288\n",
      "Iteration 84, loss = 0.24586388\n",
      "Iteration 85, loss = 0.24563313\n",
      "Iteration 86, loss = 0.24536487\n",
      "Iteration 87, loss = 0.24512714\n",
      "Iteration 88, loss = 0.24487369\n",
      "Iteration 89, loss = 0.24465737\n",
      "Iteration 90, loss = 0.24441528\n",
      "Iteration 91, loss = 0.24419434\n",
      "Iteration 92, loss = 0.24395085\n",
      "Iteration 93, loss = 0.24373321\n",
      "Iteration 94, loss = 0.24352746\n",
      "Iteration 95, loss = 0.24329881\n",
      "Iteration 96, loss = 0.24308814\n",
      "Iteration 97, loss = 0.24287958\n",
      "Iteration 98, loss = 0.24268071\n",
      "Iteration 99, loss = 0.24247739\n",
      "Iteration 100, loss = 0.24225497\n",
      "Iteration 101, loss = 0.24206176\n",
      "Iteration 102, loss = 0.24189208\n",
      "Iteration 103, loss = 0.24166632\n",
      "Iteration 104, loss = 0.24147932\n",
      "Iteration 105, loss = 0.24130323\n",
      "Iteration 106, loss = 0.24113544\n",
      "Iteration 107, loss = 0.24089740\n",
      "Iteration 108, loss = 0.24073582\n",
      "Iteration 109, loss = 0.24055875\n",
      "Iteration 110, loss = 0.24037510\n",
      "Iteration 111, loss = 0.24018592\n",
      "Iteration 112, loss = 0.24001870\n",
      "Iteration 113, loss = 0.23985825\n",
      "Iteration 114, loss = 0.23967468\n",
      "Iteration 115, loss = 0.23947899\n",
      "Iteration 116, loss = 0.23932461\n",
      "Iteration 117, loss = 0.23916466\n",
      "Iteration 118, loss = 0.23896920\n",
      "Iteration 119, loss = 0.23881677\n",
      "Iteration 120, loss = 0.23865757\n",
      "Iteration 121, loss = 0.23851726\n",
      "Iteration 122, loss = 0.23834644\n",
      "Iteration 123, loss = 0.23817092\n",
      "Iteration 124, loss = 0.23802108\n",
      "Iteration 125, loss = 0.23787821\n",
      "Iteration 126, loss = 0.23771091\n",
      "Iteration 127, loss = 0.23756960\n",
      "Iteration 128, loss = 0.23744137\n",
      "Iteration 129, loss = 0.23729792\n",
      "Iteration 130, loss = 0.23711884\n",
      "Iteration 131, loss = 0.23700551\n",
      "Iteration 132, loss = 0.23684468\n",
      "Iteration 133, loss = 0.23667162\n",
      "Iteration 134, loss = 0.23657164\n",
      "Iteration 135, loss = 0.23641263\n",
      "Iteration 136, loss = 0.23630511\n",
      "Iteration 137, loss = 0.23613334\n",
      "Iteration 138, loss = 0.23599987\n",
      "Iteration 139, loss = 0.23587098\n",
      "Iteration 140, loss = 0.23572998\n",
      "Iteration 141, loss = 0.23560184\n",
      "Iteration 142, loss = 0.23548297\n",
      "Iteration 143, loss = 0.23533547\n",
      "Iteration 144, loss = 0.23521946\n",
      "Iteration 145, loss = 0.23509139\n",
      "Iteration 146, loss = 0.23496317\n",
      "Iteration 147, loss = 0.23482674\n",
      "Iteration 148, loss = 0.23469235\n",
      "Iteration 149, loss = 0.23456958\n",
      "Iteration 150, loss = 0.23445607\n",
      "Iteration 151, loss = 0.23433939\n",
      "Iteration 152, loss = 0.23423274\n",
      "Iteration 153, loss = 0.23410794\n",
      "Iteration 154, loss = 0.23399872\n",
      "Iteration 155, loss = 0.23387160\n",
      "Iteration 156, loss = 0.23379172\n",
      "Iteration 157, loss = 0.23364316\n",
      "Iteration 158, loss = 0.23352963\n",
      "Iteration 159, loss = 0.23342228\n",
      "Iteration 160, loss = 0.23331688\n",
      "Iteration 161, loss = 0.23318331\n",
      "Iteration 162, loss = 0.23307494\n",
      "Iteration 163, loss = 0.23296697\n",
      "Iteration 164, loss = 0.23285767\n",
      "Iteration 165, loss = 0.23277987\n",
      "Iteration 166, loss = 0.23265706\n",
      "Iteration 167, loss = 0.23259957\n",
      "Iteration 168, loss = 0.23245538\n",
      "Iteration 169, loss = 0.23234669\n",
      "Iteration 170, loss = 0.23226116\n",
      "Iteration 171, loss = 0.23213597\n",
      "Iteration 172, loss = 0.23203620\n",
      "Iteration 173, loss = 0.23193265\n",
      "Iteration 174, loss = 0.23183660\n",
      "Iteration 175, loss = 0.23175756\n",
      "Iteration 176, loss = 0.23165684\n",
      "Iteration 177, loss = 0.23152996\n",
      "Iteration 178, loss = 0.23145009\n",
      "Iteration 179, loss = 0.23133889\n",
      "Iteration 180, loss = 0.23123854\n",
      "Iteration 181, loss = 0.23113512\n",
      "Iteration 182, loss = 0.23108522\n",
      "Iteration 183, loss = 0.23096377\n",
      "Iteration 184, loss = 0.23088546\n",
      "Iteration 185, loss = 0.23077696\n",
      "Iteration 186, loss = 0.23068970\n",
      "Iteration 187, loss = 0.23060729\n",
      "Iteration 188, loss = 0.23052363\n",
      "Iteration 189, loss = 0.23041626\n",
      "Iteration 190, loss = 0.23033038\n",
      "Iteration 191, loss = 0.23024444\n",
      "Iteration 192, loss = 0.23016533\n",
      "Iteration 193, loss = 0.23007833\n",
      "Iteration 194, loss = 0.22997673\n",
      "Iteration 195, loss = 0.22989719\n",
      "Iteration 196, loss = 0.22981186\n",
      "Iteration 197, loss = 0.22975032\n",
      "Iteration 198, loss = 0.22962541\n",
      "Iteration 199, loss = 0.22958940\n",
      "Iteration 200, loss = 0.22951284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84286491\n",
      "Iteration 2, loss = 0.73108972\n",
      "Iteration 3, loss = 0.63494203\n",
      "Iteration 4, loss = 0.56308067\n",
      "Iteration 5, loss = 0.50990778\n",
      "Iteration 6, loss = 0.46956048\n",
      "Iteration 7, loss = 0.43795871\n",
      "Iteration 8, loss = 0.41299647\n",
      "Iteration 9, loss = 0.39276283\n",
      "Iteration 10, loss = 0.37603595\n",
      "Iteration 11, loss = 0.36210913\n",
      "Iteration 12, loss = 0.35044839\n",
      "Iteration 13, loss = 0.34047583\n",
      "Iteration 14, loss = 0.33181593\n",
      "Iteration 15, loss = 0.32432736\n",
      "Iteration 16, loss = 0.31778532\n",
      "Iteration 17, loss = 0.31206163\n",
      "Iteration 18, loss = 0.30687894\n",
      "Iteration 19, loss = 0.30230911\n",
      "Iteration 20, loss = 0.29818560\n",
      "Iteration 21, loss = 0.29453259\n",
      "Iteration 22, loss = 0.29117758\n",
      "Iteration 23, loss = 0.28817580\n",
      "Iteration 24, loss = 0.28540454\n",
      "Iteration 25, loss = 0.28286100\n",
      "Iteration 26, loss = 0.28056352\n",
      "Iteration 27, loss = 0.27842849\n",
      "Iteration 28, loss = 0.27646650\n",
      "Iteration 29, loss = 0.27462347\n",
      "Iteration 30, loss = 0.27293879\n",
      "Iteration 31, loss = 0.27135840\n",
      "Iteration 32, loss = 0.26988155\n",
      "Iteration 33, loss = 0.26848892\n",
      "Iteration 34, loss = 0.26721506\n",
      "Iteration 35, loss = 0.26595682\n",
      "Iteration 36, loss = 0.26483691\n",
      "Iteration 37, loss = 0.26373495\n",
      "Iteration 38, loss = 0.26272925\n",
      "Iteration 39, loss = 0.26174404\n",
      "Iteration 40, loss = 0.26082780\n",
      "Iteration 41, loss = 0.25994839\n",
      "Iteration 42, loss = 0.25913245\n",
      "Iteration 43, loss = 0.25831633\n",
      "Iteration 44, loss = 0.25757016\n",
      "Iteration 45, loss = 0.25681875\n",
      "Iteration 46, loss = 0.25610545\n",
      "Iteration 47, loss = 0.25544980\n",
      "Iteration 48, loss = 0.25481471\n",
      "Iteration 49, loss = 0.25419731\n",
      "Iteration 50, loss = 0.25357880\n",
      "Iteration 51, loss = 0.25302635\n",
      "Iteration 52, loss = 0.25246083\n",
      "Iteration 53, loss = 0.25191539\n",
      "Iteration 54, loss = 0.25139964\n",
      "Iteration 55, loss = 0.25088870\n",
      "Iteration 56, loss = 0.25040765\n",
      "Iteration 57, loss = 0.24992404\n",
      "Iteration 58, loss = 0.24947498\n",
      "Iteration 59, loss = 0.24901656\n",
      "Iteration 60, loss = 0.24857068\n",
      "Iteration 61, loss = 0.24815853\n",
      "Iteration 62, loss = 0.24772988\n",
      "Iteration 63, loss = 0.24736912\n",
      "Iteration 64, loss = 0.24696227\n",
      "Iteration 65, loss = 0.24659691\n",
      "Iteration 66, loss = 0.24622230\n",
      "Iteration 67, loss = 0.24582154\n",
      "Iteration 68, loss = 0.24550545\n",
      "Iteration 69, loss = 0.24510624\n",
      "Iteration 70, loss = 0.24476349\n",
      "Iteration 71, loss = 0.24445990\n",
      "Iteration 72, loss = 0.24411518\n",
      "Iteration 73, loss = 0.24378269\n",
      "Iteration 74, loss = 0.24347217\n",
      "Iteration 75, loss = 0.24315795\n",
      "Iteration 76, loss = 0.24283750\n",
      "Iteration 77, loss = 0.24253959\n",
      "Iteration 78, loss = 0.24225464\n",
      "Iteration 79, loss = 0.24197253\n",
      "Iteration 80, loss = 0.24163792\n",
      "Iteration 81, loss = 0.24138240\n",
      "Iteration 82, loss = 0.24110277\n",
      "Iteration 83, loss = 0.24083732\n",
      "Iteration 84, loss = 0.24056289\n",
      "Iteration 85, loss = 0.24033545\n",
      "Iteration 86, loss = 0.24004591\n",
      "Iteration 87, loss = 0.23981167\n",
      "Iteration 88, loss = 0.23952336\n",
      "Iteration 89, loss = 0.23931221\n",
      "Iteration 90, loss = 0.23905505\n",
      "Iteration 91, loss = 0.23880845\n",
      "Iteration 92, loss = 0.23855201\n",
      "Iteration 93, loss = 0.23833394\n",
      "Iteration 94, loss = 0.23810429\n",
      "Iteration 95, loss = 0.23787327\n",
      "Iteration 96, loss = 0.23763796\n",
      "Iteration 97, loss = 0.23743695\n",
      "Iteration 98, loss = 0.23719978\n",
      "Iteration 99, loss = 0.23699614\n",
      "Iteration 100, loss = 0.23676729\n",
      "Iteration 101, loss = 0.23655712\n",
      "Iteration 102, loss = 0.23635508\n",
      "Iteration 103, loss = 0.23616725\n",
      "Iteration 104, loss = 0.23592842\n",
      "Iteration 105, loss = 0.23573613\n",
      "Iteration 106, loss = 0.23557413\n",
      "Iteration 107, loss = 0.23534379\n",
      "Iteration 108, loss = 0.23514998\n",
      "Iteration 109, loss = 0.23497093\n",
      "Iteration 110, loss = 0.23476610\n",
      "Iteration 111, loss = 0.23458189\n",
      "Iteration 112, loss = 0.23439057\n",
      "Iteration 113, loss = 0.23420772\n",
      "Iteration 114, loss = 0.23406364\n",
      "Iteration 115, loss = 0.23381160\n",
      "Iteration 116, loss = 0.23366563\n",
      "Iteration 117, loss = 0.23347787\n",
      "Iteration 118, loss = 0.23330445\n",
      "Iteration 119, loss = 0.23313484\n",
      "Iteration 120, loss = 0.23297994\n",
      "Iteration 121, loss = 0.23278945\n",
      "Iteration 122, loss = 0.23264369\n",
      "Iteration 123, loss = 0.23244596\n",
      "Iteration 124, loss = 0.23229319\n",
      "Iteration 125, loss = 0.23211656\n",
      "Iteration 126, loss = 0.23197879\n",
      "Iteration 127, loss = 0.23180727\n",
      "Iteration 128, loss = 0.23168978\n",
      "Iteration 129, loss = 0.23150642\n",
      "Iteration 130, loss = 0.23133297\n",
      "Iteration 131, loss = 0.23118106\n",
      "Iteration 132, loss = 0.23105416\n",
      "Iteration 133, loss = 0.23088752\n",
      "Iteration 134, loss = 0.23075984\n",
      "Iteration 135, loss = 0.23061128\n",
      "Iteration 136, loss = 0.23045515\n",
      "Iteration 137, loss = 0.23028805\n",
      "Iteration 138, loss = 0.23016946\n",
      "Iteration 139, loss = 0.23001242\n",
      "Iteration 140, loss = 0.22989333\n",
      "Iteration 141, loss = 0.22974370\n",
      "Iteration 142, loss = 0.22961760\n",
      "Iteration 143, loss = 0.22945493\n",
      "Iteration 144, loss = 0.22936917\n",
      "Iteration 145, loss = 0.22919668\n",
      "Iteration 146, loss = 0.22906501\n",
      "Iteration 147, loss = 0.22893149\n",
      "Iteration 148, loss = 0.22879527\n",
      "Iteration 149, loss = 0.22865759\n",
      "Iteration 150, loss = 0.22855098\n",
      "Iteration 151, loss = 0.22840992\n",
      "Iteration 152, loss = 0.22830743\n",
      "Iteration 153, loss = 0.22816069\n",
      "Iteration 154, loss = 0.22803815\n",
      "Iteration 155, loss = 0.22792267\n",
      "Iteration 156, loss = 0.22784552\n",
      "Iteration 157, loss = 0.22768350\n",
      "Iteration 158, loss = 0.22755654\n",
      "Iteration 159, loss = 0.22746103\n",
      "Iteration 160, loss = 0.22735146\n",
      "Iteration 161, loss = 0.22722805\n",
      "Iteration 162, loss = 0.22708945\n",
      "Iteration 163, loss = 0.22698756\n",
      "Iteration 164, loss = 0.22687127\n",
      "Iteration 165, loss = 0.22677169\n",
      "Iteration 166, loss = 0.22663271\n",
      "Iteration 167, loss = 0.22657487\n",
      "Iteration 168, loss = 0.22643084\n",
      "Iteration 169, loss = 0.22632740\n",
      "Iteration 170, loss = 0.22622205\n",
      "Iteration 171, loss = 0.22609551\n",
      "Iteration 172, loss = 0.22599906\n",
      "Iteration 173, loss = 0.22590160\n",
      "Iteration 174, loss = 0.22578986\n",
      "Iteration 175, loss = 0.22570499\n",
      "Iteration 176, loss = 0.22561314\n",
      "Iteration 177, loss = 0.22549650\n",
      "Iteration 178, loss = 0.22535795\n",
      "Iteration 179, loss = 0.22527765\n",
      "Iteration 180, loss = 0.22517326\n",
      "Iteration 181, loss = 0.22506502\n",
      "Iteration 182, loss = 0.22499744\n",
      "Iteration 183, loss = 0.22487458\n",
      "Iteration 184, loss = 0.22479470\n",
      "Iteration 185, loss = 0.22469085\n",
      "Iteration 186, loss = 0.22459895\n",
      "Iteration 187, loss = 0.22451167\n",
      "Iteration 188, loss = 0.22442862\n",
      "Iteration 189, loss = 0.22431927\n",
      "Iteration 190, loss = 0.22422869\n",
      "Iteration 191, loss = 0.22413875\n",
      "Iteration 192, loss = 0.22405110\n",
      "Iteration 193, loss = 0.22395308\n",
      "Iteration 194, loss = 0.22386563\n",
      "Iteration 195, loss = 0.22377350\n",
      "Iteration 196, loss = 0.22369366\n",
      "Iteration 197, loss = 0.22360321\n",
      "Iteration 198, loss = 0.22350237\n",
      "Iteration 199, loss = 0.22345751\n",
      "Iteration 200, loss = 0.22334466\n",
      "Iteration 1, loss = 0.84927725\n",
      "Iteration 2, loss = 0.73512048\n",
      "Iteration 3, loss = 0.63761253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.56535933\n",
      "Iteration 5, loss = 0.51188548\n",
      "Iteration 6, loss = 0.47130704\n",
      "Iteration 7, loss = 0.43989253\n",
      "Iteration 8, loss = 0.41513332\n",
      "Iteration 9, loss = 0.39499253\n",
      "Iteration 10, loss = 0.37846590\n",
      "Iteration 11, loss = 0.36472533\n",
      "Iteration 12, loss = 0.35322389\n",
      "Iteration 13, loss = 0.34340586\n",
      "Iteration 14, loss = 0.33487384\n",
      "Iteration 15, loss = 0.32754514\n",
      "Iteration 16, loss = 0.32109959\n",
      "Iteration 17, loss = 0.31546892\n",
      "Iteration 18, loss = 0.31041485\n",
      "Iteration 19, loss = 0.30595731\n",
      "Iteration 20, loss = 0.30190528\n",
      "Iteration 21, loss = 0.29832955\n",
      "Iteration 22, loss = 0.29508895\n",
      "Iteration 23, loss = 0.29212649\n",
      "Iteration 24, loss = 0.28941246\n",
      "Iteration 25, loss = 0.28693573\n",
      "Iteration 26, loss = 0.28467411\n",
      "Iteration 27, loss = 0.28260932\n",
      "Iteration 28, loss = 0.28065621\n",
      "Iteration 29, loss = 0.27889196\n",
      "Iteration 30, loss = 0.27723826\n",
      "Iteration 31, loss = 0.27572101\n",
      "Iteration 32, loss = 0.27424776\n",
      "Iteration 33, loss = 0.27289082\n",
      "Iteration 34, loss = 0.27163248\n",
      "Iteration 35, loss = 0.27045455\n",
      "Iteration 36, loss = 0.26930851\n",
      "Iteration 37, loss = 0.26826407\n",
      "Iteration 38, loss = 0.26727816\n",
      "Iteration 39, loss = 0.26631664\n",
      "Iteration 40, loss = 0.26544525\n",
      "Iteration 41, loss = 0.26456662\n",
      "Iteration 42, loss = 0.26377247\n",
      "Iteration 43, loss = 0.26298651\n",
      "Iteration 44, loss = 0.26225286\n",
      "Iteration 45, loss = 0.26154395\n",
      "Iteration 46, loss = 0.26084722\n",
      "Iteration 47, loss = 0.26020092\n",
      "Iteration 48, loss = 0.25957362\n",
      "Iteration 49, loss = 0.25898475\n",
      "Iteration 50, loss = 0.25835493\n",
      "Iteration 51, loss = 0.25783251\n",
      "Iteration 52, loss = 0.25727675\n",
      "Iteration 53, loss = 0.25673608\n",
      "Iteration 54, loss = 0.25624397\n",
      "Iteration 55, loss = 0.25573969\n",
      "Iteration 56, loss = 0.25527566\n",
      "Iteration 57, loss = 0.25480333\n",
      "Iteration 58, loss = 0.25436826\n",
      "Iteration 59, loss = 0.25391096\n",
      "Iteration 60, loss = 0.25349015\n",
      "Iteration 61, loss = 0.25307609\n",
      "Iteration 62, loss = 0.25266363\n",
      "Iteration 63, loss = 0.25231321\n",
      "Iteration 64, loss = 0.25188911\n",
      "Iteration 65, loss = 0.25152472\n",
      "Iteration 66, loss = 0.25115016\n",
      "Iteration 67, loss = 0.25080210\n",
      "Iteration 68, loss = 0.25045771\n",
      "Iteration 69, loss = 0.25008528\n",
      "Iteration 70, loss = 0.24976243\n",
      "Iteration 71, loss = 0.24942890\n",
      "Iteration 72, loss = 0.24909295\n",
      "Iteration 73, loss = 0.24877080\n",
      "Iteration 74, loss = 0.24846280\n",
      "Iteration 75, loss = 0.24817354\n",
      "Iteration 76, loss = 0.24786266\n",
      "Iteration 77, loss = 0.24754455\n",
      "Iteration 78, loss = 0.24726343\n",
      "Iteration 79, loss = 0.24699625\n",
      "Iteration 80, loss = 0.24669228\n",
      "Iteration 81, loss = 0.24640670\n",
      "Iteration 82, loss = 0.24614641\n",
      "Iteration 83, loss = 0.24588299\n",
      "Iteration 84, loss = 0.24562096\n",
      "Iteration 85, loss = 0.24536109\n",
      "Iteration 86, loss = 0.24508911\n",
      "Iteration 87, loss = 0.24484913\n",
      "Iteration 88, loss = 0.24457917\n",
      "Iteration 89, loss = 0.24433933\n",
      "Iteration 90, loss = 0.24411023\n",
      "Iteration 91, loss = 0.24386546\n",
      "Iteration 92, loss = 0.24360450\n",
      "Iteration 93, loss = 0.24340035\n",
      "Iteration 94, loss = 0.24315698\n",
      "Iteration 95, loss = 0.24293092\n",
      "Iteration 96, loss = 0.24271721\n",
      "Iteration 97, loss = 0.24247121\n",
      "Iteration 98, loss = 0.24227194\n",
      "Iteration 99, loss = 0.24205748\n",
      "Iteration 100, loss = 0.24184594\n",
      "Iteration 101, loss = 0.24162337\n",
      "Iteration 102, loss = 0.24141005\n",
      "Iteration 103, loss = 0.24123524\n",
      "Iteration 104, loss = 0.24100124\n",
      "Iteration 105, loss = 0.24081046\n",
      "Iteration 106, loss = 0.24061588\n",
      "Iteration 107, loss = 0.24040989\n",
      "Iteration 108, loss = 0.24020712\n",
      "Iteration 109, loss = 0.24007369\n",
      "Iteration 110, loss = 0.23982825\n",
      "Iteration 111, loss = 0.23963902\n",
      "Iteration 112, loss = 0.23945544\n",
      "Iteration 113, loss = 0.23928258\n",
      "Iteration 114, loss = 0.23909877\n",
      "Iteration 115, loss = 0.23890560\n",
      "Iteration 116, loss = 0.23872994\n",
      "Iteration 117, loss = 0.23855928\n",
      "Iteration 118, loss = 0.23836083\n",
      "Iteration 119, loss = 0.23820604\n",
      "Iteration 120, loss = 0.23803762\n",
      "Iteration 121, loss = 0.23785321\n",
      "Iteration 122, loss = 0.23769007\n",
      "Iteration 123, loss = 0.23753902\n",
      "Iteration 124, loss = 0.23737267\n",
      "Iteration 125, loss = 0.23718723\n",
      "Iteration 126, loss = 0.23706735\n",
      "Iteration 127, loss = 0.23686915\n",
      "Iteration 128, loss = 0.23678703\n",
      "Iteration 129, loss = 0.23658098\n",
      "Iteration 130, loss = 0.23639117\n",
      "Iteration 131, loss = 0.23624301\n",
      "Iteration 132, loss = 0.23611694\n",
      "Iteration 133, loss = 0.23593724\n",
      "Iteration 134, loss = 0.23583311\n",
      "Iteration 135, loss = 0.23566963\n",
      "Iteration 136, loss = 0.23550971\n",
      "Iteration 137, loss = 0.23538528\n",
      "Iteration 138, loss = 0.23524808\n",
      "Iteration 139, loss = 0.23507180\n",
      "Iteration 140, loss = 0.23495528\n",
      "Iteration 141, loss = 0.23481805\n",
      "Iteration 142, loss = 0.23466029\n",
      "Iteration 143, loss = 0.23452407\n",
      "Iteration 144, loss = 0.23443369\n",
      "Iteration 145, loss = 0.23425304\n",
      "Iteration 146, loss = 0.23412427\n",
      "Iteration 147, loss = 0.23398198\n",
      "Iteration 148, loss = 0.23384789\n",
      "Iteration 149, loss = 0.23370447\n",
      "Iteration 150, loss = 0.23362059\n",
      "Iteration 151, loss = 0.23346491\n",
      "Iteration 152, loss = 0.23335964\n",
      "Iteration 153, loss = 0.23320474\n",
      "Iteration 154, loss = 0.23308269\n",
      "Iteration 155, loss = 0.23296117\n",
      "Iteration 156, loss = 0.23286277\n",
      "Iteration 157, loss = 0.23272536\n",
      "Iteration 158, loss = 0.23260074\n",
      "Iteration 159, loss = 0.23248939\n",
      "Iteration 160, loss = 0.23239636\n",
      "Iteration 161, loss = 0.23225201\n",
      "Iteration 162, loss = 0.23213758\n",
      "Iteration 163, loss = 0.23201058\n",
      "Iteration 164, loss = 0.23189899\n",
      "Iteration 165, loss = 0.23181779\n",
      "Iteration 166, loss = 0.23168832\n",
      "Iteration 167, loss = 0.23159231\n",
      "Iteration 168, loss = 0.23145694\n",
      "Iteration 169, loss = 0.23135227\n",
      "Iteration 170, loss = 0.23124481\n",
      "Iteration 171, loss = 0.23113157\n",
      "Iteration 172, loss = 0.23101770\n",
      "Iteration 173, loss = 0.23092548\n",
      "Iteration 174, loss = 0.23080881\n",
      "Iteration 175, loss = 0.23071240\n",
      "Iteration 176, loss = 0.23061685\n",
      "Iteration 177, loss = 0.23051693\n",
      "Iteration 178, loss = 0.23039220\n",
      "Iteration 179, loss = 0.23029556\n",
      "Iteration 180, loss = 0.23020425\n",
      "Iteration 181, loss = 0.23007707\n",
      "Iteration 182, loss = 0.23001143\n",
      "Iteration 183, loss = 0.22988101\n",
      "Iteration 184, loss = 0.22982353\n",
      "Iteration 185, loss = 0.22972099\n",
      "Iteration 186, loss = 0.22961205\n",
      "Iteration 187, loss = 0.22953336\n",
      "Iteration 188, loss = 0.22944138\n",
      "Iteration 189, loss = 0.22933914\n",
      "Iteration 190, loss = 0.22922023\n",
      "Iteration 191, loss = 0.22917807\n",
      "Iteration 192, loss = 0.22905015\n",
      "Iteration 193, loss = 0.22897541\n",
      "Iteration 194, loss = 0.22885742\n",
      "Iteration 195, loss = 0.22876325\n",
      "Iteration 196, loss = 0.22868352\n",
      "Iteration 197, loss = 0.22859684\n",
      "Iteration 198, loss = 0.22851852\n",
      "Iteration 199, loss = 0.22844966\n",
      "Iteration 200, loss = 0.22832379\n",
      "Iteration 1, loss = 0.84476495\n",
      "Iteration 2, loss = 0.73211981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.63559504\n",
      "Iteration 4, loss = 0.56361933\n",
      "Iteration 5, loss = 0.51041556\n",
      "Iteration 6, loss = 0.46977917\n",
      "Iteration 7, loss = 0.43834969\n",
      "Iteration 8, loss = 0.41340333\n",
      "Iteration 9, loss = 0.39310191\n",
      "Iteration 10, loss = 0.37642365\n",
      "Iteration 11, loss = 0.36252292\n",
      "Iteration 12, loss = 0.35087689\n",
      "Iteration 13, loss = 0.34092224\n",
      "Iteration 14, loss = 0.33226023\n",
      "Iteration 15, loss = 0.32484748\n",
      "Iteration 16, loss = 0.31824652\n",
      "Iteration 17, loss = 0.31257151\n",
      "Iteration 18, loss = 0.30741962\n",
      "Iteration 19, loss = 0.30287508\n",
      "Iteration 20, loss = 0.29877990\n",
      "Iteration 21, loss = 0.29512758\n",
      "Iteration 22, loss = 0.29183641\n",
      "Iteration 23, loss = 0.28882871\n",
      "Iteration 24, loss = 0.28606009\n",
      "Iteration 25, loss = 0.28355856\n",
      "Iteration 26, loss = 0.28125317\n",
      "Iteration 27, loss = 0.27918004\n",
      "Iteration 28, loss = 0.27719551\n",
      "Iteration 29, loss = 0.27537244\n",
      "Iteration 30, loss = 0.27368783\n",
      "Iteration 31, loss = 0.27214350\n",
      "Iteration 32, loss = 0.27065968\n",
      "Iteration 33, loss = 0.26929752\n",
      "Iteration 34, loss = 0.26802468\n",
      "Iteration 35, loss = 0.26683602\n",
      "Iteration 36, loss = 0.26570015\n",
      "Iteration 37, loss = 0.26462287\n",
      "Iteration 38, loss = 0.26361134\n",
      "Iteration 39, loss = 0.26267114\n",
      "Iteration 40, loss = 0.26177730\n",
      "Iteration 41, loss = 0.26091135\n",
      "Iteration 42, loss = 0.26009356\n",
      "Iteration 43, loss = 0.25931311\n",
      "Iteration 44, loss = 0.25856229\n",
      "Iteration 45, loss = 0.25786349\n",
      "Iteration 46, loss = 0.25717156\n",
      "Iteration 47, loss = 0.25653286\n",
      "Iteration 48, loss = 0.25589621\n",
      "Iteration 49, loss = 0.25532034\n",
      "Iteration 50, loss = 0.25471355\n",
      "Iteration 51, loss = 0.25416736\n",
      "Iteration 52, loss = 0.25362995\n",
      "Iteration 53, loss = 0.25308955\n",
      "Iteration 54, loss = 0.25259733\n",
      "Iteration 55, loss = 0.25210893\n",
      "Iteration 56, loss = 0.25165203\n",
      "Iteration 57, loss = 0.25118607\n",
      "Iteration 58, loss = 0.25073970\n",
      "Iteration 59, loss = 0.25029807\n",
      "Iteration 60, loss = 0.24989313\n",
      "Iteration 61, loss = 0.24947422\n",
      "Iteration 62, loss = 0.24908248\n",
      "Iteration 63, loss = 0.24871607\n",
      "Iteration 64, loss = 0.24832185\n",
      "Iteration 65, loss = 0.24794899\n",
      "Iteration 66, loss = 0.24759644\n",
      "Iteration 67, loss = 0.24726284\n",
      "Iteration 68, loss = 0.24689829\n",
      "Iteration 69, loss = 0.24656841\n",
      "Iteration 70, loss = 0.24624029\n",
      "Iteration 71, loss = 0.24590461\n",
      "Iteration 72, loss = 0.24560061\n",
      "Iteration 73, loss = 0.24527223\n",
      "Iteration 74, loss = 0.24496960\n",
      "Iteration 75, loss = 0.24470097\n",
      "Iteration 76, loss = 0.24437088\n",
      "Iteration 77, loss = 0.24407101\n",
      "Iteration 78, loss = 0.24380704\n",
      "Iteration 79, loss = 0.24354329\n",
      "Iteration 80, loss = 0.24324262\n",
      "Iteration 81, loss = 0.24295887\n",
      "Iteration 82, loss = 0.24269376\n",
      "Iteration 83, loss = 0.24243455\n",
      "Iteration 84, loss = 0.24218527\n",
      "Iteration 85, loss = 0.24191154\n",
      "Iteration 86, loss = 0.24169229\n",
      "Iteration 87, loss = 0.24143913\n",
      "Iteration 88, loss = 0.24116617\n",
      "Iteration 89, loss = 0.24094976\n",
      "Iteration 90, loss = 0.24071600\n",
      "Iteration 91, loss = 0.24046831\n",
      "Iteration 92, loss = 0.24022233\n",
      "Iteration 93, loss = 0.24000377\n",
      "Iteration 94, loss = 0.23977675\n",
      "Iteration 95, loss = 0.23953521\n",
      "Iteration 96, loss = 0.23932882\n",
      "Iteration 97, loss = 0.23910143\n",
      "Iteration 98, loss = 0.23891340\n",
      "Iteration 99, loss = 0.23871316\n",
      "Iteration 100, loss = 0.23848176\n",
      "Iteration 101, loss = 0.23827565\n",
      "Iteration 102, loss = 0.23805009\n",
      "Iteration 103, loss = 0.23787660\n",
      "Iteration 104, loss = 0.23766576\n",
      "Iteration 105, loss = 0.23747272\n",
      "Iteration 106, loss = 0.23725058\n",
      "Iteration 107, loss = 0.23707664\n",
      "Iteration 108, loss = 0.23688125\n",
      "Iteration 109, loss = 0.23670774\n",
      "Iteration 110, loss = 0.23649687\n",
      "Iteration 111, loss = 0.23630214\n",
      "Iteration 112, loss = 0.23611681\n",
      "Iteration 113, loss = 0.23593930\n",
      "Iteration 114, loss = 0.23574044\n",
      "Iteration 115, loss = 0.23557054\n",
      "Iteration 116, loss = 0.23540861\n",
      "Iteration 117, loss = 0.23521841\n",
      "Iteration 118, loss = 0.23503934\n",
      "Iteration 119, loss = 0.23486819\n",
      "Iteration 120, loss = 0.23470259\n",
      "Iteration 121, loss = 0.23452357\n",
      "Iteration 122, loss = 0.23436686\n",
      "Iteration 123, loss = 0.23419648\n",
      "Iteration 124, loss = 0.23404322\n",
      "Iteration 125, loss = 0.23385468\n",
      "Iteration 126, loss = 0.23372088\n",
      "Iteration 127, loss = 0.23354460\n",
      "Iteration 128, loss = 0.23340042\n",
      "Iteration 129, loss = 0.23322624\n",
      "Iteration 130, loss = 0.23304840\n",
      "Iteration 131, loss = 0.23291600\n",
      "Iteration 132, loss = 0.23275991\n",
      "Iteration 133, loss = 0.23259267\n",
      "Iteration 134, loss = 0.23246859\n",
      "Iteration 135, loss = 0.23230741\n",
      "Iteration 136, loss = 0.23215565\n",
      "Iteration 137, loss = 0.23202251\n",
      "Iteration 138, loss = 0.23188960\n",
      "Iteration 139, loss = 0.23171050\n",
      "Iteration 140, loss = 0.23158460\n",
      "Iteration 141, loss = 0.23146759\n",
      "Iteration 142, loss = 0.23132813\n",
      "Iteration 143, loss = 0.23116160\n",
      "Iteration 144, loss = 0.23105911\n",
      "Iteration 145, loss = 0.23087878\n",
      "Iteration 146, loss = 0.23072929\n",
      "Iteration 147, loss = 0.23060058\n",
      "Iteration 148, loss = 0.23047133\n",
      "Iteration 149, loss = 0.23032146\n",
      "Iteration 150, loss = 0.23021326\n",
      "Iteration 151, loss = 0.23007702\n",
      "Iteration 152, loss = 0.22995990\n",
      "Iteration 153, loss = 0.22978883\n",
      "Iteration 154, loss = 0.22965403\n",
      "Iteration 155, loss = 0.22953681\n",
      "Iteration 156, loss = 0.22943731\n",
      "Iteration 157, loss = 0.22931147\n",
      "Iteration 158, loss = 0.22918101\n",
      "Iteration 159, loss = 0.22907389\n",
      "Iteration 160, loss = 0.22892744\n",
      "Iteration 161, loss = 0.22880994\n",
      "Iteration 162, loss = 0.22868851\n",
      "Iteration 163, loss = 0.22856684\n",
      "Iteration 164, loss = 0.22845267\n",
      "Iteration 165, loss = 0.22834329\n",
      "Iteration 166, loss = 0.22824386\n",
      "Iteration 167, loss = 0.22810818\n",
      "Iteration 168, loss = 0.22799562\n",
      "Iteration 169, loss = 0.22789498\n",
      "Iteration 170, loss = 0.22777759\n",
      "Iteration 171, loss = 0.22767093\n",
      "Iteration 172, loss = 0.22753450\n",
      "Iteration 173, loss = 0.22742982\n",
      "Iteration 174, loss = 0.22731379\n",
      "Iteration 175, loss = 0.22720671\n",
      "Iteration 176, loss = 0.22710027\n",
      "Iteration 177, loss = 0.22698166\n",
      "Iteration 178, loss = 0.22685960\n",
      "Iteration 179, loss = 0.22677751\n",
      "Iteration 180, loss = 0.22666595\n",
      "Iteration 181, loss = 0.22653114\n",
      "Iteration 182, loss = 0.22644431\n",
      "Iteration 183, loss = 0.22634255\n",
      "Iteration 184, loss = 0.22626216\n",
      "Iteration 185, loss = 0.22614888\n",
      "Iteration 186, loss = 0.22604709\n",
      "Iteration 187, loss = 0.22596682\n",
      "Iteration 188, loss = 0.22588274\n",
      "Iteration 189, loss = 0.22576912\n",
      "Iteration 190, loss = 0.22562655\n",
      "Iteration 191, loss = 0.22556182\n",
      "Iteration 192, loss = 0.22545364\n",
      "Iteration 193, loss = 0.22537511\n",
      "Iteration 194, loss = 0.22526777\n",
      "Iteration 195, loss = 0.22514766\n",
      "Iteration 196, loss = 0.22506077\n",
      "Iteration 197, loss = 0.22496409\n",
      "Iteration 198, loss = 0.22488546\n",
      "Iteration 199, loss = 0.22478056\n",
      "Iteration 200, loss = 0.22467558\n",
      "Iteration 1, loss = 0.84906207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.73617711\n",
      "Iteration 3, loss = 0.63867597\n",
      "Iteration 4, loss = 0.56661873\n",
      "Iteration 5, loss = 0.51271002\n",
      "Iteration 6, loss = 0.47170489\n",
      "Iteration 7, loss = 0.43991359\n",
      "Iteration 8, loss = 0.41477269\n",
      "Iteration 9, loss = 0.39404729\n",
      "Iteration 10, loss = 0.37723421\n",
      "Iteration 11, loss = 0.36305463\n",
      "Iteration 12, loss = 0.35114667\n",
      "Iteration 13, loss = 0.34098824\n",
      "Iteration 14, loss = 0.33215535\n",
      "Iteration 15, loss = 0.32455590\n",
      "Iteration 16, loss = 0.31778800\n",
      "Iteration 17, loss = 0.31191612\n",
      "Iteration 18, loss = 0.30663915\n",
      "Iteration 19, loss = 0.30189639\n",
      "Iteration 20, loss = 0.29768526\n",
      "Iteration 21, loss = 0.29388773\n",
      "Iteration 22, loss = 0.29043711\n",
      "Iteration 23, loss = 0.28733054\n",
      "Iteration 24, loss = 0.28443396\n",
      "Iteration 25, loss = 0.28184080\n",
      "Iteration 26, loss = 0.27941243\n",
      "Iteration 27, loss = 0.27723924\n",
      "Iteration 28, loss = 0.27517317\n",
      "Iteration 29, loss = 0.27327828\n",
      "Iteration 30, loss = 0.27150972\n",
      "Iteration 31, loss = 0.26988853\n",
      "Iteration 32, loss = 0.26830613\n",
      "Iteration 33, loss = 0.26689316\n",
      "Iteration 34, loss = 0.26553261\n",
      "Iteration 35, loss = 0.26429984\n",
      "Iteration 36, loss = 0.26307381\n",
      "Iteration 37, loss = 0.26197079\n",
      "Iteration 38, loss = 0.26090175\n",
      "Iteration 39, loss = 0.25991942\n",
      "Iteration 40, loss = 0.25899080\n",
      "Iteration 41, loss = 0.25805605\n",
      "Iteration 42, loss = 0.25719807\n",
      "Iteration 43, loss = 0.25638776\n",
      "Iteration 44, loss = 0.25558963\n",
      "Iteration 45, loss = 0.25486639\n",
      "Iteration 46, loss = 0.25414264\n",
      "Iteration 47, loss = 0.25346873\n",
      "Iteration 48, loss = 0.25282558\n",
      "Iteration 49, loss = 0.25219337\n",
      "Iteration 50, loss = 0.25155897\n",
      "Iteration 51, loss = 0.25099841\n",
      "Iteration 52, loss = 0.25044165\n",
      "Iteration 53, loss = 0.24988026\n",
      "Iteration 54, loss = 0.24935419\n",
      "Iteration 55, loss = 0.24886841\n",
      "Iteration 56, loss = 0.24836273\n",
      "Iteration 57, loss = 0.24788670\n",
      "Iteration 58, loss = 0.24744227\n",
      "Iteration 59, loss = 0.24697471\n",
      "Iteration 60, loss = 0.24655404\n",
      "Iteration 61, loss = 0.24613942\n",
      "Iteration 62, loss = 0.24573574\n",
      "Iteration 63, loss = 0.24531234\n",
      "Iteration 64, loss = 0.24496897\n",
      "Iteration 65, loss = 0.24457950\n",
      "Iteration 66, loss = 0.24420064\n",
      "Iteration 67, loss = 0.24384994\n",
      "Iteration 68, loss = 0.24348582\n",
      "Iteration 69, loss = 0.24315671\n",
      "Iteration 70, loss = 0.24284774\n",
      "Iteration 71, loss = 0.24246888\n",
      "Iteration 72, loss = 0.24216963\n",
      "Iteration 73, loss = 0.24185340\n",
      "Iteration 74, loss = 0.24153579\n",
      "Iteration 75, loss = 0.24125771\n",
      "Iteration 76, loss = 0.24094785\n",
      "Iteration 77, loss = 0.24064854\n",
      "Iteration 78, loss = 0.24038207\n",
      "Iteration 79, loss = 0.24009787\n",
      "Iteration 80, loss = 0.23982422\n",
      "Iteration 81, loss = 0.23954243\n",
      "Iteration 82, loss = 0.23928393\n",
      "Iteration 83, loss = 0.23899992\n",
      "Iteration 84, loss = 0.23875313\n",
      "Iteration 85, loss = 0.23848080\n",
      "Iteration 86, loss = 0.23827311\n",
      "Iteration 87, loss = 0.23799484\n",
      "Iteration 88, loss = 0.23776219\n",
      "Iteration 89, loss = 0.23754022\n",
      "Iteration 90, loss = 0.23727240\n",
      "Iteration 91, loss = 0.23706983\n",
      "Iteration 92, loss = 0.23682820\n",
      "Iteration 93, loss = 0.23658734\n",
      "Iteration 94, loss = 0.23638551\n",
      "Iteration 95, loss = 0.23616071\n",
      "Iteration 96, loss = 0.23594095\n",
      "Iteration 97, loss = 0.23571407\n",
      "Iteration 98, loss = 0.23551358\n",
      "Iteration 99, loss = 0.23531293\n",
      "Iteration 100, loss = 0.23509830\n",
      "Iteration 101, loss = 0.23490434\n",
      "Iteration 102, loss = 0.23469061\n",
      "Iteration 103, loss = 0.23450237\n",
      "Iteration 104, loss = 0.23432842\n",
      "Iteration 105, loss = 0.23409833\n",
      "Iteration 106, loss = 0.23391224\n",
      "Iteration 107, loss = 0.23373621\n",
      "Iteration 108, loss = 0.23355620\n",
      "Iteration 109, loss = 0.23334337\n",
      "Iteration 110, loss = 0.23317867\n",
      "Iteration 111, loss = 0.23300309\n",
      "Iteration 112, loss = 0.23281122\n",
      "Iteration 113, loss = 0.23263895\n",
      "Iteration 114, loss = 0.23243356\n",
      "Iteration 115, loss = 0.23229101\n",
      "Iteration 116, loss = 0.23211618\n",
      "Iteration 117, loss = 0.23192955\n",
      "Iteration 118, loss = 0.23175876\n",
      "Iteration 119, loss = 0.23161405\n",
      "Iteration 120, loss = 0.23143189\n",
      "Iteration 121, loss = 0.23128537\n",
      "Iteration 122, loss = 0.23110633\n",
      "Iteration 123, loss = 0.23095822\n",
      "Iteration 124, loss = 0.23080486\n",
      "Iteration 125, loss = 0.23061795\n",
      "Iteration 126, loss = 0.23048967\n",
      "Iteration 127, loss = 0.23032287\n",
      "Iteration 128, loss = 0.23017557\n",
      "Iteration 129, loss = 0.23001473\n",
      "Iteration 130, loss = 0.22986206\n",
      "Iteration 131, loss = 0.22973351\n",
      "Iteration 132, loss = 0.22960299\n",
      "Iteration 133, loss = 0.22942411\n",
      "Iteration 134, loss = 0.22930582\n",
      "Iteration 135, loss = 0.22914174\n",
      "Iteration 136, loss = 0.22902188\n",
      "Iteration 137, loss = 0.22885047\n",
      "Iteration 138, loss = 0.22875169\n",
      "Iteration 139, loss = 0.22858646\n",
      "Iteration 140, loss = 0.22845504\n",
      "Iteration 141, loss = 0.22835757\n",
      "Iteration 142, loss = 0.22820122\n",
      "Iteration 143, loss = 0.22807114\n",
      "Iteration 144, loss = 0.22795038\n",
      "Iteration 145, loss = 0.22779950\n",
      "Iteration 146, loss = 0.22767255\n",
      "Iteration 147, loss = 0.22751812\n",
      "Iteration 148, loss = 0.22741311\n",
      "Iteration 149, loss = 0.22727449\n",
      "Iteration 150, loss = 0.22716378\n",
      "Iteration 151, loss = 0.22703421\n",
      "Iteration 152, loss = 0.22692372\n",
      "Iteration 153, loss = 0.22675652\n",
      "Iteration 154, loss = 0.22664322\n",
      "Iteration 155, loss = 0.22653221\n",
      "Iteration 156, loss = 0.22640815\n",
      "Iteration 157, loss = 0.22631497\n",
      "Iteration 158, loss = 0.22619877\n",
      "Iteration 159, loss = 0.22608757\n",
      "Iteration 160, loss = 0.22593567\n",
      "Iteration 161, loss = 0.22583302\n",
      "Iteration 162, loss = 0.22571608\n",
      "Iteration 163, loss = 0.22560767\n",
      "Iteration 164, loss = 0.22550706\n",
      "Iteration 165, loss = 0.22538482\n",
      "Iteration 166, loss = 0.22530995\n",
      "Iteration 167, loss = 0.22517636\n",
      "Iteration 168, loss = 0.22505490\n",
      "Iteration 169, loss = 0.22497185\n",
      "Iteration 170, loss = 0.22486921\n",
      "Iteration 171, loss = 0.22474756\n",
      "Iteration 172, loss = 0.22466654\n",
      "Iteration 173, loss = 0.22454255\n",
      "Iteration 174, loss = 0.22443053\n",
      "Iteration 175, loss = 0.22434655\n",
      "Iteration 176, loss = 0.22423205\n",
      "Iteration 177, loss = 0.22409598\n",
      "Iteration 178, loss = 0.22401196\n",
      "Iteration 179, loss = 0.22393008\n",
      "Iteration 180, loss = 0.22382473\n",
      "Iteration 181, loss = 0.22371841\n",
      "Iteration 182, loss = 0.22360091\n",
      "Iteration 183, loss = 0.22352500\n",
      "Iteration 184, loss = 0.22344209\n",
      "Iteration 185, loss = 0.22334451\n",
      "Iteration 186, loss = 0.22325577\n",
      "Iteration 187, loss = 0.22316024\n",
      "Iteration 188, loss = 0.22306806\n",
      "Iteration 189, loss = 0.22301237\n",
      "Iteration 190, loss = 0.22284228\n",
      "Iteration 191, loss = 0.22278576\n",
      "Iteration 192, loss = 0.22269674\n",
      "Iteration 193, loss = 0.22259741\n",
      "Iteration 194, loss = 0.22251565\n",
      "Iteration 195, loss = 0.22241072\n",
      "Iteration 196, loss = 0.22231934\n",
      "Iteration 197, loss = 0.22224470\n",
      "Iteration 198, loss = 0.22213776\n",
      "Iteration 199, loss = 0.22207986\n",
      "Iteration 200, loss = 0.22200060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72251243\n",
      "Iteration 2, loss = 0.51809279\n",
      "Iteration 3, loss = 0.41508819\n",
      "Iteration 4, loss = 0.35041637\n",
      "Iteration 5, loss = 0.30970397\n",
      "Iteration 6, loss = 0.28402331\n",
      "Iteration 7, loss = 0.26678385\n",
      "Iteration 8, loss = 0.25667932\n",
      "Iteration 9, loss = 0.24915447\n",
      "Iteration 10, loss = 0.24401101\n",
      "Iteration 11, loss = 0.23884372\n",
      "Iteration 12, loss = 0.23620538\n",
      "Iteration 13, loss = 0.23328709\n",
      "Iteration 14, loss = 0.23148566\n",
      "Iteration 15, loss = 0.22964324\n",
      "Iteration 16, loss = 0.22825496\n",
      "Iteration 17, loss = 0.22684591\n",
      "Iteration 18, loss = 0.22540990\n",
      "Iteration 19, loss = 0.22370201\n",
      "Iteration 20, loss = 0.22226432\n",
      "Iteration 21, loss = 0.22105904\n",
      "Iteration 22, loss = 0.21945470\n",
      "Iteration 23, loss = 0.21887571\n",
      "Iteration 24, loss = 0.21747312\n",
      "Iteration 25, loss = 0.21610937\n",
      "Iteration 26, loss = 0.21509018\n",
      "Iteration 27, loss = 0.21349173\n",
      "Iteration 28, loss = 0.21201784\n",
      "Iteration 29, loss = 0.21065445\n",
      "Iteration 30, loss = 0.20998014\n",
      "Iteration 31, loss = 0.20767614\n",
      "Iteration 32, loss = 0.20637307\n",
      "Iteration 33, loss = 0.20539291\n",
      "Iteration 34, loss = 0.20362411\n",
      "Iteration 35, loss = 0.20125687\n",
      "Iteration 36, loss = 0.19981683\n",
      "Iteration 37, loss = 0.19849350\n",
      "Iteration 38, loss = 0.19765853\n",
      "Iteration 39, loss = 0.19522267\n",
      "Iteration 40, loss = 0.19361427\n",
      "Iteration 41, loss = 0.19175209\n",
      "Iteration 42, loss = 0.18997930\n",
      "Iteration 43, loss = 0.18786253\n",
      "Iteration 44, loss = 0.18601389\n",
      "Iteration 45, loss = 0.18431803\n",
      "Iteration 46, loss = 0.18286394\n",
      "Iteration 47, loss = 0.18080150\n",
      "Iteration 48, loss = 0.17985233\n",
      "Iteration 49, loss = 0.17738779\n",
      "Iteration 50, loss = 0.17628316\n",
      "Iteration 51, loss = 0.17400752\n",
      "Iteration 52, loss = 0.17220310\n",
      "Iteration 53, loss = 0.17053163\n",
      "Iteration 54, loss = 0.16919396\n",
      "Iteration 55, loss = 0.16667316\n",
      "Iteration 56, loss = 0.16525833\n",
      "Iteration 57, loss = 0.16293720\n",
      "Iteration 58, loss = 0.16142170\n",
      "Iteration 59, loss = 0.15982695\n",
      "Iteration 60, loss = 0.15807180\n",
      "Iteration 61, loss = 0.15624613\n",
      "Iteration 62, loss = 0.15428192\n",
      "Iteration 63, loss = 0.15329990\n",
      "Iteration 64, loss = 0.15153586\n",
      "Iteration 65, loss = 0.14894809\n",
      "Iteration 66, loss = 0.14773298\n",
      "Iteration 67, loss = 0.14669039\n",
      "Iteration 68, loss = 0.14489209\n",
      "Iteration 69, loss = 0.14261455\n",
      "Iteration 70, loss = 0.14150004\n",
      "Iteration 71, loss = 0.14028000\n",
      "Iteration 72, loss = 0.13856208\n",
      "Iteration 73, loss = 0.13664948\n",
      "Iteration 74, loss = 0.13518496\n",
      "Iteration 75, loss = 0.13401935\n",
      "Iteration 76, loss = 0.13274478\n",
      "Iteration 77, loss = 0.13099020\n",
      "Iteration 78, loss = 0.12939618\n",
      "Iteration 79, loss = 0.12829696\n",
      "Iteration 80, loss = 0.12665433\n",
      "Iteration 81, loss = 0.12550262\n",
      "Iteration 82, loss = 0.12367891\n",
      "Iteration 83, loss = 0.12206708\n",
      "Iteration 84, loss = 0.12129527\n",
      "Iteration 85, loss = 0.12057686\n",
      "Iteration 86, loss = 0.11856608\n",
      "Iteration 87, loss = 0.11746320\n",
      "Iteration 88, loss = 0.11613895\n",
      "Iteration 89, loss = 0.11531520\n",
      "Iteration 90, loss = 0.11319549\n",
      "Iteration 91, loss = 0.11272456\n",
      "Iteration 92, loss = 0.11114850\n",
      "Iteration 93, loss = 0.10978071\n",
      "Iteration 94, loss = 0.10865479\n",
      "Iteration 95, loss = 0.10772843\n",
      "Iteration 96, loss = 0.10638197\n",
      "Iteration 97, loss = 0.10495265\n",
      "Iteration 98, loss = 0.10412959\n",
      "Iteration 99, loss = 0.10300741\n",
      "Iteration 100, loss = 0.10211683\n",
      "Iteration 101, loss = 0.10038177\n",
      "Iteration 102, loss = 0.09954865\n",
      "Iteration 103, loss = 0.09832974\n",
      "Iteration 104, loss = 0.09731652\n",
      "Iteration 105, loss = 0.09615714\n",
      "Iteration 106, loss = 0.09576102\n",
      "Iteration 107, loss = 0.09387379\n",
      "Iteration 108, loss = 0.09307314\n",
      "Iteration 109, loss = 0.09236198\n",
      "Iteration 110, loss = 0.09147707\n",
      "Iteration 111, loss = 0.09028639\n",
      "Iteration 112, loss = 0.08950976\n",
      "Iteration 113, loss = 0.08830962\n",
      "Iteration 114, loss = 0.08733062\n",
      "Iteration 115, loss = 0.08677685\n",
      "Iteration 116, loss = 0.08543550\n",
      "Iteration 117, loss = 0.08431926\n",
      "Iteration 118, loss = 0.08297758\n",
      "Iteration 119, loss = 0.08233101\n",
      "Iteration 120, loss = 0.08151042\n",
      "Iteration 121, loss = 0.08057152\n",
      "Iteration 122, loss = 0.07990832\n",
      "Iteration 123, loss = 0.07848802\n",
      "Iteration 124, loss = 0.07820607\n",
      "Iteration 125, loss = 0.07675662\n",
      "Iteration 126, loss = 0.07609294\n",
      "Iteration 127, loss = 0.07550685\n",
      "Iteration 128, loss = 0.07415761\n",
      "Iteration 129, loss = 0.07357527\n",
      "Iteration 130, loss = 0.07229489\n",
      "Iteration 131, loss = 0.07208653\n",
      "Iteration 132, loss = 0.07069337\n",
      "Iteration 133, loss = 0.06970239\n",
      "Iteration 134, loss = 0.06920122\n",
      "Iteration 135, loss = 0.06866834\n",
      "Iteration 136, loss = 0.06764128\n",
      "Iteration 137, loss = 0.06679072\n",
      "Iteration 138, loss = 0.06576883\n",
      "Iteration 139, loss = 0.06482315\n",
      "Iteration 140, loss = 0.06421141\n",
      "Iteration 141, loss = 0.06346737\n",
      "Iteration 142, loss = 0.06281161\n",
      "Iteration 143, loss = 0.06171079\n",
      "Iteration 144, loss = 0.06155415\n",
      "Iteration 145, loss = 0.06082708\n",
      "Iteration 146, loss = 0.06001571\n",
      "Iteration 147, loss = 0.05900805\n",
      "Iteration 148, loss = 0.05810438\n",
      "Iteration 149, loss = 0.05746354\n",
      "Iteration 150, loss = 0.05656174\n",
      "Iteration 151, loss = 0.05628745\n",
      "Iteration 152, loss = 0.05576134\n",
      "Iteration 153, loss = 0.05464707\n",
      "Iteration 154, loss = 0.05418229\n",
      "Iteration 155, loss = 0.05382149\n",
      "Iteration 156, loss = 0.05289045\n",
      "Iteration 157, loss = 0.05237724\n",
      "Iteration 158, loss = 0.05161456\n",
      "Iteration 159, loss = 0.05108790\n",
      "Iteration 160, loss = 0.05025601\n",
      "Iteration 161, loss = 0.04975852\n",
      "Iteration 162, loss = 0.04908677\n",
      "Iteration 163, loss = 0.04837554\n",
      "Iteration 164, loss = 0.04783374\n",
      "Iteration 165, loss = 0.04780430\n",
      "Iteration 166, loss = 0.04638641\n",
      "Iteration 167, loss = 0.04606528\n",
      "Iteration 168, loss = 0.04599869\n",
      "Iteration 169, loss = 0.04495113\n",
      "Iteration 170, loss = 0.04438356\n",
      "Iteration 171, loss = 0.04406636\n",
      "Iteration 172, loss = 0.04307215\n",
      "Iteration 173, loss = 0.04254552\n",
      "Iteration 174, loss = 0.04204227\n",
      "Iteration 175, loss = 0.04166907\n",
      "Iteration 176, loss = 0.04106620\n",
      "Iteration 177, loss = 0.04037005\n",
      "Iteration 178, loss = 0.03999752\n",
      "Iteration 179, loss = 0.03947018\n",
      "Iteration 180, loss = 0.03897059\n",
      "Iteration 181, loss = 0.03802481\n",
      "Iteration 182, loss = 0.03813677\n",
      "Iteration 183, loss = 0.03754505\n",
      "Iteration 184, loss = 0.03705655\n",
      "Iteration 185, loss = 0.03639550\n",
      "Iteration 186, loss = 0.03559760\n",
      "Iteration 187, loss = 0.03529285\n",
      "Iteration 188, loss = 0.03495264\n",
      "Iteration 189, loss = 0.03424573\n",
      "Iteration 190, loss = 0.03393636\n",
      "Iteration 191, loss = 0.03380580\n",
      "Iteration 192, loss = 0.03315037\n",
      "Iteration 193, loss = 0.03263234\n",
      "Iteration 194, loss = 0.03202860\n",
      "Iteration 195, loss = 0.03170703\n",
      "Iteration 196, loss = 0.03127537\n",
      "Iteration 197, loss = 0.03147984\n",
      "Iteration 198, loss = 0.03060676\n",
      "Iteration 199, loss = 0.03010475\n",
      "Iteration 200, loss = 0.02987628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72213913\n",
      "Iteration 2, loss = 0.51612710\n",
      "Iteration 3, loss = 0.41248234\n",
      "Iteration 4, loss = 0.34830762\n",
      "Iteration 5, loss = 0.30702170\n",
      "Iteration 6, loss = 0.28141908\n",
      "Iteration 7, loss = 0.26415908\n",
      "Iteration 8, loss = 0.25347827\n",
      "Iteration 9, loss = 0.24572356\n",
      "Iteration 10, loss = 0.23963731\n",
      "Iteration 11, loss = 0.23470298\n",
      "Iteration 12, loss = 0.23235974\n",
      "Iteration 13, loss = 0.22923163\n",
      "Iteration 14, loss = 0.22645817\n",
      "Iteration 15, loss = 0.22460456\n",
      "Iteration 16, loss = 0.22295542\n",
      "Iteration 17, loss = 0.22127401\n",
      "Iteration 18, loss = 0.22000822\n",
      "Iteration 19, loss = 0.21866820\n",
      "Iteration 20, loss = 0.21746826\n",
      "Iteration 21, loss = 0.21609062\n",
      "Iteration 22, loss = 0.21446745\n",
      "Iteration 23, loss = 0.21355174\n",
      "Iteration 24, loss = 0.21271832\n",
      "Iteration 25, loss = 0.21119885\n",
      "Iteration 26, loss = 0.20953585\n",
      "Iteration 27, loss = 0.20832818\n",
      "Iteration 28, loss = 0.20728462\n",
      "Iteration 29, loss = 0.20590208\n",
      "Iteration 30, loss = 0.20483095\n",
      "Iteration 31, loss = 0.20268439\n",
      "Iteration 32, loss = 0.20130260\n",
      "Iteration 33, loss = 0.19976285\n",
      "Iteration 34, loss = 0.19887515\n",
      "Iteration 35, loss = 0.19629180\n",
      "Iteration 36, loss = 0.19520778\n",
      "Iteration 37, loss = 0.19359029\n",
      "Iteration 38, loss = 0.19194513\n",
      "Iteration 39, loss = 0.18995565\n",
      "Iteration 40, loss = 0.18817137\n",
      "Iteration 41, loss = 0.18666538\n",
      "Iteration 42, loss = 0.18516316\n",
      "Iteration 43, loss = 0.18238718\n",
      "Iteration 44, loss = 0.18093498\n",
      "Iteration 45, loss = 0.17906657\n",
      "Iteration 46, loss = 0.17703588\n",
      "Iteration 47, loss = 0.17533956\n",
      "Iteration 48, loss = 0.17371175\n",
      "Iteration 49, loss = 0.17133123\n",
      "Iteration 50, loss = 0.16976372\n",
      "Iteration 51, loss = 0.16801880\n",
      "Iteration 52, loss = 0.16565924\n",
      "Iteration 53, loss = 0.16394728\n",
      "Iteration 54, loss = 0.16218271\n",
      "Iteration 55, loss = 0.16000589\n",
      "Iteration 56, loss = 0.15877624\n",
      "Iteration 57, loss = 0.15617878\n",
      "Iteration 58, loss = 0.15492841\n",
      "Iteration 59, loss = 0.15300020\n",
      "Iteration 60, loss = 0.15103328\n",
      "Iteration 61, loss = 0.14912467\n",
      "Iteration 62, loss = 0.14729823\n",
      "Iteration 63, loss = 0.14644311\n",
      "Iteration 64, loss = 0.14434291\n",
      "Iteration 65, loss = 0.14245786\n",
      "Iteration 66, loss = 0.14122011\n",
      "Iteration 67, loss = 0.13936506\n",
      "Iteration 68, loss = 0.13810569\n",
      "Iteration 69, loss = 0.13593252\n",
      "Iteration 70, loss = 0.13410559\n",
      "Iteration 71, loss = 0.13331488\n",
      "Iteration 72, loss = 0.13218173\n",
      "Iteration 73, loss = 0.12954493\n",
      "Iteration 74, loss = 0.12782023\n",
      "Iteration 75, loss = 0.12706351\n",
      "Iteration 76, loss = 0.12535369\n",
      "Iteration 77, loss = 0.12385415\n",
      "Iteration 78, loss = 0.12256617\n",
      "Iteration 79, loss = 0.12098861\n",
      "Iteration 80, loss = 0.11955616\n",
      "Iteration 81, loss = 0.11822168\n",
      "Iteration 82, loss = 0.11694913\n",
      "Iteration 83, loss = 0.11478682\n",
      "Iteration 84, loss = 0.11390422\n",
      "Iteration 85, loss = 0.11329767\n",
      "Iteration 86, loss = 0.11108871\n",
      "Iteration 87, loss = 0.11008296\n",
      "Iteration 88, loss = 0.10881807\n",
      "Iteration 89, loss = 0.10761479\n",
      "Iteration 90, loss = 0.10586014\n",
      "Iteration 91, loss = 0.10494491\n",
      "Iteration 92, loss = 0.10346102\n",
      "Iteration 93, loss = 0.10222773\n",
      "Iteration 94, loss = 0.10099940\n",
      "Iteration 95, loss = 0.10044302\n",
      "Iteration 96, loss = 0.09869675\n",
      "Iteration 97, loss = 0.09746403\n",
      "Iteration 98, loss = 0.09636838\n",
      "Iteration 99, loss = 0.09542013\n",
      "Iteration 100, loss = 0.09449004\n",
      "Iteration 101, loss = 0.09302604\n",
      "Iteration 102, loss = 0.09229087\n",
      "Iteration 103, loss = 0.09130767\n",
      "Iteration 104, loss = 0.08992282\n",
      "Iteration 105, loss = 0.08856284\n",
      "Iteration 106, loss = 0.08816669\n",
      "Iteration 107, loss = 0.08658959\n",
      "Iteration 108, loss = 0.08552012\n",
      "Iteration 109, loss = 0.08460789\n",
      "Iteration 110, loss = 0.08372888\n",
      "Iteration 111, loss = 0.08244440\n",
      "Iteration 112, loss = 0.08157245\n",
      "Iteration 113, loss = 0.08078217\n",
      "Iteration 114, loss = 0.07984079\n",
      "Iteration 115, loss = 0.07897696\n",
      "Iteration 116, loss = 0.07745843\n",
      "Iteration 117, loss = 0.07650858\n",
      "Iteration 118, loss = 0.07556396\n",
      "Iteration 119, loss = 0.07515814\n",
      "Iteration 120, loss = 0.07415787\n",
      "Iteration 121, loss = 0.07317254\n",
      "Iteration 122, loss = 0.07227236\n",
      "Iteration 123, loss = 0.07149281\n",
      "Iteration 124, loss = 0.07062890\n",
      "Iteration 125, loss = 0.06937959\n",
      "Iteration 126, loss = 0.06867822\n",
      "Iteration 127, loss = 0.06779255\n",
      "Iteration 128, loss = 0.06711137\n",
      "Iteration 129, loss = 0.06630134\n",
      "Iteration 130, loss = 0.06511198\n",
      "Iteration 131, loss = 0.06425878\n",
      "Iteration 132, loss = 0.06363170\n",
      "Iteration 133, loss = 0.06271893\n",
      "Iteration 134, loss = 0.06199551\n",
      "Iteration 135, loss = 0.06152527\n",
      "Iteration 136, loss = 0.06035304\n",
      "Iteration 137, loss = 0.05953519\n",
      "Iteration 138, loss = 0.05885354\n",
      "Iteration 139, loss = 0.05809713\n",
      "Iteration 140, loss = 0.05750561\n",
      "Iteration 141, loss = 0.05660256\n",
      "Iteration 142, loss = 0.05627257\n",
      "Iteration 143, loss = 0.05496532\n",
      "Iteration 144, loss = 0.05510722\n",
      "Iteration 145, loss = 0.05372375\n",
      "Iteration 146, loss = 0.05310367\n",
      "Iteration 147, loss = 0.05235836\n",
      "Iteration 148, loss = 0.05153071\n",
      "Iteration 149, loss = 0.05070274\n",
      "Iteration 150, loss = 0.05005522\n",
      "Iteration 151, loss = 0.04990898\n",
      "Iteration 152, loss = 0.04938472\n",
      "Iteration 153, loss = 0.04811771\n",
      "Iteration 154, loss = 0.04742056\n",
      "Iteration 155, loss = 0.04694260\n",
      "Iteration 156, loss = 0.04660291\n",
      "Iteration 157, loss = 0.04576323\n",
      "Iteration 158, loss = 0.04517577\n",
      "Iteration 159, loss = 0.04449766\n",
      "Iteration 160, loss = 0.04420307\n",
      "Iteration 161, loss = 0.04342671\n",
      "Iteration 162, loss = 0.04262262\n",
      "Iteration 163, loss = 0.04225462\n",
      "Iteration 164, loss = 0.04151766\n",
      "Iteration 165, loss = 0.04097884\n",
      "Iteration 166, loss = 0.04014270\n",
      "Iteration 167, loss = 0.03965822\n",
      "Iteration 168, loss = 0.03924081\n",
      "Iteration 169, loss = 0.03873826\n",
      "Iteration 170, loss = 0.03822179\n",
      "Iteration 171, loss = 0.03804608\n",
      "Iteration 172, loss = 0.03697203\n",
      "Iteration 173, loss = 0.03660574\n",
      "Iteration 174, loss = 0.03607877\n",
      "Iteration 175, loss = 0.03562325\n",
      "Iteration 176, loss = 0.03503873\n",
      "Iteration 177, loss = 0.03444446\n",
      "Iteration 178, loss = 0.03409670\n",
      "Iteration 179, loss = 0.03358219\n",
      "Iteration 180, loss = 0.03305444\n",
      "Iteration 181, loss = 0.03244499\n",
      "Iteration 182, loss = 0.03197313\n",
      "Iteration 183, loss = 0.03180757\n",
      "Iteration 184, loss = 0.03104661\n",
      "Iteration 185, loss = 0.03084601\n",
      "Iteration 186, loss = 0.03008960\n",
      "Iteration 187, loss = 0.02987944\n",
      "Iteration 188, loss = 0.02960711\n",
      "Iteration 189, loss = 0.02885198\n",
      "Iteration 190, loss = 0.02853812\n",
      "Iteration 191, loss = 0.02818131\n",
      "Iteration 192, loss = 0.02777855\n",
      "Iteration 193, loss = 0.02738200\n",
      "Iteration 194, loss = 0.02676240\n",
      "Iteration 195, loss = 0.02650069\n",
      "Iteration 196, loss = 0.02602971\n",
      "Iteration 197, loss = 0.02594425\n",
      "Iteration 198, loss = 0.02535219\n",
      "Iteration 199, loss = 0.02505616\n",
      "Iteration 200, loss = 0.02469175\n",
      "Iteration 1, loss = 0.72472582\n",
      "Iteration 2, loss = 0.51719898\n",
      "Iteration 3, loss = 0.41386519\n",
      "Iteration 4, loss = 0.35005371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.30974994\n",
      "Iteration 6, loss = 0.28404133\n",
      "Iteration 7, loss = 0.26787153\n",
      "Iteration 8, loss = 0.25716951\n",
      "Iteration 9, loss = 0.24883406\n",
      "Iteration 10, loss = 0.24327859\n",
      "Iteration 11, loss = 0.23901340\n",
      "Iteration 12, loss = 0.23653891\n",
      "Iteration 13, loss = 0.23283420\n",
      "Iteration 14, loss = 0.23031185\n",
      "Iteration 15, loss = 0.22901070\n",
      "Iteration 16, loss = 0.22701373\n",
      "Iteration 17, loss = 0.22544378\n",
      "Iteration 18, loss = 0.22442087\n",
      "Iteration 19, loss = 0.22294685\n",
      "Iteration 20, loss = 0.22120517\n",
      "Iteration 21, loss = 0.21974312\n",
      "Iteration 22, loss = 0.21863178\n",
      "Iteration 23, loss = 0.21749344\n",
      "Iteration 24, loss = 0.21648464\n",
      "Iteration 25, loss = 0.21498108\n",
      "Iteration 26, loss = 0.21328311\n",
      "Iteration 27, loss = 0.21206451\n",
      "Iteration 28, loss = 0.21072967\n",
      "Iteration 29, loss = 0.20965126\n",
      "Iteration 30, loss = 0.20830673\n",
      "Iteration 31, loss = 0.20635013\n",
      "Iteration 32, loss = 0.20464409\n",
      "Iteration 33, loss = 0.20309288\n",
      "Iteration 34, loss = 0.20144368\n",
      "Iteration 35, loss = 0.19990264\n",
      "Iteration 36, loss = 0.19796668\n",
      "Iteration 37, loss = 0.19666329\n",
      "Iteration 38, loss = 0.19503634\n",
      "Iteration 39, loss = 0.19278952\n",
      "Iteration 40, loss = 0.19127466\n",
      "Iteration 41, loss = 0.18956255\n",
      "Iteration 42, loss = 0.18798799\n",
      "Iteration 43, loss = 0.18556702\n",
      "Iteration 44, loss = 0.18365394\n",
      "Iteration 45, loss = 0.18222529\n",
      "Iteration 46, loss = 0.18032926\n",
      "Iteration 47, loss = 0.17797168\n",
      "Iteration 48, loss = 0.17621694\n",
      "Iteration 49, loss = 0.17432117\n",
      "Iteration 50, loss = 0.17204331\n",
      "Iteration 51, loss = 0.17063089\n",
      "Iteration 52, loss = 0.16831845\n",
      "Iteration 53, loss = 0.16638090\n",
      "Iteration 54, loss = 0.16453212\n",
      "Iteration 55, loss = 0.16234241\n",
      "Iteration 56, loss = 0.16092075\n",
      "Iteration 57, loss = 0.15840103\n",
      "Iteration 58, loss = 0.15678199\n",
      "Iteration 59, loss = 0.15526817\n",
      "Iteration 60, loss = 0.15318683\n",
      "Iteration 61, loss = 0.15127685\n",
      "Iteration 62, loss = 0.14938787\n",
      "Iteration 63, loss = 0.14812750\n",
      "Iteration 64, loss = 0.14588388\n",
      "Iteration 65, loss = 0.14365644\n",
      "Iteration 66, loss = 0.14271048\n",
      "Iteration 67, loss = 0.14070579\n",
      "Iteration 68, loss = 0.13926772\n",
      "Iteration 69, loss = 0.13716671\n",
      "Iteration 70, loss = 0.13562041\n",
      "Iteration 71, loss = 0.13439276\n",
      "Iteration 72, loss = 0.13220567\n",
      "Iteration 73, loss = 0.13013628\n",
      "Iteration 74, loss = 0.12846378\n",
      "Iteration 75, loss = 0.12787373\n",
      "Iteration 76, loss = 0.12630382\n",
      "Iteration 77, loss = 0.12408131\n",
      "Iteration 78, loss = 0.12332694\n",
      "Iteration 79, loss = 0.12109516\n",
      "Iteration 80, loss = 0.11973904\n",
      "Iteration 81, loss = 0.11774075\n",
      "Iteration 82, loss = 0.11722001\n",
      "Iteration 83, loss = 0.11521478\n",
      "Iteration 84, loss = 0.11342915\n",
      "Iteration 85, loss = 0.11283877\n",
      "Iteration 86, loss = 0.11130390\n",
      "Iteration 87, loss = 0.10943886\n",
      "Iteration 88, loss = 0.10852101\n",
      "Iteration 89, loss = 0.10691614\n",
      "Iteration 90, loss = 0.10578075\n",
      "Iteration 91, loss = 0.10433764\n",
      "Iteration 92, loss = 0.10270608\n",
      "Iteration 93, loss = 0.10185664\n",
      "Iteration 94, loss = 0.10047802\n",
      "Iteration 95, loss = 0.09962424\n",
      "Iteration 96, loss = 0.09776536\n",
      "Iteration 97, loss = 0.09650408\n",
      "Iteration 98, loss = 0.09563899\n",
      "Iteration 99, loss = 0.09468422\n",
      "Iteration 100, loss = 0.09361371\n",
      "Iteration 101, loss = 0.09224624\n",
      "Iteration 102, loss = 0.09105254\n",
      "Iteration 103, loss = 0.09023056\n",
      "Iteration 104, loss = 0.08883401\n",
      "Iteration 105, loss = 0.08783007\n",
      "Iteration 106, loss = 0.08659052\n",
      "Iteration 107, loss = 0.08507736\n",
      "Iteration 108, loss = 0.08406673\n",
      "Iteration 109, loss = 0.08371473\n",
      "Iteration 110, loss = 0.08249352\n",
      "Iteration 111, loss = 0.08102861\n",
      "Iteration 112, loss = 0.08023535\n",
      "Iteration 113, loss = 0.07984085\n",
      "Iteration 114, loss = 0.07823274\n",
      "Iteration 115, loss = 0.07719299\n",
      "Iteration 116, loss = 0.07637199\n",
      "Iteration 117, loss = 0.07500054\n",
      "Iteration 118, loss = 0.07402570\n",
      "Iteration 119, loss = 0.07346454\n",
      "Iteration 120, loss = 0.07247876\n",
      "Iteration 121, loss = 0.07140635\n",
      "Iteration 122, loss = 0.07059673\n",
      "Iteration 123, loss = 0.06983860\n",
      "Iteration 124, loss = 0.06937934\n",
      "Iteration 125, loss = 0.06744731\n",
      "Iteration 126, loss = 0.06688251\n",
      "Iteration 127, loss = 0.06587269\n",
      "Iteration 128, loss = 0.06532561\n",
      "Iteration 129, loss = 0.06467371\n",
      "Iteration 130, loss = 0.06360859\n",
      "Iteration 131, loss = 0.06257094\n",
      "Iteration 132, loss = 0.06212131\n",
      "Iteration 133, loss = 0.06071056\n",
      "Iteration 134, loss = 0.06049942\n",
      "Iteration 135, loss = 0.05951440\n",
      "Iteration 136, loss = 0.05853709\n",
      "Iteration 137, loss = 0.05798322\n",
      "Iteration 138, loss = 0.05735191\n",
      "Iteration 139, loss = 0.05608779\n",
      "Iteration 140, loss = 0.05549285\n",
      "Iteration 141, loss = 0.05494618\n",
      "Iteration 142, loss = 0.05397057\n",
      "Iteration 143, loss = 0.05317433\n",
      "Iteration 144, loss = 0.05300093\n",
      "Iteration 145, loss = 0.05179924\n",
      "Iteration 146, loss = 0.05147605\n",
      "Iteration 147, loss = 0.05039962\n",
      "Iteration 148, loss = 0.04982670\n",
      "Iteration 149, loss = 0.04894895\n",
      "Iteration 150, loss = 0.04844980\n",
      "Iteration 151, loss = 0.04794983\n",
      "Iteration 152, loss = 0.04777668\n",
      "Iteration 153, loss = 0.04646037\n",
      "Iteration 154, loss = 0.04574953\n",
      "Iteration 155, loss = 0.04529281\n",
      "Iteration 156, loss = 0.04457666\n",
      "Iteration 157, loss = 0.04382851\n",
      "Iteration 158, loss = 0.04330744\n",
      "Iteration 159, loss = 0.04265979\n",
      "Iteration 160, loss = 0.04233842\n",
      "Iteration 161, loss = 0.04163910\n",
      "Iteration 162, loss = 0.04092038\n",
      "Iteration 163, loss = 0.04046158\n",
      "Iteration 164, loss = 0.03979725\n",
      "Iteration 165, loss = 0.03942476\n",
      "Iteration 166, loss = 0.03879825\n",
      "Iteration 167, loss = 0.03805701\n",
      "Iteration 168, loss = 0.03789977\n",
      "Iteration 169, loss = 0.03704744\n",
      "Iteration 170, loss = 0.03641811\n",
      "Iteration 171, loss = 0.03633504\n",
      "Iteration 172, loss = 0.03555517\n",
      "Iteration 173, loss = 0.03500316\n",
      "Iteration 174, loss = 0.03461498\n",
      "Iteration 175, loss = 0.03437599\n",
      "Iteration 176, loss = 0.03363748\n",
      "Iteration 177, loss = 0.03361048\n",
      "Iteration 178, loss = 0.03296227\n",
      "Iteration 179, loss = 0.03234411\n",
      "Iteration 180, loss = 0.03179581\n",
      "Iteration 181, loss = 0.03114234\n",
      "Iteration 182, loss = 0.03081868\n",
      "Iteration 183, loss = 0.03026749\n",
      "Iteration 184, loss = 0.03010627\n",
      "Iteration 185, loss = 0.02935921\n",
      "Iteration 186, loss = 0.02888290\n",
      "Iteration 187, loss = 0.02887736\n",
      "Iteration 188, loss = 0.02828344\n",
      "Iteration 189, loss = 0.02767970\n",
      "Iteration 190, loss = 0.02743772\n",
      "Iteration 191, loss = 0.02729730\n",
      "Iteration 192, loss = 0.02678868\n",
      "Iteration 193, loss = 0.02657804\n",
      "Iteration 194, loss = 0.02584012\n",
      "Iteration 195, loss = 0.02548537\n",
      "Iteration 196, loss = 0.02515282\n",
      "Iteration 197, loss = 0.02493745\n",
      "Iteration 198, loss = 0.02452793\n",
      "Iteration 199, loss = 0.02414946\n",
      "Iteration 200, loss = 0.02392390\n",
      "Iteration 1, loss = 0.71906259\n",
      "Iteration 2, loss = 0.51368735\n",
      "Iteration 3, loss = 0.41234431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.34804736\n",
      "Iteration 5, loss = 0.30779447\n",
      "Iteration 6, loss = 0.28158248\n",
      "Iteration 7, loss = 0.26536926\n",
      "Iteration 8, loss = 0.25421010\n",
      "Iteration 9, loss = 0.24616072\n",
      "Iteration 10, loss = 0.24006016\n",
      "Iteration 11, loss = 0.23644336\n",
      "Iteration 12, loss = 0.23322218\n",
      "Iteration 13, loss = 0.22952956\n",
      "Iteration 14, loss = 0.22746940\n",
      "Iteration 15, loss = 0.22535161\n",
      "Iteration 16, loss = 0.22340274\n",
      "Iteration 17, loss = 0.22190997\n",
      "Iteration 18, loss = 0.22095135\n",
      "Iteration 19, loss = 0.21929920\n",
      "Iteration 20, loss = 0.21764276\n",
      "Iteration 21, loss = 0.21623462\n",
      "Iteration 22, loss = 0.21490759\n",
      "Iteration 23, loss = 0.21378628\n",
      "Iteration 24, loss = 0.21237582\n",
      "Iteration 25, loss = 0.21087248\n",
      "Iteration 26, loss = 0.20935747\n",
      "Iteration 27, loss = 0.20835595\n",
      "Iteration 28, loss = 0.20720063\n",
      "Iteration 29, loss = 0.20566211\n",
      "Iteration 30, loss = 0.20363238\n",
      "Iteration 31, loss = 0.20218908\n",
      "Iteration 32, loss = 0.20005740\n",
      "Iteration 33, loss = 0.19875094\n",
      "Iteration 34, loss = 0.19714911\n",
      "Iteration 35, loss = 0.19525588\n",
      "Iteration 36, loss = 0.19380132\n",
      "Iteration 37, loss = 0.19200313\n",
      "Iteration 38, loss = 0.18980805\n",
      "Iteration 39, loss = 0.18812811\n",
      "Iteration 40, loss = 0.18669034\n",
      "Iteration 41, loss = 0.18479968\n",
      "Iteration 42, loss = 0.18302402\n",
      "Iteration 43, loss = 0.18067716\n",
      "Iteration 44, loss = 0.17895935\n",
      "Iteration 45, loss = 0.17714846\n",
      "Iteration 46, loss = 0.17502668\n",
      "Iteration 47, loss = 0.17326122\n",
      "Iteration 48, loss = 0.17152347\n",
      "Iteration 49, loss = 0.16937920\n",
      "Iteration 50, loss = 0.16832628\n",
      "Iteration 51, loss = 0.16606759\n",
      "Iteration 52, loss = 0.16439327\n",
      "Iteration 53, loss = 0.16198951\n",
      "Iteration 54, loss = 0.16004255\n",
      "Iteration 55, loss = 0.15840086\n",
      "Iteration 56, loss = 0.15689881\n",
      "Iteration 57, loss = 0.15480649\n",
      "Iteration 58, loss = 0.15305921\n",
      "Iteration 59, loss = 0.15125211\n",
      "Iteration 60, loss = 0.14929503\n",
      "Iteration 61, loss = 0.14745237\n",
      "Iteration 62, loss = 0.14586676\n",
      "Iteration 63, loss = 0.14436907\n",
      "Iteration 64, loss = 0.14267561\n",
      "Iteration 65, loss = 0.14061032\n",
      "Iteration 66, loss = 0.13957997\n",
      "Iteration 67, loss = 0.13766076\n",
      "Iteration 68, loss = 0.13616348\n",
      "Iteration 69, loss = 0.13442011\n",
      "Iteration 70, loss = 0.13320431\n",
      "Iteration 71, loss = 0.13145696\n",
      "Iteration 72, loss = 0.12972040\n",
      "Iteration 73, loss = 0.12810821\n",
      "Iteration 74, loss = 0.12625726\n",
      "Iteration 75, loss = 0.12541263\n",
      "Iteration 76, loss = 0.12355377\n",
      "Iteration 77, loss = 0.12179282\n",
      "Iteration 78, loss = 0.12103845\n",
      "Iteration 79, loss = 0.11947972\n",
      "Iteration 80, loss = 0.11815735\n",
      "Iteration 81, loss = 0.11608116\n",
      "Iteration 82, loss = 0.11547406\n",
      "Iteration 83, loss = 0.11358259\n",
      "Iteration 84, loss = 0.11226933\n",
      "Iteration 85, loss = 0.11091076\n",
      "Iteration 86, loss = 0.11029861\n",
      "Iteration 87, loss = 0.10865298\n",
      "Iteration 88, loss = 0.10761705\n",
      "Iteration 89, loss = 0.10613538\n",
      "Iteration 90, loss = 0.10454499\n",
      "Iteration 91, loss = 0.10326396\n",
      "Iteration 92, loss = 0.10225328\n",
      "Iteration 93, loss = 0.10085197\n",
      "Iteration 94, loss = 0.09977405\n",
      "Iteration 95, loss = 0.09836516\n",
      "Iteration 96, loss = 0.09753948\n",
      "Iteration 97, loss = 0.09620374\n",
      "Iteration 98, loss = 0.09532725\n",
      "Iteration 99, loss = 0.09428135\n",
      "Iteration 100, loss = 0.09346186\n",
      "Iteration 101, loss = 0.09216857\n",
      "Iteration 102, loss = 0.09079958\n",
      "Iteration 103, loss = 0.09015239\n",
      "Iteration 104, loss = 0.08891079\n",
      "Iteration 105, loss = 0.08781633\n",
      "Iteration 106, loss = 0.08646072\n",
      "Iteration 107, loss = 0.08621953\n",
      "Iteration 108, loss = 0.08519572\n",
      "Iteration 109, loss = 0.08418524\n",
      "Iteration 110, loss = 0.08278867\n",
      "Iteration 111, loss = 0.08166869\n",
      "Iteration 112, loss = 0.08063520\n",
      "Iteration 113, loss = 0.07963592\n",
      "Iteration 114, loss = 0.07893392\n",
      "Iteration 115, loss = 0.07754763\n",
      "Iteration 116, loss = 0.07754965\n",
      "Iteration 117, loss = 0.07590807\n",
      "Iteration 118, loss = 0.07507419\n",
      "Iteration 119, loss = 0.07425975\n",
      "Iteration 120, loss = 0.07318646\n",
      "Iteration 121, loss = 0.07268669\n",
      "Iteration 122, loss = 0.07172198\n",
      "Iteration 123, loss = 0.07080654\n",
      "Iteration 124, loss = 0.07033284\n",
      "Iteration 125, loss = 0.06864506\n",
      "Iteration 126, loss = 0.06835023\n",
      "Iteration 127, loss = 0.06758264\n",
      "Iteration 128, loss = 0.06625657\n",
      "Iteration 129, loss = 0.06595267\n",
      "Iteration 130, loss = 0.06476829\n",
      "Iteration 131, loss = 0.06396170\n",
      "Iteration 132, loss = 0.06274056\n",
      "Iteration 133, loss = 0.06222143\n",
      "Iteration 134, loss = 0.06163277\n",
      "Iteration 135, loss = 0.06084159\n",
      "Iteration 136, loss = 0.05995288\n",
      "Iteration 137, loss = 0.05966957\n",
      "Iteration 138, loss = 0.05906019\n",
      "Iteration 139, loss = 0.05762599\n",
      "Iteration 140, loss = 0.05732879\n",
      "Iteration 141, loss = 0.05653605\n",
      "Iteration 142, loss = 0.05579849\n",
      "Iteration 143, loss = 0.05494393\n",
      "Iteration 144, loss = 0.05466299\n",
      "Iteration 145, loss = 0.05342571\n",
      "Iteration 146, loss = 0.05282246\n",
      "Iteration 147, loss = 0.05209380\n",
      "Iteration 148, loss = 0.05182312\n",
      "Iteration 149, loss = 0.05093713\n",
      "Iteration 150, loss = 0.05020667\n",
      "Iteration 151, loss = 0.04958575\n",
      "Iteration 152, loss = 0.04921142\n",
      "Iteration 153, loss = 0.04853139\n",
      "Iteration 154, loss = 0.04763789\n",
      "Iteration 155, loss = 0.04678728\n",
      "Iteration 156, loss = 0.04618624\n",
      "Iteration 157, loss = 0.04553161\n",
      "Iteration 158, loss = 0.04495135\n",
      "Iteration 159, loss = 0.04461628\n",
      "Iteration 160, loss = 0.04377445\n",
      "Iteration 161, loss = 0.04340482\n",
      "Iteration 162, loss = 0.04239007\n",
      "Iteration 163, loss = 0.04216893\n",
      "Iteration 164, loss = 0.04152914\n",
      "Iteration 165, loss = 0.04133426\n",
      "Iteration 166, loss = 0.04046734\n",
      "Iteration 167, loss = 0.04076317\n",
      "Iteration 168, loss = 0.03942813\n",
      "Iteration 169, loss = 0.03916327\n",
      "Iteration 170, loss = 0.03821976\n",
      "Iteration 171, loss = 0.03792154\n",
      "Iteration 172, loss = 0.03711562\n",
      "Iteration 173, loss = 0.03673763\n",
      "Iteration 174, loss = 0.03599043\n",
      "Iteration 175, loss = 0.03600715\n",
      "Iteration 176, loss = 0.03522619\n",
      "Iteration 177, loss = 0.03498039\n",
      "Iteration 178, loss = 0.03420690\n",
      "Iteration 179, loss = 0.03377460\n",
      "Iteration 180, loss = 0.03318701\n",
      "Iteration 181, loss = 0.03271673\n",
      "Iteration 182, loss = 0.03208577\n",
      "Iteration 183, loss = 0.03161615\n",
      "Iteration 184, loss = 0.03152195\n",
      "Iteration 185, loss = 0.03097846\n",
      "Iteration 186, loss = 0.03030444\n",
      "Iteration 187, loss = 0.03017017\n",
      "Iteration 188, loss = 0.02957146\n",
      "Iteration 189, loss = 0.02932253\n",
      "Iteration 190, loss = 0.02872904\n",
      "Iteration 191, loss = 0.02837664\n",
      "Iteration 192, loss = 0.02800841\n",
      "Iteration 193, loss = 0.02778940\n",
      "Iteration 194, loss = 0.02710144\n",
      "Iteration 195, loss = 0.02663399\n",
      "Iteration 196, loss = 0.02638720\n",
      "Iteration 197, loss = 0.02620711\n",
      "Iteration 198, loss = 0.02549507\n",
      "Iteration 199, loss = 0.02520184\n",
      "Iteration 200, loss = 0.02494781\n",
      "Iteration 1, loss = 0.72215615\n",
      "Iteration 2, loss = 0.51622045\n",
      "Iteration 3, loss = 0.41247219\n",
      "Iteration 4, loss = 0.34678340\n",
      "Iteration 5, loss = 0.30538762\n",
      "Iteration 6, loss = 0.27874681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.26178222\n",
      "Iteration 8, loss = 0.25066948\n",
      "Iteration 9, loss = 0.24259375\n",
      "Iteration 10, loss = 0.23696005\n",
      "Iteration 11, loss = 0.23274848\n",
      "Iteration 12, loss = 0.22911572\n",
      "Iteration 13, loss = 0.22649935\n",
      "Iteration 14, loss = 0.22385875\n",
      "Iteration 15, loss = 0.22201645\n",
      "Iteration 16, loss = 0.22076930\n",
      "Iteration 17, loss = 0.21920433\n",
      "Iteration 18, loss = 0.21851229\n",
      "Iteration 19, loss = 0.21682594\n",
      "Iteration 20, loss = 0.21533587\n",
      "Iteration 21, loss = 0.21400349\n",
      "Iteration 22, loss = 0.21256754\n",
      "Iteration 23, loss = 0.21194152\n",
      "Iteration 24, loss = 0.21041443\n",
      "Iteration 25, loss = 0.20889891\n",
      "Iteration 26, loss = 0.20741090\n",
      "Iteration 27, loss = 0.20671650\n",
      "Iteration 28, loss = 0.20522810\n",
      "Iteration 29, loss = 0.20445778\n",
      "Iteration 30, loss = 0.20256779\n",
      "Iteration 31, loss = 0.20130436\n",
      "Iteration 32, loss = 0.19924842\n",
      "Iteration 33, loss = 0.19794397\n",
      "Iteration 34, loss = 0.19626995\n",
      "Iteration 35, loss = 0.19482083\n",
      "Iteration 36, loss = 0.19290347\n",
      "Iteration 37, loss = 0.19171370\n",
      "Iteration 38, loss = 0.18946595\n",
      "Iteration 39, loss = 0.18835383\n",
      "Iteration 40, loss = 0.18751400\n",
      "Iteration 41, loss = 0.18519046\n",
      "Iteration 42, loss = 0.18313650\n",
      "Iteration 43, loss = 0.18077436\n",
      "Iteration 44, loss = 0.17890197\n",
      "Iteration 45, loss = 0.17787837\n",
      "Iteration 46, loss = 0.17546783\n",
      "Iteration 47, loss = 0.17379011\n",
      "Iteration 48, loss = 0.17245817\n",
      "Iteration 49, loss = 0.17018853\n",
      "Iteration 50, loss = 0.16846463\n",
      "Iteration 51, loss = 0.16687887\n",
      "Iteration 52, loss = 0.16511041\n",
      "Iteration 53, loss = 0.16264481\n",
      "Iteration 54, loss = 0.16083711\n",
      "Iteration 55, loss = 0.15903545\n",
      "Iteration 56, loss = 0.15717450\n",
      "Iteration 57, loss = 0.15542235\n",
      "Iteration 58, loss = 0.15376747\n",
      "Iteration 59, loss = 0.15214298\n",
      "Iteration 60, loss = 0.15008978\n",
      "Iteration 61, loss = 0.14847634\n",
      "Iteration 62, loss = 0.14703106\n",
      "Iteration 63, loss = 0.14485504\n",
      "Iteration 64, loss = 0.14381692\n",
      "Iteration 65, loss = 0.14191725\n",
      "Iteration 66, loss = 0.13998870\n",
      "Iteration 67, loss = 0.13807507\n",
      "Iteration 68, loss = 0.13683840\n",
      "Iteration 69, loss = 0.13509515\n",
      "Iteration 70, loss = 0.13384532\n",
      "Iteration 71, loss = 0.13188636\n",
      "Iteration 72, loss = 0.13036436\n",
      "Iteration 73, loss = 0.12891184\n",
      "Iteration 74, loss = 0.12688898\n",
      "Iteration 75, loss = 0.12620392\n",
      "Iteration 76, loss = 0.12457571\n",
      "Iteration 77, loss = 0.12306126\n",
      "Iteration 78, loss = 0.12175567\n",
      "Iteration 79, loss = 0.11986907\n",
      "Iteration 80, loss = 0.11853001\n",
      "Iteration 81, loss = 0.11695023\n",
      "Iteration 82, loss = 0.11550693\n",
      "Iteration 83, loss = 0.11398449\n",
      "Iteration 84, loss = 0.11248053\n",
      "Iteration 85, loss = 0.11100957\n",
      "Iteration 86, loss = 0.11039457\n",
      "Iteration 87, loss = 0.10860244\n",
      "Iteration 88, loss = 0.10768864\n",
      "Iteration 89, loss = 0.10624417\n",
      "Iteration 90, loss = 0.10465458\n",
      "Iteration 91, loss = 0.10339407\n",
      "Iteration 92, loss = 0.10219927\n",
      "Iteration 93, loss = 0.10050095\n",
      "Iteration 94, loss = 0.09968396\n",
      "Iteration 95, loss = 0.09813297\n",
      "Iteration 96, loss = 0.09744058\n",
      "Iteration 97, loss = 0.09563593\n",
      "Iteration 98, loss = 0.09444611\n",
      "Iteration 99, loss = 0.09324422\n",
      "Iteration 100, loss = 0.09246507\n",
      "Iteration 101, loss = 0.09117300\n",
      "Iteration 102, loss = 0.08983352\n",
      "Iteration 103, loss = 0.08872435\n",
      "Iteration 104, loss = 0.08753285\n",
      "Iteration 105, loss = 0.08630176\n",
      "Iteration 106, loss = 0.08525531\n",
      "Iteration 107, loss = 0.08498820\n",
      "Iteration 108, loss = 0.08423295\n",
      "Iteration 109, loss = 0.08245374\n",
      "Iteration 110, loss = 0.08086011\n",
      "Iteration 111, loss = 0.08026785\n",
      "Iteration 112, loss = 0.07922973\n",
      "Iteration 113, loss = 0.07778101\n",
      "Iteration 114, loss = 0.07690295\n",
      "Iteration 115, loss = 0.07580017\n",
      "Iteration 116, loss = 0.07536196\n",
      "Iteration 117, loss = 0.07387336\n",
      "Iteration 118, loss = 0.07301715\n",
      "Iteration 119, loss = 0.07218032\n",
      "Iteration 120, loss = 0.07117761\n",
      "Iteration 121, loss = 0.07109949\n",
      "Iteration 122, loss = 0.06933295\n",
      "Iteration 123, loss = 0.06872120\n",
      "Iteration 124, loss = 0.06775744\n",
      "Iteration 125, loss = 0.06641900\n",
      "Iteration 126, loss = 0.06577927\n",
      "Iteration 127, loss = 0.06506380\n",
      "Iteration 128, loss = 0.06374376\n",
      "Iteration 129, loss = 0.06352985\n",
      "Iteration 130, loss = 0.06232688\n",
      "Iteration 131, loss = 0.06147895\n",
      "Iteration 132, loss = 0.06069228\n",
      "Iteration 133, loss = 0.05998697\n",
      "Iteration 134, loss = 0.05925690\n",
      "Iteration 135, loss = 0.05875135\n",
      "Iteration 136, loss = 0.05784493\n",
      "Iteration 137, loss = 0.05689770\n",
      "Iteration 138, loss = 0.05628601\n",
      "Iteration 139, loss = 0.05531048\n",
      "Iteration 140, loss = 0.05468862\n",
      "Iteration 141, loss = 0.05396484\n",
      "Iteration 142, loss = 0.05298573\n",
      "Iteration 143, loss = 0.05233092\n",
      "Iteration 144, loss = 0.05214808\n",
      "Iteration 145, loss = 0.05117740\n",
      "Iteration 146, loss = 0.05051438\n",
      "Iteration 147, loss = 0.04976729\n",
      "Iteration 148, loss = 0.04921005\n",
      "Iteration 149, loss = 0.04891199\n",
      "Iteration 150, loss = 0.04798565\n",
      "Iteration 151, loss = 0.04771009\n",
      "Iteration 152, loss = 0.04671350\n",
      "Iteration 153, loss = 0.04660571\n",
      "Iteration 154, loss = 0.04570693\n",
      "Iteration 155, loss = 0.04466849\n",
      "Iteration 156, loss = 0.04406639\n",
      "Iteration 157, loss = 0.04316522\n",
      "Iteration 158, loss = 0.04287803\n",
      "Iteration 159, loss = 0.04229605\n",
      "Iteration 160, loss = 0.04144416\n",
      "Iteration 161, loss = 0.04104558\n",
      "Iteration 162, loss = 0.04055031\n",
      "Iteration 163, loss = 0.03983310\n",
      "Iteration 164, loss = 0.03959601\n",
      "Iteration 165, loss = 0.03910076\n",
      "Iteration 166, loss = 0.03836050\n",
      "Iteration 167, loss = 0.03851566\n",
      "Iteration 168, loss = 0.03737482\n",
      "Iteration 169, loss = 0.03716057\n",
      "Iteration 170, loss = 0.03642685\n",
      "Iteration 171, loss = 0.03592651\n",
      "Iteration 172, loss = 0.03522896\n",
      "Iteration 173, loss = 0.03529441\n",
      "Iteration 174, loss = 0.03433670\n",
      "Iteration 175, loss = 0.03401344\n",
      "Iteration 176, loss = 0.03367198\n",
      "Iteration 177, loss = 0.03305631\n",
      "Iteration 178, loss = 0.03243622\n",
      "Iteration 179, loss = 0.03197187\n",
      "Iteration 180, loss = 0.03152365\n",
      "Iteration 181, loss = 0.03108253\n",
      "Iteration 182, loss = 0.03059887\n",
      "Iteration 183, loss = 0.03017767\n",
      "Iteration 184, loss = 0.02989526\n",
      "Iteration 185, loss = 0.02956208\n",
      "Iteration 186, loss = 0.02903636\n",
      "Iteration 187, loss = 0.02862687\n",
      "Iteration 188, loss = 0.02817958\n",
      "Iteration 189, loss = 0.02786518\n",
      "Iteration 190, loss = 0.02742394\n",
      "Iteration 191, loss = 0.02693596\n",
      "Iteration 192, loss = 0.02662994\n",
      "Iteration 193, loss = 0.02643315\n",
      "Iteration 194, loss = 0.02593263\n",
      "Iteration 195, loss = 0.02552099\n",
      "Iteration 196, loss = 0.02529741\n",
      "Iteration 197, loss = 0.02493527\n",
      "Iteration 198, loss = 0.02434774\n",
      "Iteration 199, loss = 0.02406572\n",
      "Iteration 200, loss = 0.02394237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84417395\n",
      "Iteration 2, loss = 0.73242823\n",
      "Iteration 3, loss = 0.63636766\n",
      "Iteration 4, loss = 0.56459624\n",
      "Iteration 5, loss = 0.51175604\n",
      "Iteration 6, loss = 0.47145104\n",
      "Iteration 7, loss = 0.44015397\n",
      "Iteration 8, loss = 0.41536734\n",
      "Iteration 9, loss = 0.39533999\n",
      "Iteration 10, loss = 0.37877388\n",
      "Iteration 11, loss = 0.36497606\n",
      "Iteration 12, loss = 0.35342595\n",
      "Iteration 13, loss = 0.34353629\n",
      "Iteration 14, loss = 0.33504673\n",
      "Iteration 15, loss = 0.32760649\n",
      "Iteration 16, loss = 0.32114925\n",
      "Iteration 17, loss = 0.31551001\n",
      "Iteration 18, loss = 0.31038220\n",
      "Iteration 19, loss = 0.30585915\n",
      "Iteration 20, loss = 0.30180838\n",
      "Iteration 21, loss = 0.29819645\n",
      "Iteration 22, loss = 0.29489893\n",
      "Iteration 23, loss = 0.29194803\n",
      "Iteration 24, loss = 0.28919191\n",
      "Iteration 25, loss = 0.28671788\n",
      "Iteration 26, loss = 0.28445137\n",
      "Iteration 27, loss = 0.28233976\n",
      "Iteration 28, loss = 0.28042054\n",
      "Iteration 29, loss = 0.27860096\n",
      "Iteration 30, loss = 0.27696101\n",
      "Iteration 31, loss = 0.27542631\n",
      "Iteration 32, loss = 0.27397758\n",
      "Iteration 33, loss = 0.27264442\n",
      "Iteration 34, loss = 0.27137220\n",
      "Iteration 35, loss = 0.27016347\n",
      "Iteration 36, loss = 0.26905484\n",
      "Iteration 37, loss = 0.26799887\n",
      "Iteration 38, loss = 0.26705309\n",
      "Iteration 39, loss = 0.26606953\n",
      "Iteration 40, loss = 0.26520123\n",
      "Iteration 41, loss = 0.26433138\n",
      "Iteration 42, loss = 0.26353074\n",
      "Iteration 43, loss = 0.26274815\n",
      "Iteration 44, loss = 0.26199923\n",
      "Iteration 45, loss = 0.26130850\n",
      "Iteration 46, loss = 0.26064408\n",
      "Iteration 47, loss = 0.25999151\n",
      "Iteration 48, loss = 0.25941357\n",
      "Iteration 49, loss = 0.25879427\n",
      "Iteration 50, loss = 0.25822447\n",
      "Iteration 51, loss = 0.25766285\n",
      "Iteration 52, loss = 0.25713588\n",
      "Iteration 53, loss = 0.25664028\n",
      "Iteration 54, loss = 0.25613837\n",
      "Iteration 55, loss = 0.25564222\n",
      "Iteration 56, loss = 0.25517371\n",
      "Iteration 57, loss = 0.25470852\n",
      "Iteration 58, loss = 0.25429721\n",
      "Iteration 59, loss = 0.25385492\n",
      "Iteration 60, loss = 0.25342431\n",
      "Iteration 61, loss = 0.25304559\n",
      "Iteration 62, loss = 0.25264073\n",
      "Iteration 63, loss = 0.25228448\n",
      "Iteration 64, loss = 0.25189022\n",
      "Iteration 65, loss = 0.25152300\n",
      "Iteration 66, loss = 0.25118924\n",
      "Iteration 67, loss = 0.25080877\n",
      "Iteration 68, loss = 0.25050425\n",
      "Iteration 69, loss = 0.25013324\n",
      "Iteration 70, loss = 0.24980888\n",
      "Iteration 71, loss = 0.24951640\n",
      "Iteration 72, loss = 0.24919954\n",
      "Iteration 73, loss = 0.24887538\n",
      "Iteration 74, loss = 0.24859069\n",
      "Iteration 75, loss = 0.24828738\n",
      "Iteration 76, loss = 0.24799986\n",
      "Iteration 77, loss = 0.24770985\n",
      "Iteration 78, loss = 0.24742839\n",
      "Iteration 79, loss = 0.24718297\n",
      "Iteration 80, loss = 0.24687029\n",
      "Iteration 81, loss = 0.24662008\n",
      "Iteration 82, loss = 0.24634614\n",
      "Iteration 83, loss = 0.24610288\n",
      "Iteration 84, loss = 0.24586388\n",
      "Iteration 85, loss = 0.24563313\n",
      "Iteration 86, loss = 0.24536487\n",
      "Iteration 87, loss = 0.24512714\n",
      "Iteration 88, loss = 0.24487369\n",
      "Iteration 89, loss = 0.24465737\n",
      "Iteration 90, loss = 0.24441528\n",
      "Iteration 91, loss = 0.24419434\n",
      "Iteration 92, loss = 0.24395085\n",
      "Iteration 93, loss = 0.24373321\n",
      "Iteration 94, loss = 0.24352746\n",
      "Iteration 95, loss = 0.24329881\n",
      "Iteration 96, loss = 0.24308814\n",
      "Iteration 97, loss = 0.24287958\n",
      "Iteration 98, loss = 0.24268071\n",
      "Iteration 99, loss = 0.24247739\n",
      "Iteration 100, loss = 0.24225497\n",
      "Iteration 101, loss = 0.24206176\n",
      "Iteration 102, loss = 0.24189208\n",
      "Iteration 103, loss = 0.24166632\n",
      "Iteration 104, loss = 0.24147932\n",
      "Iteration 105, loss = 0.24130323\n",
      "Iteration 106, loss = 0.24113544\n",
      "Iteration 107, loss = 0.24089740\n",
      "Iteration 108, loss = 0.24073582\n",
      "Iteration 109, loss = 0.24055875\n",
      "Iteration 110, loss = 0.24037510\n",
      "Iteration 111, loss = 0.24018592\n",
      "Iteration 112, loss = 0.24001870\n",
      "Iteration 113, loss = 0.23985825\n",
      "Iteration 114, loss = 0.23967468\n",
      "Iteration 115, loss = 0.23947899\n",
      "Iteration 116, loss = 0.23932461\n",
      "Iteration 117, loss = 0.23916466\n",
      "Iteration 118, loss = 0.23896920\n",
      "Iteration 119, loss = 0.23881677\n",
      "Iteration 120, loss = 0.23865757\n",
      "Iteration 121, loss = 0.23851726\n",
      "Iteration 122, loss = 0.23834644\n",
      "Iteration 123, loss = 0.23817092\n",
      "Iteration 124, loss = 0.23802108\n",
      "Iteration 125, loss = 0.23787821\n",
      "Iteration 126, loss = 0.23771091\n",
      "Iteration 127, loss = 0.23756960\n",
      "Iteration 128, loss = 0.23744137\n",
      "Iteration 129, loss = 0.23729792\n",
      "Iteration 130, loss = 0.23711884\n",
      "Iteration 131, loss = 0.23700551\n",
      "Iteration 132, loss = 0.23684468\n",
      "Iteration 133, loss = 0.23667162\n",
      "Iteration 134, loss = 0.23657164\n",
      "Iteration 135, loss = 0.23641263\n",
      "Iteration 136, loss = 0.23630511\n",
      "Iteration 137, loss = 0.23613334\n",
      "Iteration 138, loss = 0.23599987\n",
      "Iteration 139, loss = 0.23587098\n",
      "Iteration 140, loss = 0.23572998\n",
      "Iteration 141, loss = 0.23560184\n",
      "Iteration 142, loss = 0.23548297\n",
      "Iteration 143, loss = 0.23533547\n",
      "Iteration 144, loss = 0.23521946\n",
      "Iteration 145, loss = 0.23509139\n",
      "Iteration 146, loss = 0.23496317\n",
      "Iteration 147, loss = 0.23482674\n",
      "Iteration 148, loss = 0.23469235\n",
      "Iteration 149, loss = 0.23456958\n",
      "Iteration 150, loss = 0.23445607\n",
      "Iteration 151, loss = 0.23433939\n",
      "Iteration 152, loss = 0.23423274\n",
      "Iteration 153, loss = 0.23410794\n",
      "Iteration 154, loss = 0.23399872\n",
      "Iteration 155, loss = 0.23387160\n",
      "Iteration 156, loss = 0.23379172\n",
      "Iteration 157, loss = 0.23364316\n",
      "Iteration 158, loss = 0.23352963\n",
      "Iteration 159, loss = 0.23342228\n",
      "Iteration 160, loss = 0.23331688\n",
      "Iteration 161, loss = 0.23318331\n",
      "Iteration 162, loss = 0.23307494\n",
      "Iteration 163, loss = 0.23296697\n",
      "Iteration 164, loss = 0.23285767\n",
      "Iteration 165, loss = 0.23277987\n",
      "Iteration 166, loss = 0.23265706\n",
      "Iteration 167, loss = 0.23259957\n",
      "Iteration 168, loss = 0.23245538\n",
      "Iteration 169, loss = 0.23234669\n",
      "Iteration 170, loss = 0.23226116\n",
      "Iteration 171, loss = 0.23213597\n",
      "Iteration 172, loss = 0.23203620\n",
      "Iteration 173, loss = 0.23193265\n",
      "Iteration 174, loss = 0.23183660\n",
      "Iteration 175, loss = 0.23175756\n",
      "Iteration 176, loss = 0.23165684\n",
      "Iteration 177, loss = 0.23152996\n",
      "Iteration 178, loss = 0.23145009\n",
      "Iteration 179, loss = 0.23133889\n",
      "Iteration 180, loss = 0.23123854\n",
      "Iteration 181, loss = 0.23113512\n",
      "Iteration 182, loss = 0.23108522\n",
      "Iteration 183, loss = 0.23096377\n",
      "Iteration 184, loss = 0.23088546\n",
      "Iteration 185, loss = 0.23077696\n",
      "Iteration 186, loss = 0.23068970\n",
      "Iteration 187, loss = 0.23060729\n",
      "Iteration 188, loss = 0.23052363\n",
      "Iteration 189, loss = 0.23041626\n",
      "Iteration 190, loss = 0.23033038\n",
      "Iteration 191, loss = 0.23024444\n",
      "Iteration 192, loss = 0.23016533\n",
      "Iteration 193, loss = 0.23007833\n",
      "Iteration 194, loss = 0.22997673\n",
      "Iteration 195, loss = 0.22989719\n",
      "Iteration 196, loss = 0.22981186\n",
      "Iteration 197, loss = 0.22975032\n",
      "Iteration 198, loss = 0.22962541\n",
      "Iteration 199, loss = 0.22958940\n",
      "Iteration 200, loss = 0.22951284\n",
      "Iteration 1, loss = 0.84286491\n",
      "Iteration 2, loss = 0.73108972\n",
      "Iteration 3, loss = 0.63494203\n",
      "Iteration 4, loss = 0.56308067\n",
      "Iteration 5, loss = 0.50990778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.46956048\n",
      "Iteration 7, loss = 0.43795871\n",
      "Iteration 8, loss = 0.41299647\n",
      "Iteration 9, loss = 0.39276283\n",
      "Iteration 10, loss = 0.37603595\n",
      "Iteration 11, loss = 0.36210913\n",
      "Iteration 12, loss = 0.35044839\n",
      "Iteration 13, loss = 0.34047583\n",
      "Iteration 14, loss = 0.33181593\n",
      "Iteration 15, loss = 0.32432736\n",
      "Iteration 16, loss = 0.31778532\n",
      "Iteration 17, loss = 0.31206163\n",
      "Iteration 18, loss = 0.30687894\n",
      "Iteration 19, loss = 0.30230911\n",
      "Iteration 20, loss = 0.29818560\n",
      "Iteration 21, loss = 0.29453259\n",
      "Iteration 22, loss = 0.29117758\n",
      "Iteration 23, loss = 0.28817580\n",
      "Iteration 24, loss = 0.28540454\n",
      "Iteration 25, loss = 0.28286100\n",
      "Iteration 26, loss = 0.28056352\n",
      "Iteration 27, loss = 0.27842849\n",
      "Iteration 28, loss = 0.27646650\n",
      "Iteration 29, loss = 0.27462347\n",
      "Iteration 30, loss = 0.27293879\n",
      "Iteration 31, loss = 0.27135840\n",
      "Iteration 32, loss = 0.26988155\n",
      "Iteration 33, loss = 0.26848892\n",
      "Iteration 34, loss = 0.26721506\n",
      "Iteration 35, loss = 0.26595682\n",
      "Iteration 36, loss = 0.26483691\n",
      "Iteration 37, loss = 0.26373495\n",
      "Iteration 38, loss = 0.26272925\n",
      "Iteration 39, loss = 0.26174404\n",
      "Iteration 40, loss = 0.26082780\n",
      "Iteration 41, loss = 0.25994839\n",
      "Iteration 42, loss = 0.25913245\n",
      "Iteration 43, loss = 0.25831633\n",
      "Iteration 44, loss = 0.25757016\n",
      "Iteration 45, loss = 0.25681875\n",
      "Iteration 46, loss = 0.25610545\n",
      "Iteration 47, loss = 0.25544980\n",
      "Iteration 48, loss = 0.25481471\n",
      "Iteration 49, loss = 0.25419731\n",
      "Iteration 50, loss = 0.25357880\n",
      "Iteration 51, loss = 0.25302635\n",
      "Iteration 52, loss = 0.25246083\n",
      "Iteration 53, loss = 0.25191539\n",
      "Iteration 54, loss = 0.25139964\n",
      "Iteration 55, loss = 0.25088870\n",
      "Iteration 56, loss = 0.25040765\n",
      "Iteration 57, loss = 0.24992404\n",
      "Iteration 58, loss = 0.24947498\n",
      "Iteration 59, loss = 0.24901656\n",
      "Iteration 60, loss = 0.24857068\n",
      "Iteration 61, loss = 0.24815853\n",
      "Iteration 62, loss = 0.24772988\n",
      "Iteration 63, loss = 0.24736912\n",
      "Iteration 64, loss = 0.24696227\n",
      "Iteration 65, loss = 0.24659691\n",
      "Iteration 66, loss = 0.24622230\n",
      "Iteration 67, loss = 0.24582154\n",
      "Iteration 68, loss = 0.24550545\n",
      "Iteration 69, loss = 0.24510624\n",
      "Iteration 70, loss = 0.24476349\n",
      "Iteration 71, loss = 0.24445990\n",
      "Iteration 72, loss = 0.24411518\n",
      "Iteration 73, loss = 0.24378269\n",
      "Iteration 74, loss = 0.24347217\n",
      "Iteration 75, loss = 0.24315795\n",
      "Iteration 76, loss = 0.24283750\n",
      "Iteration 77, loss = 0.24253959\n",
      "Iteration 78, loss = 0.24225464\n",
      "Iteration 79, loss = 0.24197253\n",
      "Iteration 80, loss = 0.24163792\n",
      "Iteration 81, loss = 0.24138240\n",
      "Iteration 82, loss = 0.24110277\n",
      "Iteration 83, loss = 0.24083732\n",
      "Iteration 84, loss = 0.24056289\n",
      "Iteration 85, loss = 0.24033545\n",
      "Iteration 86, loss = 0.24004591\n",
      "Iteration 87, loss = 0.23981167\n",
      "Iteration 88, loss = 0.23952336\n",
      "Iteration 89, loss = 0.23931221\n",
      "Iteration 90, loss = 0.23905505\n",
      "Iteration 91, loss = 0.23880845\n",
      "Iteration 92, loss = 0.23855201\n",
      "Iteration 93, loss = 0.23833394\n",
      "Iteration 94, loss = 0.23810429\n",
      "Iteration 95, loss = 0.23787327\n",
      "Iteration 96, loss = 0.23763796\n",
      "Iteration 97, loss = 0.23743695\n",
      "Iteration 98, loss = 0.23719978\n",
      "Iteration 99, loss = 0.23699614\n",
      "Iteration 100, loss = 0.23676729\n",
      "Iteration 101, loss = 0.23655712\n",
      "Iteration 102, loss = 0.23635508\n",
      "Iteration 103, loss = 0.23616725\n",
      "Iteration 104, loss = 0.23592842\n",
      "Iteration 105, loss = 0.23573613\n",
      "Iteration 106, loss = 0.23557413\n",
      "Iteration 107, loss = 0.23534379\n",
      "Iteration 108, loss = 0.23514998\n",
      "Iteration 109, loss = 0.23497093\n",
      "Iteration 110, loss = 0.23476610\n",
      "Iteration 111, loss = 0.23458189\n",
      "Iteration 112, loss = 0.23439057\n",
      "Iteration 113, loss = 0.23420772\n",
      "Iteration 114, loss = 0.23406364\n",
      "Iteration 115, loss = 0.23381160\n",
      "Iteration 116, loss = 0.23366563\n",
      "Iteration 117, loss = 0.23347787\n",
      "Iteration 118, loss = 0.23330445\n",
      "Iteration 119, loss = 0.23313484\n",
      "Iteration 120, loss = 0.23297994\n",
      "Iteration 121, loss = 0.23278945\n",
      "Iteration 122, loss = 0.23264369\n",
      "Iteration 123, loss = 0.23244596\n",
      "Iteration 124, loss = 0.23229319\n",
      "Iteration 125, loss = 0.23211656\n",
      "Iteration 126, loss = 0.23197879\n",
      "Iteration 127, loss = 0.23180727\n",
      "Iteration 128, loss = 0.23168978\n",
      "Iteration 129, loss = 0.23150642\n",
      "Iteration 130, loss = 0.23133297\n",
      "Iteration 131, loss = 0.23118106\n",
      "Iteration 132, loss = 0.23105416\n",
      "Iteration 133, loss = 0.23088752\n",
      "Iteration 134, loss = 0.23075984\n",
      "Iteration 135, loss = 0.23061128\n",
      "Iteration 136, loss = 0.23045515\n",
      "Iteration 137, loss = 0.23028805\n",
      "Iteration 138, loss = 0.23016946\n",
      "Iteration 139, loss = 0.23001242\n",
      "Iteration 140, loss = 0.22989333\n",
      "Iteration 141, loss = 0.22974370\n",
      "Iteration 142, loss = 0.22961760\n",
      "Iteration 143, loss = 0.22945493\n",
      "Iteration 144, loss = 0.22936917\n",
      "Iteration 145, loss = 0.22919668\n",
      "Iteration 146, loss = 0.22906501\n",
      "Iteration 147, loss = 0.22893149\n",
      "Iteration 148, loss = 0.22879527\n",
      "Iteration 149, loss = 0.22865759\n",
      "Iteration 150, loss = 0.22855098\n",
      "Iteration 151, loss = 0.22840992\n",
      "Iteration 152, loss = 0.22830743\n",
      "Iteration 153, loss = 0.22816069\n",
      "Iteration 154, loss = 0.22803815\n",
      "Iteration 155, loss = 0.22792267\n",
      "Iteration 156, loss = 0.22784552\n",
      "Iteration 157, loss = 0.22768350\n",
      "Iteration 158, loss = 0.22755654\n",
      "Iteration 159, loss = 0.22746103\n",
      "Iteration 160, loss = 0.22735146\n",
      "Iteration 161, loss = 0.22722805\n",
      "Iteration 162, loss = 0.22708945\n",
      "Iteration 163, loss = 0.22698756\n",
      "Iteration 164, loss = 0.22687127\n",
      "Iteration 165, loss = 0.22677169\n",
      "Iteration 166, loss = 0.22663271\n",
      "Iteration 167, loss = 0.22657487\n",
      "Iteration 168, loss = 0.22643084\n",
      "Iteration 169, loss = 0.22632740\n",
      "Iteration 170, loss = 0.22622205\n",
      "Iteration 171, loss = 0.22609551\n",
      "Iteration 172, loss = 0.22599906\n",
      "Iteration 173, loss = 0.22590160\n",
      "Iteration 174, loss = 0.22578986\n",
      "Iteration 175, loss = 0.22570499\n",
      "Iteration 176, loss = 0.22561314\n",
      "Iteration 177, loss = 0.22549650\n",
      "Iteration 178, loss = 0.22535795\n",
      "Iteration 179, loss = 0.22527765\n",
      "Iteration 180, loss = 0.22517326\n",
      "Iteration 181, loss = 0.22506502\n",
      "Iteration 182, loss = 0.22499744\n",
      "Iteration 183, loss = 0.22487458\n",
      "Iteration 184, loss = 0.22479470\n",
      "Iteration 185, loss = 0.22469085\n",
      "Iteration 186, loss = 0.22459895\n",
      "Iteration 187, loss = 0.22451167\n",
      "Iteration 188, loss = 0.22442862\n",
      "Iteration 189, loss = 0.22431927\n",
      "Iteration 190, loss = 0.22422869\n",
      "Iteration 191, loss = 0.22413875\n",
      "Iteration 192, loss = 0.22405110\n",
      "Iteration 193, loss = 0.22395308\n",
      "Iteration 194, loss = 0.22386563\n",
      "Iteration 195, loss = 0.22377350\n",
      "Iteration 196, loss = 0.22369366\n",
      "Iteration 197, loss = 0.22360321\n",
      "Iteration 198, loss = 0.22350237\n",
      "Iteration 199, loss = 0.22345751\n",
      "Iteration 200, loss = 0.22334466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84927725\n",
      "Iteration 2, loss = 0.73512048\n",
      "Iteration 3, loss = 0.63761253\n",
      "Iteration 4, loss = 0.56535933\n",
      "Iteration 5, loss = 0.51188548\n",
      "Iteration 6, loss = 0.47130704\n",
      "Iteration 7, loss = 0.43989253\n",
      "Iteration 8, loss = 0.41513332\n",
      "Iteration 9, loss = 0.39499253\n",
      "Iteration 10, loss = 0.37846590\n",
      "Iteration 11, loss = 0.36472533\n",
      "Iteration 12, loss = 0.35322389\n",
      "Iteration 13, loss = 0.34340586\n",
      "Iteration 14, loss = 0.33487384\n",
      "Iteration 15, loss = 0.32754514\n",
      "Iteration 16, loss = 0.32109959\n",
      "Iteration 17, loss = 0.31546892\n",
      "Iteration 18, loss = 0.31041485\n",
      "Iteration 19, loss = 0.30595731\n",
      "Iteration 20, loss = 0.30190528\n",
      "Iteration 21, loss = 0.29832955\n",
      "Iteration 22, loss = 0.29508895\n",
      "Iteration 23, loss = 0.29212649\n",
      "Iteration 24, loss = 0.28941246\n",
      "Iteration 25, loss = 0.28693573\n",
      "Iteration 26, loss = 0.28467411\n",
      "Iteration 27, loss = 0.28260932\n",
      "Iteration 28, loss = 0.28065621\n",
      "Iteration 29, loss = 0.27889196\n",
      "Iteration 30, loss = 0.27723826\n",
      "Iteration 31, loss = 0.27572101\n",
      "Iteration 32, loss = 0.27424776\n",
      "Iteration 33, loss = 0.27289082\n",
      "Iteration 34, loss = 0.27163248\n",
      "Iteration 35, loss = 0.27045455\n",
      "Iteration 36, loss = 0.26930851\n",
      "Iteration 37, loss = 0.26826407\n",
      "Iteration 38, loss = 0.26727816\n",
      "Iteration 39, loss = 0.26631664\n",
      "Iteration 40, loss = 0.26544525\n",
      "Iteration 41, loss = 0.26456662\n",
      "Iteration 42, loss = 0.26377247\n",
      "Iteration 43, loss = 0.26298651\n",
      "Iteration 44, loss = 0.26225286\n",
      "Iteration 45, loss = 0.26154395\n",
      "Iteration 46, loss = 0.26084722\n",
      "Iteration 47, loss = 0.26020092\n",
      "Iteration 48, loss = 0.25957362\n",
      "Iteration 49, loss = 0.25898475\n",
      "Iteration 50, loss = 0.25835493\n",
      "Iteration 51, loss = 0.25783251\n",
      "Iteration 52, loss = 0.25727675\n",
      "Iteration 53, loss = 0.25673608\n",
      "Iteration 54, loss = 0.25624397\n",
      "Iteration 55, loss = 0.25573969\n",
      "Iteration 56, loss = 0.25527566\n",
      "Iteration 57, loss = 0.25480333\n",
      "Iteration 58, loss = 0.25436826\n",
      "Iteration 59, loss = 0.25391096\n",
      "Iteration 60, loss = 0.25349015\n",
      "Iteration 61, loss = 0.25307609\n",
      "Iteration 62, loss = 0.25266363\n",
      "Iteration 63, loss = 0.25231321\n",
      "Iteration 64, loss = 0.25188911\n",
      "Iteration 65, loss = 0.25152472\n",
      "Iteration 66, loss = 0.25115016\n",
      "Iteration 67, loss = 0.25080210\n",
      "Iteration 68, loss = 0.25045771\n",
      "Iteration 69, loss = 0.25008528\n",
      "Iteration 70, loss = 0.24976243\n",
      "Iteration 71, loss = 0.24942890\n",
      "Iteration 72, loss = 0.24909295\n",
      "Iteration 73, loss = 0.24877080\n",
      "Iteration 74, loss = 0.24846280\n",
      "Iteration 75, loss = 0.24817354\n",
      "Iteration 76, loss = 0.24786266\n",
      "Iteration 77, loss = 0.24754455\n",
      "Iteration 78, loss = 0.24726343\n",
      "Iteration 79, loss = 0.24699625\n",
      "Iteration 80, loss = 0.24669228\n",
      "Iteration 81, loss = 0.24640670\n",
      "Iteration 82, loss = 0.24614641\n",
      "Iteration 83, loss = 0.24588299\n",
      "Iteration 84, loss = 0.24562096\n",
      "Iteration 85, loss = 0.24536109\n",
      "Iteration 86, loss = 0.24508911\n",
      "Iteration 87, loss = 0.24484913\n",
      "Iteration 88, loss = 0.24457917\n",
      "Iteration 89, loss = 0.24433933\n",
      "Iteration 90, loss = 0.24411023\n",
      "Iteration 91, loss = 0.24386546\n",
      "Iteration 92, loss = 0.24360450\n",
      "Iteration 93, loss = 0.24340035\n",
      "Iteration 94, loss = 0.24315698\n",
      "Iteration 95, loss = 0.24293092\n",
      "Iteration 96, loss = 0.24271721\n",
      "Iteration 97, loss = 0.24247121\n",
      "Iteration 98, loss = 0.24227194\n",
      "Iteration 99, loss = 0.24205748\n",
      "Iteration 100, loss = 0.24184594\n",
      "Iteration 101, loss = 0.24162337\n",
      "Iteration 102, loss = 0.24141005\n",
      "Iteration 103, loss = 0.24123524\n",
      "Iteration 104, loss = 0.24100124\n",
      "Iteration 105, loss = 0.24081046\n",
      "Iteration 106, loss = 0.24061588\n",
      "Iteration 107, loss = 0.24040989\n",
      "Iteration 108, loss = 0.24020712\n",
      "Iteration 109, loss = 0.24007369\n",
      "Iteration 110, loss = 0.23982825\n",
      "Iteration 111, loss = 0.23963902\n",
      "Iteration 112, loss = 0.23945544\n",
      "Iteration 113, loss = 0.23928258\n",
      "Iteration 114, loss = 0.23909877\n",
      "Iteration 115, loss = 0.23890560\n",
      "Iteration 116, loss = 0.23872994\n",
      "Iteration 117, loss = 0.23855928\n",
      "Iteration 118, loss = 0.23836083\n",
      "Iteration 119, loss = 0.23820604\n",
      "Iteration 120, loss = 0.23803762\n",
      "Iteration 121, loss = 0.23785321\n",
      "Iteration 122, loss = 0.23769007\n",
      "Iteration 123, loss = 0.23753902\n",
      "Iteration 124, loss = 0.23737267\n",
      "Iteration 125, loss = 0.23718723\n",
      "Iteration 126, loss = 0.23706735\n",
      "Iteration 127, loss = 0.23686915\n",
      "Iteration 128, loss = 0.23678703\n",
      "Iteration 129, loss = 0.23658098\n",
      "Iteration 130, loss = 0.23639117\n",
      "Iteration 131, loss = 0.23624301\n",
      "Iteration 132, loss = 0.23611694\n",
      "Iteration 133, loss = 0.23593724\n",
      "Iteration 134, loss = 0.23583311\n",
      "Iteration 135, loss = 0.23566963\n",
      "Iteration 136, loss = 0.23550971\n",
      "Iteration 137, loss = 0.23538528\n",
      "Iteration 138, loss = 0.23524808\n",
      "Iteration 139, loss = 0.23507180\n",
      "Iteration 140, loss = 0.23495528\n",
      "Iteration 141, loss = 0.23481805\n",
      "Iteration 142, loss = 0.23466029\n",
      "Iteration 143, loss = 0.23452407\n",
      "Iteration 144, loss = 0.23443369\n",
      "Iteration 145, loss = 0.23425304\n",
      "Iteration 146, loss = 0.23412427\n",
      "Iteration 147, loss = 0.23398198\n",
      "Iteration 148, loss = 0.23384789\n",
      "Iteration 149, loss = 0.23370447\n",
      "Iteration 150, loss = 0.23362059\n",
      "Iteration 151, loss = 0.23346491\n",
      "Iteration 152, loss = 0.23335964\n",
      "Iteration 153, loss = 0.23320474\n",
      "Iteration 154, loss = 0.23308269\n",
      "Iteration 155, loss = 0.23296117\n",
      "Iteration 156, loss = 0.23286277\n",
      "Iteration 157, loss = 0.23272536\n",
      "Iteration 158, loss = 0.23260074\n",
      "Iteration 159, loss = 0.23248939\n",
      "Iteration 160, loss = 0.23239636\n",
      "Iteration 161, loss = 0.23225201\n",
      "Iteration 162, loss = 0.23213758\n",
      "Iteration 163, loss = 0.23201058\n",
      "Iteration 164, loss = 0.23189899\n",
      "Iteration 165, loss = 0.23181779\n",
      "Iteration 166, loss = 0.23168832\n",
      "Iteration 167, loss = 0.23159231\n",
      "Iteration 168, loss = 0.23145694\n",
      "Iteration 169, loss = 0.23135227\n",
      "Iteration 170, loss = 0.23124481\n",
      "Iteration 171, loss = 0.23113157\n",
      "Iteration 172, loss = 0.23101770\n",
      "Iteration 173, loss = 0.23092548\n",
      "Iteration 174, loss = 0.23080881\n",
      "Iteration 175, loss = 0.23071240\n",
      "Iteration 176, loss = 0.23061685\n",
      "Iteration 177, loss = 0.23051693\n",
      "Iteration 178, loss = 0.23039220\n",
      "Iteration 179, loss = 0.23029556\n",
      "Iteration 180, loss = 0.23020425\n",
      "Iteration 181, loss = 0.23007707\n",
      "Iteration 182, loss = 0.23001143\n",
      "Iteration 183, loss = 0.22988101\n",
      "Iteration 184, loss = 0.22982353\n",
      "Iteration 185, loss = 0.22972099\n",
      "Iteration 186, loss = 0.22961205\n",
      "Iteration 187, loss = 0.22953336\n",
      "Iteration 188, loss = 0.22944138\n",
      "Iteration 189, loss = 0.22933914\n",
      "Iteration 190, loss = 0.22922023\n",
      "Iteration 191, loss = 0.22917807\n",
      "Iteration 192, loss = 0.22905015\n",
      "Iteration 193, loss = 0.22897541\n",
      "Iteration 194, loss = 0.22885742\n",
      "Iteration 195, loss = 0.22876325\n",
      "Iteration 196, loss = 0.22868352\n",
      "Iteration 197, loss = 0.22859684\n",
      "Iteration 198, loss = 0.22851852\n",
      "Iteration 199, loss = 0.22844966\n",
      "Iteration 200, loss = 0.22832379\n",
      "Iteration 1, loss = 0.84476495\n",
      "Iteration 2, loss = 0.73211981\n",
      "Iteration 3, loss = 0.63559504\n",
      "Iteration 4, loss = 0.56361933\n",
      "Iteration 5, loss = 0.51041556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.46977917\n",
      "Iteration 7, loss = 0.43834969\n",
      "Iteration 8, loss = 0.41340333\n",
      "Iteration 9, loss = 0.39310191\n",
      "Iteration 10, loss = 0.37642365\n",
      "Iteration 11, loss = 0.36252292\n",
      "Iteration 12, loss = 0.35087689\n",
      "Iteration 13, loss = 0.34092224\n",
      "Iteration 14, loss = 0.33226023\n",
      "Iteration 15, loss = 0.32484748\n",
      "Iteration 16, loss = 0.31824652\n",
      "Iteration 17, loss = 0.31257151\n",
      "Iteration 18, loss = 0.30741962\n",
      "Iteration 19, loss = 0.30287508\n",
      "Iteration 20, loss = 0.29877990\n",
      "Iteration 21, loss = 0.29512758\n",
      "Iteration 22, loss = 0.29183641\n",
      "Iteration 23, loss = 0.28882871\n",
      "Iteration 24, loss = 0.28606009\n",
      "Iteration 25, loss = 0.28355856\n",
      "Iteration 26, loss = 0.28125317\n",
      "Iteration 27, loss = 0.27918004\n",
      "Iteration 28, loss = 0.27719551\n",
      "Iteration 29, loss = 0.27537244\n",
      "Iteration 30, loss = 0.27368783\n",
      "Iteration 31, loss = 0.27214350\n",
      "Iteration 32, loss = 0.27065968\n",
      "Iteration 33, loss = 0.26929752\n",
      "Iteration 34, loss = 0.26802468\n",
      "Iteration 35, loss = 0.26683602\n",
      "Iteration 36, loss = 0.26570015\n",
      "Iteration 37, loss = 0.26462287\n",
      "Iteration 38, loss = 0.26361134\n",
      "Iteration 39, loss = 0.26267114\n",
      "Iteration 40, loss = 0.26177730\n",
      "Iteration 41, loss = 0.26091135\n",
      "Iteration 42, loss = 0.26009356\n",
      "Iteration 43, loss = 0.25931311\n",
      "Iteration 44, loss = 0.25856229\n",
      "Iteration 45, loss = 0.25786349\n",
      "Iteration 46, loss = 0.25717156\n",
      "Iteration 47, loss = 0.25653286\n",
      "Iteration 48, loss = 0.25589621\n",
      "Iteration 49, loss = 0.25532034\n",
      "Iteration 50, loss = 0.25471355\n",
      "Iteration 51, loss = 0.25416736\n",
      "Iteration 52, loss = 0.25362995\n",
      "Iteration 53, loss = 0.25308955\n",
      "Iteration 54, loss = 0.25259733\n",
      "Iteration 55, loss = 0.25210893\n",
      "Iteration 56, loss = 0.25165203\n",
      "Iteration 57, loss = 0.25118607\n",
      "Iteration 58, loss = 0.25073970\n",
      "Iteration 59, loss = 0.25029807\n",
      "Iteration 60, loss = 0.24989313\n",
      "Iteration 61, loss = 0.24947422\n",
      "Iteration 62, loss = 0.24908248\n",
      "Iteration 63, loss = 0.24871607\n",
      "Iteration 64, loss = 0.24832185\n",
      "Iteration 65, loss = 0.24794899\n",
      "Iteration 66, loss = 0.24759644\n",
      "Iteration 67, loss = 0.24726284\n",
      "Iteration 68, loss = 0.24689829\n",
      "Iteration 69, loss = 0.24656841\n",
      "Iteration 70, loss = 0.24624029\n",
      "Iteration 71, loss = 0.24590461\n",
      "Iteration 72, loss = 0.24560061\n",
      "Iteration 73, loss = 0.24527223\n",
      "Iteration 74, loss = 0.24496960\n",
      "Iteration 75, loss = 0.24470097\n",
      "Iteration 76, loss = 0.24437088\n",
      "Iteration 77, loss = 0.24407101\n",
      "Iteration 78, loss = 0.24380704\n",
      "Iteration 79, loss = 0.24354329\n",
      "Iteration 80, loss = 0.24324262\n",
      "Iteration 81, loss = 0.24295887\n",
      "Iteration 82, loss = 0.24269376\n",
      "Iteration 83, loss = 0.24243455\n",
      "Iteration 84, loss = 0.24218527\n",
      "Iteration 85, loss = 0.24191154\n",
      "Iteration 86, loss = 0.24169229\n",
      "Iteration 87, loss = 0.24143913\n",
      "Iteration 88, loss = 0.24116617\n",
      "Iteration 89, loss = 0.24094976\n",
      "Iteration 90, loss = 0.24071600\n",
      "Iteration 91, loss = 0.24046831\n",
      "Iteration 92, loss = 0.24022233\n",
      "Iteration 93, loss = 0.24000377\n",
      "Iteration 94, loss = 0.23977675\n",
      "Iteration 95, loss = 0.23953521\n",
      "Iteration 96, loss = 0.23932882\n",
      "Iteration 97, loss = 0.23910143\n",
      "Iteration 98, loss = 0.23891340\n",
      "Iteration 99, loss = 0.23871316\n",
      "Iteration 100, loss = 0.23848176\n",
      "Iteration 101, loss = 0.23827565\n",
      "Iteration 102, loss = 0.23805009\n",
      "Iteration 103, loss = 0.23787660\n",
      "Iteration 104, loss = 0.23766576\n",
      "Iteration 105, loss = 0.23747272\n",
      "Iteration 106, loss = 0.23725058\n",
      "Iteration 107, loss = 0.23707664\n",
      "Iteration 108, loss = 0.23688125\n",
      "Iteration 109, loss = 0.23670774\n",
      "Iteration 110, loss = 0.23649687\n",
      "Iteration 111, loss = 0.23630214\n",
      "Iteration 112, loss = 0.23611681\n",
      "Iteration 113, loss = 0.23593930\n",
      "Iteration 114, loss = 0.23574044\n",
      "Iteration 115, loss = 0.23557054\n",
      "Iteration 116, loss = 0.23540861\n",
      "Iteration 117, loss = 0.23521841\n",
      "Iteration 118, loss = 0.23503934\n",
      "Iteration 119, loss = 0.23486819\n",
      "Iteration 120, loss = 0.23470259\n",
      "Iteration 121, loss = 0.23452357\n",
      "Iteration 122, loss = 0.23436686\n",
      "Iteration 123, loss = 0.23419648\n",
      "Iteration 124, loss = 0.23404322\n",
      "Iteration 125, loss = 0.23385468\n",
      "Iteration 126, loss = 0.23372088\n",
      "Iteration 127, loss = 0.23354460\n",
      "Iteration 128, loss = 0.23340042\n",
      "Iteration 129, loss = 0.23322624\n",
      "Iteration 130, loss = 0.23304840\n",
      "Iteration 131, loss = 0.23291600\n",
      "Iteration 132, loss = 0.23275991\n",
      "Iteration 133, loss = 0.23259267\n",
      "Iteration 134, loss = 0.23246859\n",
      "Iteration 135, loss = 0.23230741\n",
      "Iteration 136, loss = 0.23215565\n",
      "Iteration 137, loss = 0.23202251\n",
      "Iteration 138, loss = 0.23188960\n",
      "Iteration 139, loss = 0.23171050\n",
      "Iteration 140, loss = 0.23158460\n",
      "Iteration 141, loss = 0.23146759\n",
      "Iteration 142, loss = 0.23132813\n",
      "Iteration 143, loss = 0.23116160\n",
      "Iteration 144, loss = 0.23105911\n",
      "Iteration 145, loss = 0.23087878\n",
      "Iteration 146, loss = 0.23072929\n",
      "Iteration 147, loss = 0.23060058\n",
      "Iteration 148, loss = 0.23047133\n",
      "Iteration 149, loss = 0.23032146\n",
      "Iteration 150, loss = 0.23021326\n",
      "Iteration 151, loss = 0.23007702\n",
      "Iteration 152, loss = 0.22995990\n",
      "Iteration 153, loss = 0.22978883\n",
      "Iteration 154, loss = 0.22965403\n",
      "Iteration 155, loss = 0.22953681\n",
      "Iteration 156, loss = 0.22943731\n",
      "Iteration 157, loss = 0.22931147\n",
      "Iteration 158, loss = 0.22918101\n",
      "Iteration 159, loss = 0.22907389\n",
      "Iteration 160, loss = 0.22892744\n",
      "Iteration 161, loss = 0.22880994\n",
      "Iteration 162, loss = 0.22868851\n",
      "Iteration 163, loss = 0.22856684\n",
      "Iteration 164, loss = 0.22845267\n",
      "Iteration 165, loss = 0.22834329\n",
      "Iteration 166, loss = 0.22824386\n",
      "Iteration 167, loss = 0.22810818\n",
      "Iteration 168, loss = 0.22799562\n",
      "Iteration 169, loss = 0.22789498\n",
      "Iteration 170, loss = 0.22777759\n",
      "Iteration 171, loss = 0.22767093\n",
      "Iteration 172, loss = 0.22753450\n",
      "Iteration 173, loss = 0.22742982\n",
      "Iteration 174, loss = 0.22731379\n",
      "Iteration 175, loss = 0.22720671\n",
      "Iteration 176, loss = 0.22710027\n",
      "Iteration 177, loss = 0.22698166\n",
      "Iteration 178, loss = 0.22685960\n",
      "Iteration 179, loss = 0.22677751\n",
      "Iteration 180, loss = 0.22666595\n",
      "Iteration 181, loss = 0.22653114\n",
      "Iteration 182, loss = 0.22644431\n",
      "Iteration 183, loss = 0.22634255\n",
      "Iteration 184, loss = 0.22626216\n",
      "Iteration 185, loss = 0.22614888\n",
      "Iteration 186, loss = 0.22604709\n",
      "Iteration 187, loss = 0.22596682\n",
      "Iteration 188, loss = 0.22588274\n",
      "Iteration 189, loss = 0.22576912\n",
      "Iteration 190, loss = 0.22562655\n",
      "Iteration 191, loss = 0.22556182\n",
      "Iteration 192, loss = 0.22545364\n",
      "Iteration 193, loss = 0.22537511\n",
      "Iteration 194, loss = 0.22526777\n",
      "Iteration 195, loss = 0.22514766\n",
      "Iteration 196, loss = 0.22506077\n",
      "Iteration 197, loss = 0.22496409\n",
      "Iteration 198, loss = 0.22488546\n",
      "Iteration 199, loss = 0.22478056\n",
      "Iteration 200, loss = 0.22467558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84906207\n",
      "Iteration 2, loss = 0.73617711\n",
      "Iteration 3, loss = 0.63867597\n",
      "Iteration 4, loss = 0.56661873\n",
      "Iteration 5, loss = 0.51271002\n",
      "Iteration 6, loss = 0.47170489\n",
      "Iteration 7, loss = 0.43991359\n",
      "Iteration 8, loss = 0.41477269\n",
      "Iteration 9, loss = 0.39404729\n",
      "Iteration 10, loss = 0.37723421\n",
      "Iteration 11, loss = 0.36305463\n",
      "Iteration 12, loss = 0.35114667\n",
      "Iteration 13, loss = 0.34098824\n",
      "Iteration 14, loss = 0.33215535\n",
      "Iteration 15, loss = 0.32455590\n",
      "Iteration 16, loss = 0.31778800\n",
      "Iteration 17, loss = 0.31191612\n",
      "Iteration 18, loss = 0.30663915\n",
      "Iteration 19, loss = 0.30189639\n",
      "Iteration 20, loss = 0.29768526\n",
      "Iteration 21, loss = 0.29388773\n",
      "Iteration 22, loss = 0.29043711\n",
      "Iteration 23, loss = 0.28733054\n",
      "Iteration 24, loss = 0.28443396\n",
      "Iteration 25, loss = 0.28184080\n",
      "Iteration 26, loss = 0.27941243\n",
      "Iteration 27, loss = 0.27723924\n",
      "Iteration 28, loss = 0.27517317\n",
      "Iteration 29, loss = 0.27327828\n",
      "Iteration 30, loss = 0.27150972\n",
      "Iteration 31, loss = 0.26988853\n",
      "Iteration 32, loss = 0.26830613\n",
      "Iteration 33, loss = 0.26689316\n",
      "Iteration 34, loss = 0.26553261\n",
      "Iteration 35, loss = 0.26429984\n",
      "Iteration 36, loss = 0.26307381\n",
      "Iteration 37, loss = 0.26197079\n",
      "Iteration 38, loss = 0.26090175\n",
      "Iteration 39, loss = 0.25991942\n",
      "Iteration 40, loss = 0.25899080\n",
      "Iteration 41, loss = 0.25805605\n",
      "Iteration 42, loss = 0.25719807\n",
      "Iteration 43, loss = 0.25638776\n",
      "Iteration 44, loss = 0.25558963\n",
      "Iteration 45, loss = 0.25486639\n",
      "Iteration 46, loss = 0.25414264\n",
      "Iteration 47, loss = 0.25346873\n",
      "Iteration 48, loss = 0.25282558\n",
      "Iteration 49, loss = 0.25219337\n",
      "Iteration 50, loss = 0.25155897\n",
      "Iteration 51, loss = 0.25099841\n",
      "Iteration 52, loss = 0.25044165\n",
      "Iteration 53, loss = 0.24988026\n",
      "Iteration 54, loss = 0.24935419\n",
      "Iteration 55, loss = 0.24886841\n",
      "Iteration 56, loss = 0.24836273\n",
      "Iteration 57, loss = 0.24788670\n",
      "Iteration 58, loss = 0.24744227\n",
      "Iteration 59, loss = 0.24697471\n",
      "Iteration 60, loss = 0.24655404\n",
      "Iteration 61, loss = 0.24613942\n",
      "Iteration 62, loss = 0.24573574\n",
      "Iteration 63, loss = 0.24531234\n",
      "Iteration 64, loss = 0.24496897\n",
      "Iteration 65, loss = 0.24457950\n",
      "Iteration 66, loss = 0.24420064\n",
      "Iteration 67, loss = 0.24384994\n",
      "Iteration 68, loss = 0.24348582\n",
      "Iteration 69, loss = 0.24315671\n",
      "Iteration 70, loss = 0.24284774\n",
      "Iteration 71, loss = 0.24246888\n",
      "Iteration 72, loss = 0.24216963\n",
      "Iteration 73, loss = 0.24185340\n",
      "Iteration 74, loss = 0.24153579\n",
      "Iteration 75, loss = 0.24125771\n",
      "Iteration 76, loss = 0.24094785\n",
      "Iteration 77, loss = 0.24064854\n",
      "Iteration 78, loss = 0.24038207\n",
      "Iteration 79, loss = 0.24009787\n",
      "Iteration 80, loss = 0.23982422\n",
      "Iteration 81, loss = 0.23954243\n",
      "Iteration 82, loss = 0.23928393\n",
      "Iteration 83, loss = 0.23899992\n",
      "Iteration 84, loss = 0.23875313\n",
      "Iteration 85, loss = 0.23848080\n",
      "Iteration 86, loss = 0.23827311\n",
      "Iteration 87, loss = 0.23799484\n",
      "Iteration 88, loss = 0.23776219\n",
      "Iteration 89, loss = 0.23754022\n",
      "Iteration 90, loss = 0.23727240\n",
      "Iteration 91, loss = 0.23706983\n",
      "Iteration 92, loss = 0.23682820\n",
      "Iteration 93, loss = 0.23658734\n",
      "Iteration 94, loss = 0.23638551\n",
      "Iteration 95, loss = 0.23616071\n",
      "Iteration 96, loss = 0.23594095\n",
      "Iteration 97, loss = 0.23571407\n",
      "Iteration 98, loss = 0.23551358\n",
      "Iteration 99, loss = 0.23531293\n",
      "Iteration 100, loss = 0.23509830\n",
      "Iteration 101, loss = 0.23490434\n",
      "Iteration 102, loss = 0.23469061\n",
      "Iteration 103, loss = 0.23450237\n",
      "Iteration 104, loss = 0.23432842\n",
      "Iteration 105, loss = 0.23409833\n",
      "Iteration 106, loss = 0.23391224\n",
      "Iteration 107, loss = 0.23373621\n",
      "Iteration 108, loss = 0.23355620\n",
      "Iteration 109, loss = 0.23334337\n",
      "Iteration 110, loss = 0.23317867\n",
      "Iteration 111, loss = 0.23300309\n",
      "Iteration 112, loss = 0.23281122\n",
      "Iteration 113, loss = 0.23263895\n",
      "Iteration 114, loss = 0.23243356\n",
      "Iteration 115, loss = 0.23229101\n",
      "Iteration 116, loss = 0.23211618\n",
      "Iteration 117, loss = 0.23192955\n",
      "Iteration 118, loss = 0.23175876\n",
      "Iteration 119, loss = 0.23161405\n",
      "Iteration 120, loss = 0.23143189\n",
      "Iteration 121, loss = 0.23128537\n",
      "Iteration 122, loss = 0.23110633\n",
      "Iteration 123, loss = 0.23095822\n",
      "Iteration 124, loss = 0.23080486\n",
      "Iteration 125, loss = 0.23061795\n",
      "Iteration 126, loss = 0.23048967\n",
      "Iteration 127, loss = 0.23032287\n",
      "Iteration 128, loss = 0.23017557\n",
      "Iteration 129, loss = 0.23001473\n",
      "Iteration 130, loss = 0.22986206\n",
      "Iteration 131, loss = 0.22973351\n",
      "Iteration 132, loss = 0.22960299\n",
      "Iteration 133, loss = 0.22942411\n",
      "Iteration 134, loss = 0.22930582\n",
      "Iteration 135, loss = 0.22914174\n",
      "Iteration 136, loss = 0.22902188\n",
      "Iteration 137, loss = 0.22885047\n",
      "Iteration 138, loss = 0.22875169\n",
      "Iteration 139, loss = 0.22858646\n",
      "Iteration 140, loss = 0.22845504\n",
      "Iteration 141, loss = 0.22835757\n",
      "Iteration 142, loss = 0.22820122\n",
      "Iteration 143, loss = 0.22807114\n",
      "Iteration 144, loss = 0.22795038\n",
      "Iteration 145, loss = 0.22779950\n",
      "Iteration 146, loss = 0.22767255\n",
      "Iteration 147, loss = 0.22751812\n",
      "Iteration 148, loss = 0.22741311\n",
      "Iteration 149, loss = 0.22727449\n",
      "Iteration 150, loss = 0.22716378\n",
      "Iteration 151, loss = 0.22703421\n",
      "Iteration 152, loss = 0.22692372\n",
      "Iteration 153, loss = 0.22675652\n",
      "Iteration 154, loss = 0.22664322\n",
      "Iteration 155, loss = 0.22653221\n",
      "Iteration 156, loss = 0.22640815\n",
      "Iteration 157, loss = 0.22631497\n",
      "Iteration 158, loss = 0.22619877\n",
      "Iteration 159, loss = 0.22608757\n",
      "Iteration 160, loss = 0.22593567\n",
      "Iteration 161, loss = 0.22583302\n",
      "Iteration 162, loss = 0.22571608\n",
      "Iteration 163, loss = 0.22560767\n",
      "Iteration 164, loss = 0.22550706\n",
      "Iteration 165, loss = 0.22538482\n",
      "Iteration 166, loss = 0.22530995\n",
      "Iteration 167, loss = 0.22517636\n",
      "Iteration 168, loss = 0.22505490\n",
      "Iteration 169, loss = 0.22497185\n",
      "Iteration 170, loss = 0.22486921\n",
      "Iteration 171, loss = 0.22474756\n",
      "Iteration 172, loss = 0.22466654\n",
      "Iteration 173, loss = 0.22454255\n",
      "Iteration 174, loss = 0.22443053\n",
      "Iteration 175, loss = 0.22434655\n",
      "Iteration 176, loss = 0.22423205\n",
      "Iteration 177, loss = 0.22409598\n",
      "Iteration 178, loss = 0.22401196\n",
      "Iteration 179, loss = 0.22393008\n",
      "Iteration 180, loss = 0.22382473\n",
      "Iteration 181, loss = 0.22371841\n",
      "Iteration 182, loss = 0.22360091\n",
      "Iteration 183, loss = 0.22352500\n",
      "Iteration 184, loss = 0.22344209\n",
      "Iteration 185, loss = 0.22334451\n",
      "Iteration 186, loss = 0.22325577\n",
      "Iteration 187, loss = 0.22316024\n",
      "Iteration 188, loss = 0.22306806\n",
      "Iteration 189, loss = 0.22301237\n",
      "Iteration 190, loss = 0.22284228\n",
      "Iteration 191, loss = 0.22278576\n",
      "Iteration 192, loss = 0.22269674\n",
      "Iteration 193, loss = 0.22259741\n",
      "Iteration 194, loss = 0.22251565\n",
      "Iteration 195, loss = 0.22241072\n",
      "Iteration 196, loss = 0.22231934\n",
      "Iteration 197, loss = 0.22224470\n",
      "Iteration 198, loss = 0.22213776\n",
      "Iteration 199, loss = 0.22207986\n",
      "Iteration 200, loss = 0.22200060\n",
      "Iteration 1, loss = 0.72251243\n",
      "Iteration 2, loss = 0.51809279\n",
      "Iteration 3, loss = 0.41508819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.35041637\n",
      "Iteration 5, loss = 0.30970397\n",
      "Iteration 6, loss = 0.28402331\n",
      "Iteration 7, loss = 0.26678385\n",
      "Iteration 8, loss = 0.25667932\n",
      "Iteration 9, loss = 0.24915447\n",
      "Iteration 10, loss = 0.24401101\n",
      "Iteration 11, loss = 0.23884372\n",
      "Iteration 12, loss = 0.23620538\n",
      "Iteration 13, loss = 0.23328709\n",
      "Iteration 14, loss = 0.23148566\n",
      "Iteration 15, loss = 0.22964324\n",
      "Iteration 16, loss = 0.22825496\n",
      "Iteration 17, loss = 0.22684591\n",
      "Iteration 18, loss = 0.22540990\n",
      "Iteration 19, loss = 0.22370201\n",
      "Iteration 20, loss = 0.22226432\n",
      "Iteration 21, loss = 0.22105904\n",
      "Iteration 22, loss = 0.21945470\n",
      "Iteration 23, loss = 0.21887571\n",
      "Iteration 24, loss = 0.21747312\n",
      "Iteration 25, loss = 0.21610937\n",
      "Iteration 26, loss = 0.21509018\n",
      "Iteration 27, loss = 0.21349173\n",
      "Iteration 28, loss = 0.21201784\n",
      "Iteration 29, loss = 0.21065445\n",
      "Iteration 30, loss = 0.20998014\n",
      "Iteration 31, loss = 0.20767614\n",
      "Iteration 32, loss = 0.20637307\n",
      "Iteration 33, loss = 0.20539291\n",
      "Iteration 34, loss = 0.20362411\n",
      "Iteration 35, loss = 0.20125687\n",
      "Iteration 36, loss = 0.19981683\n",
      "Iteration 37, loss = 0.19849350\n",
      "Iteration 38, loss = 0.19765853\n",
      "Iteration 39, loss = 0.19522267\n",
      "Iteration 40, loss = 0.19361427\n",
      "Iteration 41, loss = 0.19175209\n",
      "Iteration 42, loss = 0.18997930\n",
      "Iteration 43, loss = 0.18786253\n",
      "Iteration 44, loss = 0.18601389\n",
      "Iteration 45, loss = 0.18431803\n",
      "Iteration 46, loss = 0.18286394\n",
      "Iteration 47, loss = 0.18080150\n",
      "Iteration 48, loss = 0.17985233\n",
      "Iteration 49, loss = 0.17738779\n",
      "Iteration 50, loss = 0.17628316\n",
      "Iteration 51, loss = 0.17400752\n",
      "Iteration 52, loss = 0.17220310\n",
      "Iteration 53, loss = 0.17053163\n",
      "Iteration 54, loss = 0.16919396\n",
      "Iteration 55, loss = 0.16667316\n",
      "Iteration 56, loss = 0.16525833\n",
      "Iteration 57, loss = 0.16293720\n",
      "Iteration 58, loss = 0.16142170\n",
      "Iteration 59, loss = 0.15982695\n",
      "Iteration 60, loss = 0.15807180\n",
      "Iteration 61, loss = 0.15624613\n",
      "Iteration 62, loss = 0.15428192\n",
      "Iteration 63, loss = 0.15329990\n",
      "Iteration 64, loss = 0.15153586\n",
      "Iteration 65, loss = 0.14894809\n",
      "Iteration 66, loss = 0.14773298\n",
      "Iteration 67, loss = 0.14669039\n",
      "Iteration 68, loss = 0.14489209\n",
      "Iteration 69, loss = 0.14261455\n",
      "Iteration 70, loss = 0.14150004\n",
      "Iteration 71, loss = 0.14028000\n",
      "Iteration 72, loss = 0.13856208\n",
      "Iteration 73, loss = 0.13664948\n",
      "Iteration 74, loss = 0.13518496\n",
      "Iteration 75, loss = 0.13401935\n",
      "Iteration 76, loss = 0.13274478\n",
      "Iteration 77, loss = 0.13099020\n",
      "Iteration 78, loss = 0.12939618\n",
      "Iteration 79, loss = 0.12829696\n",
      "Iteration 80, loss = 0.12665433\n",
      "Iteration 81, loss = 0.12550262\n",
      "Iteration 82, loss = 0.12367891\n",
      "Iteration 83, loss = 0.12206708\n",
      "Iteration 84, loss = 0.12129527\n",
      "Iteration 85, loss = 0.12057686\n",
      "Iteration 86, loss = 0.11856608\n",
      "Iteration 87, loss = 0.11746320\n",
      "Iteration 88, loss = 0.11613895\n",
      "Iteration 89, loss = 0.11531520\n",
      "Iteration 90, loss = 0.11319549\n",
      "Iteration 91, loss = 0.11272456\n",
      "Iteration 92, loss = 0.11114850\n",
      "Iteration 93, loss = 0.10978071\n",
      "Iteration 94, loss = 0.10865479\n",
      "Iteration 95, loss = 0.10772843\n",
      "Iteration 96, loss = 0.10638197\n",
      "Iteration 97, loss = 0.10495265\n",
      "Iteration 98, loss = 0.10412959\n",
      "Iteration 99, loss = 0.10300741\n",
      "Iteration 100, loss = 0.10211683\n",
      "Iteration 101, loss = 0.10038177\n",
      "Iteration 102, loss = 0.09954865\n",
      "Iteration 103, loss = 0.09832974\n",
      "Iteration 104, loss = 0.09731652\n",
      "Iteration 105, loss = 0.09615714\n",
      "Iteration 106, loss = 0.09576102\n",
      "Iteration 107, loss = 0.09387379\n",
      "Iteration 108, loss = 0.09307314\n",
      "Iteration 109, loss = 0.09236198\n",
      "Iteration 110, loss = 0.09147707\n",
      "Iteration 111, loss = 0.09028639\n",
      "Iteration 112, loss = 0.08950976\n",
      "Iteration 113, loss = 0.08830962\n",
      "Iteration 114, loss = 0.08733062\n",
      "Iteration 115, loss = 0.08677685\n",
      "Iteration 116, loss = 0.08543550\n",
      "Iteration 117, loss = 0.08431926\n",
      "Iteration 118, loss = 0.08297758\n",
      "Iteration 119, loss = 0.08233101\n",
      "Iteration 120, loss = 0.08151042\n",
      "Iteration 121, loss = 0.08057152\n",
      "Iteration 122, loss = 0.07990832\n",
      "Iteration 123, loss = 0.07848802\n",
      "Iteration 124, loss = 0.07820607\n",
      "Iteration 125, loss = 0.07675662\n",
      "Iteration 126, loss = 0.07609294\n",
      "Iteration 127, loss = 0.07550685\n",
      "Iteration 128, loss = 0.07415761\n",
      "Iteration 129, loss = 0.07357527\n",
      "Iteration 130, loss = 0.07229489\n",
      "Iteration 131, loss = 0.07208653\n",
      "Iteration 132, loss = 0.07069337\n",
      "Iteration 133, loss = 0.06970239\n",
      "Iteration 134, loss = 0.06920122\n",
      "Iteration 135, loss = 0.06866834\n",
      "Iteration 136, loss = 0.06764128\n",
      "Iteration 137, loss = 0.06679072\n",
      "Iteration 138, loss = 0.06576883\n",
      "Iteration 139, loss = 0.06482315\n",
      "Iteration 140, loss = 0.06421141\n",
      "Iteration 141, loss = 0.06346737\n",
      "Iteration 142, loss = 0.06281161\n",
      "Iteration 143, loss = 0.06171079\n",
      "Iteration 144, loss = 0.06155415\n",
      "Iteration 145, loss = 0.06082708\n",
      "Iteration 146, loss = 0.06001571\n",
      "Iteration 147, loss = 0.05900805\n",
      "Iteration 148, loss = 0.05810438\n",
      "Iteration 149, loss = 0.05746354\n",
      "Iteration 150, loss = 0.05656174\n",
      "Iteration 151, loss = 0.05628745\n",
      "Iteration 152, loss = 0.05576134\n",
      "Iteration 153, loss = 0.05464707\n",
      "Iteration 154, loss = 0.05418229\n",
      "Iteration 155, loss = 0.05382149\n",
      "Iteration 156, loss = 0.05289045\n",
      "Iteration 157, loss = 0.05237724\n",
      "Iteration 158, loss = 0.05161456\n",
      "Iteration 159, loss = 0.05108790\n",
      "Iteration 160, loss = 0.05025601\n",
      "Iteration 161, loss = 0.04975852\n",
      "Iteration 162, loss = 0.04908677\n",
      "Iteration 163, loss = 0.04837554\n",
      "Iteration 164, loss = 0.04783374\n",
      "Iteration 165, loss = 0.04780430\n",
      "Iteration 166, loss = 0.04638641\n",
      "Iteration 167, loss = 0.04606528\n",
      "Iteration 168, loss = 0.04599869\n",
      "Iteration 169, loss = 0.04495113\n",
      "Iteration 170, loss = 0.04438356\n",
      "Iteration 171, loss = 0.04406636\n",
      "Iteration 172, loss = 0.04307215\n",
      "Iteration 173, loss = 0.04254552\n",
      "Iteration 174, loss = 0.04204227\n",
      "Iteration 175, loss = 0.04166907\n",
      "Iteration 176, loss = 0.04106620\n",
      "Iteration 177, loss = 0.04037005\n",
      "Iteration 178, loss = 0.03999752\n",
      "Iteration 179, loss = 0.03947018\n",
      "Iteration 180, loss = 0.03897059\n",
      "Iteration 181, loss = 0.03802481\n",
      "Iteration 182, loss = 0.03813677\n",
      "Iteration 183, loss = 0.03754505\n",
      "Iteration 184, loss = 0.03705655\n",
      "Iteration 185, loss = 0.03639550\n",
      "Iteration 186, loss = 0.03559760\n",
      "Iteration 187, loss = 0.03529285\n",
      "Iteration 188, loss = 0.03495264\n",
      "Iteration 189, loss = 0.03424573\n",
      "Iteration 190, loss = 0.03393636\n",
      "Iteration 191, loss = 0.03380580\n",
      "Iteration 192, loss = 0.03315037\n",
      "Iteration 193, loss = 0.03263234\n",
      "Iteration 194, loss = 0.03202860\n",
      "Iteration 195, loss = 0.03170703\n",
      "Iteration 196, loss = 0.03127537\n",
      "Iteration 197, loss = 0.03147984\n",
      "Iteration 198, loss = 0.03060676\n",
      "Iteration 199, loss = 0.03010475\n",
      "Iteration 200, loss = 0.02987628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72213913\n",
      "Iteration 2, loss = 0.51612710\n",
      "Iteration 3, loss = 0.41248234\n",
      "Iteration 4, loss = 0.34830762\n",
      "Iteration 5, loss = 0.30702170\n",
      "Iteration 6, loss = 0.28141908\n",
      "Iteration 7, loss = 0.26415908\n",
      "Iteration 8, loss = 0.25347827\n",
      "Iteration 9, loss = 0.24572356\n",
      "Iteration 10, loss = 0.23963731\n",
      "Iteration 11, loss = 0.23470298\n",
      "Iteration 12, loss = 0.23235974\n",
      "Iteration 13, loss = 0.22923163\n",
      "Iteration 14, loss = 0.22645817\n",
      "Iteration 15, loss = 0.22460456\n",
      "Iteration 16, loss = 0.22295542\n",
      "Iteration 17, loss = 0.22127401\n",
      "Iteration 18, loss = 0.22000822\n",
      "Iteration 19, loss = 0.21866820\n",
      "Iteration 20, loss = 0.21746826\n",
      "Iteration 21, loss = 0.21609062\n",
      "Iteration 22, loss = 0.21446745\n",
      "Iteration 23, loss = 0.21355174\n",
      "Iteration 24, loss = 0.21271832\n",
      "Iteration 25, loss = 0.21119885\n",
      "Iteration 26, loss = 0.20953585\n",
      "Iteration 27, loss = 0.20832818\n",
      "Iteration 28, loss = 0.20728462\n",
      "Iteration 29, loss = 0.20590208\n",
      "Iteration 30, loss = 0.20483095\n",
      "Iteration 31, loss = 0.20268439\n",
      "Iteration 32, loss = 0.20130260\n",
      "Iteration 33, loss = 0.19976285\n",
      "Iteration 34, loss = 0.19887515\n",
      "Iteration 35, loss = 0.19629180\n",
      "Iteration 36, loss = 0.19520778\n",
      "Iteration 37, loss = 0.19359029\n",
      "Iteration 38, loss = 0.19194513\n",
      "Iteration 39, loss = 0.18995565\n",
      "Iteration 40, loss = 0.18817137\n",
      "Iteration 41, loss = 0.18666538\n",
      "Iteration 42, loss = 0.18516316\n",
      "Iteration 43, loss = 0.18238718\n",
      "Iteration 44, loss = 0.18093498\n",
      "Iteration 45, loss = 0.17906657\n",
      "Iteration 46, loss = 0.17703588\n",
      "Iteration 47, loss = 0.17533956\n",
      "Iteration 48, loss = 0.17371175\n",
      "Iteration 49, loss = 0.17133123\n",
      "Iteration 50, loss = 0.16976372\n",
      "Iteration 51, loss = 0.16801880\n",
      "Iteration 52, loss = 0.16565924\n",
      "Iteration 53, loss = 0.16394728\n",
      "Iteration 54, loss = 0.16218271\n",
      "Iteration 55, loss = 0.16000589\n",
      "Iteration 56, loss = 0.15877624\n",
      "Iteration 57, loss = 0.15617878\n",
      "Iteration 58, loss = 0.15492841\n",
      "Iteration 59, loss = 0.15300020\n",
      "Iteration 60, loss = 0.15103328\n",
      "Iteration 61, loss = 0.14912467\n",
      "Iteration 62, loss = 0.14729823\n",
      "Iteration 63, loss = 0.14644311\n",
      "Iteration 64, loss = 0.14434291\n",
      "Iteration 65, loss = 0.14245786\n",
      "Iteration 66, loss = 0.14122011\n",
      "Iteration 67, loss = 0.13936506\n",
      "Iteration 68, loss = 0.13810569\n",
      "Iteration 69, loss = 0.13593252\n",
      "Iteration 70, loss = 0.13410559\n",
      "Iteration 71, loss = 0.13331488\n",
      "Iteration 72, loss = 0.13218173\n",
      "Iteration 73, loss = 0.12954493\n",
      "Iteration 74, loss = 0.12782023\n",
      "Iteration 75, loss = 0.12706351\n",
      "Iteration 76, loss = 0.12535369\n",
      "Iteration 77, loss = 0.12385415\n",
      "Iteration 78, loss = 0.12256617\n",
      "Iteration 79, loss = 0.12098861\n",
      "Iteration 80, loss = 0.11955616\n",
      "Iteration 81, loss = 0.11822168\n",
      "Iteration 82, loss = 0.11694913\n",
      "Iteration 83, loss = 0.11478682\n",
      "Iteration 84, loss = 0.11390422\n",
      "Iteration 85, loss = 0.11329767\n",
      "Iteration 86, loss = 0.11108871\n",
      "Iteration 87, loss = 0.11008296\n",
      "Iteration 88, loss = 0.10881807\n",
      "Iteration 89, loss = 0.10761479\n",
      "Iteration 90, loss = 0.10586014\n",
      "Iteration 91, loss = 0.10494491\n",
      "Iteration 92, loss = 0.10346102\n",
      "Iteration 93, loss = 0.10222773\n",
      "Iteration 94, loss = 0.10099940\n",
      "Iteration 95, loss = 0.10044302\n",
      "Iteration 96, loss = 0.09869675\n",
      "Iteration 97, loss = 0.09746403\n",
      "Iteration 98, loss = 0.09636838\n",
      "Iteration 99, loss = 0.09542013\n",
      "Iteration 100, loss = 0.09449004\n",
      "Iteration 101, loss = 0.09302604\n",
      "Iteration 102, loss = 0.09229087\n",
      "Iteration 103, loss = 0.09130767\n",
      "Iteration 104, loss = 0.08992282\n",
      "Iteration 105, loss = 0.08856284\n",
      "Iteration 106, loss = 0.08816669\n",
      "Iteration 107, loss = 0.08658959\n",
      "Iteration 108, loss = 0.08552012\n",
      "Iteration 109, loss = 0.08460789\n",
      "Iteration 110, loss = 0.08372888\n",
      "Iteration 111, loss = 0.08244440\n",
      "Iteration 112, loss = 0.08157245\n",
      "Iteration 113, loss = 0.08078217\n",
      "Iteration 114, loss = 0.07984079\n",
      "Iteration 115, loss = 0.07897696\n",
      "Iteration 116, loss = 0.07745843\n",
      "Iteration 117, loss = 0.07650858\n",
      "Iteration 118, loss = 0.07556396\n",
      "Iteration 119, loss = 0.07515814\n",
      "Iteration 120, loss = 0.07415787\n",
      "Iteration 121, loss = 0.07317254\n",
      "Iteration 122, loss = 0.07227236\n",
      "Iteration 123, loss = 0.07149281\n",
      "Iteration 124, loss = 0.07062890\n",
      "Iteration 125, loss = 0.06937959\n",
      "Iteration 126, loss = 0.06867822\n",
      "Iteration 127, loss = 0.06779255\n",
      "Iteration 128, loss = 0.06711137\n",
      "Iteration 129, loss = 0.06630134\n",
      "Iteration 130, loss = 0.06511198\n",
      "Iteration 131, loss = 0.06425878\n",
      "Iteration 132, loss = 0.06363170\n",
      "Iteration 133, loss = 0.06271893\n",
      "Iteration 134, loss = 0.06199551\n",
      "Iteration 135, loss = 0.06152527\n",
      "Iteration 136, loss = 0.06035304\n",
      "Iteration 137, loss = 0.05953519\n",
      "Iteration 138, loss = 0.05885354\n",
      "Iteration 139, loss = 0.05809713\n",
      "Iteration 140, loss = 0.05750561\n",
      "Iteration 141, loss = 0.05660256\n",
      "Iteration 142, loss = 0.05627257\n",
      "Iteration 143, loss = 0.05496532\n",
      "Iteration 144, loss = 0.05510722\n",
      "Iteration 145, loss = 0.05372375\n",
      "Iteration 146, loss = 0.05310367\n",
      "Iteration 147, loss = 0.05235836\n",
      "Iteration 148, loss = 0.05153071\n",
      "Iteration 149, loss = 0.05070274\n",
      "Iteration 150, loss = 0.05005522\n",
      "Iteration 151, loss = 0.04990898\n",
      "Iteration 152, loss = 0.04938472\n",
      "Iteration 153, loss = 0.04811771\n",
      "Iteration 154, loss = 0.04742056\n",
      "Iteration 155, loss = 0.04694260\n",
      "Iteration 156, loss = 0.04660291\n",
      "Iteration 157, loss = 0.04576323\n",
      "Iteration 158, loss = 0.04517577\n",
      "Iteration 159, loss = 0.04449766\n",
      "Iteration 160, loss = 0.04420307\n",
      "Iteration 161, loss = 0.04342671\n",
      "Iteration 162, loss = 0.04262262\n",
      "Iteration 163, loss = 0.04225462\n",
      "Iteration 164, loss = 0.04151766\n",
      "Iteration 165, loss = 0.04097884\n",
      "Iteration 166, loss = 0.04014270\n",
      "Iteration 167, loss = 0.03965822\n",
      "Iteration 168, loss = 0.03924081\n",
      "Iteration 169, loss = 0.03873826\n",
      "Iteration 170, loss = 0.03822179\n",
      "Iteration 171, loss = 0.03804608\n",
      "Iteration 172, loss = 0.03697203\n",
      "Iteration 173, loss = 0.03660574\n",
      "Iteration 174, loss = 0.03607877\n",
      "Iteration 175, loss = 0.03562325\n",
      "Iteration 176, loss = 0.03503873\n",
      "Iteration 177, loss = 0.03444446\n",
      "Iteration 178, loss = 0.03409670\n",
      "Iteration 179, loss = 0.03358219\n",
      "Iteration 180, loss = 0.03305444\n",
      "Iteration 181, loss = 0.03244499\n",
      "Iteration 182, loss = 0.03197313\n",
      "Iteration 183, loss = 0.03180757\n",
      "Iteration 184, loss = 0.03104661\n",
      "Iteration 185, loss = 0.03084601\n",
      "Iteration 186, loss = 0.03008960\n",
      "Iteration 187, loss = 0.02987944\n",
      "Iteration 188, loss = 0.02960711\n",
      "Iteration 189, loss = 0.02885198\n",
      "Iteration 190, loss = 0.02853812\n",
      "Iteration 191, loss = 0.02818131\n",
      "Iteration 192, loss = 0.02777855\n",
      "Iteration 193, loss = 0.02738200\n",
      "Iteration 194, loss = 0.02676240\n",
      "Iteration 195, loss = 0.02650069\n",
      "Iteration 196, loss = 0.02602971\n",
      "Iteration 197, loss = 0.02594425\n",
      "Iteration 198, loss = 0.02535219\n",
      "Iteration 199, loss = 0.02505616\n",
      "Iteration 200, loss = 0.02469175\n",
      "Iteration 1, loss = 0.72472582\n",
      "Iteration 2, loss = 0.51719898\n",
      "Iteration 3, loss = 0.41386519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.35005371\n",
      "Iteration 5, loss = 0.30974994\n",
      "Iteration 6, loss = 0.28404133\n",
      "Iteration 7, loss = 0.26787153\n",
      "Iteration 8, loss = 0.25716951\n",
      "Iteration 9, loss = 0.24883406\n",
      "Iteration 10, loss = 0.24327859\n",
      "Iteration 11, loss = 0.23901340\n",
      "Iteration 12, loss = 0.23653891\n",
      "Iteration 13, loss = 0.23283420\n",
      "Iteration 14, loss = 0.23031185\n",
      "Iteration 15, loss = 0.22901070\n",
      "Iteration 16, loss = 0.22701373\n",
      "Iteration 17, loss = 0.22544378\n",
      "Iteration 18, loss = 0.22442087\n",
      "Iteration 19, loss = 0.22294685\n",
      "Iteration 20, loss = 0.22120517\n",
      "Iteration 21, loss = 0.21974312\n",
      "Iteration 22, loss = 0.21863178\n",
      "Iteration 23, loss = 0.21749344\n",
      "Iteration 24, loss = 0.21648464\n",
      "Iteration 25, loss = 0.21498108\n",
      "Iteration 26, loss = 0.21328311\n",
      "Iteration 27, loss = 0.21206451\n",
      "Iteration 28, loss = 0.21072967\n",
      "Iteration 29, loss = 0.20965126\n",
      "Iteration 30, loss = 0.20830673\n",
      "Iteration 31, loss = 0.20635013\n",
      "Iteration 32, loss = 0.20464409\n",
      "Iteration 33, loss = 0.20309288\n",
      "Iteration 34, loss = 0.20144368\n",
      "Iteration 35, loss = 0.19990264\n",
      "Iteration 36, loss = 0.19796668\n",
      "Iteration 37, loss = 0.19666329\n",
      "Iteration 38, loss = 0.19503634\n",
      "Iteration 39, loss = 0.19278952\n",
      "Iteration 40, loss = 0.19127466\n",
      "Iteration 41, loss = 0.18956255\n",
      "Iteration 42, loss = 0.18798799\n",
      "Iteration 43, loss = 0.18556702\n",
      "Iteration 44, loss = 0.18365394\n",
      "Iteration 45, loss = 0.18222529\n",
      "Iteration 46, loss = 0.18032926\n",
      "Iteration 47, loss = 0.17797168\n",
      "Iteration 48, loss = 0.17621694\n",
      "Iteration 49, loss = 0.17432117\n",
      "Iteration 50, loss = 0.17204331\n",
      "Iteration 51, loss = 0.17063089\n",
      "Iteration 52, loss = 0.16831845\n",
      "Iteration 53, loss = 0.16638090\n",
      "Iteration 54, loss = 0.16453212\n",
      "Iteration 55, loss = 0.16234241\n",
      "Iteration 56, loss = 0.16092075\n",
      "Iteration 57, loss = 0.15840103\n",
      "Iteration 58, loss = 0.15678199\n",
      "Iteration 59, loss = 0.15526817\n",
      "Iteration 60, loss = 0.15318683\n",
      "Iteration 61, loss = 0.15127685\n",
      "Iteration 62, loss = 0.14938787\n",
      "Iteration 63, loss = 0.14812750\n",
      "Iteration 64, loss = 0.14588388\n",
      "Iteration 65, loss = 0.14365644\n",
      "Iteration 66, loss = 0.14271048\n",
      "Iteration 67, loss = 0.14070579\n",
      "Iteration 68, loss = 0.13926772\n",
      "Iteration 69, loss = 0.13716671\n",
      "Iteration 70, loss = 0.13562041\n",
      "Iteration 71, loss = 0.13439276\n",
      "Iteration 72, loss = 0.13220567\n",
      "Iteration 73, loss = 0.13013628\n",
      "Iteration 74, loss = 0.12846378\n",
      "Iteration 75, loss = 0.12787373\n",
      "Iteration 76, loss = 0.12630382\n",
      "Iteration 77, loss = 0.12408131\n",
      "Iteration 78, loss = 0.12332694\n",
      "Iteration 79, loss = 0.12109516\n",
      "Iteration 80, loss = 0.11973904\n",
      "Iteration 81, loss = 0.11774075\n",
      "Iteration 82, loss = 0.11722001\n",
      "Iteration 83, loss = 0.11521478\n",
      "Iteration 84, loss = 0.11342915\n",
      "Iteration 85, loss = 0.11283877\n",
      "Iteration 86, loss = 0.11130390\n",
      "Iteration 87, loss = 0.10943886\n",
      "Iteration 88, loss = 0.10852101\n",
      "Iteration 89, loss = 0.10691614\n",
      "Iteration 90, loss = 0.10578075\n",
      "Iteration 91, loss = 0.10433764\n",
      "Iteration 92, loss = 0.10270608\n",
      "Iteration 93, loss = 0.10185664\n",
      "Iteration 94, loss = 0.10047802\n",
      "Iteration 95, loss = 0.09962424\n",
      "Iteration 96, loss = 0.09776536\n",
      "Iteration 97, loss = 0.09650408\n",
      "Iteration 98, loss = 0.09563899\n",
      "Iteration 99, loss = 0.09468422\n",
      "Iteration 100, loss = 0.09361371\n",
      "Iteration 101, loss = 0.09224624\n",
      "Iteration 102, loss = 0.09105254\n",
      "Iteration 103, loss = 0.09023056\n",
      "Iteration 104, loss = 0.08883401\n",
      "Iteration 105, loss = 0.08783007\n",
      "Iteration 106, loss = 0.08659052\n",
      "Iteration 107, loss = 0.08507736\n",
      "Iteration 108, loss = 0.08406673\n",
      "Iteration 109, loss = 0.08371473\n",
      "Iteration 110, loss = 0.08249352\n",
      "Iteration 111, loss = 0.08102861\n",
      "Iteration 112, loss = 0.08023535\n",
      "Iteration 113, loss = 0.07984085\n",
      "Iteration 114, loss = 0.07823274\n",
      "Iteration 115, loss = 0.07719299\n",
      "Iteration 116, loss = 0.07637199\n",
      "Iteration 117, loss = 0.07500054\n",
      "Iteration 118, loss = 0.07402570\n",
      "Iteration 119, loss = 0.07346454\n",
      "Iteration 120, loss = 0.07247876\n",
      "Iteration 121, loss = 0.07140635\n",
      "Iteration 122, loss = 0.07059673\n",
      "Iteration 123, loss = 0.06983860\n",
      "Iteration 124, loss = 0.06937934\n",
      "Iteration 125, loss = 0.06744731\n",
      "Iteration 126, loss = 0.06688251\n",
      "Iteration 127, loss = 0.06587269\n",
      "Iteration 128, loss = 0.06532561\n",
      "Iteration 129, loss = 0.06467371\n",
      "Iteration 130, loss = 0.06360859\n",
      "Iteration 131, loss = 0.06257094\n",
      "Iteration 132, loss = 0.06212131\n",
      "Iteration 133, loss = 0.06071056\n",
      "Iteration 134, loss = 0.06049942\n",
      "Iteration 135, loss = 0.05951440\n",
      "Iteration 136, loss = 0.05853709\n",
      "Iteration 137, loss = 0.05798322\n",
      "Iteration 138, loss = 0.05735191\n",
      "Iteration 139, loss = 0.05608779\n",
      "Iteration 140, loss = 0.05549285\n",
      "Iteration 141, loss = 0.05494618\n",
      "Iteration 142, loss = 0.05397057\n",
      "Iteration 143, loss = 0.05317433\n",
      "Iteration 144, loss = 0.05300093\n",
      "Iteration 145, loss = 0.05179924\n",
      "Iteration 146, loss = 0.05147605\n",
      "Iteration 147, loss = 0.05039962\n",
      "Iteration 148, loss = 0.04982670\n",
      "Iteration 149, loss = 0.04894895\n",
      "Iteration 150, loss = 0.04844980\n",
      "Iteration 151, loss = 0.04794983\n",
      "Iteration 152, loss = 0.04777668\n",
      "Iteration 153, loss = 0.04646037\n",
      "Iteration 154, loss = 0.04574953\n",
      "Iteration 155, loss = 0.04529281\n",
      "Iteration 156, loss = 0.04457666\n",
      "Iteration 157, loss = 0.04382851\n",
      "Iteration 158, loss = 0.04330744\n",
      "Iteration 159, loss = 0.04265979\n",
      "Iteration 160, loss = 0.04233842\n",
      "Iteration 161, loss = 0.04163910\n",
      "Iteration 162, loss = 0.04092038\n",
      "Iteration 163, loss = 0.04046158\n",
      "Iteration 164, loss = 0.03979725\n",
      "Iteration 165, loss = 0.03942476\n",
      "Iteration 166, loss = 0.03879825\n",
      "Iteration 167, loss = 0.03805701\n",
      "Iteration 168, loss = 0.03789977\n",
      "Iteration 169, loss = 0.03704744\n",
      "Iteration 170, loss = 0.03641811\n",
      "Iteration 171, loss = 0.03633504\n",
      "Iteration 172, loss = 0.03555517\n",
      "Iteration 173, loss = 0.03500316\n",
      "Iteration 174, loss = 0.03461498\n",
      "Iteration 175, loss = 0.03437599\n",
      "Iteration 176, loss = 0.03363748\n",
      "Iteration 177, loss = 0.03361048\n",
      "Iteration 178, loss = 0.03296227\n",
      "Iteration 179, loss = 0.03234411\n",
      "Iteration 180, loss = 0.03179581\n",
      "Iteration 181, loss = 0.03114234\n",
      "Iteration 182, loss = 0.03081868\n",
      "Iteration 183, loss = 0.03026749\n",
      "Iteration 184, loss = 0.03010627\n",
      "Iteration 185, loss = 0.02935921\n",
      "Iteration 186, loss = 0.02888290\n",
      "Iteration 187, loss = 0.02887736\n",
      "Iteration 188, loss = 0.02828344\n",
      "Iteration 189, loss = 0.02767970\n",
      "Iteration 190, loss = 0.02743772\n",
      "Iteration 191, loss = 0.02729730\n",
      "Iteration 192, loss = 0.02678868\n",
      "Iteration 193, loss = 0.02657804\n",
      "Iteration 194, loss = 0.02584012\n",
      "Iteration 195, loss = 0.02548537\n",
      "Iteration 196, loss = 0.02515282\n",
      "Iteration 197, loss = 0.02493745\n",
      "Iteration 198, loss = 0.02452793\n",
      "Iteration 199, loss = 0.02414946\n",
      "Iteration 200, loss = 0.02392390\n",
      "Iteration 1, loss = 0.71906259\n",
      "Iteration 2, loss = 0.51368735\n",
      "Iteration 3, loss = 0.41234431\n",
      "Iteration 4, loss = 0.34804736\n",
      "Iteration 5, loss = 0.30779447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.28158248\n",
      "Iteration 7, loss = 0.26536926\n",
      "Iteration 8, loss = 0.25421010\n",
      "Iteration 9, loss = 0.24616072\n",
      "Iteration 10, loss = 0.24006016\n",
      "Iteration 11, loss = 0.23644336\n",
      "Iteration 12, loss = 0.23322218\n",
      "Iteration 13, loss = 0.22952956\n",
      "Iteration 14, loss = 0.22746940\n",
      "Iteration 15, loss = 0.22535161\n",
      "Iteration 16, loss = 0.22340274\n",
      "Iteration 17, loss = 0.22190997\n",
      "Iteration 18, loss = 0.22095135\n",
      "Iteration 19, loss = 0.21929920\n",
      "Iteration 20, loss = 0.21764276\n",
      "Iteration 21, loss = 0.21623462\n",
      "Iteration 22, loss = 0.21490759\n",
      "Iteration 23, loss = 0.21378628\n",
      "Iteration 24, loss = 0.21237582\n",
      "Iteration 25, loss = 0.21087248\n",
      "Iteration 26, loss = 0.20935747\n",
      "Iteration 27, loss = 0.20835595\n",
      "Iteration 28, loss = 0.20720063\n",
      "Iteration 29, loss = 0.20566211\n",
      "Iteration 30, loss = 0.20363238\n",
      "Iteration 31, loss = 0.20218908\n",
      "Iteration 32, loss = 0.20005740\n",
      "Iteration 33, loss = 0.19875094\n",
      "Iteration 34, loss = 0.19714911\n",
      "Iteration 35, loss = 0.19525588\n",
      "Iteration 36, loss = 0.19380132\n",
      "Iteration 37, loss = 0.19200313\n",
      "Iteration 38, loss = 0.18980805\n",
      "Iteration 39, loss = 0.18812811\n",
      "Iteration 40, loss = 0.18669034\n",
      "Iteration 41, loss = 0.18479968\n",
      "Iteration 42, loss = 0.18302402\n",
      "Iteration 43, loss = 0.18067716\n",
      "Iteration 44, loss = 0.17895935\n",
      "Iteration 45, loss = 0.17714846\n",
      "Iteration 46, loss = 0.17502668\n",
      "Iteration 47, loss = 0.17326122\n",
      "Iteration 48, loss = 0.17152347\n",
      "Iteration 49, loss = 0.16937920\n",
      "Iteration 50, loss = 0.16832628\n",
      "Iteration 51, loss = 0.16606759\n",
      "Iteration 52, loss = 0.16439327\n",
      "Iteration 53, loss = 0.16198951\n",
      "Iteration 54, loss = 0.16004255\n",
      "Iteration 55, loss = 0.15840086\n",
      "Iteration 56, loss = 0.15689881\n",
      "Iteration 57, loss = 0.15480649\n",
      "Iteration 58, loss = 0.15305921\n",
      "Iteration 59, loss = 0.15125211\n",
      "Iteration 60, loss = 0.14929503\n",
      "Iteration 61, loss = 0.14745237\n",
      "Iteration 62, loss = 0.14586676\n",
      "Iteration 63, loss = 0.14436907\n",
      "Iteration 64, loss = 0.14267561\n",
      "Iteration 65, loss = 0.14061032\n",
      "Iteration 66, loss = 0.13957997\n",
      "Iteration 67, loss = 0.13766076\n",
      "Iteration 68, loss = 0.13616348\n",
      "Iteration 69, loss = 0.13442011\n",
      "Iteration 70, loss = 0.13320431\n",
      "Iteration 71, loss = 0.13145696\n",
      "Iteration 72, loss = 0.12972040\n",
      "Iteration 73, loss = 0.12810821\n",
      "Iteration 74, loss = 0.12625726\n",
      "Iteration 75, loss = 0.12541263\n",
      "Iteration 76, loss = 0.12355377\n",
      "Iteration 77, loss = 0.12179282\n",
      "Iteration 78, loss = 0.12103845\n",
      "Iteration 79, loss = 0.11947972\n",
      "Iteration 80, loss = 0.11815735\n",
      "Iteration 81, loss = 0.11608116\n",
      "Iteration 82, loss = 0.11547406\n",
      "Iteration 83, loss = 0.11358259\n",
      "Iteration 84, loss = 0.11226933\n",
      "Iteration 85, loss = 0.11091076\n",
      "Iteration 86, loss = 0.11029861\n",
      "Iteration 87, loss = 0.10865298\n",
      "Iteration 88, loss = 0.10761705\n",
      "Iteration 89, loss = 0.10613538\n",
      "Iteration 90, loss = 0.10454499\n",
      "Iteration 91, loss = 0.10326396\n",
      "Iteration 92, loss = 0.10225328\n",
      "Iteration 93, loss = 0.10085197\n",
      "Iteration 94, loss = 0.09977405\n",
      "Iteration 95, loss = 0.09836516\n",
      "Iteration 96, loss = 0.09753948\n",
      "Iteration 97, loss = 0.09620374\n",
      "Iteration 98, loss = 0.09532725\n",
      "Iteration 99, loss = 0.09428135\n",
      "Iteration 100, loss = 0.09346186\n",
      "Iteration 101, loss = 0.09216857\n",
      "Iteration 102, loss = 0.09079958\n",
      "Iteration 103, loss = 0.09015239\n",
      "Iteration 104, loss = 0.08891079\n",
      "Iteration 105, loss = 0.08781633\n",
      "Iteration 106, loss = 0.08646072\n",
      "Iteration 107, loss = 0.08621953\n",
      "Iteration 108, loss = 0.08519572\n",
      "Iteration 109, loss = 0.08418524\n",
      "Iteration 110, loss = 0.08278867\n",
      "Iteration 111, loss = 0.08166869\n",
      "Iteration 112, loss = 0.08063520\n",
      "Iteration 113, loss = 0.07963592\n",
      "Iteration 114, loss = 0.07893392\n",
      "Iteration 115, loss = 0.07754763\n",
      "Iteration 116, loss = 0.07754965\n",
      "Iteration 117, loss = 0.07590807\n",
      "Iteration 118, loss = 0.07507419\n",
      "Iteration 119, loss = 0.07425975\n",
      "Iteration 120, loss = 0.07318646\n",
      "Iteration 121, loss = 0.07268669\n",
      "Iteration 122, loss = 0.07172198\n",
      "Iteration 123, loss = 0.07080654\n",
      "Iteration 124, loss = 0.07033284\n",
      "Iteration 125, loss = 0.06864506\n",
      "Iteration 126, loss = 0.06835023\n",
      "Iteration 127, loss = 0.06758264\n",
      "Iteration 128, loss = 0.06625657\n",
      "Iteration 129, loss = 0.06595267\n",
      "Iteration 130, loss = 0.06476829\n",
      "Iteration 131, loss = 0.06396170\n",
      "Iteration 132, loss = 0.06274056\n",
      "Iteration 133, loss = 0.06222143\n",
      "Iteration 134, loss = 0.06163277\n",
      "Iteration 135, loss = 0.06084159\n",
      "Iteration 136, loss = 0.05995288\n",
      "Iteration 137, loss = 0.05966957\n",
      "Iteration 138, loss = 0.05906019\n",
      "Iteration 139, loss = 0.05762599\n",
      "Iteration 140, loss = 0.05732879\n",
      "Iteration 141, loss = 0.05653605\n",
      "Iteration 142, loss = 0.05579849\n",
      "Iteration 143, loss = 0.05494393\n",
      "Iteration 144, loss = 0.05466299\n",
      "Iteration 145, loss = 0.05342571\n",
      "Iteration 146, loss = 0.05282246\n",
      "Iteration 147, loss = 0.05209380\n",
      "Iteration 148, loss = 0.05182312\n",
      "Iteration 149, loss = 0.05093713\n",
      "Iteration 150, loss = 0.05020667\n",
      "Iteration 151, loss = 0.04958575\n",
      "Iteration 152, loss = 0.04921142\n",
      "Iteration 153, loss = 0.04853139\n",
      "Iteration 154, loss = 0.04763789\n",
      "Iteration 155, loss = 0.04678728\n",
      "Iteration 156, loss = 0.04618624\n",
      "Iteration 157, loss = 0.04553161\n",
      "Iteration 158, loss = 0.04495135\n",
      "Iteration 159, loss = 0.04461628\n",
      "Iteration 160, loss = 0.04377445\n",
      "Iteration 161, loss = 0.04340482\n",
      "Iteration 162, loss = 0.04239007\n",
      "Iteration 163, loss = 0.04216893\n",
      "Iteration 164, loss = 0.04152914\n",
      "Iteration 165, loss = 0.04133426\n",
      "Iteration 166, loss = 0.04046734\n",
      "Iteration 167, loss = 0.04076317\n",
      "Iteration 168, loss = 0.03942813\n",
      "Iteration 169, loss = 0.03916327\n",
      "Iteration 170, loss = 0.03821976\n",
      "Iteration 171, loss = 0.03792154\n",
      "Iteration 172, loss = 0.03711562\n",
      "Iteration 173, loss = 0.03673763\n",
      "Iteration 174, loss = 0.03599043\n",
      "Iteration 175, loss = 0.03600715\n",
      "Iteration 176, loss = 0.03522619\n",
      "Iteration 177, loss = 0.03498039\n",
      "Iteration 178, loss = 0.03420690\n",
      "Iteration 179, loss = 0.03377460\n",
      "Iteration 180, loss = 0.03318701\n",
      "Iteration 181, loss = 0.03271673\n",
      "Iteration 182, loss = 0.03208577\n",
      "Iteration 183, loss = 0.03161615\n",
      "Iteration 184, loss = 0.03152195\n",
      "Iteration 185, loss = 0.03097846\n",
      "Iteration 186, loss = 0.03030444\n",
      "Iteration 187, loss = 0.03017017\n",
      "Iteration 188, loss = 0.02957146\n",
      "Iteration 189, loss = 0.02932253\n",
      "Iteration 190, loss = 0.02872904\n",
      "Iteration 191, loss = 0.02837664\n",
      "Iteration 192, loss = 0.02800841\n",
      "Iteration 193, loss = 0.02778940\n",
      "Iteration 194, loss = 0.02710144\n",
      "Iteration 195, loss = 0.02663399\n",
      "Iteration 196, loss = 0.02638720\n",
      "Iteration 197, loss = 0.02620711\n",
      "Iteration 198, loss = 0.02549507\n",
      "Iteration 199, loss = 0.02520184\n",
      "Iteration 200, loss = 0.02494781\n",
      "Iteration 1, loss = 0.72215615\n",
      "Iteration 2, loss = 0.51622045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.41247219\n",
      "Iteration 4, loss = 0.34678340\n",
      "Iteration 5, loss = 0.30538762\n",
      "Iteration 6, loss = 0.27874681\n",
      "Iteration 7, loss = 0.26178222\n",
      "Iteration 8, loss = 0.25066948\n",
      "Iteration 9, loss = 0.24259375\n",
      "Iteration 10, loss = 0.23696005\n",
      "Iteration 11, loss = 0.23274848\n",
      "Iteration 12, loss = 0.22911572\n",
      "Iteration 13, loss = 0.22649935\n",
      "Iteration 14, loss = 0.22385875\n",
      "Iteration 15, loss = 0.22201645\n",
      "Iteration 16, loss = 0.22076930\n",
      "Iteration 17, loss = 0.21920433\n",
      "Iteration 18, loss = 0.21851229\n",
      "Iteration 19, loss = 0.21682594\n",
      "Iteration 20, loss = 0.21533587\n",
      "Iteration 21, loss = 0.21400349\n",
      "Iteration 22, loss = 0.21256754\n",
      "Iteration 23, loss = 0.21194152\n",
      "Iteration 24, loss = 0.21041443\n",
      "Iteration 25, loss = 0.20889891\n",
      "Iteration 26, loss = 0.20741090\n",
      "Iteration 27, loss = 0.20671650\n",
      "Iteration 28, loss = 0.20522810\n",
      "Iteration 29, loss = 0.20445778\n",
      "Iteration 30, loss = 0.20256779\n",
      "Iteration 31, loss = 0.20130436\n",
      "Iteration 32, loss = 0.19924842\n",
      "Iteration 33, loss = 0.19794397\n",
      "Iteration 34, loss = 0.19626995\n",
      "Iteration 35, loss = 0.19482083\n",
      "Iteration 36, loss = 0.19290347\n",
      "Iteration 37, loss = 0.19171370\n",
      "Iteration 38, loss = 0.18946595\n",
      "Iteration 39, loss = 0.18835383\n",
      "Iteration 40, loss = 0.18751400\n",
      "Iteration 41, loss = 0.18519046\n",
      "Iteration 42, loss = 0.18313650\n",
      "Iteration 43, loss = 0.18077436\n",
      "Iteration 44, loss = 0.17890197\n",
      "Iteration 45, loss = 0.17787837\n",
      "Iteration 46, loss = 0.17546783\n",
      "Iteration 47, loss = 0.17379011\n",
      "Iteration 48, loss = 0.17245817\n",
      "Iteration 49, loss = 0.17018853\n",
      "Iteration 50, loss = 0.16846463\n",
      "Iteration 51, loss = 0.16687887\n",
      "Iteration 52, loss = 0.16511041\n",
      "Iteration 53, loss = 0.16264481\n",
      "Iteration 54, loss = 0.16083711\n",
      "Iteration 55, loss = 0.15903545\n",
      "Iteration 56, loss = 0.15717450\n",
      "Iteration 57, loss = 0.15542235\n",
      "Iteration 58, loss = 0.15376747\n",
      "Iteration 59, loss = 0.15214298\n",
      "Iteration 60, loss = 0.15008978\n",
      "Iteration 61, loss = 0.14847634\n",
      "Iteration 62, loss = 0.14703106\n",
      "Iteration 63, loss = 0.14485504\n",
      "Iteration 64, loss = 0.14381692\n",
      "Iteration 65, loss = 0.14191725\n",
      "Iteration 66, loss = 0.13998870\n",
      "Iteration 67, loss = 0.13807507\n",
      "Iteration 68, loss = 0.13683840\n",
      "Iteration 69, loss = 0.13509515\n",
      "Iteration 70, loss = 0.13384532\n",
      "Iteration 71, loss = 0.13188636\n",
      "Iteration 72, loss = 0.13036436\n",
      "Iteration 73, loss = 0.12891184\n",
      "Iteration 74, loss = 0.12688898\n",
      "Iteration 75, loss = 0.12620392\n",
      "Iteration 76, loss = 0.12457571\n",
      "Iteration 77, loss = 0.12306126\n",
      "Iteration 78, loss = 0.12175567\n",
      "Iteration 79, loss = 0.11986907\n",
      "Iteration 80, loss = 0.11853001\n",
      "Iteration 81, loss = 0.11695023\n",
      "Iteration 82, loss = 0.11550693\n",
      "Iteration 83, loss = 0.11398449\n",
      "Iteration 84, loss = 0.11248053\n",
      "Iteration 85, loss = 0.11100957\n",
      "Iteration 86, loss = 0.11039457\n",
      "Iteration 87, loss = 0.10860244\n",
      "Iteration 88, loss = 0.10768864\n",
      "Iteration 89, loss = 0.10624417\n",
      "Iteration 90, loss = 0.10465458\n",
      "Iteration 91, loss = 0.10339407\n",
      "Iteration 92, loss = 0.10219927\n",
      "Iteration 93, loss = 0.10050095\n",
      "Iteration 94, loss = 0.09968396\n",
      "Iteration 95, loss = 0.09813297\n",
      "Iteration 96, loss = 0.09744058\n",
      "Iteration 97, loss = 0.09563593\n",
      "Iteration 98, loss = 0.09444611\n",
      "Iteration 99, loss = 0.09324422\n",
      "Iteration 100, loss = 0.09246507\n",
      "Iteration 101, loss = 0.09117300\n",
      "Iteration 102, loss = 0.08983352\n",
      "Iteration 103, loss = 0.08872435\n",
      "Iteration 104, loss = 0.08753285\n",
      "Iteration 105, loss = 0.08630176\n",
      "Iteration 106, loss = 0.08525531\n",
      "Iteration 107, loss = 0.08498820\n",
      "Iteration 108, loss = 0.08423295\n",
      "Iteration 109, loss = 0.08245374\n",
      "Iteration 110, loss = 0.08086011\n",
      "Iteration 111, loss = 0.08026785\n",
      "Iteration 112, loss = 0.07922973\n",
      "Iteration 113, loss = 0.07778101\n",
      "Iteration 114, loss = 0.07690295\n",
      "Iteration 115, loss = 0.07580017\n",
      "Iteration 116, loss = 0.07536196\n",
      "Iteration 117, loss = 0.07387336\n",
      "Iteration 118, loss = 0.07301715\n",
      "Iteration 119, loss = 0.07218032\n",
      "Iteration 120, loss = 0.07117761\n",
      "Iteration 121, loss = 0.07109949\n",
      "Iteration 122, loss = 0.06933295\n",
      "Iteration 123, loss = 0.06872120\n",
      "Iteration 124, loss = 0.06775744\n",
      "Iteration 125, loss = 0.06641900\n",
      "Iteration 126, loss = 0.06577927\n",
      "Iteration 127, loss = 0.06506380\n",
      "Iteration 128, loss = 0.06374376\n",
      "Iteration 129, loss = 0.06352985\n",
      "Iteration 130, loss = 0.06232688\n",
      "Iteration 131, loss = 0.06147895\n",
      "Iteration 132, loss = 0.06069228\n",
      "Iteration 133, loss = 0.05998697\n",
      "Iteration 134, loss = 0.05925690\n",
      "Iteration 135, loss = 0.05875135\n",
      "Iteration 136, loss = 0.05784493\n",
      "Iteration 137, loss = 0.05689770\n",
      "Iteration 138, loss = 0.05628601\n",
      "Iteration 139, loss = 0.05531048\n",
      "Iteration 140, loss = 0.05468862\n",
      "Iteration 141, loss = 0.05396484\n",
      "Iteration 142, loss = 0.05298573\n",
      "Iteration 143, loss = 0.05233092\n",
      "Iteration 144, loss = 0.05214808\n",
      "Iteration 145, loss = 0.05117740\n",
      "Iteration 146, loss = 0.05051438\n",
      "Iteration 147, loss = 0.04976729\n",
      "Iteration 148, loss = 0.04921005\n",
      "Iteration 149, loss = 0.04891199\n",
      "Iteration 150, loss = 0.04798565\n",
      "Iteration 151, loss = 0.04771009\n",
      "Iteration 152, loss = 0.04671350\n",
      "Iteration 153, loss = 0.04660571\n",
      "Iteration 154, loss = 0.04570693\n",
      "Iteration 155, loss = 0.04466849\n",
      "Iteration 156, loss = 0.04406639\n",
      "Iteration 157, loss = 0.04316522\n",
      "Iteration 158, loss = 0.04287803\n",
      "Iteration 159, loss = 0.04229605\n",
      "Iteration 160, loss = 0.04144416\n",
      "Iteration 161, loss = 0.04104558\n",
      "Iteration 162, loss = 0.04055031\n",
      "Iteration 163, loss = 0.03983310\n",
      "Iteration 164, loss = 0.03959601\n",
      "Iteration 165, loss = 0.03910076\n",
      "Iteration 166, loss = 0.03836050\n",
      "Iteration 167, loss = 0.03851566\n",
      "Iteration 168, loss = 0.03737482\n",
      "Iteration 169, loss = 0.03716057\n",
      "Iteration 170, loss = 0.03642685\n",
      "Iteration 171, loss = 0.03592651\n",
      "Iteration 172, loss = 0.03522896\n",
      "Iteration 173, loss = 0.03529441\n",
      "Iteration 174, loss = 0.03433670\n",
      "Iteration 175, loss = 0.03401344\n",
      "Iteration 176, loss = 0.03367198\n",
      "Iteration 177, loss = 0.03305631\n",
      "Iteration 178, loss = 0.03243622\n",
      "Iteration 179, loss = 0.03197187\n",
      "Iteration 180, loss = 0.03152365\n",
      "Iteration 181, loss = 0.03108253\n",
      "Iteration 182, loss = 0.03059887\n",
      "Iteration 183, loss = 0.03017767\n",
      "Iteration 184, loss = 0.02989526\n",
      "Iteration 185, loss = 0.02956208\n",
      "Iteration 186, loss = 0.02903636\n",
      "Iteration 187, loss = 0.02862687\n",
      "Iteration 188, loss = 0.02817958\n",
      "Iteration 189, loss = 0.02786518\n",
      "Iteration 190, loss = 0.02742394\n",
      "Iteration 191, loss = 0.02693596\n",
      "Iteration 192, loss = 0.02662994\n",
      "Iteration 193, loss = 0.02643315\n",
      "Iteration 194, loss = 0.02593263\n",
      "Iteration 195, loss = 0.02552099\n",
      "Iteration 196, loss = 0.02529741\n",
      "Iteration 197, loss = 0.02493527\n",
      "Iteration 198, loss = 0.02434774\n",
      "Iteration 199, loss = 0.02406572\n",
      "Iteration 200, loss = 0.02394237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72967515\n",
      "Iteration 2, loss = 0.59496349\n",
      "Iteration 3, loss = 0.52267396\n",
      "Iteration 4, loss = 0.48603518\n",
      "Iteration 5, loss = 0.46360083\n",
      "Iteration 6, loss = 0.44687568\n",
      "Iteration 7, loss = 0.43318800\n",
      "Iteration 8, loss = 0.42139958\n",
      "Iteration 9, loss = 0.41099244\n",
      "Iteration 10, loss = 0.40158733\n",
      "Iteration 11, loss = 0.39294219\n",
      "Iteration 12, loss = 0.38507323\n",
      "Iteration 13, loss = 0.37779074\n",
      "Iteration 14, loss = 0.37106081\n",
      "Iteration 15, loss = 0.36479777\n",
      "Iteration 16, loss = 0.35894824\n",
      "Iteration 17, loss = 0.35352208\n",
      "Iteration 18, loss = 0.34843874\n",
      "Iteration 19, loss = 0.34365369\n",
      "Iteration 20, loss = 0.33909521\n",
      "Iteration 21, loss = 0.33488506\n",
      "Iteration 22, loss = 0.33087924\n",
      "Iteration 23, loss = 0.32716166\n",
      "Iteration 24, loss = 0.32350217\n",
      "Iteration 25, loss = 0.32018488\n",
      "Iteration 26, loss = 0.31690850\n",
      "Iteration 27, loss = 0.31384015\n",
      "Iteration 28, loss = 0.31094233\n",
      "Iteration 29, loss = 0.30813766\n",
      "Iteration 30, loss = 0.30552326\n",
      "Iteration 31, loss = 0.30299643\n",
      "Iteration 32, loss = 0.30058484\n",
      "Iteration 33, loss = 0.29836358\n",
      "Iteration 34, loss = 0.29610344\n",
      "Iteration 35, loss = 0.29400862\n",
      "Iteration 36, loss = 0.29202058\n",
      "Iteration 37, loss = 0.29004289\n",
      "Iteration 38, loss = 0.28822140\n",
      "Iteration 39, loss = 0.28642563\n",
      "Iteration 40, loss = 0.28472328\n",
      "Iteration 41, loss = 0.28306945\n",
      "Iteration 42, loss = 0.28151655\n",
      "Iteration 43, loss = 0.27999753\n",
      "Iteration 44, loss = 0.27852785\n",
      "Iteration 45, loss = 0.27713756\n",
      "Iteration 46, loss = 0.27583600\n",
      "Iteration 47, loss = 0.27450975\n",
      "Iteration 48, loss = 0.27330732\n",
      "Iteration 49, loss = 0.27210031\n",
      "Iteration 50, loss = 0.27091533\n",
      "Iteration 51, loss = 0.26981104\n",
      "Iteration 52, loss = 0.26873697\n",
      "Iteration 53, loss = 0.26770050\n",
      "Iteration 54, loss = 0.26668517\n",
      "Iteration 55, loss = 0.26570769\n",
      "Iteration 56, loss = 0.26477348\n",
      "Iteration 57, loss = 0.26386535\n",
      "Iteration 58, loss = 0.26299075\n",
      "Iteration 59, loss = 0.26214345\n",
      "Iteration 60, loss = 0.26131226\n",
      "Iteration 61, loss = 0.26052288\n",
      "Iteration 62, loss = 0.25974440\n",
      "Iteration 63, loss = 0.25898880\n",
      "Iteration 64, loss = 0.25823875\n",
      "Iteration 65, loss = 0.25751844\n",
      "Iteration 66, loss = 0.25683288\n",
      "Iteration 67, loss = 0.25612261\n",
      "Iteration 68, loss = 0.25548442\n",
      "Iteration 69, loss = 0.25479196\n",
      "Iteration 70, loss = 0.25416262\n",
      "Iteration 71, loss = 0.25355139\n",
      "Iteration 72, loss = 0.25292892\n",
      "Iteration 73, loss = 0.25232183\n",
      "Iteration 74, loss = 0.25173352\n",
      "Iteration 75, loss = 0.25114753\n",
      "Iteration 76, loss = 0.25058930\n",
      "Iteration 77, loss = 0.25001407\n",
      "Iteration 78, loss = 0.24946835\n",
      "Iteration 79, loss = 0.24894282\n",
      "Iteration 80, loss = 0.24838588\n",
      "Iteration 81, loss = 0.24788959\n",
      "Iteration 82, loss = 0.24736023\n",
      "Iteration 83, loss = 0.24686764\n",
      "Iteration 84, loss = 0.24638570\n",
      "Iteration 85, loss = 0.24589638\n",
      "Iteration 86, loss = 0.24540381\n",
      "Iteration 87, loss = 0.24493713\n",
      "Iteration 88, loss = 0.24445836\n",
      "Iteration 89, loss = 0.24401059\n",
      "Iteration 90, loss = 0.24354564\n",
      "Iteration 91, loss = 0.24309945\n",
      "Iteration 92, loss = 0.24265059\n",
      "Iteration 93, loss = 0.24220520\n",
      "Iteration 94, loss = 0.24179482\n",
      "Iteration 95, loss = 0.24134619\n",
      "Iteration 96, loss = 0.24093313\n",
      "Iteration 97, loss = 0.24051270\n",
      "Iteration 98, loss = 0.24009681\n",
      "Iteration 99, loss = 0.23970027\n",
      "Iteration 100, loss = 0.23927917\n",
      "Iteration 101, loss = 0.23888870\n",
      "Iteration 102, loss = 0.23850412\n",
      "Iteration 103, loss = 0.23808996\n",
      "Iteration 104, loss = 0.23770163\n",
      "Iteration 105, loss = 0.23733672\n",
      "Iteration 106, loss = 0.23698955\n",
      "Iteration 107, loss = 0.23655183\n",
      "Iteration 108, loss = 0.23620925\n",
      "Iteration 109, loss = 0.23582105\n",
      "Iteration 110, loss = 0.23545277\n",
      "Iteration 111, loss = 0.23508471\n",
      "Iteration 112, loss = 0.23472944\n",
      "Iteration 113, loss = 0.23437848\n",
      "Iteration 114, loss = 0.23401532\n",
      "Iteration 115, loss = 0.23364535\n",
      "Iteration 116, loss = 0.23329638\n",
      "Iteration 117, loss = 0.23295389\n",
      "Iteration 118, loss = 0.23258596\n",
      "Iteration 119, loss = 0.23224601\n",
      "Iteration 120, loss = 0.23190709\n",
      "Iteration 121, loss = 0.23157753\n",
      "Iteration 122, loss = 0.23123151\n",
      "Iteration 123, loss = 0.23088911\n",
      "Iteration 124, loss = 0.23055133\n",
      "Iteration 125, loss = 0.23022509\n",
      "Iteration 126, loss = 0.22987881\n",
      "Iteration 127, loss = 0.22955877\n",
      "Iteration 128, loss = 0.22922846\n",
      "Iteration 129, loss = 0.22890830\n",
      "Iteration 130, loss = 0.22856634\n",
      "Iteration 131, loss = 0.22825019\n",
      "Iteration 132, loss = 0.22791357\n",
      "Iteration 133, loss = 0.22757762\n",
      "Iteration 134, loss = 0.22729005\n",
      "Iteration 135, loss = 0.22694849\n",
      "Iteration 136, loss = 0.22665351\n",
      "Iteration 137, loss = 0.22631780\n",
      "Iteration 138, loss = 0.22600891\n",
      "Iteration 139, loss = 0.22570425\n",
      "Iteration 140, loss = 0.22539231\n",
      "Iteration 141, loss = 0.22508149\n",
      "Iteration 142, loss = 0.22478908\n",
      "Iteration 143, loss = 0.22446510\n",
      "Iteration 144, loss = 0.22417731\n",
      "Iteration 145, loss = 0.22386600\n",
      "Iteration 146, loss = 0.22357747\n",
      "Iteration 147, loss = 0.22325232\n",
      "Iteration 148, loss = 0.22295496\n",
      "Iteration 149, loss = 0.22265045\n",
      "Iteration 150, loss = 0.22235511\n",
      "Iteration 151, loss = 0.22207129\n",
      "Iteration 152, loss = 0.22177714\n",
      "Iteration 153, loss = 0.22148772\n",
      "Iteration 154, loss = 0.22121101\n",
      "Iteration 155, loss = 0.22089977\n",
      "Iteration 156, loss = 0.22063146\n",
      "Iteration 157, loss = 0.22032830\n",
      "Iteration 158, loss = 0.22004154\n",
      "Iteration 159, loss = 0.21975948\n",
      "Iteration 160, loss = 0.21948630\n",
      "Iteration 161, loss = 0.21918051\n",
      "Iteration 162, loss = 0.21890949\n",
      "Iteration 163, loss = 0.21862583\n",
      "Iteration 164, loss = 0.21834256\n",
      "Iteration 165, loss = 0.21808627\n",
      "Iteration 166, loss = 0.21780285\n",
      "Iteration 167, loss = 0.21756027\n",
      "Iteration 168, loss = 0.21726407\n",
      "Iteration 169, loss = 0.21698194\n",
      "Iteration 170, loss = 0.21672310\n",
      "Iteration 171, loss = 0.21643688\n",
      "Iteration 172, loss = 0.21616610\n",
      "Iteration 173, loss = 0.21589673\n",
      "Iteration 174, loss = 0.21562858\n",
      "Iteration 175, loss = 0.21538780\n",
      "Iteration 176, loss = 0.21510378\n",
      "Iteration 177, loss = 0.21482038\n",
      "Iteration 178, loss = 0.21456311\n",
      "Iteration 179, loss = 0.21430086\n",
      "Iteration 180, loss = 0.21402932\n",
      "Iteration 181, loss = 0.21376015\n",
      "Iteration 182, loss = 0.21352900\n",
      "Iteration 183, loss = 0.21325617\n",
      "Iteration 184, loss = 0.21301007\n",
      "Iteration 185, loss = 0.21273966\n",
      "Iteration 186, loss = 0.21248057\n",
      "Iteration 187, loss = 0.21222890\n",
      "Iteration 188, loss = 0.21197692\n",
      "Iteration 189, loss = 0.21171181\n",
      "Iteration 190, loss = 0.21145247\n",
      "Iteration 191, loss = 0.21120283\n",
      "Iteration 192, loss = 0.21095948\n",
      "Iteration 193, loss = 0.21069879\n",
      "Iteration 194, loss = 0.21044047\n",
      "Iteration 195, loss = 0.21019854\n",
      "Iteration 196, loss = 0.20994756\n",
      "Iteration 197, loss = 0.20970925\n",
      "Iteration 198, loss = 0.20942679\n",
      "Iteration 199, loss = 0.20920456\n",
      "Iteration 200, loss = 0.20895829\n",
      "Iteration 1, loss = 0.73016497\n",
      "Iteration 2, loss = 0.59469330\n",
      "Iteration 3, loss = 0.52194651\n",
      "Iteration 4, loss = 0.48495748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.46195429\n",
      "Iteration 6, loss = 0.44506392\n",
      "Iteration 7, loss = 0.43107907\n",
      "Iteration 8, loss = 0.41908235\n",
      "Iteration 9, loss = 0.40847157\n",
      "Iteration 10, loss = 0.39897103\n",
      "Iteration 11, loss = 0.39008481\n",
      "Iteration 12, loss = 0.38215268\n",
      "Iteration 13, loss = 0.37471941\n",
      "Iteration 14, loss = 0.36787484\n",
      "Iteration 15, loss = 0.36152775\n",
      "Iteration 16, loss = 0.35563352\n",
      "Iteration 17, loss = 0.35014521\n",
      "Iteration 18, loss = 0.34497516\n",
      "Iteration 19, loss = 0.34014470\n",
      "Iteration 20, loss = 0.33556125\n",
      "Iteration 21, loss = 0.33133248\n",
      "Iteration 22, loss = 0.32730253\n",
      "Iteration 23, loss = 0.32355588\n",
      "Iteration 24, loss = 0.31993990\n",
      "Iteration 25, loss = 0.31655647\n",
      "Iteration 26, loss = 0.31331544\n",
      "Iteration 27, loss = 0.31026913\n",
      "Iteration 28, loss = 0.30737392\n",
      "Iteration 29, loss = 0.30461104\n",
      "Iteration 30, loss = 0.30199895\n",
      "Iteration 31, loss = 0.29949049\n",
      "Iteration 32, loss = 0.29712140\n",
      "Iteration 33, loss = 0.29487808\n",
      "Iteration 34, loss = 0.29267368\n",
      "Iteration 35, loss = 0.29056947\n",
      "Iteration 36, loss = 0.28859007\n",
      "Iteration 37, loss = 0.28664964\n",
      "Iteration 38, loss = 0.28481805\n",
      "Iteration 39, loss = 0.28305051\n",
      "Iteration 40, loss = 0.28134725\n",
      "Iteration 41, loss = 0.27971418\n",
      "Iteration 42, loss = 0.27817829\n",
      "Iteration 43, loss = 0.27663725\n",
      "Iteration 44, loss = 0.27518640\n",
      "Iteration 45, loss = 0.27373794\n",
      "Iteration 46, loss = 0.27239852\n",
      "Iteration 47, loss = 0.27106637\n",
      "Iteration 48, loss = 0.26980275\n",
      "Iteration 49, loss = 0.26858711\n",
      "Iteration 50, loss = 0.26737730\n",
      "Iteration 51, loss = 0.26626073\n",
      "Iteration 52, loss = 0.26514494\n",
      "Iteration 53, loss = 0.26407676\n",
      "Iteration 54, loss = 0.26303844\n",
      "Iteration 55, loss = 0.26203322\n",
      "Iteration 56, loss = 0.26106552\n",
      "Iteration 57, loss = 0.26014805\n",
      "Iteration 58, loss = 0.25923131\n",
      "Iteration 59, loss = 0.25834289\n",
      "Iteration 60, loss = 0.25748318\n",
      "Iteration 61, loss = 0.25665955\n",
      "Iteration 62, loss = 0.25584812\n",
      "Iteration 63, loss = 0.25507876\n",
      "Iteration 64, loss = 0.25431840\n",
      "Iteration 65, loss = 0.25358718\n",
      "Iteration 66, loss = 0.25285067\n",
      "Iteration 67, loss = 0.25214093\n",
      "Iteration 68, loss = 0.25148354\n",
      "Iteration 69, loss = 0.25077363\n",
      "Iteration 70, loss = 0.25011821\n",
      "Iteration 71, loss = 0.24950789\n",
      "Iteration 72, loss = 0.24886162\n",
      "Iteration 73, loss = 0.24825035\n",
      "Iteration 74, loss = 0.24765504\n",
      "Iteration 75, loss = 0.24706282\n",
      "Iteration 76, loss = 0.24647462\n",
      "Iteration 77, loss = 0.24590456\n",
      "Iteration 78, loss = 0.24536379\n",
      "Iteration 79, loss = 0.24481621\n",
      "Iteration 80, loss = 0.24424974\n",
      "Iteration 81, loss = 0.24373390\n",
      "Iteration 82, loss = 0.24320876\n",
      "Iteration 83, loss = 0.24271470\n",
      "Iteration 84, loss = 0.24220338\n",
      "Iteration 85, loss = 0.24172144\n",
      "Iteration 86, loss = 0.24122724\n",
      "Iteration 87, loss = 0.24077606\n",
      "Iteration 88, loss = 0.24025846\n",
      "Iteration 89, loss = 0.23982493\n",
      "Iteration 90, loss = 0.23936230\n",
      "Iteration 91, loss = 0.23890329\n",
      "Iteration 92, loss = 0.23844928\n",
      "Iteration 93, loss = 0.23800721\n",
      "Iteration 94, loss = 0.23758197\n",
      "Iteration 95, loss = 0.23713636\n",
      "Iteration 96, loss = 0.23671078\n",
      "Iteration 97, loss = 0.23630121\n",
      "Iteration 98, loss = 0.23586285\n",
      "Iteration 99, loss = 0.23547859\n",
      "Iteration 100, loss = 0.23504530\n",
      "Iteration 101, loss = 0.23463872\n",
      "Iteration 102, loss = 0.23423834\n",
      "Iteration 103, loss = 0.23385375\n",
      "Iteration 104, loss = 0.23344130\n",
      "Iteration 105, loss = 0.23305669\n",
      "Iteration 106, loss = 0.23271060\n",
      "Iteration 107, loss = 0.23228804\n",
      "Iteration 108, loss = 0.23191204\n",
      "Iteration 109, loss = 0.23154112\n",
      "Iteration 110, loss = 0.23115869\n",
      "Iteration 111, loss = 0.23079293\n",
      "Iteration 112, loss = 0.23041418\n",
      "Iteration 113, loss = 0.23006224\n",
      "Iteration 114, loss = 0.22972067\n",
      "Iteration 115, loss = 0.22931879\n",
      "Iteration 116, loss = 0.22897441\n",
      "Iteration 117, loss = 0.22860650\n",
      "Iteration 118, loss = 0.22826408\n",
      "Iteration 119, loss = 0.22791106\n",
      "Iteration 120, loss = 0.22757614\n",
      "Iteration 121, loss = 0.22721944\n",
      "Iteration 122, loss = 0.22688612\n",
      "Iteration 123, loss = 0.22652369\n",
      "Iteration 124, loss = 0.22619288\n",
      "Iteration 125, loss = 0.22584482\n",
      "Iteration 126, loss = 0.22552537\n",
      "Iteration 127, loss = 0.22518213\n",
      "Iteration 128, loss = 0.22486653\n",
      "Iteration 129, loss = 0.22453028\n",
      "Iteration 130, loss = 0.22418298\n",
      "Iteration 131, loss = 0.22385269\n",
      "Iteration 132, loss = 0.22354413\n",
      "Iteration 133, loss = 0.22320195\n",
      "Iteration 134, loss = 0.22288925\n",
      "Iteration 135, loss = 0.22256650\n",
      "Iteration 136, loss = 0.22224665\n",
      "Iteration 137, loss = 0.22191235\n",
      "Iteration 138, loss = 0.22161974\n",
      "Iteration 139, loss = 0.22129461\n",
      "Iteration 140, loss = 0.22098924\n",
      "Iteration 141, loss = 0.22068039\n",
      "Iteration 142, loss = 0.22036836\n",
      "Iteration 143, loss = 0.22005045\n",
      "Iteration 144, loss = 0.21977291\n",
      "Iteration 145, loss = 0.21944309\n",
      "Iteration 146, loss = 0.21914195\n",
      "Iteration 147, loss = 0.21884314\n",
      "Iteration 148, loss = 0.21853491\n",
      "Iteration 149, loss = 0.21822994\n",
      "Iteration 150, loss = 0.21793967\n",
      "Iteration 151, loss = 0.21764107\n",
      "Iteration 152, loss = 0.21736028\n",
      "Iteration 153, loss = 0.21706705\n",
      "Iteration 154, loss = 0.21677570\n",
      "Iteration 155, loss = 0.21648312\n",
      "Iteration 156, loss = 0.21621961\n",
      "Iteration 157, loss = 0.21591602\n",
      "Iteration 158, loss = 0.21561733\n",
      "Iteration 159, loss = 0.21534834\n",
      "Iteration 160, loss = 0.21507417\n",
      "Iteration 161, loss = 0.21477829\n",
      "Iteration 162, loss = 0.21447863\n",
      "Iteration 163, loss = 0.21421225\n",
      "Iteration 164, loss = 0.21392443\n",
      "Iteration 165, loss = 0.21366421\n",
      "Iteration 166, loss = 0.21337032\n",
      "Iteration 167, loss = 0.21311809\n",
      "Iteration 168, loss = 0.21282110\n",
      "Iteration 169, loss = 0.21255207\n",
      "Iteration 170, loss = 0.21228331\n",
      "Iteration 171, loss = 0.21199603\n",
      "Iteration 172, loss = 0.21173158\n",
      "Iteration 173, loss = 0.21146196\n",
      "Iteration 174, loss = 0.21118243\n",
      "Iteration 175, loss = 0.21092039\n",
      "Iteration 176, loss = 0.21066977\n",
      "Iteration 177, loss = 0.21037598\n",
      "Iteration 178, loss = 0.21009020\n",
      "Iteration 179, loss = 0.20984659\n",
      "Iteration 180, loss = 0.20956610\n",
      "Iteration 181, loss = 0.20929495\n",
      "Iteration 182, loss = 0.20904426\n",
      "Iteration 183, loss = 0.20878338\n",
      "Iteration 184, loss = 0.20853286\n",
      "Iteration 185, loss = 0.20825724\n",
      "Iteration 186, loss = 0.20800715\n",
      "Iteration 187, loss = 0.20775403\n",
      "Iteration 188, loss = 0.20750879\n",
      "Iteration 189, loss = 0.20724016\n",
      "Iteration 190, loss = 0.20698467\n",
      "Iteration 191, loss = 0.20673869\n",
      "Iteration 192, loss = 0.20649234\n",
      "Iteration 193, loss = 0.20623821\n",
      "Iteration 194, loss = 0.20598891\n",
      "Iteration 195, loss = 0.20574729\n",
      "Iteration 196, loss = 0.20551250\n",
      "Iteration 197, loss = 0.20526056\n",
      "Iteration 198, loss = 0.20500924\n",
      "Iteration 199, loss = 0.20477873\n",
      "Iteration 200, loss = 0.20452477\n",
      "Iteration 1, loss = 0.73357680\n",
      "Iteration 2, loss = 0.59795252\n",
      "Iteration 3, loss = 0.52619670\n",
      "Iteration 4, loss = 0.48966843\n",
      "Iteration 5, loss = 0.46696666\n",
      "Iteration 6, loss = 0.45016318\n",
      "Iteration 7, loss = 0.43642280\n",
      "Iteration 8, loss = 0.42460146\n",
      "Iteration 9, loss = 0.41411600\n",
      "Iteration 10, loss = 0.40465310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39593745\n",
      "Iteration 12, loss = 0.38808816\n",
      "Iteration 13, loss = 0.38072010\n",
      "Iteration 14, loss = 0.37395850\n",
      "Iteration 15, loss = 0.36762905\n",
      "Iteration 16, loss = 0.36178655\n",
      "Iteration 17, loss = 0.35633681\n",
      "Iteration 18, loss = 0.35117674\n",
      "Iteration 19, loss = 0.34639423\n",
      "Iteration 20, loss = 0.34184409\n",
      "Iteration 21, loss = 0.33759442\n",
      "Iteration 22, loss = 0.33361576\n",
      "Iteration 23, loss = 0.32984179\n",
      "Iteration 24, loss = 0.32626554\n",
      "Iteration 25, loss = 0.32288766\n",
      "Iteration 26, loss = 0.31964200\n",
      "Iteration 27, loss = 0.31661408\n",
      "Iteration 28, loss = 0.31369329\n",
      "Iteration 29, loss = 0.31094233\n",
      "Iteration 30, loss = 0.30832083\n",
      "Iteration 31, loss = 0.30581275\n",
      "Iteration 32, loss = 0.30341467\n",
      "Iteration 33, loss = 0.30113481\n",
      "Iteration 34, loss = 0.29891759\n",
      "Iteration 35, loss = 0.29683815\n",
      "Iteration 36, loss = 0.29479691\n",
      "Iteration 37, loss = 0.29288295\n",
      "Iteration 38, loss = 0.29102878\n",
      "Iteration 39, loss = 0.28924011\n",
      "Iteration 40, loss = 0.28755582\n",
      "Iteration 41, loss = 0.28590074\n",
      "Iteration 42, loss = 0.28434042\n",
      "Iteration 43, loss = 0.28278559\n",
      "Iteration 44, loss = 0.28131498\n",
      "Iteration 45, loss = 0.27990490\n",
      "Iteration 46, loss = 0.27854964\n",
      "Iteration 47, loss = 0.27722497\n",
      "Iteration 48, loss = 0.27593850\n",
      "Iteration 49, loss = 0.27472677\n",
      "Iteration 50, loss = 0.27350691\n",
      "Iteration 51, loss = 0.27242874\n",
      "Iteration 52, loss = 0.27127329\n",
      "Iteration 53, loss = 0.27019426\n",
      "Iteration 54, loss = 0.26916949\n",
      "Iteration 55, loss = 0.26815098\n",
      "Iteration 56, loss = 0.26719611\n",
      "Iteration 57, loss = 0.26625107\n",
      "Iteration 58, loss = 0.26534740\n",
      "Iteration 59, loss = 0.26443709\n",
      "Iteration 60, loss = 0.26357459\n",
      "Iteration 61, loss = 0.26273935\n",
      "Iteration 62, loss = 0.26192497\n",
      "Iteration 63, loss = 0.26113885\n",
      "Iteration 64, loss = 0.26036693\n",
      "Iteration 65, loss = 0.25961751\n",
      "Iteration 66, loss = 0.25887745\n",
      "Iteration 67, loss = 0.25818925\n",
      "Iteration 68, loss = 0.25747789\n",
      "Iteration 69, loss = 0.25678372\n",
      "Iteration 70, loss = 0.25612640\n",
      "Iteration 71, loss = 0.25548044\n",
      "Iteration 72, loss = 0.25483342\n",
      "Iteration 73, loss = 0.25420434\n",
      "Iteration 74, loss = 0.25359126\n",
      "Iteration 75, loss = 0.25299439\n",
      "Iteration 76, loss = 0.25241680\n",
      "Iteration 77, loss = 0.25180930\n",
      "Iteration 78, loss = 0.25124698\n",
      "Iteration 79, loss = 0.25069549\n",
      "Iteration 80, loss = 0.25013422\n",
      "Iteration 81, loss = 0.24958869\n",
      "Iteration 82, loss = 0.24905591\n",
      "Iteration 83, loss = 0.24854193\n",
      "Iteration 84, loss = 0.24801386\n",
      "Iteration 85, loss = 0.24749825\n",
      "Iteration 86, loss = 0.24699469\n",
      "Iteration 87, loss = 0.24650974\n",
      "Iteration 88, loss = 0.24599038\n",
      "Iteration 89, loss = 0.24551823\n",
      "Iteration 90, loss = 0.24504262\n",
      "Iteration 91, loss = 0.24457789\n",
      "Iteration 92, loss = 0.24409162\n",
      "Iteration 93, loss = 0.24365688\n",
      "Iteration 94, loss = 0.24319782\n",
      "Iteration 95, loss = 0.24273734\n",
      "Iteration 96, loss = 0.24230912\n",
      "Iteration 97, loss = 0.24185495\n",
      "Iteration 98, loss = 0.24142671\n",
      "Iteration 99, loss = 0.24100024\n",
      "Iteration 100, loss = 0.24057707\n",
      "Iteration 101, loss = 0.24013371\n",
      "Iteration 102, loss = 0.23971385\n",
      "Iteration 103, loss = 0.23931777\n",
      "Iteration 104, loss = 0.23888516\n",
      "Iteration 105, loss = 0.23848580\n",
      "Iteration 106, loss = 0.23809197\n",
      "Iteration 107, loss = 0.23767378\n",
      "Iteration 108, loss = 0.23725825\n",
      "Iteration 109, loss = 0.23690598\n",
      "Iteration 110, loss = 0.23647334\n",
      "Iteration 111, loss = 0.23609142\n",
      "Iteration 112, loss = 0.23568833\n",
      "Iteration 113, loss = 0.23531812\n",
      "Iteration 114, loss = 0.23492767\n",
      "Iteration 115, loss = 0.23454804\n",
      "Iteration 116, loss = 0.23415496\n",
      "Iteration 117, loss = 0.23377781\n",
      "Iteration 118, loss = 0.23339762\n",
      "Iteration 119, loss = 0.23302564\n",
      "Iteration 120, loss = 0.23265216\n",
      "Iteration 121, loss = 0.23227979\n",
      "Iteration 122, loss = 0.23192028\n",
      "Iteration 123, loss = 0.23155390\n",
      "Iteration 124, loss = 0.23119528\n",
      "Iteration 125, loss = 0.23081998\n",
      "Iteration 126, loss = 0.23049944\n",
      "Iteration 127, loss = 0.23011219\n",
      "Iteration 128, loss = 0.22978902\n",
      "Iteration 129, loss = 0.22942394\n",
      "Iteration 130, loss = 0.22905068\n",
      "Iteration 131, loss = 0.22870555\n",
      "Iteration 132, loss = 0.22837592\n",
      "Iteration 133, loss = 0.22801224\n",
      "Iteration 134, loss = 0.22768596\n",
      "Iteration 135, loss = 0.22733714\n",
      "Iteration 136, loss = 0.22699832\n",
      "Iteration 137, loss = 0.22666569\n",
      "Iteration 138, loss = 0.22633329\n",
      "Iteration 139, loss = 0.22597948\n",
      "Iteration 140, loss = 0.22564191\n",
      "Iteration 141, loss = 0.22531794\n",
      "Iteration 142, loss = 0.22496294\n",
      "Iteration 143, loss = 0.22463682\n",
      "Iteration 144, loss = 0.22434135\n",
      "Iteration 145, loss = 0.22397952\n",
      "Iteration 146, loss = 0.22365718\n",
      "Iteration 147, loss = 0.22334632\n",
      "Iteration 148, loss = 0.22299738\n",
      "Iteration 149, loss = 0.22267715\n",
      "Iteration 150, loss = 0.22237609\n",
      "Iteration 151, loss = 0.22204983\n",
      "Iteration 152, loss = 0.22175063\n",
      "Iteration 153, loss = 0.22141926\n",
      "Iteration 154, loss = 0.22111720\n",
      "Iteration 155, loss = 0.22080211\n",
      "Iteration 156, loss = 0.22050381\n",
      "Iteration 157, loss = 0.22018061\n",
      "Iteration 158, loss = 0.21986564\n",
      "Iteration 159, loss = 0.21957225\n",
      "Iteration 160, loss = 0.21927835\n",
      "Iteration 161, loss = 0.21895782\n",
      "Iteration 162, loss = 0.21865109\n",
      "Iteration 163, loss = 0.21834741\n",
      "Iteration 164, loss = 0.21804218\n",
      "Iteration 165, loss = 0.21776827\n",
      "Iteration 166, loss = 0.21747058\n",
      "Iteration 167, loss = 0.21716112\n",
      "Iteration 168, loss = 0.21685407\n",
      "Iteration 169, loss = 0.21657403\n",
      "Iteration 170, loss = 0.21626677\n",
      "Iteration 171, loss = 0.21597574\n",
      "Iteration 172, loss = 0.21567698\n",
      "Iteration 173, loss = 0.21539134\n",
      "Iteration 174, loss = 0.21510207\n",
      "Iteration 175, loss = 0.21480375\n",
      "Iteration 176, loss = 0.21452053\n",
      "Iteration 177, loss = 0.21422726\n",
      "Iteration 178, loss = 0.21393583\n",
      "Iteration 179, loss = 0.21366385\n",
      "Iteration 180, loss = 0.21337904\n",
      "Iteration 181, loss = 0.21307168\n",
      "Iteration 182, loss = 0.21280767\n",
      "Iteration 183, loss = 0.21252331\n",
      "Iteration 184, loss = 0.21226841\n",
      "Iteration 185, loss = 0.21197915\n",
      "Iteration 186, loss = 0.21170295\n",
      "Iteration 187, loss = 0.21144948\n",
      "Iteration 188, loss = 0.21116860\n",
      "Iteration 189, loss = 0.21088808\n",
      "Iteration 190, loss = 0.21060764\n",
      "Iteration 191, loss = 0.21037599\n",
      "Iteration 192, loss = 0.21008089\n",
      "Iteration 193, loss = 0.20980927\n",
      "Iteration 194, loss = 0.20952437\n",
      "Iteration 195, loss = 0.20925907\n",
      "Iteration 196, loss = 0.20899650\n",
      "Iteration 197, loss = 0.20873749\n",
      "Iteration 198, loss = 0.20846526\n",
      "Iteration 199, loss = 0.20822103\n",
      "Iteration 200, loss = 0.20793117\n",
      "Iteration 1, loss = 0.72893468\n",
      "Iteration 2, loss = 0.59345408\n",
      "Iteration 3, loss = 0.52125208\n",
      "Iteration 4, loss = 0.48444341\n",
      "Iteration 5, loss = 0.46179237\n",
      "Iteration 6, loss = 0.44491724\n",
      "Iteration 7, loss = 0.43123666\n",
      "Iteration 8, loss = 0.41945931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.40899003\n",
      "Iteration 10, loss = 0.39956630\n",
      "Iteration 11, loss = 0.39090363\n",
      "Iteration 12, loss = 0.38304279\n",
      "Iteration 13, loss = 0.37576833\n",
      "Iteration 14, loss = 0.36902327\n",
      "Iteration 15, loss = 0.36272223\n",
      "Iteration 16, loss = 0.35687914\n",
      "Iteration 17, loss = 0.35151520\n",
      "Iteration 18, loss = 0.34636486\n",
      "Iteration 19, loss = 0.34161578\n",
      "Iteration 20, loss = 0.33716266\n",
      "Iteration 21, loss = 0.33293989\n",
      "Iteration 22, loss = 0.32900450\n",
      "Iteration 23, loss = 0.32527878\n",
      "Iteration 24, loss = 0.32174315\n",
      "Iteration 25, loss = 0.31840113\n",
      "Iteration 26, loss = 0.31523132\n",
      "Iteration 27, loss = 0.31225524\n",
      "Iteration 28, loss = 0.30940627\n",
      "Iteration 29, loss = 0.30667651\n",
      "Iteration 30, loss = 0.30409987\n",
      "Iteration 31, loss = 0.30167117\n",
      "Iteration 32, loss = 0.29932198\n",
      "Iteration 33, loss = 0.29709615\n",
      "Iteration 34, loss = 0.29494401\n",
      "Iteration 35, loss = 0.29293420\n",
      "Iteration 36, loss = 0.29095191\n",
      "Iteration 37, loss = 0.28909129\n",
      "Iteration 38, loss = 0.28728174\n",
      "Iteration 39, loss = 0.28558573\n",
      "Iteration 40, loss = 0.28392736\n",
      "Iteration 41, loss = 0.28234066\n",
      "Iteration 42, loss = 0.28079986\n",
      "Iteration 43, loss = 0.27933755\n",
      "Iteration 44, loss = 0.27789603\n",
      "Iteration 45, loss = 0.27655744\n",
      "Iteration 46, loss = 0.27521941\n",
      "Iteration 47, loss = 0.27395959\n",
      "Iteration 48, loss = 0.27269583\n",
      "Iteration 49, loss = 0.27155836\n",
      "Iteration 50, loss = 0.27036410\n",
      "Iteration 51, loss = 0.26925624\n",
      "Iteration 52, loss = 0.26818284\n",
      "Iteration 53, loss = 0.26712358\n",
      "Iteration 54, loss = 0.26612941\n",
      "Iteration 55, loss = 0.26512032\n",
      "Iteration 56, loss = 0.26419549\n",
      "Iteration 57, loss = 0.26325549\n",
      "Iteration 58, loss = 0.26235266\n",
      "Iteration 59, loss = 0.26146200\n",
      "Iteration 60, loss = 0.26062309\n",
      "Iteration 61, loss = 0.25978129\n",
      "Iteration 62, loss = 0.25896126\n",
      "Iteration 63, loss = 0.25818213\n",
      "Iteration 64, loss = 0.25741146\n",
      "Iteration 65, loss = 0.25665081\n",
      "Iteration 66, loss = 0.25590844\n",
      "Iteration 67, loss = 0.25522626\n",
      "Iteration 68, loss = 0.25449620\n",
      "Iteration 69, loss = 0.25380908\n",
      "Iteration 70, loss = 0.25315344\n",
      "Iteration 71, loss = 0.25248970\n",
      "Iteration 72, loss = 0.25183146\n",
      "Iteration 73, loss = 0.25119526\n",
      "Iteration 74, loss = 0.25057437\n",
      "Iteration 75, loss = 0.24998828\n",
      "Iteration 76, loss = 0.24938114\n",
      "Iteration 77, loss = 0.24877760\n",
      "Iteration 78, loss = 0.24820870\n",
      "Iteration 79, loss = 0.24765569\n",
      "Iteration 80, loss = 0.24707910\n",
      "Iteration 81, loss = 0.24653124\n",
      "Iteration 82, loss = 0.24599520\n",
      "Iteration 83, loss = 0.24548250\n",
      "Iteration 84, loss = 0.24495873\n",
      "Iteration 85, loss = 0.24444118\n",
      "Iteration 86, loss = 0.24396740\n",
      "Iteration 87, loss = 0.24347436\n",
      "Iteration 88, loss = 0.24296423\n",
      "Iteration 89, loss = 0.24250493\n",
      "Iteration 90, loss = 0.24204104\n",
      "Iteration 91, loss = 0.24158138\n",
      "Iteration 92, loss = 0.24110116\n",
      "Iteration 93, loss = 0.24066591\n",
      "Iteration 94, loss = 0.24021676\n",
      "Iteration 95, loss = 0.23976493\n",
      "Iteration 96, loss = 0.23935042\n",
      "Iteration 97, loss = 0.23891884\n",
      "Iteration 98, loss = 0.23850578\n",
      "Iteration 99, loss = 0.23810690\n",
      "Iteration 100, loss = 0.23768072\n",
      "Iteration 101, loss = 0.23726411\n",
      "Iteration 102, loss = 0.23685048\n",
      "Iteration 103, loss = 0.23646581\n",
      "Iteration 104, loss = 0.23606034\n",
      "Iteration 105, loss = 0.23568130\n",
      "Iteration 106, loss = 0.23527255\n",
      "Iteration 107, loss = 0.23489355\n",
      "Iteration 108, loss = 0.23451303\n",
      "Iteration 109, loss = 0.23415078\n",
      "Iteration 110, loss = 0.23376219\n",
      "Iteration 111, loss = 0.23338269\n",
      "Iteration 112, loss = 0.23300735\n",
      "Iteration 113, loss = 0.23264360\n",
      "Iteration 114, loss = 0.23226352\n",
      "Iteration 115, loss = 0.23192806\n",
      "Iteration 116, loss = 0.23157238\n",
      "Iteration 117, loss = 0.23119733\n",
      "Iteration 118, loss = 0.23085450\n",
      "Iteration 119, loss = 0.23050794\n",
      "Iteration 120, loss = 0.23015642\n",
      "Iteration 121, loss = 0.22979639\n",
      "Iteration 122, loss = 0.22946655\n",
      "Iteration 123, loss = 0.22911173\n",
      "Iteration 124, loss = 0.22878058\n",
      "Iteration 125, loss = 0.22842794\n",
      "Iteration 126, loss = 0.22811266\n",
      "Iteration 127, loss = 0.22776656\n",
      "Iteration 128, loss = 0.22743204\n",
      "Iteration 129, loss = 0.22710209\n",
      "Iteration 130, loss = 0.22676668\n",
      "Iteration 131, loss = 0.22644967\n",
      "Iteration 132, loss = 0.22612754\n",
      "Iteration 133, loss = 0.22578710\n",
      "Iteration 134, loss = 0.22548855\n",
      "Iteration 135, loss = 0.22515319\n",
      "Iteration 136, loss = 0.22483967\n",
      "Iteration 137, loss = 0.22453697\n",
      "Iteration 138, loss = 0.22423310\n",
      "Iteration 139, loss = 0.22388804\n",
      "Iteration 140, loss = 0.22359246\n",
      "Iteration 141, loss = 0.22329571\n",
      "Iteration 142, loss = 0.22298071\n",
      "Iteration 143, loss = 0.22265477\n",
      "Iteration 144, loss = 0.22237851\n",
      "Iteration 145, loss = 0.22203109\n",
      "Iteration 146, loss = 0.22172346\n",
      "Iteration 147, loss = 0.22145075\n",
      "Iteration 148, loss = 0.22112634\n",
      "Iteration 149, loss = 0.22080033\n",
      "Iteration 150, loss = 0.22050696\n",
      "Iteration 151, loss = 0.22021741\n",
      "Iteration 152, loss = 0.21991828\n",
      "Iteration 153, loss = 0.21959702\n",
      "Iteration 154, loss = 0.21930589\n",
      "Iteration 155, loss = 0.21900327\n",
      "Iteration 156, loss = 0.21871954\n",
      "Iteration 157, loss = 0.21842563\n",
      "Iteration 158, loss = 0.21811440\n",
      "Iteration 159, loss = 0.21784542\n",
      "Iteration 160, loss = 0.21753856\n",
      "Iteration 161, loss = 0.21725110\n",
      "Iteration 162, loss = 0.21695231\n",
      "Iteration 163, loss = 0.21667254\n",
      "Iteration 164, loss = 0.21638829\n",
      "Iteration 165, loss = 0.21609941\n",
      "Iteration 166, loss = 0.21583989\n",
      "Iteration 167, loss = 0.21552909\n",
      "Iteration 168, loss = 0.21526229\n",
      "Iteration 169, loss = 0.21500236\n",
      "Iteration 170, loss = 0.21469297\n",
      "Iteration 171, loss = 0.21442786\n",
      "Iteration 172, loss = 0.21413779\n",
      "Iteration 173, loss = 0.21385666\n",
      "Iteration 174, loss = 0.21358024\n",
      "Iteration 175, loss = 0.21330568\n",
      "Iteration 176, loss = 0.21303561\n",
      "Iteration 177, loss = 0.21274642\n",
      "Iteration 178, loss = 0.21247379\n",
      "Iteration 179, loss = 0.21221991\n",
      "Iteration 180, loss = 0.21193819\n",
      "Iteration 181, loss = 0.21164953\n",
      "Iteration 182, loss = 0.21138911\n",
      "Iteration 183, loss = 0.21113201\n",
      "Iteration 184, loss = 0.21087094\n",
      "Iteration 185, loss = 0.21059218\n",
      "Iteration 186, loss = 0.21033509\n",
      "Iteration 187, loss = 0.21008675\n",
      "Iteration 188, loss = 0.20983242\n",
      "Iteration 189, loss = 0.20955037\n",
      "Iteration 190, loss = 0.20925822\n",
      "Iteration 191, loss = 0.20903181\n",
      "Iteration 192, loss = 0.20876089\n",
      "Iteration 193, loss = 0.20850156\n",
      "Iteration 194, loss = 0.20822451\n",
      "Iteration 195, loss = 0.20795698\n",
      "Iteration 196, loss = 0.20770586\n",
      "Iteration 197, loss = 0.20745022\n",
      "Iteration 198, loss = 0.20719749\n",
      "Iteration 199, loss = 0.20693841\n",
      "Iteration 200, loss = 0.20667739\n",
      "Iteration 1, loss = 0.73466796\n",
      "Iteration 2, loss = 0.60031035\n",
      "Iteration 3, loss = 0.52826222\n",
      "Iteration 4, loss = 0.49127231\n",
      "Iteration 5, loss = 0.46812641\n",
      "Iteration 6, loss = 0.45089711\n",
      "Iteration 7, loss = 0.43686770\n",
      "Iteration 8, loss = 0.42480478\n",
      "Iteration 9, loss = 0.41392412\n",
      "Iteration 10, loss = 0.40421073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39519409\n",
      "Iteration 12, loss = 0.38698513\n",
      "Iteration 13, loss = 0.37936569\n",
      "Iteration 14, loss = 0.37239618\n",
      "Iteration 15, loss = 0.36574379\n",
      "Iteration 16, loss = 0.35959780\n",
      "Iteration 17, loss = 0.35397943\n",
      "Iteration 18, loss = 0.34854622\n",
      "Iteration 19, loss = 0.34346724\n",
      "Iteration 20, loss = 0.33877395\n",
      "Iteration 21, loss = 0.33430448\n",
      "Iteration 22, loss = 0.33004932\n",
      "Iteration 23, loss = 0.32608461\n",
      "Iteration 24, loss = 0.32228521\n",
      "Iteration 25, loss = 0.31871524\n",
      "Iteration 26, loss = 0.31529099\n",
      "Iteration 27, loss = 0.31205902\n",
      "Iteration 28, loss = 0.30895420\n",
      "Iteration 29, loss = 0.30603629\n",
      "Iteration 30, loss = 0.30317435\n",
      "Iteration 31, loss = 0.30052864\n",
      "Iteration 32, loss = 0.29795150\n",
      "Iteration 33, loss = 0.29548601\n",
      "Iteration 34, loss = 0.29313654\n",
      "Iteration 35, loss = 0.29092817\n",
      "Iteration 36, loss = 0.28872589\n",
      "Iteration 37, loss = 0.28668316\n",
      "Iteration 38, loss = 0.28471139\n",
      "Iteration 39, loss = 0.28281469\n",
      "Iteration 40, loss = 0.28104778\n",
      "Iteration 41, loss = 0.27927080\n",
      "Iteration 42, loss = 0.27760096\n",
      "Iteration 43, loss = 0.27603357\n",
      "Iteration 44, loss = 0.27447098\n",
      "Iteration 45, loss = 0.27300693\n",
      "Iteration 46, loss = 0.27158380\n",
      "Iteration 47, loss = 0.27020957\n",
      "Iteration 48, loss = 0.26888751\n",
      "Iteration 49, loss = 0.26767182\n",
      "Iteration 50, loss = 0.26639560\n",
      "Iteration 51, loss = 0.26523676\n",
      "Iteration 52, loss = 0.26410391\n",
      "Iteration 53, loss = 0.26300820\n",
      "Iteration 54, loss = 0.26195758\n",
      "Iteration 55, loss = 0.26094010\n",
      "Iteration 56, loss = 0.25995197\n",
      "Iteration 57, loss = 0.25898670\n",
      "Iteration 58, loss = 0.25807363\n",
      "Iteration 59, loss = 0.25716800\n",
      "Iteration 60, loss = 0.25630555\n",
      "Iteration 61, loss = 0.25546071\n",
      "Iteration 62, loss = 0.25463255\n",
      "Iteration 63, loss = 0.25383224\n",
      "Iteration 64, loss = 0.25308173\n",
      "Iteration 65, loss = 0.25232113\n",
      "Iteration 66, loss = 0.25158801\n",
      "Iteration 67, loss = 0.25088018\n",
      "Iteration 68, loss = 0.25014958\n",
      "Iteration 69, loss = 0.24948497\n",
      "Iteration 70, loss = 0.24884500\n",
      "Iteration 71, loss = 0.24816366\n",
      "Iteration 72, loss = 0.24754021\n",
      "Iteration 73, loss = 0.24690514\n",
      "Iteration 74, loss = 0.24629926\n",
      "Iteration 75, loss = 0.24571492\n",
      "Iteration 76, loss = 0.24511443\n",
      "Iteration 77, loss = 0.24453929\n",
      "Iteration 78, loss = 0.24398284\n",
      "Iteration 79, loss = 0.24343108\n",
      "Iteration 80, loss = 0.24289318\n",
      "Iteration 81, loss = 0.24234605\n",
      "Iteration 82, loss = 0.24182595\n",
      "Iteration 83, loss = 0.24130443\n",
      "Iteration 84, loss = 0.24079584\n",
      "Iteration 85, loss = 0.24029399\n",
      "Iteration 86, loss = 0.23983398\n",
      "Iteration 87, loss = 0.23933380\n",
      "Iteration 88, loss = 0.23885575\n",
      "Iteration 89, loss = 0.23841735\n",
      "Iteration 90, loss = 0.23791877\n",
      "Iteration 91, loss = 0.23749815\n",
      "Iteration 92, loss = 0.23703474\n",
      "Iteration 93, loss = 0.23658265\n",
      "Iteration 94, loss = 0.23615519\n",
      "Iteration 95, loss = 0.23572018\n",
      "Iteration 96, loss = 0.23529412\n",
      "Iteration 97, loss = 0.23487186\n",
      "Iteration 98, loss = 0.23446877\n",
      "Iteration 99, loss = 0.23404616\n",
      "Iteration 100, loss = 0.23363306\n",
      "Iteration 101, loss = 0.23322741\n",
      "Iteration 102, loss = 0.23281742\n",
      "Iteration 103, loss = 0.23243873\n",
      "Iteration 104, loss = 0.23206682\n",
      "Iteration 105, loss = 0.23164519\n",
      "Iteration 106, loss = 0.23125510\n",
      "Iteration 107, loss = 0.23088852\n",
      "Iteration 108, loss = 0.23052293\n",
      "Iteration 109, loss = 0.23013288\n",
      "Iteration 110, loss = 0.22978921\n",
      "Iteration 111, loss = 0.22940561\n",
      "Iteration 112, loss = 0.22903926\n",
      "Iteration 113, loss = 0.22868046\n",
      "Iteration 114, loss = 0.22830775\n",
      "Iteration 115, loss = 0.22797857\n",
      "Iteration 116, loss = 0.22764334\n",
      "Iteration 117, loss = 0.22725804\n",
      "Iteration 118, loss = 0.22692837\n",
      "Iteration 119, loss = 0.22659959\n",
      "Iteration 120, loss = 0.22623563\n",
      "Iteration 121, loss = 0.22591295\n",
      "Iteration 122, loss = 0.22557212\n",
      "Iteration 123, loss = 0.22523762\n",
      "Iteration 124, loss = 0.22490650\n",
      "Iteration 125, loss = 0.22456068\n",
      "Iteration 126, loss = 0.22426002\n",
      "Iteration 127, loss = 0.22392423\n",
      "Iteration 128, loss = 0.22359627\n",
      "Iteration 129, loss = 0.22327169\n",
      "Iteration 130, loss = 0.22295901\n",
      "Iteration 131, loss = 0.22264639\n",
      "Iteration 132, loss = 0.22235842\n",
      "Iteration 133, loss = 0.22200632\n",
      "Iteration 134, loss = 0.22172227\n",
      "Iteration 135, loss = 0.22139036\n",
      "Iteration 136, loss = 0.22109440\n",
      "Iteration 137, loss = 0.22077047\n",
      "Iteration 138, loss = 0.22049804\n",
      "Iteration 139, loss = 0.22016575\n",
      "Iteration 140, loss = 0.21986844\n",
      "Iteration 141, loss = 0.21959819\n",
      "Iteration 142, loss = 0.21928169\n",
      "Iteration 143, loss = 0.21898736\n",
      "Iteration 144, loss = 0.21869746\n",
      "Iteration 145, loss = 0.21839213\n",
      "Iteration 146, loss = 0.21808515\n",
      "Iteration 147, loss = 0.21777955\n",
      "Iteration 148, loss = 0.21749945\n",
      "Iteration 149, loss = 0.21719169\n",
      "Iteration 150, loss = 0.21690473\n",
      "Iteration 151, loss = 0.21662984\n",
      "Iteration 152, loss = 0.21633872\n",
      "Iteration 153, loss = 0.21602067\n",
      "Iteration 154, loss = 0.21574586\n",
      "Iteration 155, loss = 0.21546842\n",
      "Iteration 156, loss = 0.21518309\n",
      "Iteration 157, loss = 0.21491590\n",
      "Iteration 158, loss = 0.21462396\n",
      "Iteration 159, loss = 0.21436196\n",
      "Iteration 160, loss = 0.21406524\n",
      "Iteration 161, loss = 0.21379900\n",
      "Iteration 162, loss = 0.21351573\n",
      "Iteration 163, loss = 0.21325518\n",
      "Iteration 164, loss = 0.21298679\n",
      "Iteration 165, loss = 0.21270814\n",
      "Iteration 166, loss = 0.21246291\n",
      "Iteration 167, loss = 0.21217582\n",
      "Iteration 168, loss = 0.21190404\n",
      "Iteration 169, loss = 0.21165016\n",
      "Iteration 170, loss = 0.21137641\n",
      "Iteration 171, loss = 0.21110610\n",
      "Iteration 172, loss = 0.21086216\n",
      "Iteration 173, loss = 0.21058261\n",
      "Iteration 174, loss = 0.21030888\n",
      "Iteration 175, loss = 0.21007233\n",
      "Iteration 176, loss = 0.20979416\n",
      "Iteration 177, loss = 0.20950955\n",
      "Iteration 178, loss = 0.20926547\n",
      "Iteration 179, loss = 0.20900969\n",
      "Iteration 180, loss = 0.20874737\n",
      "Iteration 181, loss = 0.20848121\n",
      "Iteration 182, loss = 0.20822089\n",
      "Iteration 183, loss = 0.20797951\n",
      "Iteration 184, loss = 0.20773290\n",
      "Iteration 185, loss = 0.20747871\n",
      "Iteration 186, loss = 0.20723448\n",
      "Iteration 187, loss = 0.20699262\n",
      "Iteration 188, loss = 0.20674368\n",
      "Iteration 189, loss = 0.20651438\n",
      "Iteration 190, loss = 0.20622007\n",
      "Iteration 191, loss = 0.20600720\n",
      "Iteration 192, loss = 0.20574940\n",
      "Iteration 193, loss = 0.20550540\n",
      "Iteration 194, loss = 0.20526467\n",
      "Iteration 195, loss = 0.20501190\n",
      "Iteration 196, loss = 0.20476971\n",
      "Iteration 197, loss = 0.20453755\n",
      "Iteration 198, loss = 0.20428089\n",
      "Iteration 199, loss = 0.20406620\n",
      "Iteration 200, loss = 0.20382559\n",
      "Iteration 1, loss = 0.61178200\n",
      "Iteration 2, loss = 0.40865526\n",
      "Iteration 3, loss = 0.33969611\n",
      "Iteration 4, loss = 0.30564416\n",
      "Iteration 5, loss = 0.28502539\n",
      "Iteration 6, loss = 0.27111660\n",
      "Iteration 7, loss = 0.25981296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.25138070\n",
      "Iteration 9, loss = 0.24372301\n",
      "Iteration 10, loss = 0.23759264\n",
      "Iteration 11, loss = 0.23106439\n",
      "Iteration 12, loss = 0.22599849\n",
      "Iteration 13, loss = 0.22056539\n",
      "Iteration 14, loss = 0.21621559\n",
      "Iteration 15, loss = 0.21151929\n",
      "Iteration 16, loss = 0.20748680\n",
      "Iteration 17, loss = 0.20373690\n",
      "Iteration 18, loss = 0.19922601\n",
      "Iteration 19, loss = 0.19537058\n",
      "Iteration 20, loss = 0.19156793\n",
      "Iteration 21, loss = 0.18796337\n",
      "Iteration 22, loss = 0.18445988\n",
      "Iteration 23, loss = 0.18137578\n",
      "Iteration 24, loss = 0.17805770\n",
      "Iteration 25, loss = 0.17501600\n",
      "Iteration 26, loss = 0.17191046\n",
      "Iteration 27, loss = 0.16862790\n",
      "Iteration 28, loss = 0.16583810\n",
      "Iteration 29, loss = 0.16291970\n",
      "Iteration 30, loss = 0.16098964\n",
      "Iteration 31, loss = 0.15775033\n",
      "Iteration 32, loss = 0.15519684\n",
      "Iteration 33, loss = 0.15328559\n",
      "Iteration 34, loss = 0.15067169\n",
      "Iteration 35, loss = 0.14778130\n",
      "Iteration 36, loss = 0.14575235\n",
      "Iteration 37, loss = 0.14368061\n",
      "Iteration 38, loss = 0.14243026\n",
      "Iteration 39, loss = 0.13996932\n",
      "Iteration 40, loss = 0.13806745\n",
      "Iteration 41, loss = 0.13630989\n",
      "Iteration 42, loss = 0.13463977\n",
      "Iteration 43, loss = 0.13269713\n",
      "Iteration 44, loss = 0.13116192\n",
      "Iteration 45, loss = 0.12969729\n",
      "Iteration 46, loss = 0.12828011\n",
      "Iteration 47, loss = 0.12657102\n",
      "Iteration 48, loss = 0.12552413\n",
      "Iteration 49, loss = 0.12390973\n",
      "Iteration 50, loss = 0.12282534\n",
      "Iteration 51, loss = 0.12107470\n",
      "Iteration 52, loss = 0.12009367\n",
      "Iteration 53, loss = 0.11870750\n",
      "Iteration 54, loss = 0.11794921\n",
      "Iteration 55, loss = 0.11632019\n",
      "Iteration 56, loss = 0.11532708\n",
      "Iteration 57, loss = 0.11400294\n",
      "Iteration 58, loss = 0.11302230\n",
      "Iteration 59, loss = 0.11187047\n",
      "Iteration 60, loss = 0.11086870\n",
      "Iteration 61, loss = 0.11017558\n",
      "Iteration 62, loss = 0.10887762\n",
      "Iteration 63, loss = 0.10824635\n",
      "Iteration 64, loss = 0.10727604\n",
      "Iteration 65, loss = 0.10575394\n",
      "Iteration 66, loss = 0.10521215\n",
      "Iteration 67, loss = 0.10407692\n",
      "Iteration 68, loss = 0.10336761\n",
      "Iteration 69, loss = 0.10243476\n",
      "Iteration 70, loss = 0.10149383\n",
      "Iteration 71, loss = 0.10138517\n",
      "Iteration 72, loss = 0.10011927\n",
      "Iteration 73, loss = 0.09886374\n",
      "Iteration 74, loss = 0.09810332\n",
      "Iteration 75, loss = 0.09739136\n",
      "Iteration 76, loss = 0.09705084\n",
      "Iteration 77, loss = 0.09568711\n",
      "Iteration 78, loss = 0.09511683\n",
      "Iteration 79, loss = 0.09446564\n",
      "Iteration 80, loss = 0.09355579\n",
      "Iteration 81, loss = 0.09319645\n",
      "Iteration 82, loss = 0.09201266\n",
      "Iteration 83, loss = 0.09090748\n",
      "Iteration 84, loss = 0.09078119\n",
      "Iteration 85, loss = 0.09025928\n",
      "Iteration 86, loss = 0.08914280\n",
      "Iteration 87, loss = 0.08846112\n",
      "Iteration 88, loss = 0.08811550\n",
      "Iteration 89, loss = 0.08777682\n",
      "Iteration 90, loss = 0.08626436\n",
      "Iteration 91, loss = 0.08581448\n",
      "Iteration 92, loss = 0.08478331\n",
      "Iteration 93, loss = 0.08449751\n",
      "Iteration 94, loss = 0.08398635\n",
      "Iteration 95, loss = 0.08419764\n",
      "Iteration 96, loss = 0.08233489\n",
      "Iteration 97, loss = 0.08165606\n",
      "Iteration 98, loss = 0.08121896\n",
      "Iteration 99, loss = 0.08061396\n",
      "Iteration 100, loss = 0.07993569\n",
      "Iteration 101, loss = 0.07912533\n",
      "Iteration 102, loss = 0.07899240\n",
      "Iteration 103, loss = 0.07810856\n",
      "Iteration 104, loss = 0.07748439\n",
      "Iteration 105, loss = 0.07729359\n",
      "Iteration 106, loss = 0.07712430\n",
      "Iteration 107, loss = 0.07563672\n",
      "Iteration 108, loss = 0.07499895\n",
      "Iteration 109, loss = 0.07507868\n",
      "Iteration 110, loss = 0.07447945\n",
      "Iteration 111, loss = 0.07375823\n",
      "Iteration 112, loss = 0.07342622\n",
      "Iteration 113, loss = 0.07273986\n",
      "Iteration 114, loss = 0.07239135\n",
      "Iteration 115, loss = 0.07255489\n",
      "Iteration 116, loss = 0.07111610\n",
      "Iteration 117, loss = 0.07022672\n",
      "Iteration 118, loss = 0.06942044\n",
      "Iteration 119, loss = 0.06924061\n",
      "Iteration 120, loss = 0.06866123\n",
      "Iteration 121, loss = 0.06844194\n",
      "Iteration 122, loss = 0.06802413\n",
      "Iteration 123, loss = 0.06692940\n",
      "Iteration 124, loss = 0.06688662\n",
      "Iteration 125, loss = 0.06582312\n",
      "Iteration 126, loss = 0.06569655\n",
      "Iteration 127, loss = 0.06521644\n",
      "Iteration 128, loss = 0.06452882\n",
      "Iteration 129, loss = 0.06464444\n",
      "Iteration 130, loss = 0.06351282\n",
      "Iteration 131, loss = 0.06326880\n",
      "Iteration 132, loss = 0.06265072\n",
      "Iteration 133, loss = 0.06176966\n",
      "Iteration 134, loss = 0.06174364\n",
      "Iteration 135, loss = 0.06108039\n",
      "Iteration 136, loss = 0.06088749\n",
      "Iteration 137, loss = 0.06043701\n",
      "Iteration 138, loss = 0.05967798\n",
      "Iteration 139, loss = 0.05918515\n",
      "Iteration 140, loss = 0.05882868\n",
      "Iteration 141, loss = 0.05855221\n",
      "Iteration 142, loss = 0.05796824\n",
      "Iteration 143, loss = 0.05742025\n",
      "Iteration 144, loss = 0.05725206\n",
      "Iteration 145, loss = 0.05694955\n",
      "Iteration 146, loss = 0.05665379\n",
      "Iteration 147, loss = 0.05531129\n",
      "Iteration 148, loss = 0.05522310\n",
      "Iteration 149, loss = 0.05480291\n",
      "Iteration 150, loss = 0.05385208\n",
      "Iteration 151, loss = 0.05403819\n",
      "Iteration 152, loss = 0.05413695\n",
      "Iteration 153, loss = 0.05292894\n",
      "Iteration 154, loss = 0.05306515\n",
      "Iteration 155, loss = 0.05237053\n",
      "Iteration 156, loss = 0.05161722\n",
      "Iteration 157, loss = 0.05153891\n",
      "Iteration 158, loss = 0.05074966\n",
      "Iteration 159, loss = 0.05074117\n",
      "Iteration 160, loss = 0.05058981\n",
      "Iteration 161, loss = 0.05034434\n",
      "Iteration 162, loss = 0.04988983\n",
      "Iteration 163, loss = 0.04892205\n",
      "Iteration 164, loss = 0.04856762\n",
      "Iteration 165, loss = 0.04862319\n",
      "Iteration 166, loss = 0.04779570\n",
      "Iteration 167, loss = 0.04773939\n",
      "Iteration 168, loss = 0.04737443\n",
      "Iteration 169, loss = 0.04714145\n",
      "Iteration 170, loss = 0.04656912\n",
      "Iteration 171, loss = 0.04649081\n",
      "Iteration 172, loss = 0.04582530\n",
      "Iteration 173, loss = 0.04535580\n",
      "Iteration 174, loss = 0.04492716\n",
      "Iteration 175, loss = 0.04483674\n",
      "Iteration 176, loss = 0.04438325\n",
      "Iteration 177, loss = 0.04396035\n",
      "Iteration 178, loss = 0.04349794\n",
      "Iteration 179, loss = 0.04289762\n",
      "Iteration 180, loss = 0.04275633\n",
      "Iteration 181, loss = 0.04216203\n",
      "Iteration 182, loss = 0.04237716\n",
      "Iteration 183, loss = 0.04249451\n",
      "Iteration 184, loss = 0.04165654\n",
      "Iteration 185, loss = 0.04120665\n",
      "Iteration 186, loss = 0.04051007\n",
      "Iteration 187, loss = 0.04052644\n",
      "Iteration 188, loss = 0.04048275\n",
      "Iteration 189, loss = 0.03965810\n",
      "Iteration 190, loss = 0.03946775\n",
      "Iteration 191, loss = 0.03931165\n",
      "Iteration 192, loss = 0.03891929\n",
      "Iteration 193, loss = 0.03864001\n",
      "Iteration 194, loss = 0.03824437\n",
      "Iteration 195, loss = 0.03805576\n",
      "Iteration 196, loss = 0.03802029\n",
      "Iteration 197, loss = 0.03797715\n",
      "Iteration 198, loss = 0.03705639\n",
      "Iteration 199, loss = 0.03714901\n",
      "Iteration 200, loss = 0.03667557\n",
      "Iteration 1, loss = 0.61264317\n",
      "Iteration 2, loss = 0.40833922\n",
      "Iteration 3, loss = 0.33834903\n",
      "Iteration 4, loss = 0.30378487\n",
      "Iteration 5, loss = 0.28289018\n",
      "Iteration 6, loss = 0.26876067\n",
      "Iteration 7, loss = 0.25746674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.24892800\n",
      "Iteration 9, loss = 0.24128364\n",
      "Iteration 10, loss = 0.23492774\n",
      "Iteration 11, loss = 0.22853789\n",
      "Iteration 12, loss = 0.22371652\n",
      "Iteration 13, loss = 0.21814242\n",
      "Iteration 14, loss = 0.21319592\n",
      "Iteration 15, loss = 0.20866385\n",
      "Iteration 16, loss = 0.20435662\n",
      "Iteration 17, loss = 0.20032369\n",
      "Iteration 18, loss = 0.19636800\n",
      "Iteration 19, loss = 0.19269990\n",
      "Iteration 20, loss = 0.18892335\n",
      "Iteration 21, loss = 0.18558164\n",
      "Iteration 22, loss = 0.18202488\n",
      "Iteration 23, loss = 0.17895786\n",
      "Iteration 24, loss = 0.17606286\n",
      "Iteration 25, loss = 0.17303217\n",
      "Iteration 26, loss = 0.16992772\n",
      "Iteration 27, loss = 0.16716374\n",
      "Iteration 28, loss = 0.16452085\n",
      "Iteration 29, loss = 0.16201464\n",
      "Iteration 30, loss = 0.15964338\n",
      "Iteration 31, loss = 0.15674766\n",
      "Iteration 32, loss = 0.15445127\n",
      "Iteration 33, loss = 0.15208565\n",
      "Iteration 34, loss = 0.15032243\n",
      "Iteration 35, loss = 0.14748638\n",
      "Iteration 36, loss = 0.14568255\n",
      "Iteration 37, loss = 0.14359594\n",
      "Iteration 38, loss = 0.14178923\n",
      "Iteration 39, loss = 0.13983468\n",
      "Iteration 40, loss = 0.13776153\n",
      "Iteration 41, loss = 0.13611923\n",
      "Iteration 42, loss = 0.13450804\n",
      "Iteration 43, loss = 0.13225557\n",
      "Iteration 44, loss = 0.13093320\n",
      "Iteration 45, loss = 0.12938098\n",
      "Iteration 46, loss = 0.12759641\n",
      "Iteration 47, loss = 0.12608139\n",
      "Iteration 48, loss = 0.12472660\n",
      "Iteration 49, loss = 0.12327183\n",
      "Iteration 50, loss = 0.12192187\n",
      "Iteration 51, loss = 0.12031877\n",
      "Iteration 52, loss = 0.11891030\n",
      "Iteration 53, loss = 0.11741274\n",
      "Iteration 54, loss = 0.11655232\n",
      "Iteration 55, loss = 0.11487351\n",
      "Iteration 56, loss = 0.11398249\n",
      "Iteration 57, loss = 0.11254936\n",
      "Iteration 58, loss = 0.11152226\n",
      "Iteration 59, loss = 0.11040542\n",
      "Iteration 60, loss = 0.10900801\n",
      "Iteration 61, loss = 0.10805230\n",
      "Iteration 62, loss = 0.10671673\n",
      "Iteration 63, loss = 0.10647315\n",
      "Iteration 64, loss = 0.10495215\n",
      "Iteration 65, loss = 0.10382984\n",
      "Iteration 66, loss = 0.10317777\n",
      "Iteration 67, loss = 0.10167008\n",
      "Iteration 68, loss = 0.10096053\n",
      "Iteration 69, loss = 0.10016603\n",
      "Iteration 70, loss = 0.09857125\n",
      "Iteration 71, loss = 0.09863003\n",
      "Iteration 72, loss = 0.09773251\n",
      "Iteration 73, loss = 0.09607211\n",
      "Iteration 74, loss = 0.09513330\n",
      "Iteration 75, loss = 0.09450133\n",
      "Iteration 76, loss = 0.09321119\n",
      "Iteration 77, loss = 0.09250441\n",
      "Iteration 78, loss = 0.09201875\n",
      "Iteration 79, loss = 0.09136779\n",
      "Iteration 80, loss = 0.09021122\n",
      "Iteration 81, loss = 0.08942798\n",
      "Iteration 82, loss = 0.08864224\n",
      "Iteration 83, loss = 0.08744675\n",
      "Iteration 84, loss = 0.08696232\n",
      "Iteration 85, loss = 0.08633966\n",
      "Iteration 86, loss = 0.08524814\n",
      "Iteration 87, loss = 0.08495856\n",
      "Iteration 88, loss = 0.08413056\n",
      "Iteration 89, loss = 0.08341199\n",
      "Iteration 90, loss = 0.08218890\n",
      "Iteration 91, loss = 0.08161621\n",
      "Iteration 92, loss = 0.08043940\n",
      "Iteration 93, loss = 0.08007804\n",
      "Iteration 94, loss = 0.07946631\n",
      "Iteration 95, loss = 0.07960786\n",
      "Iteration 96, loss = 0.07801727\n",
      "Iteration 97, loss = 0.07710966\n",
      "Iteration 98, loss = 0.07658578\n",
      "Iteration 99, loss = 0.07622739\n",
      "Iteration 100, loss = 0.07575949\n",
      "Iteration 101, loss = 0.07462274\n",
      "Iteration 102, loss = 0.07489969\n",
      "Iteration 103, loss = 0.07385981\n",
      "Iteration 104, loss = 0.07270560\n",
      "Iteration 105, loss = 0.07228918\n",
      "Iteration 106, loss = 0.07202343\n",
      "Iteration 107, loss = 0.07122886\n",
      "Iteration 108, loss = 0.07054562\n",
      "Iteration 109, loss = 0.07006268\n",
      "Iteration 110, loss = 0.06964285\n",
      "Iteration 111, loss = 0.06864485\n",
      "Iteration 112, loss = 0.06902564\n",
      "Iteration 113, loss = 0.06748758\n",
      "Iteration 114, loss = 0.06688930\n",
      "Iteration 115, loss = 0.06705448\n",
      "Iteration 116, loss = 0.06596391\n",
      "Iteration 117, loss = 0.06500087\n",
      "Iteration 118, loss = 0.06447290\n",
      "Iteration 119, loss = 0.06421666\n",
      "Iteration 120, loss = 0.06404548\n",
      "Iteration 121, loss = 0.06367043\n",
      "Iteration 122, loss = 0.06269307\n",
      "Iteration 123, loss = 0.06195342\n",
      "Iteration 124, loss = 0.06180956\n",
      "Iteration 125, loss = 0.06081742\n",
      "Iteration 126, loss = 0.06097036\n",
      "Iteration 127, loss = 0.05994155\n",
      "Iteration 128, loss = 0.05942769\n",
      "Iteration 129, loss = 0.05925416\n",
      "Iteration 130, loss = 0.05860797\n",
      "Iteration 131, loss = 0.05792337\n",
      "Iteration 132, loss = 0.05740739\n",
      "Iteration 133, loss = 0.05684168\n",
      "Iteration 134, loss = 0.05643028\n",
      "Iteration 135, loss = 0.05610259\n",
      "Iteration 136, loss = 0.05545926\n",
      "Iteration 137, loss = 0.05519648\n",
      "Iteration 138, loss = 0.05442069\n",
      "Iteration 139, loss = 0.05442714\n",
      "Iteration 140, loss = 0.05424678\n",
      "Iteration 141, loss = 0.05346997\n",
      "Iteration 142, loss = 0.05294121\n",
      "Iteration 143, loss = 0.05209012\n",
      "Iteration 144, loss = 0.05243004\n",
      "Iteration 145, loss = 0.05185101\n",
      "Iteration 146, loss = 0.05148452\n",
      "Iteration 147, loss = 0.05046319\n",
      "Iteration 148, loss = 0.05063533\n",
      "Iteration 149, loss = 0.04978075\n",
      "Iteration 150, loss = 0.04928835\n",
      "Iteration 151, loss = 0.04918945\n",
      "Iteration 152, loss = 0.04916061\n",
      "Iteration 153, loss = 0.04809840\n",
      "Iteration 154, loss = 0.04793280\n",
      "Iteration 155, loss = 0.04743618\n",
      "Iteration 156, loss = 0.04708061\n",
      "Iteration 157, loss = 0.04685296\n",
      "Iteration 158, loss = 0.04617519\n",
      "Iteration 159, loss = 0.04595409\n",
      "Iteration 160, loss = 0.04562445\n",
      "Iteration 161, loss = 0.04529640\n",
      "Iteration 162, loss = 0.04464271\n",
      "Iteration 163, loss = 0.04452484\n",
      "Iteration 164, loss = 0.04377003\n",
      "Iteration 165, loss = 0.04361059\n",
      "Iteration 166, loss = 0.04289770\n",
      "Iteration 167, loss = 0.04272576\n",
      "Iteration 168, loss = 0.04254758\n",
      "Iteration 169, loss = 0.04233104\n",
      "Iteration 170, loss = 0.04180097\n",
      "Iteration 171, loss = 0.04161654\n",
      "Iteration 172, loss = 0.04121054\n",
      "Iteration 173, loss = 0.04081498\n",
      "Iteration 174, loss = 0.04057085\n",
      "Iteration 175, loss = 0.04015835\n",
      "Iteration 176, loss = 0.03981025\n",
      "Iteration 177, loss = 0.03914189\n",
      "Iteration 178, loss = 0.03897262\n",
      "Iteration 179, loss = 0.03854726\n",
      "Iteration 180, loss = 0.03847821\n",
      "Iteration 181, loss = 0.03784923\n",
      "Iteration 182, loss = 0.03764722\n",
      "Iteration 183, loss = 0.03775552\n",
      "Iteration 184, loss = 0.03690138\n",
      "Iteration 185, loss = 0.03701714\n",
      "Iteration 186, loss = 0.03594848\n",
      "Iteration 187, loss = 0.03591565\n",
      "Iteration 188, loss = 0.03620897\n",
      "Iteration 189, loss = 0.03521290\n",
      "Iteration 190, loss = 0.03526885\n",
      "Iteration 191, loss = 0.03496823\n",
      "Iteration 192, loss = 0.03487542\n",
      "Iteration 193, loss = 0.03436144\n",
      "Iteration 194, loss = 0.03391609\n",
      "Iteration 195, loss = 0.03357753\n",
      "Iteration 196, loss = 0.03351516\n",
      "Iteration 197, loss = 0.03346195\n",
      "Iteration 198, loss = 0.03271383\n",
      "Iteration 199, loss = 0.03282878\n",
      "Iteration 200, loss = 0.03231911\n",
      "Iteration 1, loss = 0.61603689\n",
      "Iteration 2, loss = 0.41181678\n",
      "Iteration 3, loss = 0.34208453\n",
      "Iteration 4, loss = 0.30758688\n",
      "Iteration 5, loss = 0.28698856\n",
      "Iteration 6, loss = 0.27261017\n",
      "Iteration 7, loss = 0.26163808\n",
      "Iteration 8, loss = 0.25264971\n",
      "Iteration 9, loss = 0.24459064\n",
      "Iteration 10, loss = 0.23796412\n",
      "Iteration 11, loss = 0.23136471\n",
      "Iteration 12, loss = 0.22620232\n",
      "Iteration 13, loss = 0.22020925\n",
      "Iteration 14, loss = 0.21491433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.21037787\n",
      "Iteration 16, loss = 0.20554184\n",
      "Iteration 17, loss = 0.20130142\n",
      "Iteration 18, loss = 0.19711975\n",
      "Iteration 19, loss = 0.19318314\n",
      "Iteration 20, loss = 0.18893134\n",
      "Iteration 21, loss = 0.18545958\n",
      "Iteration 22, loss = 0.18190028\n",
      "Iteration 23, loss = 0.17843825\n",
      "Iteration 24, loss = 0.17539544\n",
      "Iteration 25, loss = 0.17222526\n",
      "Iteration 26, loss = 0.16885198\n",
      "Iteration 27, loss = 0.16583450\n",
      "Iteration 28, loss = 0.16292445\n",
      "Iteration 29, loss = 0.16040767\n",
      "Iteration 30, loss = 0.15768662\n",
      "Iteration 31, loss = 0.15467919\n",
      "Iteration 32, loss = 0.15208621\n",
      "Iteration 33, loss = 0.14962364\n",
      "Iteration 34, loss = 0.14743137\n",
      "Iteration 35, loss = 0.14510649\n",
      "Iteration 36, loss = 0.14278471\n",
      "Iteration 37, loss = 0.14087481\n",
      "Iteration 38, loss = 0.13900024\n",
      "Iteration 39, loss = 0.13686133\n",
      "Iteration 40, loss = 0.13491609\n",
      "Iteration 41, loss = 0.13335247\n",
      "Iteration 42, loss = 0.13162956\n",
      "Iteration 43, loss = 0.12943556\n",
      "Iteration 44, loss = 0.12813539\n",
      "Iteration 45, loss = 0.12681952\n",
      "Iteration 46, loss = 0.12536906\n",
      "Iteration 47, loss = 0.12348861\n",
      "Iteration 48, loss = 0.12202962\n",
      "Iteration 49, loss = 0.12056164\n",
      "Iteration 50, loss = 0.11922356\n",
      "Iteration 51, loss = 0.11837826\n",
      "Iteration 52, loss = 0.11700083\n",
      "Iteration 53, loss = 0.11586036\n",
      "Iteration 54, loss = 0.11479083\n",
      "Iteration 55, loss = 0.11313735\n",
      "Iteration 56, loss = 0.11219761\n",
      "Iteration 57, loss = 0.11108830\n",
      "Iteration 58, loss = 0.11010333\n",
      "Iteration 59, loss = 0.10950964\n",
      "Iteration 60, loss = 0.10827772\n",
      "Iteration 61, loss = 0.10710212\n",
      "Iteration 62, loss = 0.10595498\n",
      "Iteration 63, loss = 0.10524157\n",
      "Iteration 64, loss = 0.10396467\n",
      "Iteration 65, loss = 0.10293678\n",
      "Iteration 66, loss = 0.10262718\n",
      "Iteration 67, loss = 0.10135153\n",
      "Iteration 68, loss = 0.10089861\n",
      "Iteration 69, loss = 0.10019180\n",
      "Iteration 70, loss = 0.09906240\n",
      "Iteration 71, loss = 0.09866343\n",
      "Iteration 72, loss = 0.09764906\n",
      "Iteration 73, loss = 0.09630878\n",
      "Iteration 74, loss = 0.09543785\n",
      "Iteration 75, loss = 0.09504421\n",
      "Iteration 76, loss = 0.09450330\n",
      "Iteration 77, loss = 0.09347152\n",
      "Iteration 78, loss = 0.09286054\n",
      "Iteration 79, loss = 0.09184298\n",
      "Iteration 80, loss = 0.09109732\n",
      "Iteration 81, loss = 0.08973139\n",
      "Iteration 82, loss = 0.08978727\n",
      "Iteration 83, loss = 0.08847652\n",
      "Iteration 84, loss = 0.08797864\n",
      "Iteration 85, loss = 0.08741321\n",
      "Iteration 86, loss = 0.08657085\n",
      "Iteration 87, loss = 0.08579070\n",
      "Iteration 88, loss = 0.08555773\n",
      "Iteration 89, loss = 0.08439324\n",
      "Iteration 90, loss = 0.08402092\n",
      "Iteration 91, loss = 0.08326185\n",
      "Iteration 92, loss = 0.08235110\n",
      "Iteration 93, loss = 0.08193059\n",
      "Iteration 94, loss = 0.08148059\n",
      "Iteration 95, loss = 0.08072853\n",
      "Iteration 96, loss = 0.07964739\n",
      "Iteration 97, loss = 0.07903692\n",
      "Iteration 98, loss = 0.07844464\n",
      "Iteration 99, loss = 0.07790977\n",
      "Iteration 100, loss = 0.07747130\n",
      "Iteration 101, loss = 0.07649280\n",
      "Iteration 102, loss = 0.07671244\n",
      "Iteration 103, loss = 0.07554168\n",
      "Iteration 104, loss = 0.07485434\n",
      "Iteration 105, loss = 0.07448727\n",
      "Iteration 106, loss = 0.07381903\n",
      "Iteration 107, loss = 0.07278278\n",
      "Iteration 108, loss = 0.07240903\n",
      "Iteration 109, loss = 0.07240167\n",
      "Iteration 110, loss = 0.07150210\n",
      "Iteration 111, loss = 0.07085482\n",
      "Iteration 112, loss = 0.07029673\n",
      "Iteration 113, loss = 0.06974130\n",
      "Iteration 114, loss = 0.06888250\n",
      "Iteration 115, loss = 0.06865611\n",
      "Iteration 116, loss = 0.06815577\n",
      "Iteration 117, loss = 0.06737297\n",
      "Iteration 118, loss = 0.06675086\n",
      "Iteration 119, loss = 0.06646085\n",
      "Iteration 120, loss = 0.06601282\n",
      "Iteration 121, loss = 0.06548582\n",
      "Iteration 122, loss = 0.06469271\n",
      "Iteration 123, loss = 0.06419482\n",
      "Iteration 124, loss = 0.06495263\n",
      "Iteration 125, loss = 0.06271672\n",
      "Iteration 126, loss = 0.06287216\n",
      "Iteration 127, loss = 0.06170348\n",
      "Iteration 128, loss = 0.06157954\n",
      "Iteration 129, loss = 0.06151562\n",
      "Iteration 130, loss = 0.06062940\n",
      "Iteration 131, loss = 0.06040816\n",
      "Iteration 132, loss = 0.05965442\n",
      "Iteration 133, loss = 0.05875545\n",
      "Iteration 134, loss = 0.05889163\n",
      "Iteration 135, loss = 0.05825985\n",
      "Iteration 136, loss = 0.05785803\n",
      "Iteration 137, loss = 0.05743671\n",
      "Iteration 138, loss = 0.05687137\n",
      "Iteration 139, loss = 0.05614562\n",
      "Iteration 140, loss = 0.05619436\n",
      "Iteration 141, loss = 0.05543760\n",
      "Iteration 142, loss = 0.05471046\n",
      "Iteration 143, loss = 0.05415792\n",
      "Iteration 144, loss = 0.05437291\n",
      "Iteration 145, loss = 0.05344728\n",
      "Iteration 146, loss = 0.05375953\n",
      "Iteration 147, loss = 0.05275767\n",
      "Iteration 148, loss = 0.05251213\n",
      "Iteration 149, loss = 0.05183222\n",
      "Iteration 150, loss = 0.05167573\n",
      "Iteration 151, loss = 0.05103255\n",
      "Iteration 152, loss = 0.05102812\n",
      "Iteration 153, loss = 0.05018187\n",
      "Iteration 154, loss = 0.04962025\n",
      "Iteration 155, loss = 0.04933967\n",
      "Iteration 156, loss = 0.04892433\n",
      "Iteration 157, loss = 0.04844702\n",
      "Iteration 158, loss = 0.04824916\n",
      "Iteration 159, loss = 0.04754267\n",
      "Iteration 160, loss = 0.04757924\n",
      "Iteration 161, loss = 0.04720394\n",
      "Iteration 162, loss = 0.04671199\n",
      "Iteration 163, loss = 0.04620501\n",
      "Iteration 164, loss = 0.04553593\n",
      "Iteration 165, loss = 0.04535843\n",
      "Iteration 166, loss = 0.04517744\n",
      "Iteration 167, loss = 0.04458413\n",
      "Iteration 168, loss = 0.04450867\n",
      "Iteration 169, loss = 0.04385076\n",
      "Iteration 170, loss = 0.04352655\n",
      "Iteration 171, loss = 0.04313371\n",
      "Iteration 172, loss = 0.04333724\n",
      "Iteration 173, loss = 0.04245310\n",
      "Iteration 174, loss = 0.04237551\n",
      "Iteration 175, loss = 0.04244031\n",
      "Iteration 176, loss = 0.04134669\n",
      "Iteration 177, loss = 0.04137846\n",
      "Iteration 178, loss = 0.04097140\n",
      "Iteration 179, loss = 0.04048888\n",
      "Iteration 180, loss = 0.04023776\n",
      "Iteration 181, loss = 0.03954457\n",
      "Iteration 182, loss = 0.03974031\n",
      "Iteration 183, loss = 0.03864950\n",
      "Iteration 184, loss = 0.03850581\n",
      "Iteration 185, loss = 0.03831219\n",
      "Iteration 186, loss = 0.03763812\n",
      "Iteration 187, loss = 0.03791873\n",
      "Iteration 188, loss = 0.03797055\n",
      "Iteration 189, loss = 0.03674475\n",
      "Iteration 190, loss = 0.03684319\n",
      "Iteration 191, loss = 0.03674523\n",
      "Iteration 192, loss = 0.03644268\n",
      "Iteration 193, loss = 0.03599993\n",
      "Iteration 194, loss = 0.03539878\n",
      "Iteration 195, loss = 0.03519259\n",
      "Iteration 196, loss = 0.03498417\n",
      "Iteration 197, loss = 0.03460491\n",
      "Iteration 198, loss = 0.03435171\n",
      "Iteration 199, loss = 0.03412471\n",
      "Iteration 200, loss = 0.03384361\n",
      "Iteration 1, loss = 0.61096914\n",
      "Iteration 2, loss = 0.40827804\n",
      "Iteration 3, loss = 0.33941493\n",
      "Iteration 4, loss = 0.30506645\n",
      "Iteration 5, loss = 0.28472963\n",
      "Iteration 6, loss = 0.27003333\n",
      "Iteration 7, loss = 0.25917016\n",
      "Iteration 8, loss = 0.25022302\n",
      "Iteration 9, loss = 0.24244499\n",
      "Iteration 10, loss = 0.23568113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.22983068\n",
      "Iteration 12, loss = 0.22440417\n",
      "Iteration 13, loss = 0.21889089\n",
      "Iteration 14, loss = 0.21417813\n",
      "Iteration 15, loss = 0.20920076\n",
      "Iteration 16, loss = 0.20464580\n",
      "Iteration 17, loss = 0.20068200\n",
      "Iteration 18, loss = 0.19691113\n",
      "Iteration 19, loss = 0.19293751\n",
      "Iteration 20, loss = 0.18887067\n",
      "Iteration 21, loss = 0.18541545\n",
      "Iteration 22, loss = 0.18182989\n",
      "Iteration 23, loss = 0.17865301\n",
      "Iteration 24, loss = 0.17550694\n",
      "Iteration 25, loss = 0.17224123\n",
      "Iteration 26, loss = 0.16919741\n",
      "Iteration 27, loss = 0.16659298\n",
      "Iteration 28, loss = 0.16363294\n",
      "Iteration 29, loss = 0.16104212\n",
      "Iteration 30, loss = 0.15840704\n",
      "Iteration 31, loss = 0.15568603\n",
      "Iteration 32, loss = 0.15321116\n",
      "Iteration 33, loss = 0.15104235\n",
      "Iteration 34, loss = 0.14882924\n",
      "Iteration 35, loss = 0.14657261\n",
      "Iteration 36, loss = 0.14439652\n",
      "Iteration 37, loss = 0.14242032\n",
      "Iteration 38, loss = 0.14029441\n",
      "Iteration 39, loss = 0.13833189\n",
      "Iteration 40, loss = 0.13669305\n",
      "Iteration 41, loss = 0.13490787\n",
      "Iteration 42, loss = 0.13303371\n",
      "Iteration 43, loss = 0.13129509\n",
      "Iteration 44, loss = 0.12979205\n",
      "Iteration 45, loss = 0.12826098\n",
      "Iteration 46, loss = 0.12665160\n",
      "Iteration 47, loss = 0.12537702\n",
      "Iteration 48, loss = 0.12391266\n",
      "Iteration 49, loss = 0.12240274\n",
      "Iteration 50, loss = 0.12156320\n",
      "Iteration 51, loss = 0.11988849\n",
      "Iteration 52, loss = 0.11913398\n",
      "Iteration 53, loss = 0.11761922\n",
      "Iteration 54, loss = 0.11600044\n",
      "Iteration 55, loss = 0.11509347\n",
      "Iteration 56, loss = 0.11397442\n",
      "Iteration 57, loss = 0.11280124\n",
      "Iteration 58, loss = 0.11152826\n",
      "Iteration 59, loss = 0.11056224\n",
      "Iteration 60, loss = 0.10940557\n",
      "Iteration 61, loss = 0.10806107\n",
      "Iteration 62, loss = 0.10713668\n",
      "Iteration 63, loss = 0.10647072\n",
      "Iteration 64, loss = 0.10513803\n",
      "Iteration 65, loss = 0.10414739\n",
      "Iteration 66, loss = 0.10336299\n",
      "Iteration 67, loss = 0.10252959\n",
      "Iteration 68, loss = 0.10136751\n",
      "Iteration 69, loss = 0.10082449\n",
      "Iteration 70, loss = 0.09981332\n",
      "Iteration 71, loss = 0.09920868\n",
      "Iteration 72, loss = 0.09789525\n",
      "Iteration 73, loss = 0.09680482\n",
      "Iteration 74, loss = 0.09561331\n",
      "Iteration 75, loss = 0.09522389\n",
      "Iteration 76, loss = 0.09425283\n",
      "Iteration 77, loss = 0.09342903\n",
      "Iteration 78, loss = 0.09253446\n",
      "Iteration 79, loss = 0.09202263\n",
      "Iteration 80, loss = 0.09127072\n",
      "Iteration 81, loss = 0.08984838\n",
      "Iteration 82, loss = 0.08974192\n",
      "Iteration 83, loss = 0.08868925\n",
      "Iteration 84, loss = 0.08795116\n",
      "Iteration 85, loss = 0.08710056\n",
      "Iteration 86, loss = 0.08668400\n",
      "Iteration 87, loss = 0.08565658\n",
      "Iteration 88, loss = 0.08497791\n",
      "Iteration 89, loss = 0.08435972\n",
      "Iteration 90, loss = 0.08342301\n",
      "Iteration 91, loss = 0.08233724\n",
      "Iteration 92, loss = 0.08186960\n",
      "Iteration 93, loss = 0.08092915\n",
      "Iteration 94, loss = 0.08067000\n",
      "Iteration 95, loss = 0.07966829\n",
      "Iteration 96, loss = 0.07926309\n",
      "Iteration 97, loss = 0.07814706\n",
      "Iteration 98, loss = 0.07794077\n",
      "Iteration 99, loss = 0.07738799\n",
      "Iteration 100, loss = 0.07672308\n",
      "Iteration 101, loss = 0.07588823\n",
      "Iteration 102, loss = 0.07565607\n",
      "Iteration 103, loss = 0.07485160\n",
      "Iteration 104, loss = 0.07429632\n",
      "Iteration 105, loss = 0.07374893\n",
      "Iteration 106, loss = 0.07274713\n",
      "Iteration 107, loss = 0.07237583\n",
      "Iteration 108, loss = 0.07211831\n",
      "Iteration 109, loss = 0.07139923\n",
      "Iteration 110, loss = 0.07066915\n",
      "Iteration 111, loss = 0.06982182\n",
      "Iteration 112, loss = 0.06927475\n",
      "Iteration 113, loss = 0.06852071\n",
      "Iteration 114, loss = 0.06779834\n",
      "Iteration 115, loss = 0.06729634\n",
      "Iteration 116, loss = 0.06758950\n",
      "Iteration 117, loss = 0.06599782\n",
      "Iteration 118, loss = 0.06582859\n",
      "Iteration 119, loss = 0.06525009\n",
      "Iteration 120, loss = 0.06475382\n",
      "Iteration 121, loss = 0.06430373\n",
      "Iteration 122, loss = 0.06348316\n",
      "Iteration 123, loss = 0.06318292\n",
      "Iteration 124, loss = 0.06300751\n",
      "Iteration 125, loss = 0.06156145\n",
      "Iteration 126, loss = 0.06165256\n",
      "Iteration 127, loss = 0.06106056\n",
      "Iteration 128, loss = 0.06009621\n",
      "Iteration 129, loss = 0.06049348\n",
      "Iteration 130, loss = 0.05928344\n",
      "Iteration 131, loss = 0.05878106\n",
      "Iteration 132, loss = 0.05806337\n",
      "Iteration 133, loss = 0.05783011\n",
      "Iteration 134, loss = 0.05759120\n",
      "Iteration 135, loss = 0.05744406\n",
      "Iteration 136, loss = 0.05632800\n",
      "Iteration 137, loss = 0.05613746\n",
      "Iteration 138, loss = 0.05558276\n",
      "Iteration 139, loss = 0.05475530\n",
      "Iteration 140, loss = 0.05460721\n",
      "Iteration 141, loss = 0.05417518\n",
      "Iteration 142, loss = 0.05365534\n",
      "Iteration 143, loss = 0.05307791\n",
      "Iteration 144, loss = 0.05300298\n",
      "Iteration 145, loss = 0.05222035\n",
      "Iteration 146, loss = 0.05173774\n",
      "Iteration 147, loss = 0.05139814\n",
      "Iteration 148, loss = 0.05160922\n",
      "Iteration 149, loss = 0.05093725\n",
      "Iteration 150, loss = 0.05043694\n",
      "Iteration 151, loss = 0.05005588\n",
      "Iteration 152, loss = 0.04957131\n",
      "Iteration 153, loss = 0.04927779\n",
      "Iteration 154, loss = 0.04813304\n",
      "Iteration 155, loss = 0.04778045\n",
      "Iteration 156, loss = 0.04778040\n",
      "Iteration 157, loss = 0.04664308\n",
      "Iteration 158, loss = 0.04653692\n",
      "Iteration 159, loss = 0.04617294\n",
      "Iteration 160, loss = 0.04579641\n",
      "Iteration 161, loss = 0.04554948\n",
      "Iteration 162, loss = 0.04489952\n",
      "Iteration 163, loss = 0.04448222\n",
      "Iteration 164, loss = 0.04402882\n",
      "Iteration 165, loss = 0.04409940\n",
      "Iteration 166, loss = 0.04363106\n",
      "Iteration 167, loss = 0.04395191\n",
      "Iteration 168, loss = 0.04264124\n",
      "Iteration 169, loss = 0.04270325\n",
      "Iteration 170, loss = 0.04234891\n",
      "Iteration 171, loss = 0.04210814\n",
      "Iteration 172, loss = 0.04152899\n",
      "Iteration 173, loss = 0.04142623\n",
      "Iteration 174, loss = 0.04021804\n",
      "Iteration 175, loss = 0.04133616\n",
      "Iteration 176, loss = 0.04009894\n",
      "Iteration 177, loss = 0.03938533\n",
      "Iteration 178, loss = 0.03938045\n",
      "Iteration 179, loss = 0.03881933\n",
      "Iteration 180, loss = 0.03835633\n",
      "Iteration 181, loss = 0.03789370\n",
      "Iteration 182, loss = 0.03775242\n",
      "Iteration 183, loss = 0.03725233\n",
      "Iteration 184, loss = 0.03721706\n",
      "Iteration 185, loss = 0.03690289\n",
      "Iteration 186, loss = 0.03624860\n",
      "Iteration 187, loss = 0.03619964\n",
      "Iteration 188, loss = 0.03621514\n",
      "Iteration 189, loss = 0.03574653\n",
      "Iteration 190, loss = 0.03531475\n",
      "Iteration 191, loss = 0.03515930\n",
      "Iteration 192, loss = 0.03459595\n",
      "Iteration 193, loss = 0.03477471\n",
      "Iteration 194, loss = 0.03401162\n",
      "Iteration 195, loss = 0.03354345\n",
      "Iteration 196, loss = 0.03334074\n",
      "Iteration 197, loss = 0.03311679\n",
      "Iteration 198, loss = 0.03278323\n",
      "Iteration 199, loss = 0.03244625\n",
      "Iteration 200, loss = 0.03228196\n",
      "Iteration 1, loss = 0.61443599\n",
      "Iteration 2, loss = 0.41124507\n",
      "Iteration 3, loss = 0.34086817\n",
      "Iteration 4, loss = 0.30539251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.28324845\n",
      "Iteration 6, loss = 0.26771142\n",
      "Iteration 7, loss = 0.25626075\n",
      "Iteration 8, loss = 0.24687524\n",
      "Iteration 9, loss = 0.23882976\n",
      "Iteration 10, loss = 0.23199458\n",
      "Iteration 11, loss = 0.22594089\n",
      "Iteration 12, loss = 0.22038276\n",
      "Iteration 13, loss = 0.21532472\n",
      "Iteration 14, loss = 0.21042936\n",
      "Iteration 15, loss = 0.20563170\n",
      "Iteration 16, loss = 0.20165529\n",
      "Iteration 17, loss = 0.19777822\n",
      "Iteration 18, loss = 0.19438728\n",
      "Iteration 19, loss = 0.19022226\n",
      "Iteration 20, loss = 0.18678957\n",
      "Iteration 21, loss = 0.18346746\n",
      "Iteration 22, loss = 0.18001516\n",
      "Iteration 23, loss = 0.17730317\n",
      "Iteration 24, loss = 0.17410757\n",
      "Iteration 25, loss = 0.17087472\n",
      "Iteration 26, loss = 0.16819949\n",
      "Iteration 27, loss = 0.16584097\n",
      "Iteration 28, loss = 0.16290026\n",
      "Iteration 29, loss = 0.16063910\n",
      "Iteration 30, loss = 0.15805801\n",
      "Iteration 31, loss = 0.15554701\n",
      "Iteration 32, loss = 0.15300829\n",
      "Iteration 33, loss = 0.15109563\n",
      "Iteration 34, loss = 0.14864170\n",
      "Iteration 35, loss = 0.14639027\n",
      "Iteration 36, loss = 0.14425770\n",
      "Iteration 37, loss = 0.14251680\n",
      "Iteration 38, loss = 0.14028337\n",
      "Iteration 39, loss = 0.13868494\n",
      "Iteration 40, loss = 0.13734187\n",
      "Iteration 41, loss = 0.13512850\n",
      "Iteration 42, loss = 0.13304322\n",
      "Iteration 43, loss = 0.13124007\n",
      "Iteration 44, loss = 0.12988481\n",
      "Iteration 45, loss = 0.12846578\n",
      "Iteration 46, loss = 0.12666448\n",
      "Iteration 47, loss = 0.12545201\n",
      "Iteration 48, loss = 0.12399893\n",
      "Iteration 49, loss = 0.12239113\n",
      "Iteration 50, loss = 0.12115829\n",
      "Iteration 51, loss = 0.11998524\n",
      "Iteration 52, loss = 0.11888991\n",
      "Iteration 53, loss = 0.11720851\n",
      "Iteration 54, loss = 0.11587287\n",
      "Iteration 55, loss = 0.11485322\n",
      "Iteration 56, loss = 0.11369341\n",
      "Iteration 57, loss = 0.11236163\n",
      "Iteration 58, loss = 0.11127379\n",
      "Iteration 59, loss = 0.11034620\n",
      "Iteration 60, loss = 0.10935111\n",
      "Iteration 61, loss = 0.10814800\n",
      "Iteration 62, loss = 0.10738815\n",
      "Iteration 63, loss = 0.10623859\n",
      "Iteration 64, loss = 0.10504777\n",
      "Iteration 65, loss = 0.10413736\n",
      "Iteration 66, loss = 0.10295900\n",
      "Iteration 67, loss = 0.10221435\n",
      "Iteration 68, loss = 0.10117001\n",
      "Iteration 69, loss = 0.10050455\n",
      "Iteration 70, loss = 0.09991207\n",
      "Iteration 71, loss = 0.09854453\n",
      "Iteration 72, loss = 0.09771330\n",
      "Iteration 73, loss = 0.09668364\n",
      "Iteration 74, loss = 0.09549337\n",
      "Iteration 75, loss = 0.09516516\n",
      "Iteration 76, loss = 0.09453725\n",
      "Iteration 77, loss = 0.09361568\n",
      "Iteration 78, loss = 0.09277893\n",
      "Iteration 79, loss = 0.09165237\n",
      "Iteration 80, loss = 0.09117660\n",
      "Iteration 81, loss = 0.09009282\n",
      "Iteration 82, loss = 0.08948860\n",
      "Iteration 83, loss = 0.08850640\n",
      "Iteration 84, loss = 0.08782236\n",
      "Iteration 85, loss = 0.08708670\n",
      "Iteration 86, loss = 0.08680682\n",
      "Iteration 87, loss = 0.08543874\n",
      "Iteration 88, loss = 0.08534317\n",
      "Iteration 89, loss = 0.08433626\n",
      "Iteration 90, loss = 0.08361074\n",
      "Iteration 91, loss = 0.08303687\n",
      "Iteration 92, loss = 0.08215066\n",
      "Iteration 93, loss = 0.08127863\n",
      "Iteration 94, loss = 0.08131432\n",
      "Iteration 95, loss = 0.08011283\n",
      "Iteration 96, loss = 0.07985154\n",
      "Iteration 97, loss = 0.07869085\n",
      "Iteration 98, loss = 0.07813965\n",
      "Iteration 99, loss = 0.07759829\n",
      "Iteration 100, loss = 0.07722759\n",
      "Iteration 101, loss = 0.07634828\n",
      "Iteration 102, loss = 0.07589902\n",
      "Iteration 103, loss = 0.07523982\n",
      "Iteration 104, loss = 0.07440472\n",
      "Iteration 105, loss = 0.07401384\n",
      "Iteration 106, loss = 0.07345761\n",
      "Iteration 107, loss = 0.07290434\n",
      "Iteration 108, loss = 0.07333445\n",
      "Iteration 109, loss = 0.07189508\n",
      "Iteration 110, loss = 0.07112936\n",
      "Iteration 111, loss = 0.07060112\n",
      "Iteration 112, loss = 0.06998613\n",
      "Iteration 113, loss = 0.06952714\n",
      "Iteration 114, loss = 0.06866650\n",
      "Iteration 115, loss = 0.06821419\n",
      "Iteration 116, loss = 0.06802012\n",
      "Iteration 117, loss = 0.06690022\n",
      "Iteration 118, loss = 0.06673080\n",
      "Iteration 119, loss = 0.06577783\n",
      "Iteration 120, loss = 0.06540917\n",
      "Iteration 121, loss = 0.06564986\n",
      "Iteration 122, loss = 0.06441439\n",
      "Iteration 123, loss = 0.06422903\n",
      "Iteration 124, loss = 0.06350931\n",
      "Iteration 125, loss = 0.06252964\n",
      "Iteration 126, loss = 0.06231430\n",
      "Iteration 127, loss = 0.06184115\n",
      "Iteration 128, loss = 0.06075727\n",
      "Iteration 129, loss = 0.06119931\n",
      "Iteration 130, loss = 0.06002592\n",
      "Iteration 131, loss = 0.05960538\n",
      "Iteration 132, loss = 0.05892782\n",
      "Iteration 133, loss = 0.05895952\n",
      "Iteration 134, loss = 0.05840843\n",
      "Iteration 135, loss = 0.05875908\n",
      "Iteration 136, loss = 0.05741961\n",
      "Iteration 137, loss = 0.05683561\n",
      "Iteration 138, loss = 0.05646130\n",
      "Iteration 139, loss = 0.05605222\n",
      "Iteration 140, loss = 0.05567342\n",
      "Iteration 141, loss = 0.05474126\n",
      "Iteration 142, loss = 0.05451551\n",
      "Iteration 143, loss = 0.05404908\n",
      "Iteration 144, loss = 0.05411944\n",
      "Iteration 145, loss = 0.05318320\n",
      "Iteration 146, loss = 0.05254322\n",
      "Iteration 147, loss = 0.05212208\n",
      "Iteration 148, loss = 0.05201717\n",
      "Iteration 149, loss = 0.05184073\n",
      "Iteration 150, loss = 0.05121949\n",
      "Iteration 151, loss = 0.05111667\n",
      "Iteration 152, loss = 0.05069461\n",
      "Iteration 153, loss = 0.05044699\n",
      "Iteration 154, loss = 0.04950020\n",
      "Iteration 155, loss = 0.04874022\n",
      "Iteration 156, loss = 0.04884293\n",
      "Iteration 157, loss = 0.04788966\n",
      "Iteration 158, loss = 0.04753196\n",
      "Iteration 159, loss = 0.04708561\n",
      "Iteration 160, loss = 0.04645616\n",
      "Iteration 161, loss = 0.04633816\n",
      "Iteration 162, loss = 0.04647625\n",
      "Iteration 163, loss = 0.04574133\n",
      "Iteration 164, loss = 0.04534456\n",
      "Iteration 165, loss = 0.04535816\n",
      "Iteration 166, loss = 0.04473205\n",
      "Iteration 167, loss = 0.04461526\n",
      "Iteration 168, loss = 0.04375577\n",
      "Iteration 169, loss = 0.04350152\n",
      "Iteration 170, loss = 0.04346499\n",
      "Iteration 171, loss = 0.04307188\n",
      "Iteration 172, loss = 0.04228218\n",
      "Iteration 173, loss = 0.04223306\n",
      "Iteration 174, loss = 0.04130428\n",
      "Iteration 175, loss = 0.04185617\n",
      "Iteration 176, loss = 0.04143577\n",
      "Iteration 177, loss = 0.04072763\n",
      "Iteration 178, loss = 0.04015934\n",
      "Iteration 179, loss = 0.03952326\n",
      "Iteration 180, loss = 0.03951312\n",
      "Iteration 181, loss = 0.03895394\n",
      "Iteration 182, loss = 0.03869973\n",
      "Iteration 183, loss = 0.03832793\n",
      "Iteration 184, loss = 0.03832007\n",
      "Iteration 185, loss = 0.03821863\n",
      "Iteration 186, loss = 0.03739880\n",
      "Iteration 187, loss = 0.03712704\n",
      "Iteration 188, loss = 0.03665415\n",
      "Iteration 189, loss = 0.03677228\n",
      "Iteration 190, loss = 0.03632805\n",
      "Iteration 191, loss = 0.03591643\n",
      "Iteration 192, loss = 0.03559389\n",
      "Iteration 193, loss = 0.03560692\n",
      "Iteration 194, loss = 0.03511067\n",
      "Iteration 195, loss = 0.03471964\n",
      "Iteration 196, loss = 0.03457461\n",
      "Iteration 197, loss = 0.03431713\n",
      "Iteration 198, loss = 0.03370291\n",
      "Iteration 199, loss = 0.03361809\n",
      "Iteration 200, loss = 0.03345449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72967515\n",
      "Iteration 2, loss = 0.59496349\n",
      "Iteration 3, loss = 0.52267396\n",
      "Iteration 4, loss = 0.48603518\n",
      "Iteration 5, loss = 0.46360083\n",
      "Iteration 6, loss = 0.44687568\n",
      "Iteration 7, loss = 0.43318800\n",
      "Iteration 8, loss = 0.42139958\n",
      "Iteration 9, loss = 0.41099244\n",
      "Iteration 10, loss = 0.40158733\n",
      "Iteration 11, loss = 0.39294219\n",
      "Iteration 12, loss = 0.38507323\n",
      "Iteration 13, loss = 0.37779074\n",
      "Iteration 14, loss = 0.37106081\n",
      "Iteration 15, loss = 0.36479777\n",
      "Iteration 16, loss = 0.35894824\n",
      "Iteration 17, loss = 0.35352208\n",
      "Iteration 18, loss = 0.34843874\n",
      "Iteration 19, loss = 0.34365369\n",
      "Iteration 20, loss = 0.33909521\n",
      "Iteration 21, loss = 0.33488506\n",
      "Iteration 22, loss = 0.33087924\n",
      "Iteration 23, loss = 0.32716166\n",
      "Iteration 24, loss = 0.32350217\n",
      "Iteration 25, loss = 0.32018488\n",
      "Iteration 26, loss = 0.31690850\n",
      "Iteration 27, loss = 0.31384015\n",
      "Iteration 28, loss = 0.31094233\n",
      "Iteration 29, loss = 0.30813766\n",
      "Iteration 30, loss = 0.30552326\n",
      "Iteration 31, loss = 0.30299643\n",
      "Iteration 32, loss = 0.30058484\n",
      "Iteration 33, loss = 0.29836358\n",
      "Iteration 34, loss = 0.29610344\n",
      "Iteration 35, loss = 0.29400862\n",
      "Iteration 36, loss = 0.29202058\n",
      "Iteration 37, loss = 0.29004289\n",
      "Iteration 38, loss = 0.28822140\n",
      "Iteration 39, loss = 0.28642563\n",
      "Iteration 40, loss = 0.28472328\n",
      "Iteration 41, loss = 0.28306945\n",
      "Iteration 42, loss = 0.28151655\n",
      "Iteration 43, loss = 0.27999753\n",
      "Iteration 44, loss = 0.27852785\n",
      "Iteration 45, loss = 0.27713756\n",
      "Iteration 46, loss = 0.27583600\n",
      "Iteration 47, loss = 0.27450975\n",
      "Iteration 48, loss = 0.27330732\n",
      "Iteration 49, loss = 0.27210031\n",
      "Iteration 50, loss = 0.27091533\n",
      "Iteration 51, loss = 0.26981104\n",
      "Iteration 52, loss = 0.26873697\n",
      "Iteration 53, loss = 0.26770050\n",
      "Iteration 54, loss = 0.26668517\n",
      "Iteration 55, loss = 0.26570769\n",
      "Iteration 56, loss = 0.26477348\n",
      "Iteration 57, loss = 0.26386535\n",
      "Iteration 58, loss = 0.26299075\n",
      "Iteration 59, loss = 0.26214345\n",
      "Iteration 60, loss = 0.26131226\n",
      "Iteration 61, loss = 0.26052288\n",
      "Iteration 62, loss = 0.25974440\n",
      "Iteration 63, loss = 0.25898880\n",
      "Iteration 64, loss = 0.25823875\n",
      "Iteration 65, loss = 0.25751844\n",
      "Iteration 66, loss = 0.25683288\n",
      "Iteration 67, loss = 0.25612261\n",
      "Iteration 68, loss = 0.25548442\n",
      "Iteration 69, loss = 0.25479196\n",
      "Iteration 70, loss = 0.25416262\n",
      "Iteration 71, loss = 0.25355139\n",
      "Iteration 72, loss = 0.25292892\n",
      "Iteration 73, loss = 0.25232183\n",
      "Iteration 74, loss = 0.25173352\n",
      "Iteration 75, loss = 0.25114753\n",
      "Iteration 76, loss = 0.25058930\n",
      "Iteration 77, loss = 0.25001407\n",
      "Iteration 78, loss = 0.24946835\n",
      "Iteration 79, loss = 0.24894282\n",
      "Iteration 80, loss = 0.24838588\n",
      "Iteration 81, loss = 0.24788959\n",
      "Iteration 82, loss = 0.24736023\n",
      "Iteration 83, loss = 0.24686764\n",
      "Iteration 84, loss = 0.24638570\n",
      "Iteration 85, loss = 0.24589638\n",
      "Iteration 86, loss = 0.24540381\n",
      "Iteration 87, loss = 0.24493713\n",
      "Iteration 88, loss = 0.24445836\n",
      "Iteration 89, loss = 0.24401059\n",
      "Iteration 90, loss = 0.24354564\n",
      "Iteration 91, loss = 0.24309945\n",
      "Iteration 92, loss = 0.24265059\n",
      "Iteration 93, loss = 0.24220520\n",
      "Iteration 94, loss = 0.24179482\n",
      "Iteration 95, loss = 0.24134619\n",
      "Iteration 96, loss = 0.24093313\n",
      "Iteration 97, loss = 0.24051270\n",
      "Iteration 98, loss = 0.24009681\n",
      "Iteration 99, loss = 0.23970027\n",
      "Iteration 100, loss = 0.23927917\n",
      "Iteration 101, loss = 0.23888870\n",
      "Iteration 102, loss = 0.23850412\n",
      "Iteration 103, loss = 0.23808996\n",
      "Iteration 104, loss = 0.23770163\n",
      "Iteration 105, loss = 0.23733672\n",
      "Iteration 106, loss = 0.23698955\n",
      "Iteration 107, loss = 0.23655183\n",
      "Iteration 108, loss = 0.23620925\n",
      "Iteration 109, loss = 0.23582105\n",
      "Iteration 110, loss = 0.23545277\n",
      "Iteration 111, loss = 0.23508471\n",
      "Iteration 112, loss = 0.23472944\n",
      "Iteration 113, loss = 0.23437848\n",
      "Iteration 114, loss = 0.23401532\n",
      "Iteration 115, loss = 0.23364535\n",
      "Iteration 116, loss = 0.23329638\n",
      "Iteration 117, loss = 0.23295389\n",
      "Iteration 118, loss = 0.23258596\n",
      "Iteration 119, loss = 0.23224601\n",
      "Iteration 120, loss = 0.23190709\n",
      "Iteration 121, loss = 0.23157753\n",
      "Iteration 122, loss = 0.23123151\n",
      "Iteration 123, loss = 0.23088911\n",
      "Iteration 124, loss = 0.23055133\n",
      "Iteration 125, loss = 0.23022509\n",
      "Iteration 126, loss = 0.22987881\n",
      "Iteration 127, loss = 0.22955877\n",
      "Iteration 128, loss = 0.22922846\n",
      "Iteration 129, loss = 0.22890830\n",
      "Iteration 130, loss = 0.22856634\n",
      "Iteration 131, loss = 0.22825019\n",
      "Iteration 132, loss = 0.22791357\n",
      "Iteration 133, loss = 0.22757762\n",
      "Iteration 134, loss = 0.22729005\n",
      "Iteration 135, loss = 0.22694849\n",
      "Iteration 136, loss = 0.22665351\n",
      "Iteration 137, loss = 0.22631780\n",
      "Iteration 138, loss = 0.22600891\n",
      "Iteration 139, loss = 0.22570425\n",
      "Iteration 140, loss = 0.22539231\n",
      "Iteration 141, loss = 0.22508149\n",
      "Iteration 142, loss = 0.22478908\n",
      "Iteration 143, loss = 0.22446510\n",
      "Iteration 144, loss = 0.22417731\n",
      "Iteration 145, loss = 0.22386600\n",
      "Iteration 146, loss = 0.22357747\n",
      "Iteration 147, loss = 0.22325232\n",
      "Iteration 148, loss = 0.22295496\n",
      "Iteration 149, loss = 0.22265045\n",
      "Iteration 150, loss = 0.22235511\n",
      "Iteration 151, loss = 0.22207129\n",
      "Iteration 152, loss = 0.22177714\n",
      "Iteration 153, loss = 0.22148772\n",
      "Iteration 154, loss = 0.22121101\n",
      "Iteration 155, loss = 0.22089977\n",
      "Iteration 156, loss = 0.22063146\n",
      "Iteration 157, loss = 0.22032830\n",
      "Iteration 158, loss = 0.22004154\n",
      "Iteration 159, loss = 0.21975948\n",
      "Iteration 160, loss = 0.21948630\n",
      "Iteration 161, loss = 0.21918051\n",
      "Iteration 162, loss = 0.21890949\n",
      "Iteration 163, loss = 0.21862583\n",
      "Iteration 164, loss = 0.21834256\n",
      "Iteration 165, loss = 0.21808627\n",
      "Iteration 166, loss = 0.21780285\n",
      "Iteration 167, loss = 0.21756027\n",
      "Iteration 168, loss = 0.21726407\n",
      "Iteration 169, loss = 0.21698194\n",
      "Iteration 170, loss = 0.21672310\n",
      "Iteration 171, loss = 0.21643688\n",
      "Iteration 172, loss = 0.21616610\n",
      "Iteration 173, loss = 0.21589673\n",
      "Iteration 174, loss = 0.21562858\n",
      "Iteration 175, loss = 0.21538780\n",
      "Iteration 176, loss = 0.21510378\n",
      "Iteration 177, loss = 0.21482038\n",
      "Iteration 178, loss = 0.21456311\n",
      "Iteration 179, loss = 0.21430086\n",
      "Iteration 180, loss = 0.21402932\n",
      "Iteration 181, loss = 0.21376015\n",
      "Iteration 182, loss = 0.21352900\n",
      "Iteration 183, loss = 0.21325617\n",
      "Iteration 184, loss = 0.21301007\n",
      "Iteration 185, loss = 0.21273966\n",
      "Iteration 186, loss = 0.21248057\n",
      "Iteration 187, loss = 0.21222890\n",
      "Iteration 188, loss = 0.21197692\n",
      "Iteration 189, loss = 0.21171181\n",
      "Iteration 190, loss = 0.21145247\n",
      "Iteration 191, loss = 0.21120283\n",
      "Iteration 192, loss = 0.21095948\n",
      "Iteration 193, loss = 0.21069879\n",
      "Iteration 194, loss = 0.21044047\n",
      "Iteration 195, loss = 0.21019854\n",
      "Iteration 196, loss = 0.20994756\n",
      "Iteration 197, loss = 0.20970925\n",
      "Iteration 198, loss = 0.20942679\n",
      "Iteration 199, loss = 0.20920456\n",
      "Iteration 200, loss = 0.20895829\n",
      "Iteration 1, loss = 0.73016497\n",
      "Iteration 2, loss = 0.59469330\n",
      "Iteration 3, loss = 0.52194651\n",
      "Iteration 4, loss = 0.48495748\n",
      "Iteration 5, loss = 0.46195429\n",
      "Iteration 6, loss = 0.44506392\n",
      "Iteration 7, loss = 0.43107907\n",
      "Iteration 8, loss = 0.41908235\n",
      "Iteration 9, loss = 0.40847157\n",
      "Iteration 10, loss = 0.39897103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39008481\n",
      "Iteration 12, loss = 0.38215268\n",
      "Iteration 13, loss = 0.37471941\n",
      "Iteration 14, loss = 0.36787484\n",
      "Iteration 15, loss = 0.36152775\n",
      "Iteration 16, loss = 0.35563352\n",
      "Iteration 17, loss = 0.35014521\n",
      "Iteration 18, loss = 0.34497516\n",
      "Iteration 19, loss = 0.34014470\n",
      "Iteration 20, loss = 0.33556125\n",
      "Iteration 21, loss = 0.33133248\n",
      "Iteration 22, loss = 0.32730253\n",
      "Iteration 23, loss = 0.32355588\n",
      "Iteration 24, loss = 0.31993990\n",
      "Iteration 25, loss = 0.31655647\n",
      "Iteration 26, loss = 0.31331544\n",
      "Iteration 27, loss = 0.31026913\n",
      "Iteration 28, loss = 0.30737392\n",
      "Iteration 29, loss = 0.30461104\n",
      "Iteration 30, loss = 0.30199895\n",
      "Iteration 31, loss = 0.29949049\n",
      "Iteration 32, loss = 0.29712140\n",
      "Iteration 33, loss = 0.29487808\n",
      "Iteration 34, loss = 0.29267368\n",
      "Iteration 35, loss = 0.29056947\n",
      "Iteration 36, loss = 0.28859007\n",
      "Iteration 37, loss = 0.28664964\n",
      "Iteration 38, loss = 0.28481805\n",
      "Iteration 39, loss = 0.28305051\n",
      "Iteration 40, loss = 0.28134725\n",
      "Iteration 41, loss = 0.27971418\n",
      "Iteration 42, loss = 0.27817829\n",
      "Iteration 43, loss = 0.27663725\n",
      "Iteration 44, loss = 0.27518640\n",
      "Iteration 45, loss = 0.27373794\n",
      "Iteration 46, loss = 0.27239852\n",
      "Iteration 47, loss = 0.27106637\n",
      "Iteration 48, loss = 0.26980275\n",
      "Iteration 49, loss = 0.26858711\n",
      "Iteration 50, loss = 0.26737730\n",
      "Iteration 51, loss = 0.26626073\n",
      "Iteration 52, loss = 0.26514494\n",
      "Iteration 53, loss = 0.26407676\n",
      "Iteration 54, loss = 0.26303844\n",
      "Iteration 55, loss = 0.26203322\n",
      "Iteration 56, loss = 0.26106552\n",
      "Iteration 57, loss = 0.26014805\n",
      "Iteration 58, loss = 0.25923131\n",
      "Iteration 59, loss = 0.25834289\n",
      "Iteration 60, loss = 0.25748318\n",
      "Iteration 61, loss = 0.25665955\n",
      "Iteration 62, loss = 0.25584812\n",
      "Iteration 63, loss = 0.25507876\n",
      "Iteration 64, loss = 0.25431840\n",
      "Iteration 65, loss = 0.25358718\n",
      "Iteration 66, loss = 0.25285067\n",
      "Iteration 67, loss = 0.25214093\n",
      "Iteration 68, loss = 0.25148354\n",
      "Iteration 69, loss = 0.25077363\n",
      "Iteration 70, loss = 0.25011821\n",
      "Iteration 71, loss = 0.24950789\n",
      "Iteration 72, loss = 0.24886162\n",
      "Iteration 73, loss = 0.24825035\n",
      "Iteration 74, loss = 0.24765504\n",
      "Iteration 75, loss = 0.24706282\n",
      "Iteration 76, loss = 0.24647462\n",
      "Iteration 77, loss = 0.24590456\n",
      "Iteration 78, loss = 0.24536379\n",
      "Iteration 79, loss = 0.24481621\n",
      "Iteration 80, loss = 0.24424974\n",
      "Iteration 81, loss = 0.24373390\n",
      "Iteration 82, loss = 0.24320876\n",
      "Iteration 83, loss = 0.24271470\n",
      "Iteration 84, loss = 0.24220338\n",
      "Iteration 85, loss = 0.24172144\n",
      "Iteration 86, loss = 0.24122724\n",
      "Iteration 87, loss = 0.24077606\n",
      "Iteration 88, loss = 0.24025846\n",
      "Iteration 89, loss = 0.23982493\n",
      "Iteration 90, loss = 0.23936230\n",
      "Iteration 91, loss = 0.23890329\n",
      "Iteration 92, loss = 0.23844928\n",
      "Iteration 93, loss = 0.23800721\n",
      "Iteration 94, loss = 0.23758197\n",
      "Iteration 95, loss = 0.23713636\n",
      "Iteration 96, loss = 0.23671078\n",
      "Iteration 97, loss = 0.23630121\n",
      "Iteration 98, loss = 0.23586285\n",
      "Iteration 99, loss = 0.23547859\n",
      "Iteration 100, loss = 0.23504530\n",
      "Iteration 101, loss = 0.23463872\n",
      "Iteration 102, loss = 0.23423834\n",
      "Iteration 103, loss = 0.23385375\n",
      "Iteration 104, loss = 0.23344130\n",
      "Iteration 105, loss = 0.23305669\n",
      "Iteration 106, loss = 0.23271060\n",
      "Iteration 107, loss = 0.23228804\n",
      "Iteration 108, loss = 0.23191204\n",
      "Iteration 109, loss = 0.23154112\n",
      "Iteration 110, loss = 0.23115869\n",
      "Iteration 111, loss = 0.23079293\n",
      "Iteration 112, loss = 0.23041418\n",
      "Iteration 113, loss = 0.23006224\n",
      "Iteration 114, loss = 0.22972067\n",
      "Iteration 115, loss = 0.22931879\n",
      "Iteration 116, loss = 0.22897441\n",
      "Iteration 117, loss = 0.22860650\n",
      "Iteration 118, loss = 0.22826408\n",
      "Iteration 119, loss = 0.22791106\n",
      "Iteration 120, loss = 0.22757614\n",
      "Iteration 121, loss = 0.22721944\n",
      "Iteration 122, loss = 0.22688612\n",
      "Iteration 123, loss = 0.22652369\n",
      "Iteration 124, loss = 0.22619288\n",
      "Iteration 125, loss = 0.22584482\n",
      "Iteration 126, loss = 0.22552537\n",
      "Iteration 127, loss = 0.22518213\n",
      "Iteration 128, loss = 0.22486653\n",
      "Iteration 129, loss = 0.22453028\n",
      "Iteration 130, loss = 0.22418298\n",
      "Iteration 131, loss = 0.22385269\n",
      "Iteration 132, loss = 0.22354413\n",
      "Iteration 133, loss = 0.22320195\n",
      "Iteration 134, loss = 0.22288925\n",
      "Iteration 135, loss = 0.22256650\n",
      "Iteration 136, loss = 0.22224665\n",
      "Iteration 137, loss = 0.22191235\n",
      "Iteration 138, loss = 0.22161974\n",
      "Iteration 139, loss = 0.22129461\n",
      "Iteration 140, loss = 0.22098924\n",
      "Iteration 141, loss = 0.22068039\n",
      "Iteration 142, loss = 0.22036836\n",
      "Iteration 143, loss = 0.22005045\n",
      "Iteration 144, loss = 0.21977291\n",
      "Iteration 145, loss = 0.21944309\n",
      "Iteration 146, loss = 0.21914195\n",
      "Iteration 147, loss = 0.21884314\n",
      "Iteration 148, loss = 0.21853491\n",
      "Iteration 149, loss = 0.21822994\n",
      "Iteration 150, loss = 0.21793967\n",
      "Iteration 151, loss = 0.21764107\n",
      "Iteration 152, loss = 0.21736028\n",
      "Iteration 153, loss = 0.21706705\n",
      "Iteration 154, loss = 0.21677570\n",
      "Iteration 155, loss = 0.21648312\n",
      "Iteration 156, loss = 0.21621961\n",
      "Iteration 157, loss = 0.21591602\n",
      "Iteration 158, loss = 0.21561733\n",
      "Iteration 159, loss = 0.21534834\n",
      "Iteration 160, loss = 0.21507417\n",
      "Iteration 161, loss = 0.21477829\n",
      "Iteration 162, loss = 0.21447863\n",
      "Iteration 163, loss = 0.21421225\n",
      "Iteration 164, loss = 0.21392443\n",
      "Iteration 165, loss = 0.21366421\n",
      "Iteration 166, loss = 0.21337032\n",
      "Iteration 167, loss = 0.21311809\n",
      "Iteration 168, loss = 0.21282110\n",
      "Iteration 169, loss = 0.21255207\n",
      "Iteration 170, loss = 0.21228331\n",
      "Iteration 171, loss = 0.21199603\n",
      "Iteration 172, loss = 0.21173158\n",
      "Iteration 173, loss = 0.21146196\n",
      "Iteration 174, loss = 0.21118243\n",
      "Iteration 175, loss = 0.21092039\n",
      "Iteration 176, loss = 0.21066977\n",
      "Iteration 177, loss = 0.21037598\n",
      "Iteration 178, loss = 0.21009020\n",
      "Iteration 179, loss = 0.20984659\n",
      "Iteration 180, loss = 0.20956610\n",
      "Iteration 181, loss = 0.20929495\n",
      "Iteration 182, loss = 0.20904426\n",
      "Iteration 183, loss = 0.20878338\n",
      "Iteration 184, loss = 0.20853286\n",
      "Iteration 185, loss = 0.20825724\n",
      "Iteration 186, loss = 0.20800715\n",
      "Iteration 187, loss = 0.20775403\n",
      "Iteration 188, loss = 0.20750879\n",
      "Iteration 189, loss = 0.20724016\n",
      "Iteration 190, loss = 0.20698467\n",
      "Iteration 191, loss = 0.20673869\n",
      "Iteration 192, loss = 0.20649234\n",
      "Iteration 193, loss = 0.20623821\n",
      "Iteration 194, loss = 0.20598891\n",
      "Iteration 195, loss = 0.20574729\n",
      "Iteration 196, loss = 0.20551250\n",
      "Iteration 197, loss = 0.20526056\n",
      "Iteration 198, loss = 0.20500924\n",
      "Iteration 199, loss = 0.20477873\n",
      "Iteration 200, loss = 0.20452477\n",
      "Iteration 1, loss = 0.73357680\n",
      "Iteration 2, loss = 0.59795252\n",
      "Iteration 3, loss = 0.52619670\n",
      "Iteration 4, loss = 0.48966843\n",
      "Iteration 5, loss = 0.46696666\n",
      "Iteration 6, loss = 0.45016318\n",
      "Iteration 7, loss = 0.43642280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.42460146\n",
      "Iteration 9, loss = 0.41411600\n",
      "Iteration 10, loss = 0.40465310\n",
      "Iteration 11, loss = 0.39593745\n",
      "Iteration 12, loss = 0.38808816\n",
      "Iteration 13, loss = 0.38072010\n",
      "Iteration 14, loss = 0.37395850\n",
      "Iteration 15, loss = 0.36762905\n",
      "Iteration 16, loss = 0.36178655\n",
      "Iteration 17, loss = 0.35633681\n",
      "Iteration 18, loss = 0.35117674\n",
      "Iteration 19, loss = 0.34639423\n",
      "Iteration 20, loss = 0.34184409\n",
      "Iteration 21, loss = 0.33759442\n",
      "Iteration 22, loss = 0.33361576\n",
      "Iteration 23, loss = 0.32984179\n",
      "Iteration 24, loss = 0.32626554\n",
      "Iteration 25, loss = 0.32288766\n",
      "Iteration 26, loss = 0.31964200\n",
      "Iteration 27, loss = 0.31661408\n",
      "Iteration 28, loss = 0.31369329\n",
      "Iteration 29, loss = 0.31094233\n",
      "Iteration 30, loss = 0.30832083\n",
      "Iteration 31, loss = 0.30581275\n",
      "Iteration 32, loss = 0.30341467\n",
      "Iteration 33, loss = 0.30113481\n",
      "Iteration 34, loss = 0.29891759\n",
      "Iteration 35, loss = 0.29683815\n",
      "Iteration 36, loss = 0.29479691\n",
      "Iteration 37, loss = 0.29288295\n",
      "Iteration 38, loss = 0.29102878\n",
      "Iteration 39, loss = 0.28924011\n",
      "Iteration 40, loss = 0.28755582\n",
      "Iteration 41, loss = 0.28590074\n",
      "Iteration 42, loss = 0.28434042\n",
      "Iteration 43, loss = 0.28278559\n",
      "Iteration 44, loss = 0.28131498\n",
      "Iteration 45, loss = 0.27990490\n",
      "Iteration 46, loss = 0.27854964\n",
      "Iteration 47, loss = 0.27722497\n",
      "Iteration 48, loss = 0.27593850\n",
      "Iteration 49, loss = 0.27472677\n",
      "Iteration 50, loss = 0.27350691\n",
      "Iteration 51, loss = 0.27242874\n",
      "Iteration 52, loss = 0.27127329\n",
      "Iteration 53, loss = 0.27019426\n",
      "Iteration 54, loss = 0.26916949\n",
      "Iteration 55, loss = 0.26815098\n",
      "Iteration 56, loss = 0.26719611\n",
      "Iteration 57, loss = 0.26625107\n",
      "Iteration 58, loss = 0.26534740\n",
      "Iteration 59, loss = 0.26443709\n",
      "Iteration 60, loss = 0.26357459\n",
      "Iteration 61, loss = 0.26273935\n",
      "Iteration 62, loss = 0.26192497\n",
      "Iteration 63, loss = 0.26113885\n",
      "Iteration 64, loss = 0.26036693\n",
      "Iteration 65, loss = 0.25961751\n",
      "Iteration 66, loss = 0.25887745\n",
      "Iteration 67, loss = 0.25818925\n",
      "Iteration 68, loss = 0.25747789\n",
      "Iteration 69, loss = 0.25678372\n",
      "Iteration 70, loss = 0.25612640\n",
      "Iteration 71, loss = 0.25548044\n",
      "Iteration 72, loss = 0.25483342\n",
      "Iteration 73, loss = 0.25420434\n",
      "Iteration 74, loss = 0.25359126\n",
      "Iteration 75, loss = 0.25299439\n",
      "Iteration 76, loss = 0.25241680\n",
      "Iteration 77, loss = 0.25180930\n",
      "Iteration 78, loss = 0.25124698\n",
      "Iteration 79, loss = 0.25069549\n",
      "Iteration 80, loss = 0.25013422\n",
      "Iteration 81, loss = 0.24958869\n",
      "Iteration 82, loss = 0.24905591\n",
      "Iteration 83, loss = 0.24854193\n",
      "Iteration 84, loss = 0.24801386\n",
      "Iteration 85, loss = 0.24749825\n",
      "Iteration 86, loss = 0.24699469\n",
      "Iteration 87, loss = 0.24650974\n",
      "Iteration 88, loss = 0.24599038\n",
      "Iteration 89, loss = 0.24551823\n",
      "Iteration 90, loss = 0.24504262\n",
      "Iteration 91, loss = 0.24457789\n",
      "Iteration 92, loss = 0.24409162\n",
      "Iteration 93, loss = 0.24365688\n",
      "Iteration 94, loss = 0.24319782\n",
      "Iteration 95, loss = 0.24273734\n",
      "Iteration 96, loss = 0.24230912\n",
      "Iteration 97, loss = 0.24185495\n",
      "Iteration 98, loss = 0.24142671\n",
      "Iteration 99, loss = 0.24100024\n",
      "Iteration 100, loss = 0.24057707\n",
      "Iteration 101, loss = 0.24013371\n",
      "Iteration 102, loss = 0.23971385\n",
      "Iteration 103, loss = 0.23931777\n",
      "Iteration 104, loss = 0.23888516\n",
      "Iteration 105, loss = 0.23848580\n",
      "Iteration 106, loss = 0.23809197\n",
      "Iteration 107, loss = 0.23767378\n",
      "Iteration 108, loss = 0.23725825\n",
      "Iteration 109, loss = 0.23690598\n",
      "Iteration 110, loss = 0.23647334\n",
      "Iteration 111, loss = 0.23609142\n",
      "Iteration 112, loss = 0.23568833\n",
      "Iteration 113, loss = 0.23531812\n",
      "Iteration 114, loss = 0.23492767\n",
      "Iteration 115, loss = 0.23454804\n",
      "Iteration 116, loss = 0.23415496\n",
      "Iteration 117, loss = 0.23377781\n",
      "Iteration 118, loss = 0.23339762\n",
      "Iteration 119, loss = 0.23302564\n",
      "Iteration 120, loss = 0.23265216\n",
      "Iteration 121, loss = 0.23227979\n",
      "Iteration 122, loss = 0.23192028\n",
      "Iteration 123, loss = 0.23155390\n",
      "Iteration 124, loss = 0.23119528\n",
      "Iteration 125, loss = 0.23081998\n",
      "Iteration 126, loss = 0.23049944\n",
      "Iteration 127, loss = 0.23011219\n",
      "Iteration 128, loss = 0.22978902\n",
      "Iteration 129, loss = 0.22942394\n",
      "Iteration 130, loss = 0.22905068\n",
      "Iteration 131, loss = 0.22870555\n",
      "Iteration 132, loss = 0.22837592\n",
      "Iteration 133, loss = 0.22801224\n",
      "Iteration 134, loss = 0.22768596\n",
      "Iteration 135, loss = 0.22733714\n",
      "Iteration 136, loss = 0.22699832\n",
      "Iteration 137, loss = 0.22666569\n",
      "Iteration 138, loss = 0.22633329\n",
      "Iteration 139, loss = 0.22597948\n",
      "Iteration 140, loss = 0.22564191\n",
      "Iteration 141, loss = 0.22531794\n",
      "Iteration 142, loss = 0.22496294\n",
      "Iteration 143, loss = 0.22463682\n",
      "Iteration 144, loss = 0.22434135\n",
      "Iteration 145, loss = 0.22397952\n",
      "Iteration 146, loss = 0.22365718\n",
      "Iteration 147, loss = 0.22334632\n",
      "Iteration 148, loss = 0.22299738\n",
      "Iteration 149, loss = 0.22267715\n",
      "Iteration 150, loss = 0.22237609\n",
      "Iteration 151, loss = 0.22204983\n",
      "Iteration 152, loss = 0.22175063\n",
      "Iteration 153, loss = 0.22141926\n",
      "Iteration 154, loss = 0.22111720\n",
      "Iteration 155, loss = 0.22080211\n",
      "Iteration 156, loss = 0.22050381\n",
      "Iteration 157, loss = 0.22018061\n",
      "Iteration 158, loss = 0.21986564\n",
      "Iteration 159, loss = 0.21957225\n",
      "Iteration 160, loss = 0.21927835\n",
      "Iteration 161, loss = 0.21895782\n",
      "Iteration 162, loss = 0.21865109\n",
      "Iteration 163, loss = 0.21834741\n",
      "Iteration 164, loss = 0.21804218\n",
      "Iteration 165, loss = 0.21776827\n",
      "Iteration 166, loss = 0.21747058\n",
      "Iteration 167, loss = 0.21716112\n",
      "Iteration 168, loss = 0.21685407\n",
      "Iteration 169, loss = 0.21657403\n",
      "Iteration 170, loss = 0.21626677\n",
      "Iteration 171, loss = 0.21597574\n",
      "Iteration 172, loss = 0.21567698\n",
      "Iteration 173, loss = 0.21539134\n",
      "Iteration 174, loss = 0.21510207\n",
      "Iteration 175, loss = 0.21480375\n",
      "Iteration 176, loss = 0.21452053\n",
      "Iteration 177, loss = 0.21422726\n",
      "Iteration 178, loss = 0.21393583\n",
      "Iteration 179, loss = 0.21366385\n",
      "Iteration 180, loss = 0.21337904\n",
      "Iteration 181, loss = 0.21307168\n",
      "Iteration 182, loss = 0.21280767\n",
      "Iteration 183, loss = 0.21252331\n",
      "Iteration 184, loss = 0.21226841\n",
      "Iteration 185, loss = 0.21197915\n",
      "Iteration 186, loss = 0.21170295\n",
      "Iteration 187, loss = 0.21144948\n",
      "Iteration 188, loss = 0.21116860\n",
      "Iteration 189, loss = 0.21088808\n",
      "Iteration 190, loss = 0.21060764\n",
      "Iteration 191, loss = 0.21037599\n",
      "Iteration 192, loss = 0.21008089\n",
      "Iteration 193, loss = 0.20980927\n",
      "Iteration 194, loss = 0.20952437\n",
      "Iteration 195, loss = 0.20925907\n",
      "Iteration 196, loss = 0.20899650\n",
      "Iteration 197, loss = 0.20873749\n",
      "Iteration 198, loss = 0.20846526\n",
      "Iteration 199, loss = 0.20822103\n",
      "Iteration 200, loss = 0.20793117\n",
      "Iteration 1, loss = 0.72893468\n",
      "Iteration 2, loss = 0.59345408\n",
      "Iteration 3, loss = 0.52125208\n",
      "Iteration 4, loss = 0.48444341\n",
      "Iteration 5, loss = 0.46179237\n",
      "Iteration 6, loss = 0.44491724\n",
      "Iteration 7, loss = 0.43123666\n",
      "Iteration 8, loss = 0.41945931\n",
      "Iteration 9, loss = 0.40899003\n",
      "Iteration 10, loss = 0.39956630\n",
      "Iteration 11, loss = 0.39090363\n",
      "Iteration 12, loss = 0.38304279\n",
      "Iteration 13, loss = 0.37576833\n",
      "Iteration 14, loss = 0.36902327\n",
      "Iteration 15, loss = 0.36272223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.35687914\n",
      "Iteration 17, loss = 0.35151520\n",
      "Iteration 18, loss = 0.34636486\n",
      "Iteration 19, loss = 0.34161578\n",
      "Iteration 20, loss = 0.33716266\n",
      "Iteration 21, loss = 0.33293989\n",
      "Iteration 22, loss = 0.32900450\n",
      "Iteration 23, loss = 0.32527878\n",
      "Iteration 24, loss = 0.32174315\n",
      "Iteration 25, loss = 0.31840113\n",
      "Iteration 26, loss = 0.31523132\n",
      "Iteration 27, loss = 0.31225524\n",
      "Iteration 28, loss = 0.30940627\n",
      "Iteration 29, loss = 0.30667651\n",
      "Iteration 30, loss = 0.30409987\n",
      "Iteration 31, loss = 0.30167117\n",
      "Iteration 32, loss = 0.29932198\n",
      "Iteration 33, loss = 0.29709615\n",
      "Iteration 34, loss = 0.29494401\n",
      "Iteration 35, loss = 0.29293420\n",
      "Iteration 36, loss = 0.29095191\n",
      "Iteration 37, loss = 0.28909129\n",
      "Iteration 38, loss = 0.28728174\n",
      "Iteration 39, loss = 0.28558573\n",
      "Iteration 40, loss = 0.28392736\n",
      "Iteration 41, loss = 0.28234066\n",
      "Iteration 42, loss = 0.28079986\n",
      "Iteration 43, loss = 0.27933755\n",
      "Iteration 44, loss = 0.27789603\n",
      "Iteration 45, loss = 0.27655744\n",
      "Iteration 46, loss = 0.27521941\n",
      "Iteration 47, loss = 0.27395959\n",
      "Iteration 48, loss = 0.27269583\n",
      "Iteration 49, loss = 0.27155836\n",
      "Iteration 50, loss = 0.27036410\n",
      "Iteration 51, loss = 0.26925624\n",
      "Iteration 52, loss = 0.26818284\n",
      "Iteration 53, loss = 0.26712358\n",
      "Iteration 54, loss = 0.26612941\n",
      "Iteration 55, loss = 0.26512032\n",
      "Iteration 56, loss = 0.26419549\n",
      "Iteration 57, loss = 0.26325549\n",
      "Iteration 58, loss = 0.26235266\n",
      "Iteration 59, loss = 0.26146200\n",
      "Iteration 60, loss = 0.26062309\n",
      "Iteration 61, loss = 0.25978129\n",
      "Iteration 62, loss = 0.25896126\n",
      "Iteration 63, loss = 0.25818213\n",
      "Iteration 64, loss = 0.25741146\n",
      "Iteration 65, loss = 0.25665081\n",
      "Iteration 66, loss = 0.25590844\n",
      "Iteration 67, loss = 0.25522626\n",
      "Iteration 68, loss = 0.25449620\n",
      "Iteration 69, loss = 0.25380908\n",
      "Iteration 70, loss = 0.25315344\n",
      "Iteration 71, loss = 0.25248970\n",
      "Iteration 72, loss = 0.25183146\n",
      "Iteration 73, loss = 0.25119526\n",
      "Iteration 74, loss = 0.25057437\n",
      "Iteration 75, loss = 0.24998828\n",
      "Iteration 76, loss = 0.24938114\n",
      "Iteration 77, loss = 0.24877760\n",
      "Iteration 78, loss = 0.24820870\n",
      "Iteration 79, loss = 0.24765569\n",
      "Iteration 80, loss = 0.24707910\n",
      "Iteration 81, loss = 0.24653124\n",
      "Iteration 82, loss = 0.24599520\n",
      "Iteration 83, loss = 0.24548250\n",
      "Iteration 84, loss = 0.24495873\n",
      "Iteration 85, loss = 0.24444118\n",
      "Iteration 86, loss = 0.24396740\n",
      "Iteration 87, loss = 0.24347436\n",
      "Iteration 88, loss = 0.24296423\n",
      "Iteration 89, loss = 0.24250493\n",
      "Iteration 90, loss = 0.24204104\n",
      "Iteration 91, loss = 0.24158138\n",
      "Iteration 92, loss = 0.24110116\n",
      "Iteration 93, loss = 0.24066591\n",
      "Iteration 94, loss = 0.24021676\n",
      "Iteration 95, loss = 0.23976493\n",
      "Iteration 96, loss = 0.23935042\n",
      "Iteration 97, loss = 0.23891884\n",
      "Iteration 98, loss = 0.23850578\n",
      "Iteration 99, loss = 0.23810690\n",
      "Iteration 100, loss = 0.23768072\n",
      "Iteration 101, loss = 0.23726411\n",
      "Iteration 102, loss = 0.23685048\n",
      "Iteration 103, loss = 0.23646581\n",
      "Iteration 104, loss = 0.23606034\n",
      "Iteration 105, loss = 0.23568130\n",
      "Iteration 106, loss = 0.23527255\n",
      "Iteration 107, loss = 0.23489355\n",
      "Iteration 108, loss = 0.23451303\n",
      "Iteration 109, loss = 0.23415078\n",
      "Iteration 110, loss = 0.23376219\n",
      "Iteration 111, loss = 0.23338269\n",
      "Iteration 112, loss = 0.23300735\n",
      "Iteration 113, loss = 0.23264360\n",
      "Iteration 114, loss = 0.23226352\n",
      "Iteration 115, loss = 0.23192806\n",
      "Iteration 116, loss = 0.23157238\n",
      "Iteration 117, loss = 0.23119733\n",
      "Iteration 118, loss = 0.23085450\n",
      "Iteration 119, loss = 0.23050794\n",
      "Iteration 120, loss = 0.23015642\n",
      "Iteration 121, loss = 0.22979639\n",
      "Iteration 122, loss = 0.22946655\n",
      "Iteration 123, loss = 0.22911173\n",
      "Iteration 124, loss = 0.22878058\n",
      "Iteration 125, loss = 0.22842794\n",
      "Iteration 126, loss = 0.22811266\n",
      "Iteration 127, loss = 0.22776656\n",
      "Iteration 128, loss = 0.22743204\n",
      "Iteration 129, loss = 0.22710209\n",
      "Iteration 130, loss = 0.22676668\n",
      "Iteration 131, loss = 0.22644967\n",
      "Iteration 132, loss = 0.22612754\n",
      "Iteration 133, loss = 0.22578710\n",
      "Iteration 134, loss = 0.22548855\n",
      "Iteration 135, loss = 0.22515319\n",
      "Iteration 136, loss = 0.22483967\n",
      "Iteration 137, loss = 0.22453697\n",
      "Iteration 138, loss = 0.22423310\n",
      "Iteration 139, loss = 0.22388804\n",
      "Iteration 140, loss = 0.22359246\n",
      "Iteration 141, loss = 0.22329571\n",
      "Iteration 142, loss = 0.22298071\n",
      "Iteration 143, loss = 0.22265477\n",
      "Iteration 144, loss = 0.22237851\n",
      "Iteration 145, loss = 0.22203109\n",
      "Iteration 146, loss = 0.22172346\n",
      "Iteration 147, loss = 0.22145075\n",
      "Iteration 148, loss = 0.22112634\n",
      "Iteration 149, loss = 0.22080033\n",
      "Iteration 150, loss = 0.22050696\n",
      "Iteration 151, loss = 0.22021741\n",
      "Iteration 152, loss = 0.21991828\n",
      "Iteration 153, loss = 0.21959702\n",
      "Iteration 154, loss = 0.21930589\n",
      "Iteration 155, loss = 0.21900327\n",
      "Iteration 156, loss = 0.21871954\n",
      "Iteration 157, loss = 0.21842563\n",
      "Iteration 158, loss = 0.21811440\n",
      "Iteration 159, loss = 0.21784542\n",
      "Iteration 160, loss = 0.21753856\n",
      "Iteration 161, loss = 0.21725110\n",
      "Iteration 162, loss = 0.21695231\n",
      "Iteration 163, loss = 0.21667254\n",
      "Iteration 164, loss = 0.21638829\n",
      "Iteration 165, loss = 0.21609941\n",
      "Iteration 166, loss = 0.21583989\n",
      "Iteration 167, loss = 0.21552909\n",
      "Iteration 168, loss = 0.21526229\n",
      "Iteration 169, loss = 0.21500236\n",
      "Iteration 170, loss = 0.21469297\n",
      "Iteration 171, loss = 0.21442786\n",
      "Iteration 172, loss = 0.21413779\n",
      "Iteration 173, loss = 0.21385666\n",
      "Iteration 174, loss = 0.21358024\n",
      "Iteration 175, loss = 0.21330568\n",
      "Iteration 176, loss = 0.21303561\n",
      "Iteration 177, loss = 0.21274642\n",
      "Iteration 178, loss = 0.21247379\n",
      "Iteration 179, loss = 0.21221991\n",
      "Iteration 180, loss = 0.21193819\n",
      "Iteration 181, loss = 0.21164953\n",
      "Iteration 182, loss = 0.21138911\n",
      "Iteration 183, loss = 0.21113201\n",
      "Iteration 184, loss = 0.21087094\n",
      "Iteration 185, loss = 0.21059218\n",
      "Iteration 186, loss = 0.21033509\n",
      "Iteration 187, loss = 0.21008675\n",
      "Iteration 188, loss = 0.20983242\n",
      "Iteration 189, loss = 0.20955037\n",
      "Iteration 190, loss = 0.20925822\n",
      "Iteration 191, loss = 0.20903181\n",
      "Iteration 192, loss = 0.20876089\n",
      "Iteration 193, loss = 0.20850156\n",
      "Iteration 194, loss = 0.20822451\n",
      "Iteration 195, loss = 0.20795698\n",
      "Iteration 196, loss = 0.20770586\n",
      "Iteration 197, loss = 0.20745022\n",
      "Iteration 198, loss = 0.20719749\n",
      "Iteration 199, loss = 0.20693841\n",
      "Iteration 200, loss = 0.20667739\n",
      "Iteration 1, loss = 0.73466796\n",
      "Iteration 2, loss = 0.60031035\n",
      "Iteration 3, loss = 0.52826222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.49127231\n",
      "Iteration 5, loss = 0.46812641\n",
      "Iteration 6, loss = 0.45089711\n",
      "Iteration 7, loss = 0.43686770\n",
      "Iteration 8, loss = 0.42480478\n",
      "Iteration 9, loss = 0.41392412\n",
      "Iteration 10, loss = 0.40421073\n",
      "Iteration 11, loss = 0.39519409\n",
      "Iteration 12, loss = 0.38698513\n",
      "Iteration 13, loss = 0.37936569\n",
      "Iteration 14, loss = 0.37239618\n",
      "Iteration 15, loss = 0.36574379\n",
      "Iteration 16, loss = 0.35959780\n",
      "Iteration 17, loss = 0.35397943\n",
      "Iteration 18, loss = 0.34854622\n",
      "Iteration 19, loss = 0.34346724\n",
      "Iteration 20, loss = 0.33877395\n",
      "Iteration 21, loss = 0.33430448\n",
      "Iteration 22, loss = 0.33004932\n",
      "Iteration 23, loss = 0.32608461\n",
      "Iteration 24, loss = 0.32228521\n",
      "Iteration 25, loss = 0.31871524\n",
      "Iteration 26, loss = 0.31529099\n",
      "Iteration 27, loss = 0.31205902\n",
      "Iteration 28, loss = 0.30895420\n",
      "Iteration 29, loss = 0.30603629\n",
      "Iteration 30, loss = 0.30317435\n",
      "Iteration 31, loss = 0.30052864\n",
      "Iteration 32, loss = 0.29795150\n",
      "Iteration 33, loss = 0.29548601\n",
      "Iteration 34, loss = 0.29313654\n",
      "Iteration 35, loss = 0.29092817\n",
      "Iteration 36, loss = 0.28872589\n",
      "Iteration 37, loss = 0.28668316\n",
      "Iteration 38, loss = 0.28471139\n",
      "Iteration 39, loss = 0.28281469\n",
      "Iteration 40, loss = 0.28104778\n",
      "Iteration 41, loss = 0.27927080\n",
      "Iteration 42, loss = 0.27760096\n",
      "Iteration 43, loss = 0.27603357\n",
      "Iteration 44, loss = 0.27447098\n",
      "Iteration 45, loss = 0.27300693\n",
      "Iteration 46, loss = 0.27158380\n",
      "Iteration 47, loss = 0.27020957\n",
      "Iteration 48, loss = 0.26888751\n",
      "Iteration 49, loss = 0.26767182\n",
      "Iteration 50, loss = 0.26639560\n",
      "Iteration 51, loss = 0.26523676\n",
      "Iteration 52, loss = 0.26410391\n",
      "Iteration 53, loss = 0.26300820\n",
      "Iteration 54, loss = 0.26195758\n",
      "Iteration 55, loss = 0.26094010\n",
      "Iteration 56, loss = 0.25995197\n",
      "Iteration 57, loss = 0.25898670\n",
      "Iteration 58, loss = 0.25807363\n",
      "Iteration 59, loss = 0.25716800\n",
      "Iteration 60, loss = 0.25630555\n",
      "Iteration 61, loss = 0.25546071\n",
      "Iteration 62, loss = 0.25463255\n",
      "Iteration 63, loss = 0.25383224\n",
      "Iteration 64, loss = 0.25308173\n",
      "Iteration 65, loss = 0.25232113\n",
      "Iteration 66, loss = 0.25158801\n",
      "Iteration 67, loss = 0.25088018\n",
      "Iteration 68, loss = 0.25014958\n",
      "Iteration 69, loss = 0.24948497\n",
      "Iteration 70, loss = 0.24884500\n",
      "Iteration 71, loss = 0.24816366\n",
      "Iteration 72, loss = 0.24754021\n",
      "Iteration 73, loss = 0.24690514\n",
      "Iteration 74, loss = 0.24629926\n",
      "Iteration 75, loss = 0.24571492\n",
      "Iteration 76, loss = 0.24511443\n",
      "Iteration 77, loss = 0.24453929\n",
      "Iteration 78, loss = 0.24398284\n",
      "Iteration 79, loss = 0.24343108\n",
      "Iteration 80, loss = 0.24289318\n",
      "Iteration 81, loss = 0.24234605\n",
      "Iteration 82, loss = 0.24182595\n",
      "Iteration 83, loss = 0.24130443\n",
      "Iteration 84, loss = 0.24079584\n",
      "Iteration 85, loss = 0.24029399\n",
      "Iteration 86, loss = 0.23983398\n",
      "Iteration 87, loss = 0.23933380\n",
      "Iteration 88, loss = 0.23885575\n",
      "Iteration 89, loss = 0.23841735\n",
      "Iteration 90, loss = 0.23791877\n",
      "Iteration 91, loss = 0.23749815\n",
      "Iteration 92, loss = 0.23703474\n",
      "Iteration 93, loss = 0.23658265\n",
      "Iteration 94, loss = 0.23615519\n",
      "Iteration 95, loss = 0.23572018\n",
      "Iteration 96, loss = 0.23529412\n",
      "Iteration 97, loss = 0.23487186\n",
      "Iteration 98, loss = 0.23446877\n",
      "Iteration 99, loss = 0.23404616\n",
      "Iteration 100, loss = 0.23363306\n",
      "Iteration 101, loss = 0.23322741\n",
      "Iteration 102, loss = 0.23281742\n",
      "Iteration 103, loss = 0.23243873\n",
      "Iteration 104, loss = 0.23206682\n",
      "Iteration 105, loss = 0.23164519\n",
      "Iteration 106, loss = 0.23125510\n",
      "Iteration 107, loss = 0.23088852\n",
      "Iteration 108, loss = 0.23052293\n",
      "Iteration 109, loss = 0.23013288\n",
      "Iteration 110, loss = 0.22978921\n",
      "Iteration 111, loss = 0.22940561\n",
      "Iteration 112, loss = 0.22903926\n",
      "Iteration 113, loss = 0.22868046\n",
      "Iteration 114, loss = 0.22830775\n",
      "Iteration 115, loss = 0.22797857\n",
      "Iteration 116, loss = 0.22764334\n",
      "Iteration 117, loss = 0.22725804\n",
      "Iteration 118, loss = 0.22692837\n",
      "Iteration 119, loss = 0.22659959\n",
      "Iteration 120, loss = 0.22623563\n",
      "Iteration 121, loss = 0.22591295\n",
      "Iteration 122, loss = 0.22557212\n",
      "Iteration 123, loss = 0.22523762\n",
      "Iteration 124, loss = 0.22490650\n",
      "Iteration 125, loss = 0.22456068\n",
      "Iteration 126, loss = 0.22426002\n",
      "Iteration 127, loss = 0.22392423\n",
      "Iteration 128, loss = 0.22359627\n",
      "Iteration 129, loss = 0.22327169\n",
      "Iteration 130, loss = 0.22295901\n",
      "Iteration 131, loss = 0.22264639\n",
      "Iteration 132, loss = 0.22235842\n",
      "Iteration 133, loss = 0.22200632\n",
      "Iteration 134, loss = 0.22172227\n",
      "Iteration 135, loss = 0.22139036\n",
      "Iteration 136, loss = 0.22109440\n",
      "Iteration 137, loss = 0.22077047\n",
      "Iteration 138, loss = 0.22049804\n",
      "Iteration 139, loss = 0.22016575\n",
      "Iteration 140, loss = 0.21986844\n",
      "Iteration 141, loss = 0.21959819\n",
      "Iteration 142, loss = 0.21928169\n",
      "Iteration 143, loss = 0.21898736\n",
      "Iteration 144, loss = 0.21869746\n",
      "Iteration 145, loss = 0.21839213\n",
      "Iteration 146, loss = 0.21808515\n",
      "Iteration 147, loss = 0.21777955\n",
      "Iteration 148, loss = 0.21749945\n",
      "Iteration 149, loss = 0.21719169\n",
      "Iteration 150, loss = 0.21690473\n",
      "Iteration 151, loss = 0.21662984\n",
      "Iteration 152, loss = 0.21633872\n",
      "Iteration 153, loss = 0.21602067\n",
      "Iteration 154, loss = 0.21574586\n",
      "Iteration 155, loss = 0.21546842\n",
      "Iteration 156, loss = 0.21518309\n",
      "Iteration 157, loss = 0.21491590\n",
      "Iteration 158, loss = 0.21462396\n",
      "Iteration 159, loss = 0.21436196\n",
      "Iteration 160, loss = 0.21406524\n",
      "Iteration 161, loss = 0.21379900\n",
      "Iteration 162, loss = 0.21351573\n",
      "Iteration 163, loss = 0.21325518\n",
      "Iteration 164, loss = 0.21298679\n",
      "Iteration 165, loss = 0.21270814\n",
      "Iteration 166, loss = 0.21246291\n",
      "Iteration 167, loss = 0.21217582\n",
      "Iteration 168, loss = 0.21190404\n",
      "Iteration 169, loss = 0.21165016\n",
      "Iteration 170, loss = 0.21137641\n",
      "Iteration 171, loss = 0.21110610\n",
      "Iteration 172, loss = 0.21086216\n",
      "Iteration 173, loss = 0.21058261\n",
      "Iteration 174, loss = 0.21030888\n",
      "Iteration 175, loss = 0.21007233\n",
      "Iteration 176, loss = 0.20979416\n",
      "Iteration 177, loss = 0.20950955\n",
      "Iteration 178, loss = 0.20926547\n",
      "Iteration 179, loss = 0.20900969\n",
      "Iteration 180, loss = 0.20874737\n",
      "Iteration 181, loss = 0.20848121\n",
      "Iteration 182, loss = 0.20822089\n",
      "Iteration 183, loss = 0.20797951\n",
      "Iteration 184, loss = 0.20773290\n",
      "Iteration 185, loss = 0.20747871\n",
      "Iteration 186, loss = 0.20723448\n",
      "Iteration 187, loss = 0.20699262\n",
      "Iteration 188, loss = 0.20674368\n",
      "Iteration 189, loss = 0.20651438\n",
      "Iteration 190, loss = 0.20622007\n",
      "Iteration 191, loss = 0.20600720\n",
      "Iteration 192, loss = 0.20574940\n",
      "Iteration 193, loss = 0.20550540\n",
      "Iteration 194, loss = 0.20526467\n",
      "Iteration 195, loss = 0.20501190\n",
      "Iteration 196, loss = 0.20476971\n",
      "Iteration 197, loss = 0.20453755\n",
      "Iteration 198, loss = 0.20428089\n",
      "Iteration 199, loss = 0.20406620\n",
      "Iteration 200, loss = 0.20382559\n",
      "Iteration 1, loss = 0.61178200\n",
      "Iteration 2, loss = 0.40865526\n",
      "Iteration 3, loss = 0.33969611\n",
      "Iteration 4, loss = 0.30564416\n",
      "Iteration 5, loss = 0.28502539\n",
      "Iteration 6, loss = 0.27111660\n",
      "Iteration 7, loss = 0.25981296\n",
      "Iteration 8, loss = 0.25138070\n",
      "Iteration 9, loss = 0.24372301\n",
      "Iteration 10, loss = 0.23759264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.23106439\n",
      "Iteration 12, loss = 0.22599849\n",
      "Iteration 13, loss = 0.22056539\n",
      "Iteration 14, loss = 0.21621559\n",
      "Iteration 15, loss = 0.21151929\n",
      "Iteration 16, loss = 0.20748680\n",
      "Iteration 17, loss = 0.20373690\n",
      "Iteration 18, loss = 0.19922601\n",
      "Iteration 19, loss = 0.19537058\n",
      "Iteration 20, loss = 0.19156793\n",
      "Iteration 21, loss = 0.18796337\n",
      "Iteration 22, loss = 0.18445988\n",
      "Iteration 23, loss = 0.18137578\n",
      "Iteration 24, loss = 0.17805770\n",
      "Iteration 25, loss = 0.17501600\n",
      "Iteration 26, loss = 0.17191046\n",
      "Iteration 27, loss = 0.16862790\n",
      "Iteration 28, loss = 0.16583810\n",
      "Iteration 29, loss = 0.16291970\n",
      "Iteration 30, loss = 0.16098964\n",
      "Iteration 31, loss = 0.15775033\n",
      "Iteration 32, loss = 0.15519684\n",
      "Iteration 33, loss = 0.15328559\n",
      "Iteration 34, loss = 0.15067169\n",
      "Iteration 35, loss = 0.14778130\n",
      "Iteration 36, loss = 0.14575235\n",
      "Iteration 37, loss = 0.14368061\n",
      "Iteration 38, loss = 0.14243026\n",
      "Iteration 39, loss = 0.13996932\n",
      "Iteration 40, loss = 0.13806745\n",
      "Iteration 41, loss = 0.13630989\n",
      "Iteration 42, loss = 0.13463977\n",
      "Iteration 43, loss = 0.13269713\n",
      "Iteration 44, loss = 0.13116192\n",
      "Iteration 45, loss = 0.12969729\n",
      "Iteration 46, loss = 0.12828011\n",
      "Iteration 47, loss = 0.12657102\n",
      "Iteration 48, loss = 0.12552413\n",
      "Iteration 49, loss = 0.12390973\n",
      "Iteration 50, loss = 0.12282534\n",
      "Iteration 51, loss = 0.12107470\n",
      "Iteration 52, loss = 0.12009367\n",
      "Iteration 53, loss = 0.11870750\n",
      "Iteration 54, loss = 0.11794921\n",
      "Iteration 55, loss = 0.11632019\n",
      "Iteration 56, loss = 0.11532708\n",
      "Iteration 57, loss = 0.11400294\n",
      "Iteration 58, loss = 0.11302230\n",
      "Iteration 59, loss = 0.11187047\n",
      "Iteration 60, loss = 0.11086870\n",
      "Iteration 61, loss = 0.11017558\n",
      "Iteration 62, loss = 0.10887762\n",
      "Iteration 63, loss = 0.10824635\n",
      "Iteration 64, loss = 0.10727604\n",
      "Iteration 65, loss = 0.10575394\n",
      "Iteration 66, loss = 0.10521215\n",
      "Iteration 67, loss = 0.10407692\n",
      "Iteration 68, loss = 0.10336761\n",
      "Iteration 69, loss = 0.10243476\n",
      "Iteration 70, loss = 0.10149383\n",
      "Iteration 71, loss = 0.10138517\n",
      "Iteration 72, loss = 0.10011927\n",
      "Iteration 73, loss = 0.09886374\n",
      "Iteration 74, loss = 0.09810332\n",
      "Iteration 75, loss = 0.09739136\n",
      "Iteration 76, loss = 0.09705084\n",
      "Iteration 77, loss = 0.09568711\n",
      "Iteration 78, loss = 0.09511683\n",
      "Iteration 79, loss = 0.09446564\n",
      "Iteration 80, loss = 0.09355579\n",
      "Iteration 81, loss = 0.09319645\n",
      "Iteration 82, loss = 0.09201266\n",
      "Iteration 83, loss = 0.09090748\n",
      "Iteration 84, loss = 0.09078119\n",
      "Iteration 85, loss = 0.09025928\n",
      "Iteration 86, loss = 0.08914280\n",
      "Iteration 87, loss = 0.08846112\n",
      "Iteration 88, loss = 0.08811550\n",
      "Iteration 89, loss = 0.08777682\n",
      "Iteration 90, loss = 0.08626436\n",
      "Iteration 91, loss = 0.08581448\n",
      "Iteration 92, loss = 0.08478331\n",
      "Iteration 93, loss = 0.08449751\n",
      "Iteration 94, loss = 0.08398635\n",
      "Iteration 95, loss = 0.08419764\n",
      "Iteration 96, loss = 0.08233489\n",
      "Iteration 97, loss = 0.08165606\n",
      "Iteration 98, loss = 0.08121896\n",
      "Iteration 99, loss = 0.08061396\n",
      "Iteration 100, loss = 0.07993569\n",
      "Iteration 101, loss = 0.07912533\n",
      "Iteration 102, loss = 0.07899240\n",
      "Iteration 103, loss = 0.07810856\n",
      "Iteration 104, loss = 0.07748439\n",
      "Iteration 105, loss = 0.07729359\n",
      "Iteration 106, loss = 0.07712430\n",
      "Iteration 107, loss = 0.07563672\n",
      "Iteration 108, loss = 0.07499895\n",
      "Iteration 109, loss = 0.07507868\n",
      "Iteration 110, loss = 0.07447945\n",
      "Iteration 111, loss = 0.07375823\n",
      "Iteration 112, loss = 0.07342622\n",
      "Iteration 113, loss = 0.07273986\n",
      "Iteration 114, loss = 0.07239135\n",
      "Iteration 115, loss = 0.07255489\n",
      "Iteration 116, loss = 0.07111610\n",
      "Iteration 117, loss = 0.07022672\n",
      "Iteration 118, loss = 0.06942044\n",
      "Iteration 119, loss = 0.06924061\n",
      "Iteration 120, loss = 0.06866123\n",
      "Iteration 121, loss = 0.06844194\n",
      "Iteration 122, loss = 0.06802413\n",
      "Iteration 123, loss = 0.06692940\n",
      "Iteration 124, loss = 0.06688662\n",
      "Iteration 125, loss = 0.06582312\n",
      "Iteration 126, loss = 0.06569655\n",
      "Iteration 127, loss = 0.06521644\n",
      "Iteration 128, loss = 0.06452882\n",
      "Iteration 129, loss = 0.06464444\n",
      "Iteration 130, loss = 0.06351282\n",
      "Iteration 131, loss = 0.06326880\n",
      "Iteration 132, loss = 0.06265072\n",
      "Iteration 133, loss = 0.06176966\n",
      "Iteration 134, loss = 0.06174364\n",
      "Iteration 135, loss = 0.06108039\n",
      "Iteration 136, loss = 0.06088749\n",
      "Iteration 137, loss = 0.06043701\n",
      "Iteration 138, loss = 0.05967798\n",
      "Iteration 139, loss = 0.05918515\n",
      "Iteration 140, loss = 0.05882868\n",
      "Iteration 141, loss = 0.05855221\n",
      "Iteration 142, loss = 0.05796824\n",
      "Iteration 143, loss = 0.05742025\n",
      "Iteration 144, loss = 0.05725206\n",
      "Iteration 145, loss = 0.05694955\n",
      "Iteration 146, loss = 0.05665379\n",
      "Iteration 147, loss = 0.05531129\n",
      "Iteration 148, loss = 0.05522310\n",
      "Iteration 149, loss = 0.05480291\n",
      "Iteration 150, loss = 0.05385208\n",
      "Iteration 151, loss = 0.05403819\n",
      "Iteration 152, loss = 0.05413695\n",
      "Iteration 153, loss = 0.05292894\n",
      "Iteration 154, loss = 0.05306515\n",
      "Iteration 155, loss = 0.05237053\n",
      "Iteration 156, loss = 0.05161722\n",
      "Iteration 157, loss = 0.05153891\n",
      "Iteration 158, loss = 0.05074966\n",
      "Iteration 159, loss = 0.05074117\n",
      "Iteration 160, loss = 0.05058981\n",
      "Iteration 161, loss = 0.05034434\n",
      "Iteration 162, loss = 0.04988983\n",
      "Iteration 163, loss = 0.04892205\n",
      "Iteration 164, loss = 0.04856762\n",
      "Iteration 165, loss = 0.04862319\n",
      "Iteration 166, loss = 0.04779570\n",
      "Iteration 167, loss = 0.04773939\n",
      "Iteration 168, loss = 0.04737443\n",
      "Iteration 169, loss = 0.04714145\n",
      "Iteration 170, loss = 0.04656912\n",
      "Iteration 171, loss = 0.04649081\n",
      "Iteration 172, loss = 0.04582530\n",
      "Iteration 173, loss = 0.04535580\n",
      "Iteration 174, loss = 0.04492716\n",
      "Iteration 175, loss = 0.04483674\n",
      "Iteration 176, loss = 0.04438325\n",
      "Iteration 177, loss = 0.04396035\n",
      "Iteration 178, loss = 0.04349794\n",
      "Iteration 179, loss = 0.04289762\n",
      "Iteration 180, loss = 0.04275633\n",
      "Iteration 181, loss = 0.04216203\n",
      "Iteration 182, loss = 0.04237716\n",
      "Iteration 183, loss = 0.04249451\n",
      "Iteration 184, loss = 0.04165654\n",
      "Iteration 185, loss = 0.04120665\n",
      "Iteration 186, loss = 0.04051007\n",
      "Iteration 187, loss = 0.04052644\n",
      "Iteration 188, loss = 0.04048275\n",
      "Iteration 189, loss = 0.03965810\n",
      "Iteration 190, loss = 0.03946775\n",
      "Iteration 191, loss = 0.03931165\n",
      "Iteration 192, loss = 0.03891929\n",
      "Iteration 193, loss = 0.03864001\n",
      "Iteration 194, loss = 0.03824437\n",
      "Iteration 195, loss = 0.03805576\n",
      "Iteration 196, loss = 0.03802029\n",
      "Iteration 197, loss = 0.03797715\n",
      "Iteration 198, loss = 0.03705639\n",
      "Iteration 199, loss = 0.03714901\n",
      "Iteration 200, loss = 0.03667557\n",
      "Iteration 1, loss = 0.61264317\n",
      "Iteration 2, loss = 0.40833922\n",
      "Iteration 3, loss = 0.33834903\n",
      "Iteration 4, loss = 0.30378487\n",
      "Iteration 5, loss = 0.28289018\n",
      "Iteration 6, loss = 0.26876067\n",
      "Iteration 7, loss = 0.25746674\n",
      "Iteration 8, loss = 0.24892800\n",
      "Iteration 9, loss = 0.24128364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.23492774\n",
      "Iteration 11, loss = 0.22853789\n",
      "Iteration 12, loss = 0.22371652\n",
      "Iteration 13, loss = 0.21814242\n",
      "Iteration 14, loss = 0.21319592\n",
      "Iteration 15, loss = 0.20866385\n",
      "Iteration 16, loss = 0.20435662\n",
      "Iteration 17, loss = 0.20032369\n",
      "Iteration 18, loss = 0.19636800\n",
      "Iteration 19, loss = 0.19269990\n",
      "Iteration 20, loss = 0.18892335\n",
      "Iteration 21, loss = 0.18558164\n",
      "Iteration 22, loss = 0.18202488\n",
      "Iteration 23, loss = 0.17895786\n",
      "Iteration 24, loss = 0.17606286\n",
      "Iteration 25, loss = 0.17303217\n",
      "Iteration 26, loss = 0.16992772\n",
      "Iteration 27, loss = 0.16716374\n",
      "Iteration 28, loss = 0.16452085\n",
      "Iteration 29, loss = 0.16201464\n",
      "Iteration 30, loss = 0.15964338\n",
      "Iteration 31, loss = 0.15674766\n",
      "Iteration 32, loss = 0.15445127\n",
      "Iteration 33, loss = 0.15208565\n",
      "Iteration 34, loss = 0.15032243\n",
      "Iteration 35, loss = 0.14748638\n",
      "Iteration 36, loss = 0.14568255\n",
      "Iteration 37, loss = 0.14359594\n",
      "Iteration 38, loss = 0.14178923\n",
      "Iteration 39, loss = 0.13983468\n",
      "Iteration 40, loss = 0.13776153\n",
      "Iteration 41, loss = 0.13611923\n",
      "Iteration 42, loss = 0.13450804\n",
      "Iteration 43, loss = 0.13225557\n",
      "Iteration 44, loss = 0.13093320\n",
      "Iteration 45, loss = 0.12938098\n",
      "Iteration 46, loss = 0.12759641\n",
      "Iteration 47, loss = 0.12608139\n",
      "Iteration 48, loss = 0.12472660\n",
      "Iteration 49, loss = 0.12327183\n",
      "Iteration 50, loss = 0.12192187\n",
      "Iteration 51, loss = 0.12031877\n",
      "Iteration 52, loss = 0.11891030\n",
      "Iteration 53, loss = 0.11741274\n",
      "Iteration 54, loss = 0.11655232\n",
      "Iteration 55, loss = 0.11487351\n",
      "Iteration 56, loss = 0.11398249\n",
      "Iteration 57, loss = 0.11254936\n",
      "Iteration 58, loss = 0.11152226\n",
      "Iteration 59, loss = 0.11040542\n",
      "Iteration 60, loss = 0.10900801\n",
      "Iteration 61, loss = 0.10805230\n",
      "Iteration 62, loss = 0.10671673\n",
      "Iteration 63, loss = 0.10647315\n",
      "Iteration 64, loss = 0.10495215\n",
      "Iteration 65, loss = 0.10382984\n",
      "Iteration 66, loss = 0.10317777\n",
      "Iteration 67, loss = 0.10167008\n",
      "Iteration 68, loss = 0.10096053\n",
      "Iteration 69, loss = 0.10016603\n",
      "Iteration 70, loss = 0.09857125\n",
      "Iteration 71, loss = 0.09863003\n",
      "Iteration 72, loss = 0.09773251\n",
      "Iteration 73, loss = 0.09607211\n",
      "Iteration 74, loss = 0.09513330\n",
      "Iteration 75, loss = 0.09450133\n",
      "Iteration 76, loss = 0.09321119\n",
      "Iteration 77, loss = 0.09250441\n",
      "Iteration 78, loss = 0.09201875\n",
      "Iteration 79, loss = 0.09136779\n",
      "Iteration 80, loss = 0.09021122\n",
      "Iteration 81, loss = 0.08942798\n",
      "Iteration 82, loss = 0.08864224\n",
      "Iteration 83, loss = 0.08744675\n",
      "Iteration 84, loss = 0.08696232\n",
      "Iteration 85, loss = 0.08633966\n",
      "Iteration 86, loss = 0.08524814\n",
      "Iteration 87, loss = 0.08495856\n",
      "Iteration 88, loss = 0.08413056\n",
      "Iteration 89, loss = 0.08341199\n",
      "Iteration 90, loss = 0.08218890\n",
      "Iteration 91, loss = 0.08161621\n",
      "Iteration 92, loss = 0.08043940\n",
      "Iteration 93, loss = 0.08007804\n",
      "Iteration 94, loss = 0.07946631\n",
      "Iteration 95, loss = 0.07960786\n",
      "Iteration 96, loss = 0.07801727\n",
      "Iteration 97, loss = 0.07710966\n",
      "Iteration 98, loss = 0.07658578\n",
      "Iteration 99, loss = 0.07622739\n",
      "Iteration 100, loss = 0.07575949\n",
      "Iteration 101, loss = 0.07462274\n",
      "Iteration 102, loss = 0.07489969\n",
      "Iteration 103, loss = 0.07385981\n",
      "Iteration 104, loss = 0.07270560\n",
      "Iteration 105, loss = 0.07228918\n",
      "Iteration 106, loss = 0.07202343\n",
      "Iteration 107, loss = 0.07122886\n",
      "Iteration 108, loss = 0.07054562\n",
      "Iteration 109, loss = 0.07006268\n",
      "Iteration 110, loss = 0.06964285\n",
      "Iteration 111, loss = 0.06864485\n",
      "Iteration 112, loss = 0.06902564\n",
      "Iteration 113, loss = 0.06748758\n",
      "Iteration 114, loss = 0.06688930\n",
      "Iteration 115, loss = 0.06705448\n",
      "Iteration 116, loss = 0.06596391\n",
      "Iteration 117, loss = 0.06500087\n",
      "Iteration 118, loss = 0.06447290\n",
      "Iteration 119, loss = 0.06421666\n",
      "Iteration 120, loss = 0.06404548\n",
      "Iteration 121, loss = 0.06367043\n",
      "Iteration 122, loss = 0.06269307\n",
      "Iteration 123, loss = 0.06195342\n",
      "Iteration 124, loss = 0.06180956\n",
      "Iteration 125, loss = 0.06081742\n",
      "Iteration 126, loss = 0.06097036\n",
      "Iteration 127, loss = 0.05994155\n",
      "Iteration 128, loss = 0.05942769\n",
      "Iteration 129, loss = 0.05925416\n",
      "Iteration 130, loss = 0.05860797\n",
      "Iteration 131, loss = 0.05792337\n",
      "Iteration 132, loss = 0.05740739\n",
      "Iteration 133, loss = 0.05684168\n",
      "Iteration 134, loss = 0.05643028\n",
      "Iteration 135, loss = 0.05610259\n",
      "Iteration 136, loss = 0.05545926\n",
      "Iteration 137, loss = 0.05519648\n",
      "Iteration 138, loss = 0.05442069\n",
      "Iteration 139, loss = 0.05442714\n",
      "Iteration 140, loss = 0.05424678\n",
      "Iteration 141, loss = 0.05346997\n",
      "Iteration 142, loss = 0.05294121\n",
      "Iteration 143, loss = 0.05209012\n",
      "Iteration 144, loss = 0.05243004\n",
      "Iteration 145, loss = 0.05185101\n",
      "Iteration 146, loss = 0.05148452\n",
      "Iteration 147, loss = 0.05046319\n",
      "Iteration 148, loss = 0.05063533\n",
      "Iteration 149, loss = 0.04978075\n",
      "Iteration 150, loss = 0.04928835\n",
      "Iteration 151, loss = 0.04918945\n",
      "Iteration 152, loss = 0.04916061\n",
      "Iteration 153, loss = 0.04809840\n",
      "Iteration 154, loss = 0.04793280\n",
      "Iteration 155, loss = 0.04743618\n",
      "Iteration 156, loss = 0.04708061\n",
      "Iteration 157, loss = 0.04685296\n",
      "Iteration 158, loss = 0.04617519\n",
      "Iteration 159, loss = 0.04595409\n",
      "Iteration 160, loss = 0.04562445\n",
      "Iteration 161, loss = 0.04529640\n",
      "Iteration 162, loss = 0.04464271\n",
      "Iteration 163, loss = 0.04452484\n",
      "Iteration 164, loss = 0.04377003\n",
      "Iteration 165, loss = 0.04361059\n",
      "Iteration 166, loss = 0.04289770\n",
      "Iteration 167, loss = 0.04272576\n",
      "Iteration 168, loss = 0.04254758\n",
      "Iteration 169, loss = 0.04233104\n",
      "Iteration 170, loss = 0.04180097\n",
      "Iteration 171, loss = 0.04161654\n",
      "Iteration 172, loss = 0.04121054\n",
      "Iteration 173, loss = 0.04081498\n",
      "Iteration 174, loss = 0.04057085\n",
      "Iteration 175, loss = 0.04015835\n",
      "Iteration 176, loss = 0.03981025\n",
      "Iteration 177, loss = 0.03914189\n",
      "Iteration 178, loss = 0.03897262\n",
      "Iteration 179, loss = 0.03854726\n",
      "Iteration 180, loss = 0.03847821\n",
      "Iteration 181, loss = 0.03784923\n",
      "Iteration 182, loss = 0.03764722\n",
      "Iteration 183, loss = 0.03775552\n",
      "Iteration 184, loss = 0.03690138\n",
      "Iteration 185, loss = 0.03701714\n",
      "Iteration 186, loss = 0.03594848\n",
      "Iteration 187, loss = 0.03591565\n",
      "Iteration 188, loss = 0.03620897\n",
      "Iteration 189, loss = 0.03521290\n",
      "Iteration 190, loss = 0.03526885\n",
      "Iteration 191, loss = 0.03496823\n",
      "Iteration 192, loss = 0.03487542\n",
      "Iteration 193, loss = 0.03436144\n",
      "Iteration 194, loss = 0.03391609\n",
      "Iteration 195, loss = 0.03357753\n",
      "Iteration 196, loss = 0.03351516\n",
      "Iteration 197, loss = 0.03346195\n",
      "Iteration 198, loss = 0.03271383\n",
      "Iteration 199, loss = 0.03282878\n",
      "Iteration 200, loss = 0.03231911\n",
      "Iteration 1, loss = 0.61603689\n",
      "Iteration 2, loss = 0.41181678\n",
      "Iteration 3, loss = 0.34208453\n",
      "Iteration 4, loss = 0.30758688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.28698856\n",
      "Iteration 6, loss = 0.27261017\n",
      "Iteration 7, loss = 0.26163808\n",
      "Iteration 8, loss = 0.25264971\n",
      "Iteration 9, loss = 0.24459064\n",
      "Iteration 10, loss = 0.23796412\n",
      "Iteration 11, loss = 0.23136471\n",
      "Iteration 12, loss = 0.22620232\n",
      "Iteration 13, loss = 0.22020925\n",
      "Iteration 14, loss = 0.21491433\n",
      "Iteration 15, loss = 0.21037787\n",
      "Iteration 16, loss = 0.20554184\n",
      "Iteration 17, loss = 0.20130142\n",
      "Iteration 18, loss = 0.19711975\n",
      "Iteration 19, loss = 0.19318314\n",
      "Iteration 20, loss = 0.18893134\n",
      "Iteration 21, loss = 0.18545958\n",
      "Iteration 22, loss = 0.18190028\n",
      "Iteration 23, loss = 0.17843825\n",
      "Iteration 24, loss = 0.17539544\n",
      "Iteration 25, loss = 0.17222526\n",
      "Iteration 26, loss = 0.16885198\n",
      "Iteration 27, loss = 0.16583450\n",
      "Iteration 28, loss = 0.16292445\n",
      "Iteration 29, loss = 0.16040767\n",
      "Iteration 30, loss = 0.15768662\n",
      "Iteration 31, loss = 0.15467919\n",
      "Iteration 32, loss = 0.15208621\n",
      "Iteration 33, loss = 0.14962364\n",
      "Iteration 34, loss = 0.14743137\n",
      "Iteration 35, loss = 0.14510649\n",
      "Iteration 36, loss = 0.14278471\n",
      "Iteration 37, loss = 0.14087481\n",
      "Iteration 38, loss = 0.13900024\n",
      "Iteration 39, loss = 0.13686133\n",
      "Iteration 40, loss = 0.13491609\n",
      "Iteration 41, loss = 0.13335247\n",
      "Iteration 42, loss = 0.13162956\n",
      "Iteration 43, loss = 0.12943556\n",
      "Iteration 44, loss = 0.12813539\n",
      "Iteration 45, loss = 0.12681952\n",
      "Iteration 46, loss = 0.12536906\n",
      "Iteration 47, loss = 0.12348861\n",
      "Iteration 48, loss = 0.12202962\n",
      "Iteration 49, loss = 0.12056164\n",
      "Iteration 50, loss = 0.11922356\n",
      "Iteration 51, loss = 0.11837826\n",
      "Iteration 52, loss = 0.11700083\n",
      "Iteration 53, loss = 0.11586036\n",
      "Iteration 54, loss = 0.11479083\n",
      "Iteration 55, loss = 0.11313735\n",
      "Iteration 56, loss = 0.11219761\n",
      "Iteration 57, loss = 0.11108830\n",
      "Iteration 58, loss = 0.11010333\n",
      "Iteration 59, loss = 0.10950964\n",
      "Iteration 60, loss = 0.10827772\n",
      "Iteration 61, loss = 0.10710212\n",
      "Iteration 62, loss = 0.10595498\n",
      "Iteration 63, loss = 0.10524157\n",
      "Iteration 64, loss = 0.10396467\n",
      "Iteration 65, loss = 0.10293678\n",
      "Iteration 66, loss = 0.10262718\n",
      "Iteration 67, loss = 0.10135153\n",
      "Iteration 68, loss = 0.10089861\n",
      "Iteration 69, loss = 0.10019180\n",
      "Iteration 70, loss = 0.09906240\n",
      "Iteration 71, loss = 0.09866343\n",
      "Iteration 72, loss = 0.09764906\n",
      "Iteration 73, loss = 0.09630878\n",
      "Iteration 74, loss = 0.09543785\n",
      "Iteration 75, loss = 0.09504421\n",
      "Iteration 76, loss = 0.09450330\n",
      "Iteration 77, loss = 0.09347152\n",
      "Iteration 78, loss = 0.09286054\n",
      "Iteration 79, loss = 0.09184298\n",
      "Iteration 80, loss = 0.09109732\n",
      "Iteration 81, loss = 0.08973139\n",
      "Iteration 82, loss = 0.08978727\n",
      "Iteration 83, loss = 0.08847652\n",
      "Iteration 84, loss = 0.08797864\n",
      "Iteration 85, loss = 0.08741321\n",
      "Iteration 86, loss = 0.08657085\n",
      "Iteration 87, loss = 0.08579070\n",
      "Iteration 88, loss = 0.08555773\n",
      "Iteration 89, loss = 0.08439324\n",
      "Iteration 90, loss = 0.08402092\n",
      "Iteration 91, loss = 0.08326185\n",
      "Iteration 92, loss = 0.08235110\n",
      "Iteration 93, loss = 0.08193059\n",
      "Iteration 94, loss = 0.08148059\n",
      "Iteration 95, loss = 0.08072853\n",
      "Iteration 96, loss = 0.07964739\n",
      "Iteration 97, loss = 0.07903692\n",
      "Iteration 98, loss = 0.07844464\n",
      "Iteration 99, loss = 0.07790977\n",
      "Iteration 100, loss = 0.07747130\n",
      "Iteration 101, loss = 0.07649280\n",
      "Iteration 102, loss = 0.07671244\n",
      "Iteration 103, loss = 0.07554168\n",
      "Iteration 104, loss = 0.07485434\n",
      "Iteration 105, loss = 0.07448727\n",
      "Iteration 106, loss = 0.07381903\n",
      "Iteration 107, loss = 0.07278278\n",
      "Iteration 108, loss = 0.07240903\n",
      "Iteration 109, loss = 0.07240167\n",
      "Iteration 110, loss = 0.07150210\n",
      "Iteration 111, loss = 0.07085482\n",
      "Iteration 112, loss = 0.07029673\n",
      "Iteration 113, loss = 0.06974130\n",
      "Iteration 114, loss = 0.06888250\n",
      "Iteration 115, loss = 0.06865611\n",
      "Iteration 116, loss = 0.06815577\n",
      "Iteration 117, loss = 0.06737297\n",
      "Iteration 118, loss = 0.06675086\n",
      "Iteration 119, loss = 0.06646085\n",
      "Iteration 120, loss = 0.06601282\n",
      "Iteration 121, loss = 0.06548582\n",
      "Iteration 122, loss = 0.06469271\n",
      "Iteration 123, loss = 0.06419482\n",
      "Iteration 124, loss = 0.06495263\n",
      "Iteration 125, loss = 0.06271672\n",
      "Iteration 126, loss = 0.06287216\n",
      "Iteration 127, loss = 0.06170348\n",
      "Iteration 128, loss = 0.06157954\n",
      "Iteration 129, loss = 0.06151562\n",
      "Iteration 130, loss = 0.06062940\n",
      "Iteration 131, loss = 0.06040816\n",
      "Iteration 132, loss = 0.05965442\n",
      "Iteration 133, loss = 0.05875545\n",
      "Iteration 134, loss = 0.05889163\n",
      "Iteration 135, loss = 0.05825985\n",
      "Iteration 136, loss = 0.05785803\n",
      "Iteration 137, loss = 0.05743671\n",
      "Iteration 138, loss = 0.05687137\n",
      "Iteration 139, loss = 0.05614562\n",
      "Iteration 140, loss = 0.05619436\n",
      "Iteration 141, loss = 0.05543760\n",
      "Iteration 142, loss = 0.05471046\n",
      "Iteration 143, loss = 0.05415792\n",
      "Iteration 144, loss = 0.05437291\n",
      "Iteration 145, loss = 0.05344728\n",
      "Iteration 146, loss = 0.05375953\n",
      "Iteration 147, loss = 0.05275767\n",
      "Iteration 148, loss = 0.05251213\n",
      "Iteration 149, loss = 0.05183222\n",
      "Iteration 150, loss = 0.05167573\n",
      "Iteration 151, loss = 0.05103255\n",
      "Iteration 152, loss = 0.05102812\n",
      "Iteration 153, loss = 0.05018187\n",
      "Iteration 154, loss = 0.04962025\n",
      "Iteration 155, loss = 0.04933967\n",
      "Iteration 156, loss = 0.04892433\n",
      "Iteration 157, loss = 0.04844702\n",
      "Iteration 158, loss = 0.04824916\n",
      "Iteration 159, loss = 0.04754267\n",
      "Iteration 160, loss = 0.04757924\n",
      "Iteration 161, loss = 0.04720394\n",
      "Iteration 162, loss = 0.04671199\n",
      "Iteration 163, loss = 0.04620501\n",
      "Iteration 164, loss = 0.04553593\n",
      "Iteration 165, loss = 0.04535843\n",
      "Iteration 166, loss = 0.04517744\n",
      "Iteration 167, loss = 0.04458413\n",
      "Iteration 168, loss = 0.04450867\n",
      "Iteration 169, loss = 0.04385076\n",
      "Iteration 170, loss = 0.04352655\n",
      "Iteration 171, loss = 0.04313371\n",
      "Iteration 172, loss = 0.04333724\n",
      "Iteration 173, loss = 0.04245310\n",
      "Iteration 174, loss = 0.04237551\n",
      "Iteration 175, loss = 0.04244031\n",
      "Iteration 176, loss = 0.04134669\n",
      "Iteration 177, loss = 0.04137846\n",
      "Iteration 178, loss = 0.04097140\n",
      "Iteration 179, loss = 0.04048888\n",
      "Iteration 180, loss = 0.04023776\n",
      "Iteration 181, loss = 0.03954457\n",
      "Iteration 182, loss = 0.03974031\n",
      "Iteration 183, loss = 0.03864950\n",
      "Iteration 184, loss = 0.03850581\n",
      "Iteration 185, loss = 0.03831219\n",
      "Iteration 186, loss = 0.03763812\n",
      "Iteration 187, loss = 0.03791873\n",
      "Iteration 188, loss = 0.03797055\n",
      "Iteration 189, loss = 0.03674475\n",
      "Iteration 190, loss = 0.03684319\n",
      "Iteration 191, loss = 0.03674523\n",
      "Iteration 192, loss = 0.03644268\n",
      "Iteration 193, loss = 0.03599993\n",
      "Iteration 194, loss = 0.03539878\n",
      "Iteration 195, loss = 0.03519259\n",
      "Iteration 196, loss = 0.03498417\n",
      "Iteration 197, loss = 0.03460491\n",
      "Iteration 198, loss = 0.03435171\n",
      "Iteration 199, loss = 0.03412471\n",
      "Iteration 200, loss = 0.03384361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61096914\n",
      "Iteration 2, loss = 0.40827804\n",
      "Iteration 3, loss = 0.33941493\n",
      "Iteration 4, loss = 0.30506645\n",
      "Iteration 5, loss = 0.28472963\n",
      "Iteration 6, loss = 0.27003333\n",
      "Iteration 7, loss = 0.25917016\n",
      "Iteration 8, loss = 0.25022302\n",
      "Iteration 9, loss = 0.24244499\n",
      "Iteration 10, loss = 0.23568113\n",
      "Iteration 11, loss = 0.22983068\n",
      "Iteration 12, loss = 0.22440417\n",
      "Iteration 13, loss = 0.21889089\n",
      "Iteration 14, loss = 0.21417813\n",
      "Iteration 15, loss = 0.20920076\n",
      "Iteration 16, loss = 0.20464580\n",
      "Iteration 17, loss = 0.20068200\n",
      "Iteration 18, loss = 0.19691113\n",
      "Iteration 19, loss = 0.19293751\n",
      "Iteration 20, loss = 0.18887067\n",
      "Iteration 21, loss = 0.18541545\n",
      "Iteration 22, loss = 0.18182989\n",
      "Iteration 23, loss = 0.17865301\n",
      "Iteration 24, loss = 0.17550694\n",
      "Iteration 25, loss = 0.17224123\n",
      "Iteration 26, loss = 0.16919741\n",
      "Iteration 27, loss = 0.16659298\n",
      "Iteration 28, loss = 0.16363294\n",
      "Iteration 29, loss = 0.16104212\n",
      "Iteration 30, loss = 0.15840704\n",
      "Iteration 31, loss = 0.15568603\n",
      "Iteration 32, loss = 0.15321116\n",
      "Iteration 33, loss = 0.15104235\n",
      "Iteration 34, loss = 0.14882924\n",
      "Iteration 35, loss = 0.14657261\n",
      "Iteration 36, loss = 0.14439652\n",
      "Iteration 37, loss = 0.14242032\n",
      "Iteration 38, loss = 0.14029441\n",
      "Iteration 39, loss = 0.13833189\n",
      "Iteration 40, loss = 0.13669305\n",
      "Iteration 41, loss = 0.13490787\n",
      "Iteration 42, loss = 0.13303371\n",
      "Iteration 43, loss = 0.13129509\n",
      "Iteration 44, loss = 0.12979205\n",
      "Iteration 45, loss = 0.12826098\n",
      "Iteration 46, loss = 0.12665160\n",
      "Iteration 47, loss = 0.12537702\n",
      "Iteration 48, loss = 0.12391266\n",
      "Iteration 49, loss = 0.12240274\n",
      "Iteration 50, loss = 0.12156320\n",
      "Iteration 51, loss = 0.11988849\n",
      "Iteration 52, loss = 0.11913398\n",
      "Iteration 53, loss = 0.11761922\n",
      "Iteration 54, loss = 0.11600044\n",
      "Iteration 55, loss = 0.11509347\n",
      "Iteration 56, loss = 0.11397442\n",
      "Iteration 57, loss = 0.11280124\n",
      "Iteration 58, loss = 0.11152826\n",
      "Iteration 59, loss = 0.11056224\n",
      "Iteration 60, loss = 0.10940557\n",
      "Iteration 61, loss = 0.10806107\n",
      "Iteration 62, loss = 0.10713668\n",
      "Iteration 63, loss = 0.10647072\n",
      "Iteration 64, loss = 0.10513803\n",
      "Iteration 65, loss = 0.10414739\n",
      "Iteration 66, loss = 0.10336299\n",
      "Iteration 67, loss = 0.10252959\n",
      "Iteration 68, loss = 0.10136751\n",
      "Iteration 69, loss = 0.10082449\n",
      "Iteration 70, loss = 0.09981332\n",
      "Iteration 71, loss = 0.09920868\n",
      "Iteration 72, loss = 0.09789525\n",
      "Iteration 73, loss = 0.09680482\n",
      "Iteration 74, loss = 0.09561331\n",
      "Iteration 75, loss = 0.09522389\n",
      "Iteration 76, loss = 0.09425283\n",
      "Iteration 77, loss = 0.09342903\n",
      "Iteration 78, loss = 0.09253446\n",
      "Iteration 79, loss = 0.09202263\n",
      "Iteration 80, loss = 0.09127072\n",
      "Iteration 81, loss = 0.08984838\n",
      "Iteration 82, loss = 0.08974192\n",
      "Iteration 83, loss = 0.08868925\n",
      "Iteration 84, loss = 0.08795116\n",
      "Iteration 85, loss = 0.08710056\n",
      "Iteration 86, loss = 0.08668400\n",
      "Iteration 87, loss = 0.08565658\n",
      "Iteration 88, loss = 0.08497791\n",
      "Iteration 89, loss = 0.08435972\n",
      "Iteration 90, loss = 0.08342301\n",
      "Iteration 91, loss = 0.08233724\n",
      "Iteration 92, loss = 0.08186960\n",
      "Iteration 93, loss = 0.08092915\n",
      "Iteration 94, loss = 0.08067000\n",
      "Iteration 95, loss = 0.07966829\n",
      "Iteration 96, loss = 0.07926309\n",
      "Iteration 97, loss = 0.07814706\n",
      "Iteration 98, loss = 0.07794077\n",
      "Iteration 99, loss = 0.07738799\n",
      "Iteration 100, loss = 0.07672308\n",
      "Iteration 101, loss = 0.07588823\n",
      "Iteration 102, loss = 0.07565607\n",
      "Iteration 103, loss = 0.07485160\n",
      "Iteration 104, loss = 0.07429632\n",
      "Iteration 105, loss = 0.07374893\n",
      "Iteration 106, loss = 0.07274713\n",
      "Iteration 107, loss = 0.07237583\n",
      "Iteration 108, loss = 0.07211831\n",
      "Iteration 109, loss = 0.07139923\n",
      "Iteration 110, loss = 0.07066915\n",
      "Iteration 111, loss = 0.06982182\n",
      "Iteration 112, loss = 0.06927475\n",
      "Iteration 113, loss = 0.06852071\n",
      "Iteration 114, loss = 0.06779834\n",
      "Iteration 115, loss = 0.06729634\n",
      "Iteration 116, loss = 0.06758950\n",
      "Iteration 117, loss = 0.06599782\n",
      "Iteration 118, loss = 0.06582859\n",
      "Iteration 119, loss = 0.06525009\n",
      "Iteration 120, loss = 0.06475382\n",
      "Iteration 121, loss = 0.06430373\n",
      "Iteration 122, loss = 0.06348316\n",
      "Iteration 123, loss = 0.06318292\n",
      "Iteration 124, loss = 0.06300751\n",
      "Iteration 125, loss = 0.06156145\n",
      "Iteration 126, loss = 0.06165256\n",
      "Iteration 127, loss = 0.06106056\n",
      "Iteration 128, loss = 0.06009621\n",
      "Iteration 129, loss = 0.06049348\n",
      "Iteration 130, loss = 0.05928344\n",
      "Iteration 131, loss = 0.05878106\n",
      "Iteration 132, loss = 0.05806337\n",
      "Iteration 133, loss = 0.05783011\n",
      "Iteration 134, loss = 0.05759120\n",
      "Iteration 135, loss = 0.05744406\n",
      "Iteration 136, loss = 0.05632800\n",
      "Iteration 137, loss = 0.05613746\n",
      "Iteration 138, loss = 0.05558276\n",
      "Iteration 139, loss = 0.05475530\n",
      "Iteration 140, loss = 0.05460721\n",
      "Iteration 141, loss = 0.05417518\n",
      "Iteration 142, loss = 0.05365534\n",
      "Iteration 143, loss = 0.05307791\n",
      "Iteration 144, loss = 0.05300298\n",
      "Iteration 145, loss = 0.05222035\n",
      "Iteration 146, loss = 0.05173774\n",
      "Iteration 147, loss = 0.05139814\n",
      "Iteration 148, loss = 0.05160922\n",
      "Iteration 149, loss = 0.05093725\n",
      "Iteration 150, loss = 0.05043694\n",
      "Iteration 151, loss = 0.05005588\n",
      "Iteration 152, loss = 0.04957131\n",
      "Iteration 153, loss = 0.04927779\n",
      "Iteration 154, loss = 0.04813304\n",
      "Iteration 155, loss = 0.04778045\n",
      "Iteration 156, loss = 0.04778040\n",
      "Iteration 157, loss = 0.04664308\n",
      "Iteration 158, loss = 0.04653692\n",
      "Iteration 159, loss = 0.04617294\n",
      "Iteration 160, loss = 0.04579641\n",
      "Iteration 161, loss = 0.04554948\n",
      "Iteration 162, loss = 0.04489952\n",
      "Iteration 163, loss = 0.04448222\n",
      "Iteration 164, loss = 0.04402882\n",
      "Iteration 165, loss = 0.04409940\n",
      "Iteration 166, loss = 0.04363106\n",
      "Iteration 167, loss = 0.04395191\n",
      "Iteration 168, loss = 0.04264124\n",
      "Iteration 169, loss = 0.04270325\n",
      "Iteration 170, loss = 0.04234891\n",
      "Iteration 171, loss = 0.04210814\n",
      "Iteration 172, loss = 0.04152899\n",
      "Iteration 173, loss = 0.04142623\n",
      "Iteration 174, loss = 0.04021804\n",
      "Iteration 175, loss = 0.04133616\n",
      "Iteration 176, loss = 0.04009894\n",
      "Iteration 177, loss = 0.03938533\n",
      "Iteration 178, loss = 0.03938045\n",
      "Iteration 179, loss = 0.03881933\n",
      "Iteration 180, loss = 0.03835633\n",
      "Iteration 181, loss = 0.03789370\n",
      "Iteration 182, loss = 0.03775242\n",
      "Iteration 183, loss = 0.03725233\n",
      "Iteration 184, loss = 0.03721706\n",
      "Iteration 185, loss = 0.03690289\n",
      "Iteration 186, loss = 0.03624860\n",
      "Iteration 187, loss = 0.03619964\n",
      "Iteration 188, loss = 0.03621514\n",
      "Iteration 189, loss = 0.03574653\n",
      "Iteration 190, loss = 0.03531475\n",
      "Iteration 191, loss = 0.03515930\n",
      "Iteration 192, loss = 0.03459595\n",
      "Iteration 193, loss = 0.03477471\n",
      "Iteration 194, loss = 0.03401162\n",
      "Iteration 195, loss = 0.03354345\n",
      "Iteration 196, loss = 0.03334074\n",
      "Iteration 197, loss = 0.03311679\n",
      "Iteration 198, loss = 0.03278323\n",
      "Iteration 199, loss = 0.03244625\n",
      "Iteration 200, loss = 0.03228196\n",
      "Iteration 1, loss = 0.61443599\n",
      "Iteration 2, loss = 0.41124507\n",
      "Iteration 3, loss = 0.34086817\n",
      "Iteration 4, loss = 0.30539251\n",
      "Iteration 5, loss = 0.28324845\n",
      "Iteration 6, loss = 0.26771142\n",
      "Iteration 7, loss = 0.25626075\n",
      "Iteration 8, loss = 0.24687524\n",
      "Iteration 9, loss = 0.23882976\n",
      "Iteration 10, loss = 0.23199458\n",
      "Iteration 11, loss = 0.22594089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.22038276\n",
      "Iteration 13, loss = 0.21532472\n",
      "Iteration 14, loss = 0.21042936\n",
      "Iteration 15, loss = 0.20563170\n",
      "Iteration 16, loss = 0.20165529\n",
      "Iteration 17, loss = 0.19777822\n",
      "Iteration 18, loss = 0.19438728\n",
      "Iteration 19, loss = 0.19022226\n",
      "Iteration 20, loss = 0.18678957\n",
      "Iteration 21, loss = 0.18346746\n",
      "Iteration 22, loss = 0.18001516\n",
      "Iteration 23, loss = 0.17730317\n",
      "Iteration 24, loss = 0.17410757\n",
      "Iteration 25, loss = 0.17087472\n",
      "Iteration 26, loss = 0.16819949\n",
      "Iteration 27, loss = 0.16584097\n",
      "Iteration 28, loss = 0.16290026\n",
      "Iteration 29, loss = 0.16063910\n",
      "Iteration 30, loss = 0.15805801\n",
      "Iteration 31, loss = 0.15554701\n",
      "Iteration 32, loss = 0.15300829\n",
      "Iteration 33, loss = 0.15109563\n",
      "Iteration 34, loss = 0.14864170\n",
      "Iteration 35, loss = 0.14639027\n",
      "Iteration 36, loss = 0.14425770\n",
      "Iteration 37, loss = 0.14251680\n",
      "Iteration 38, loss = 0.14028337\n",
      "Iteration 39, loss = 0.13868494\n",
      "Iteration 40, loss = 0.13734187\n",
      "Iteration 41, loss = 0.13512850\n",
      "Iteration 42, loss = 0.13304322\n",
      "Iteration 43, loss = 0.13124007\n",
      "Iteration 44, loss = 0.12988481\n",
      "Iteration 45, loss = 0.12846578\n",
      "Iteration 46, loss = 0.12666448\n",
      "Iteration 47, loss = 0.12545201\n",
      "Iteration 48, loss = 0.12399893\n",
      "Iteration 49, loss = 0.12239113\n",
      "Iteration 50, loss = 0.12115829\n",
      "Iteration 51, loss = 0.11998524\n",
      "Iteration 52, loss = 0.11888991\n",
      "Iteration 53, loss = 0.11720851\n",
      "Iteration 54, loss = 0.11587287\n",
      "Iteration 55, loss = 0.11485322\n",
      "Iteration 56, loss = 0.11369341\n",
      "Iteration 57, loss = 0.11236163\n",
      "Iteration 58, loss = 0.11127379\n",
      "Iteration 59, loss = 0.11034620\n",
      "Iteration 60, loss = 0.10935111\n",
      "Iteration 61, loss = 0.10814800\n",
      "Iteration 62, loss = 0.10738815\n",
      "Iteration 63, loss = 0.10623859\n",
      "Iteration 64, loss = 0.10504777\n",
      "Iteration 65, loss = 0.10413736\n",
      "Iteration 66, loss = 0.10295900\n",
      "Iteration 67, loss = 0.10221435\n",
      "Iteration 68, loss = 0.10117001\n",
      "Iteration 69, loss = 0.10050455\n",
      "Iteration 70, loss = 0.09991207\n",
      "Iteration 71, loss = 0.09854453\n",
      "Iteration 72, loss = 0.09771330\n",
      "Iteration 73, loss = 0.09668364\n",
      "Iteration 74, loss = 0.09549337\n",
      "Iteration 75, loss = 0.09516516\n",
      "Iteration 76, loss = 0.09453725\n",
      "Iteration 77, loss = 0.09361568\n",
      "Iteration 78, loss = 0.09277893\n",
      "Iteration 79, loss = 0.09165237\n",
      "Iteration 80, loss = 0.09117660\n",
      "Iteration 81, loss = 0.09009282\n",
      "Iteration 82, loss = 0.08948860\n",
      "Iteration 83, loss = 0.08850640\n",
      "Iteration 84, loss = 0.08782236\n",
      "Iteration 85, loss = 0.08708670\n",
      "Iteration 86, loss = 0.08680682\n",
      "Iteration 87, loss = 0.08543874\n",
      "Iteration 88, loss = 0.08534317\n",
      "Iteration 89, loss = 0.08433626\n",
      "Iteration 90, loss = 0.08361074\n",
      "Iteration 91, loss = 0.08303687\n",
      "Iteration 92, loss = 0.08215066\n",
      "Iteration 93, loss = 0.08127863\n",
      "Iteration 94, loss = 0.08131432\n",
      "Iteration 95, loss = 0.08011283\n",
      "Iteration 96, loss = 0.07985154\n",
      "Iteration 97, loss = 0.07869085\n",
      "Iteration 98, loss = 0.07813965\n",
      "Iteration 99, loss = 0.07759829\n",
      "Iteration 100, loss = 0.07722759\n",
      "Iteration 101, loss = 0.07634828\n",
      "Iteration 102, loss = 0.07589902\n",
      "Iteration 103, loss = 0.07523982\n",
      "Iteration 104, loss = 0.07440472\n",
      "Iteration 105, loss = 0.07401384\n",
      "Iteration 106, loss = 0.07345761\n",
      "Iteration 107, loss = 0.07290434\n",
      "Iteration 108, loss = 0.07333445\n",
      "Iteration 109, loss = 0.07189508\n",
      "Iteration 110, loss = 0.07112936\n",
      "Iteration 111, loss = 0.07060112\n",
      "Iteration 112, loss = 0.06998613\n",
      "Iteration 113, loss = 0.06952714\n",
      "Iteration 114, loss = 0.06866650\n",
      "Iteration 115, loss = 0.06821419\n",
      "Iteration 116, loss = 0.06802012\n",
      "Iteration 117, loss = 0.06690022\n",
      "Iteration 118, loss = 0.06673080\n",
      "Iteration 119, loss = 0.06577783\n",
      "Iteration 120, loss = 0.06540917\n",
      "Iteration 121, loss = 0.06564986\n",
      "Iteration 122, loss = 0.06441439\n",
      "Iteration 123, loss = 0.06422903\n",
      "Iteration 124, loss = 0.06350931\n",
      "Iteration 125, loss = 0.06252964\n",
      "Iteration 126, loss = 0.06231430\n",
      "Iteration 127, loss = 0.06184115\n",
      "Iteration 128, loss = 0.06075727\n",
      "Iteration 129, loss = 0.06119931\n",
      "Iteration 130, loss = 0.06002592\n",
      "Iteration 131, loss = 0.05960538\n",
      "Iteration 132, loss = 0.05892782\n",
      "Iteration 133, loss = 0.05895952\n",
      "Iteration 134, loss = 0.05840843\n",
      "Iteration 135, loss = 0.05875908\n",
      "Iteration 136, loss = 0.05741961\n",
      "Iteration 137, loss = 0.05683561\n",
      "Iteration 138, loss = 0.05646130\n",
      "Iteration 139, loss = 0.05605222\n",
      "Iteration 140, loss = 0.05567342\n",
      "Iteration 141, loss = 0.05474126\n",
      "Iteration 142, loss = 0.05451551\n",
      "Iteration 143, loss = 0.05404908\n",
      "Iteration 144, loss = 0.05411944\n",
      "Iteration 145, loss = 0.05318320\n",
      "Iteration 146, loss = 0.05254322\n",
      "Iteration 147, loss = 0.05212208\n",
      "Iteration 148, loss = 0.05201717\n",
      "Iteration 149, loss = 0.05184073\n",
      "Iteration 150, loss = 0.05121949\n",
      "Iteration 151, loss = 0.05111667\n",
      "Iteration 152, loss = 0.05069461\n",
      "Iteration 153, loss = 0.05044699\n",
      "Iteration 154, loss = 0.04950020\n",
      "Iteration 155, loss = 0.04874022\n",
      "Iteration 156, loss = 0.04884293\n",
      "Iteration 157, loss = 0.04788966\n",
      "Iteration 158, loss = 0.04753196\n",
      "Iteration 159, loss = 0.04708561\n",
      "Iteration 160, loss = 0.04645616\n",
      "Iteration 161, loss = 0.04633816\n",
      "Iteration 162, loss = 0.04647625\n",
      "Iteration 163, loss = 0.04574133\n",
      "Iteration 164, loss = 0.04534456\n",
      "Iteration 165, loss = 0.04535816\n",
      "Iteration 166, loss = 0.04473205\n",
      "Iteration 167, loss = 0.04461526\n",
      "Iteration 168, loss = 0.04375577\n",
      "Iteration 169, loss = 0.04350152\n",
      "Iteration 170, loss = 0.04346499\n",
      "Iteration 171, loss = 0.04307188\n",
      "Iteration 172, loss = 0.04228218\n",
      "Iteration 173, loss = 0.04223306\n",
      "Iteration 174, loss = 0.04130428\n",
      "Iteration 175, loss = 0.04185617\n",
      "Iteration 176, loss = 0.04143577\n",
      "Iteration 177, loss = 0.04072763\n",
      "Iteration 178, loss = 0.04015934\n",
      "Iteration 179, loss = 0.03952326\n",
      "Iteration 180, loss = 0.03951312\n",
      "Iteration 181, loss = 0.03895394\n",
      "Iteration 182, loss = 0.03869973\n",
      "Iteration 183, loss = 0.03832793\n",
      "Iteration 184, loss = 0.03832007\n",
      "Iteration 185, loss = 0.03821863\n",
      "Iteration 186, loss = 0.03739880\n",
      "Iteration 187, loss = 0.03712704\n",
      "Iteration 188, loss = 0.03665415\n",
      "Iteration 189, loss = 0.03677228\n",
      "Iteration 190, loss = 0.03632805\n",
      "Iteration 191, loss = 0.03591643\n",
      "Iteration 192, loss = 0.03559389\n",
      "Iteration 193, loss = 0.03560692\n",
      "Iteration 194, loss = 0.03511067\n",
      "Iteration 195, loss = 0.03471964\n",
      "Iteration 196, loss = 0.03457461\n",
      "Iteration 197, loss = 0.03431713\n",
      "Iteration 198, loss = 0.03370291\n",
      "Iteration 199, loss = 0.03361809\n",
      "Iteration 200, loss = 0.03345449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72967515\n",
      "Iteration 2, loss = 0.59496349\n",
      "Iteration 3, loss = 0.52267396\n",
      "Iteration 4, loss = 0.48603518\n",
      "Iteration 5, loss = 0.46360083\n",
      "Iteration 6, loss = 0.44687568\n",
      "Iteration 7, loss = 0.43318800\n",
      "Iteration 8, loss = 0.42139958\n",
      "Iteration 9, loss = 0.41099244\n",
      "Iteration 10, loss = 0.40158733\n",
      "Iteration 11, loss = 0.39294219\n",
      "Iteration 12, loss = 0.38507323\n",
      "Iteration 13, loss = 0.37779074\n",
      "Iteration 14, loss = 0.37106081\n",
      "Iteration 15, loss = 0.36479777\n",
      "Iteration 16, loss = 0.35894824\n",
      "Iteration 17, loss = 0.35352208\n",
      "Iteration 18, loss = 0.34843874\n",
      "Iteration 19, loss = 0.34365369\n",
      "Iteration 20, loss = 0.33909521\n",
      "Iteration 21, loss = 0.33488506\n",
      "Iteration 22, loss = 0.33087924\n",
      "Iteration 23, loss = 0.32716166\n",
      "Iteration 24, loss = 0.32350217\n",
      "Iteration 25, loss = 0.32018488\n",
      "Iteration 26, loss = 0.31690850\n",
      "Iteration 27, loss = 0.31384015\n",
      "Iteration 28, loss = 0.31094233\n",
      "Iteration 29, loss = 0.30813766\n",
      "Iteration 30, loss = 0.30552326\n",
      "Iteration 31, loss = 0.30299643\n",
      "Iteration 32, loss = 0.30058484\n",
      "Iteration 33, loss = 0.29836358\n",
      "Iteration 34, loss = 0.29610344\n",
      "Iteration 35, loss = 0.29400862\n",
      "Iteration 36, loss = 0.29202058\n",
      "Iteration 37, loss = 0.29004289\n",
      "Iteration 38, loss = 0.28822140\n",
      "Iteration 39, loss = 0.28642563\n",
      "Iteration 40, loss = 0.28472328\n",
      "Iteration 41, loss = 0.28306945\n",
      "Iteration 42, loss = 0.28151655\n",
      "Iteration 43, loss = 0.27999753\n",
      "Iteration 44, loss = 0.27852785\n",
      "Iteration 45, loss = 0.27713756\n",
      "Iteration 46, loss = 0.27583600\n",
      "Iteration 47, loss = 0.27450975\n",
      "Iteration 48, loss = 0.27330732\n",
      "Iteration 49, loss = 0.27210031\n",
      "Iteration 50, loss = 0.27091533\n",
      "Iteration 51, loss = 0.26981104\n",
      "Iteration 52, loss = 0.26873697\n",
      "Iteration 53, loss = 0.26770050\n",
      "Iteration 54, loss = 0.26668517\n",
      "Iteration 55, loss = 0.26570769\n",
      "Iteration 56, loss = 0.26477348\n",
      "Iteration 57, loss = 0.26386535\n",
      "Iteration 58, loss = 0.26299075\n",
      "Iteration 59, loss = 0.26214345\n",
      "Iteration 60, loss = 0.26131226\n",
      "Iteration 61, loss = 0.26052288\n",
      "Iteration 62, loss = 0.25974440\n",
      "Iteration 63, loss = 0.25898880\n",
      "Iteration 64, loss = 0.25823875\n",
      "Iteration 65, loss = 0.25751844\n",
      "Iteration 66, loss = 0.25683288\n",
      "Iteration 67, loss = 0.25612261\n",
      "Iteration 68, loss = 0.25548442\n",
      "Iteration 69, loss = 0.25479196\n",
      "Iteration 70, loss = 0.25416262\n",
      "Iteration 71, loss = 0.25355139\n",
      "Iteration 72, loss = 0.25292892\n",
      "Iteration 73, loss = 0.25232183\n",
      "Iteration 74, loss = 0.25173352\n",
      "Iteration 75, loss = 0.25114753\n",
      "Iteration 76, loss = 0.25058930\n",
      "Iteration 77, loss = 0.25001407\n",
      "Iteration 78, loss = 0.24946835\n",
      "Iteration 79, loss = 0.24894282\n",
      "Iteration 80, loss = 0.24838588\n",
      "Iteration 81, loss = 0.24788959\n",
      "Iteration 82, loss = 0.24736023\n",
      "Iteration 83, loss = 0.24686764\n",
      "Iteration 84, loss = 0.24638570\n",
      "Iteration 85, loss = 0.24589638\n",
      "Iteration 86, loss = 0.24540381\n",
      "Iteration 87, loss = 0.24493713\n",
      "Iteration 88, loss = 0.24445836\n",
      "Iteration 89, loss = 0.24401059\n",
      "Iteration 90, loss = 0.24354564\n",
      "Iteration 91, loss = 0.24309945\n",
      "Iteration 92, loss = 0.24265059\n",
      "Iteration 93, loss = 0.24220520\n",
      "Iteration 94, loss = 0.24179482\n",
      "Iteration 95, loss = 0.24134619\n",
      "Iteration 96, loss = 0.24093313\n",
      "Iteration 97, loss = 0.24051270\n",
      "Iteration 98, loss = 0.24009681\n",
      "Iteration 99, loss = 0.23970027\n",
      "Iteration 100, loss = 0.23927917\n",
      "Iteration 101, loss = 0.23888870\n",
      "Iteration 102, loss = 0.23850412\n",
      "Iteration 103, loss = 0.23808996\n",
      "Iteration 104, loss = 0.23770163\n",
      "Iteration 105, loss = 0.23733672\n",
      "Iteration 106, loss = 0.23698955\n",
      "Iteration 107, loss = 0.23655183\n",
      "Iteration 108, loss = 0.23620925\n",
      "Iteration 109, loss = 0.23582105\n",
      "Iteration 110, loss = 0.23545277\n",
      "Iteration 111, loss = 0.23508471\n",
      "Iteration 112, loss = 0.23472944\n",
      "Iteration 113, loss = 0.23437848\n",
      "Iteration 114, loss = 0.23401532\n",
      "Iteration 115, loss = 0.23364535\n",
      "Iteration 116, loss = 0.23329638\n",
      "Iteration 117, loss = 0.23295389\n",
      "Iteration 118, loss = 0.23258596\n",
      "Iteration 119, loss = 0.23224601\n",
      "Iteration 120, loss = 0.23190709\n",
      "Iteration 121, loss = 0.23157753\n",
      "Iteration 122, loss = 0.23123151\n",
      "Iteration 123, loss = 0.23088911\n",
      "Iteration 124, loss = 0.23055133\n",
      "Iteration 125, loss = 0.23022509\n",
      "Iteration 126, loss = 0.22987881\n",
      "Iteration 127, loss = 0.22955877\n",
      "Iteration 128, loss = 0.22922846\n",
      "Iteration 129, loss = 0.22890830\n",
      "Iteration 130, loss = 0.22856634\n",
      "Iteration 131, loss = 0.22825019\n",
      "Iteration 132, loss = 0.22791357\n",
      "Iteration 133, loss = 0.22757762\n",
      "Iteration 134, loss = 0.22729005\n",
      "Iteration 135, loss = 0.22694849\n",
      "Iteration 136, loss = 0.22665351\n",
      "Iteration 137, loss = 0.22631780\n",
      "Iteration 138, loss = 0.22600891\n",
      "Iteration 139, loss = 0.22570425\n",
      "Iteration 140, loss = 0.22539231\n",
      "Iteration 141, loss = 0.22508149\n",
      "Iteration 142, loss = 0.22478908\n",
      "Iteration 143, loss = 0.22446510\n",
      "Iteration 144, loss = 0.22417731\n",
      "Iteration 145, loss = 0.22386600\n",
      "Iteration 146, loss = 0.22357747\n",
      "Iteration 147, loss = 0.22325232\n",
      "Iteration 148, loss = 0.22295496\n",
      "Iteration 149, loss = 0.22265045\n",
      "Iteration 150, loss = 0.22235511\n",
      "Iteration 151, loss = 0.22207129\n",
      "Iteration 152, loss = 0.22177714\n",
      "Iteration 153, loss = 0.22148772\n",
      "Iteration 154, loss = 0.22121101\n",
      "Iteration 155, loss = 0.22089977\n",
      "Iteration 156, loss = 0.22063146\n",
      "Iteration 157, loss = 0.22032830\n",
      "Iteration 158, loss = 0.22004154\n",
      "Iteration 159, loss = 0.21975948\n",
      "Iteration 160, loss = 0.21948630\n",
      "Iteration 161, loss = 0.21918051\n",
      "Iteration 162, loss = 0.21890949\n",
      "Iteration 163, loss = 0.21862583\n",
      "Iteration 164, loss = 0.21834256\n",
      "Iteration 165, loss = 0.21808627\n",
      "Iteration 166, loss = 0.21780285\n",
      "Iteration 167, loss = 0.21756027\n",
      "Iteration 168, loss = 0.21726407\n",
      "Iteration 169, loss = 0.21698194\n",
      "Iteration 170, loss = 0.21672310\n",
      "Iteration 171, loss = 0.21643688\n",
      "Iteration 172, loss = 0.21616610\n",
      "Iteration 173, loss = 0.21589673\n",
      "Iteration 174, loss = 0.21562858\n",
      "Iteration 175, loss = 0.21538780\n",
      "Iteration 176, loss = 0.21510378\n",
      "Iteration 177, loss = 0.21482038\n",
      "Iteration 178, loss = 0.21456311\n",
      "Iteration 179, loss = 0.21430086\n",
      "Iteration 180, loss = 0.21402932\n",
      "Iteration 181, loss = 0.21376015\n",
      "Iteration 182, loss = 0.21352900\n",
      "Iteration 183, loss = 0.21325617\n",
      "Iteration 184, loss = 0.21301007\n",
      "Iteration 185, loss = 0.21273966\n",
      "Iteration 186, loss = 0.21248057\n",
      "Iteration 187, loss = 0.21222890\n",
      "Iteration 188, loss = 0.21197692\n",
      "Iteration 189, loss = 0.21171181\n",
      "Iteration 190, loss = 0.21145247\n",
      "Iteration 191, loss = 0.21120283\n",
      "Iteration 192, loss = 0.21095948\n",
      "Iteration 193, loss = 0.21069879\n",
      "Iteration 194, loss = 0.21044047\n",
      "Iteration 195, loss = 0.21019854\n",
      "Iteration 196, loss = 0.20994756\n",
      "Iteration 197, loss = 0.20970925\n",
      "Iteration 198, loss = 0.20942679\n",
      "Iteration 199, loss = 0.20920456\n",
      "Iteration 200, loss = 0.20895829\n",
      "Iteration 1, loss = 0.73016497\n",
      "Iteration 2, loss = 0.59469330\n",
      "Iteration 3, loss = 0.52194651\n",
      "Iteration 4, loss = 0.48495748\n",
      "Iteration 5, loss = 0.46195429\n",
      "Iteration 6, loss = 0.44506392\n",
      "Iteration 7, loss = 0.43107907\n",
      "Iteration 8, loss = 0.41908235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.40847157\n",
      "Iteration 10, loss = 0.39897103\n",
      "Iteration 11, loss = 0.39008481\n",
      "Iteration 12, loss = 0.38215268\n",
      "Iteration 13, loss = 0.37471941\n",
      "Iteration 14, loss = 0.36787484\n",
      "Iteration 15, loss = 0.36152775\n",
      "Iteration 16, loss = 0.35563352\n",
      "Iteration 17, loss = 0.35014521\n",
      "Iteration 18, loss = 0.34497516\n",
      "Iteration 19, loss = 0.34014470\n",
      "Iteration 20, loss = 0.33556125\n",
      "Iteration 21, loss = 0.33133248\n",
      "Iteration 22, loss = 0.32730253\n",
      "Iteration 23, loss = 0.32355588\n",
      "Iteration 24, loss = 0.31993990\n",
      "Iteration 25, loss = 0.31655647\n",
      "Iteration 26, loss = 0.31331544\n",
      "Iteration 27, loss = 0.31026913\n",
      "Iteration 28, loss = 0.30737392\n",
      "Iteration 29, loss = 0.30461104\n",
      "Iteration 30, loss = 0.30199895\n",
      "Iteration 31, loss = 0.29949049\n",
      "Iteration 32, loss = 0.29712140\n",
      "Iteration 33, loss = 0.29487808\n",
      "Iteration 34, loss = 0.29267368\n",
      "Iteration 35, loss = 0.29056947\n",
      "Iteration 36, loss = 0.28859007\n",
      "Iteration 37, loss = 0.28664964\n",
      "Iteration 38, loss = 0.28481805\n",
      "Iteration 39, loss = 0.28305051\n",
      "Iteration 40, loss = 0.28134725\n",
      "Iteration 41, loss = 0.27971418\n",
      "Iteration 42, loss = 0.27817829\n",
      "Iteration 43, loss = 0.27663725\n",
      "Iteration 44, loss = 0.27518640\n",
      "Iteration 45, loss = 0.27373794\n",
      "Iteration 46, loss = 0.27239852\n",
      "Iteration 47, loss = 0.27106637\n",
      "Iteration 48, loss = 0.26980275\n",
      "Iteration 49, loss = 0.26858711\n",
      "Iteration 50, loss = 0.26737730\n",
      "Iteration 51, loss = 0.26626073\n",
      "Iteration 52, loss = 0.26514494\n",
      "Iteration 53, loss = 0.26407676\n",
      "Iteration 54, loss = 0.26303844\n",
      "Iteration 55, loss = 0.26203322\n",
      "Iteration 56, loss = 0.26106552\n",
      "Iteration 57, loss = 0.26014805\n",
      "Iteration 58, loss = 0.25923131\n",
      "Iteration 59, loss = 0.25834289\n",
      "Iteration 60, loss = 0.25748318\n",
      "Iteration 61, loss = 0.25665955\n",
      "Iteration 62, loss = 0.25584812\n",
      "Iteration 63, loss = 0.25507876\n",
      "Iteration 64, loss = 0.25431840\n",
      "Iteration 65, loss = 0.25358718\n",
      "Iteration 66, loss = 0.25285067\n",
      "Iteration 67, loss = 0.25214093\n",
      "Iteration 68, loss = 0.25148354\n",
      "Iteration 69, loss = 0.25077363\n",
      "Iteration 70, loss = 0.25011821\n",
      "Iteration 71, loss = 0.24950789\n",
      "Iteration 72, loss = 0.24886162\n",
      "Iteration 73, loss = 0.24825035\n",
      "Iteration 74, loss = 0.24765504\n",
      "Iteration 75, loss = 0.24706282\n",
      "Iteration 76, loss = 0.24647462\n",
      "Iteration 77, loss = 0.24590456\n",
      "Iteration 78, loss = 0.24536379\n",
      "Iteration 79, loss = 0.24481621\n",
      "Iteration 80, loss = 0.24424974\n",
      "Iteration 81, loss = 0.24373390\n",
      "Iteration 82, loss = 0.24320876\n",
      "Iteration 83, loss = 0.24271470\n",
      "Iteration 84, loss = 0.24220338\n",
      "Iteration 85, loss = 0.24172144\n",
      "Iteration 86, loss = 0.24122724\n",
      "Iteration 87, loss = 0.24077606\n",
      "Iteration 88, loss = 0.24025846\n",
      "Iteration 89, loss = 0.23982493\n",
      "Iteration 90, loss = 0.23936230\n",
      "Iteration 91, loss = 0.23890329\n",
      "Iteration 92, loss = 0.23844928\n",
      "Iteration 93, loss = 0.23800721\n",
      "Iteration 94, loss = 0.23758197\n",
      "Iteration 95, loss = 0.23713636\n",
      "Iteration 96, loss = 0.23671078\n",
      "Iteration 97, loss = 0.23630121\n",
      "Iteration 98, loss = 0.23586285\n",
      "Iteration 99, loss = 0.23547859\n",
      "Iteration 100, loss = 0.23504530\n",
      "Iteration 101, loss = 0.23463872\n",
      "Iteration 102, loss = 0.23423834\n",
      "Iteration 103, loss = 0.23385375\n",
      "Iteration 104, loss = 0.23344130\n",
      "Iteration 105, loss = 0.23305669\n",
      "Iteration 106, loss = 0.23271060\n",
      "Iteration 107, loss = 0.23228804\n",
      "Iteration 108, loss = 0.23191204\n",
      "Iteration 109, loss = 0.23154112\n",
      "Iteration 110, loss = 0.23115869\n",
      "Iteration 111, loss = 0.23079293\n",
      "Iteration 112, loss = 0.23041418\n",
      "Iteration 113, loss = 0.23006224\n",
      "Iteration 114, loss = 0.22972067\n",
      "Iteration 115, loss = 0.22931879\n",
      "Iteration 116, loss = 0.22897441\n",
      "Iteration 117, loss = 0.22860650\n",
      "Iteration 118, loss = 0.22826408\n",
      "Iteration 119, loss = 0.22791106\n",
      "Iteration 120, loss = 0.22757614\n",
      "Iteration 121, loss = 0.22721944\n",
      "Iteration 122, loss = 0.22688612\n",
      "Iteration 123, loss = 0.22652369\n",
      "Iteration 124, loss = 0.22619288\n",
      "Iteration 125, loss = 0.22584482\n",
      "Iteration 126, loss = 0.22552537\n",
      "Iteration 127, loss = 0.22518213\n",
      "Iteration 128, loss = 0.22486653\n",
      "Iteration 129, loss = 0.22453028\n",
      "Iteration 130, loss = 0.22418298\n",
      "Iteration 131, loss = 0.22385269\n",
      "Iteration 132, loss = 0.22354413\n",
      "Iteration 133, loss = 0.22320195\n",
      "Iteration 134, loss = 0.22288925\n",
      "Iteration 135, loss = 0.22256650\n",
      "Iteration 136, loss = 0.22224665\n",
      "Iteration 137, loss = 0.22191235\n",
      "Iteration 138, loss = 0.22161974\n",
      "Iteration 139, loss = 0.22129461\n",
      "Iteration 140, loss = 0.22098924\n",
      "Iteration 141, loss = 0.22068039\n",
      "Iteration 142, loss = 0.22036836\n",
      "Iteration 143, loss = 0.22005045\n",
      "Iteration 144, loss = 0.21977291\n",
      "Iteration 145, loss = 0.21944309\n",
      "Iteration 146, loss = 0.21914195\n",
      "Iteration 147, loss = 0.21884314\n",
      "Iteration 148, loss = 0.21853491\n",
      "Iteration 149, loss = 0.21822994\n",
      "Iteration 150, loss = 0.21793967\n",
      "Iteration 151, loss = 0.21764107\n",
      "Iteration 152, loss = 0.21736028\n",
      "Iteration 153, loss = 0.21706705\n",
      "Iteration 154, loss = 0.21677570\n",
      "Iteration 155, loss = 0.21648312\n",
      "Iteration 156, loss = 0.21621961\n",
      "Iteration 157, loss = 0.21591602\n",
      "Iteration 158, loss = 0.21561733\n",
      "Iteration 159, loss = 0.21534834\n",
      "Iteration 160, loss = 0.21507417\n",
      "Iteration 161, loss = 0.21477829\n",
      "Iteration 162, loss = 0.21447863\n",
      "Iteration 163, loss = 0.21421225\n",
      "Iteration 164, loss = 0.21392443\n",
      "Iteration 165, loss = 0.21366421\n",
      "Iteration 166, loss = 0.21337032\n",
      "Iteration 167, loss = 0.21311809\n",
      "Iteration 168, loss = 0.21282110\n",
      "Iteration 169, loss = 0.21255207\n",
      "Iteration 170, loss = 0.21228331\n",
      "Iteration 171, loss = 0.21199603\n",
      "Iteration 172, loss = 0.21173158\n",
      "Iteration 173, loss = 0.21146196\n",
      "Iteration 174, loss = 0.21118243\n",
      "Iteration 175, loss = 0.21092039\n",
      "Iteration 176, loss = 0.21066977\n",
      "Iteration 177, loss = 0.21037598\n",
      "Iteration 178, loss = 0.21009020\n",
      "Iteration 179, loss = 0.20984659\n",
      "Iteration 180, loss = 0.20956610\n",
      "Iteration 181, loss = 0.20929495\n",
      "Iteration 182, loss = 0.20904426\n",
      "Iteration 183, loss = 0.20878338\n",
      "Iteration 184, loss = 0.20853286\n",
      "Iteration 185, loss = 0.20825724\n",
      "Iteration 186, loss = 0.20800715\n",
      "Iteration 187, loss = 0.20775403\n",
      "Iteration 188, loss = 0.20750879\n",
      "Iteration 189, loss = 0.20724016\n",
      "Iteration 190, loss = 0.20698467\n",
      "Iteration 191, loss = 0.20673869\n",
      "Iteration 192, loss = 0.20649234\n",
      "Iteration 193, loss = 0.20623821\n",
      "Iteration 194, loss = 0.20598891\n",
      "Iteration 195, loss = 0.20574729\n",
      "Iteration 196, loss = 0.20551250\n",
      "Iteration 197, loss = 0.20526056\n",
      "Iteration 198, loss = 0.20500924\n",
      "Iteration 199, loss = 0.20477873\n",
      "Iteration 200, loss = 0.20452477\n",
      "Iteration 1, loss = 0.73357680\n",
      "Iteration 2, loss = 0.59795252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.52619670\n",
      "Iteration 4, loss = 0.48966843\n",
      "Iteration 5, loss = 0.46696666\n",
      "Iteration 6, loss = 0.45016318\n",
      "Iteration 7, loss = 0.43642280\n",
      "Iteration 8, loss = 0.42460146\n",
      "Iteration 9, loss = 0.41411600\n",
      "Iteration 10, loss = 0.40465310\n",
      "Iteration 11, loss = 0.39593745\n",
      "Iteration 12, loss = 0.38808816\n",
      "Iteration 13, loss = 0.38072010\n",
      "Iteration 14, loss = 0.37395850\n",
      "Iteration 15, loss = 0.36762905\n",
      "Iteration 16, loss = 0.36178655\n",
      "Iteration 17, loss = 0.35633681\n",
      "Iteration 18, loss = 0.35117674\n",
      "Iteration 19, loss = 0.34639423\n",
      "Iteration 20, loss = 0.34184409\n",
      "Iteration 21, loss = 0.33759442\n",
      "Iteration 22, loss = 0.33361576\n",
      "Iteration 23, loss = 0.32984179\n",
      "Iteration 24, loss = 0.32626554\n",
      "Iteration 25, loss = 0.32288766\n",
      "Iteration 26, loss = 0.31964200\n",
      "Iteration 27, loss = 0.31661408\n",
      "Iteration 28, loss = 0.31369329\n",
      "Iteration 29, loss = 0.31094233\n",
      "Iteration 30, loss = 0.30832083\n",
      "Iteration 31, loss = 0.30581275\n",
      "Iteration 32, loss = 0.30341467\n",
      "Iteration 33, loss = 0.30113481\n",
      "Iteration 34, loss = 0.29891759\n",
      "Iteration 35, loss = 0.29683815\n",
      "Iteration 36, loss = 0.29479691\n",
      "Iteration 37, loss = 0.29288295\n",
      "Iteration 38, loss = 0.29102878\n",
      "Iteration 39, loss = 0.28924011\n",
      "Iteration 40, loss = 0.28755582\n",
      "Iteration 41, loss = 0.28590074\n",
      "Iteration 42, loss = 0.28434042\n",
      "Iteration 43, loss = 0.28278559\n",
      "Iteration 44, loss = 0.28131498\n",
      "Iteration 45, loss = 0.27990490\n",
      "Iteration 46, loss = 0.27854964\n",
      "Iteration 47, loss = 0.27722497\n",
      "Iteration 48, loss = 0.27593850\n",
      "Iteration 49, loss = 0.27472677\n",
      "Iteration 50, loss = 0.27350691\n",
      "Iteration 51, loss = 0.27242874\n",
      "Iteration 52, loss = 0.27127329\n",
      "Iteration 53, loss = 0.27019426\n",
      "Iteration 54, loss = 0.26916949\n",
      "Iteration 55, loss = 0.26815098\n",
      "Iteration 56, loss = 0.26719611\n",
      "Iteration 57, loss = 0.26625107\n",
      "Iteration 58, loss = 0.26534740\n",
      "Iteration 59, loss = 0.26443709\n",
      "Iteration 60, loss = 0.26357459\n",
      "Iteration 61, loss = 0.26273935\n",
      "Iteration 62, loss = 0.26192497\n",
      "Iteration 63, loss = 0.26113885\n",
      "Iteration 64, loss = 0.26036693\n",
      "Iteration 65, loss = 0.25961751\n",
      "Iteration 66, loss = 0.25887745\n",
      "Iteration 67, loss = 0.25818925\n",
      "Iteration 68, loss = 0.25747789\n",
      "Iteration 69, loss = 0.25678372\n",
      "Iteration 70, loss = 0.25612640\n",
      "Iteration 71, loss = 0.25548044\n",
      "Iteration 72, loss = 0.25483342\n",
      "Iteration 73, loss = 0.25420434\n",
      "Iteration 74, loss = 0.25359126\n",
      "Iteration 75, loss = 0.25299439\n",
      "Iteration 76, loss = 0.25241680\n",
      "Iteration 77, loss = 0.25180930\n",
      "Iteration 78, loss = 0.25124698\n",
      "Iteration 79, loss = 0.25069549\n",
      "Iteration 80, loss = 0.25013422\n",
      "Iteration 81, loss = 0.24958869\n",
      "Iteration 82, loss = 0.24905591\n",
      "Iteration 83, loss = 0.24854193\n",
      "Iteration 84, loss = 0.24801386\n",
      "Iteration 85, loss = 0.24749825\n",
      "Iteration 86, loss = 0.24699469\n",
      "Iteration 87, loss = 0.24650974\n",
      "Iteration 88, loss = 0.24599038\n",
      "Iteration 89, loss = 0.24551823\n",
      "Iteration 90, loss = 0.24504262\n",
      "Iteration 91, loss = 0.24457789\n",
      "Iteration 92, loss = 0.24409162\n",
      "Iteration 93, loss = 0.24365688\n",
      "Iteration 94, loss = 0.24319782\n",
      "Iteration 95, loss = 0.24273734\n",
      "Iteration 96, loss = 0.24230912\n",
      "Iteration 97, loss = 0.24185495\n",
      "Iteration 98, loss = 0.24142671\n",
      "Iteration 99, loss = 0.24100024\n",
      "Iteration 100, loss = 0.24057707\n",
      "Iteration 101, loss = 0.24013371\n",
      "Iteration 102, loss = 0.23971385\n",
      "Iteration 103, loss = 0.23931777\n",
      "Iteration 104, loss = 0.23888516\n",
      "Iteration 105, loss = 0.23848580\n",
      "Iteration 106, loss = 0.23809197\n",
      "Iteration 107, loss = 0.23767378\n",
      "Iteration 108, loss = 0.23725825\n",
      "Iteration 109, loss = 0.23690598\n",
      "Iteration 110, loss = 0.23647334\n",
      "Iteration 111, loss = 0.23609142\n",
      "Iteration 112, loss = 0.23568833\n",
      "Iteration 113, loss = 0.23531812\n",
      "Iteration 114, loss = 0.23492767\n",
      "Iteration 115, loss = 0.23454804\n",
      "Iteration 116, loss = 0.23415496\n",
      "Iteration 117, loss = 0.23377781\n",
      "Iteration 118, loss = 0.23339762\n",
      "Iteration 119, loss = 0.23302564\n",
      "Iteration 120, loss = 0.23265216\n",
      "Iteration 121, loss = 0.23227979\n",
      "Iteration 122, loss = 0.23192028\n",
      "Iteration 123, loss = 0.23155390\n",
      "Iteration 124, loss = 0.23119528\n",
      "Iteration 125, loss = 0.23081998\n",
      "Iteration 126, loss = 0.23049944\n",
      "Iteration 127, loss = 0.23011219\n",
      "Iteration 128, loss = 0.22978902\n",
      "Iteration 129, loss = 0.22942394\n",
      "Iteration 130, loss = 0.22905068\n",
      "Iteration 131, loss = 0.22870555\n",
      "Iteration 132, loss = 0.22837592\n",
      "Iteration 133, loss = 0.22801224\n",
      "Iteration 134, loss = 0.22768596\n",
      "Iteration 135, loss = 0.22733714\n",
      "Iteration 136, loss = 0.22699832\n",
      "Iteration 137, loss = 0.22666569\n",
      "Iteration 138, loss = 0.22633329\n",
      "Iteration 139, loss = 0.22597948\n",
      "Iteration 140, loss = 0.22564191\n",
      "Iteration 141, loss = 0.22531794\n",
      "Iteration 142, loss = 0.22496294\n",
      "Iteration 143, loss = 0.22463682\n",
      "Iteration 144, loss = 0.22434135\n",
      "Iteration 145, loss = 0.22397952\n",
      "Iteration 146, loss = 0.22365718\n",
      "Iteration 147, loss = 0.22334632\n",
      "Iteration 148, loss = 0.22299738\n",
      "Iteration 149, loss = 0.22267715\n",
      "Iteration 150, loss = 0.22237609\n",
      "Iteration 151, loss = 0.22204983\n",
      "Iteration 152, loss = 0.22175063\n",
      "Iteration 153, loss = 0.22141926\n",
      "Iteration 154, loss = 0.22111720\n",
      "Iteration 155, loss = 0.22080211\n",
      "Iteration 156, loss = 0.22050381\n",
      "Iteration 157, loss = 0.22018061\n",
      "Iteration 158, loss = 0.21986564\n",
      "Iteration 159, loss = 0.21957225\n",
      "Iteration 160, loss = 0.21927835\n",
      "Iteration 161, loss = 0.21895782\n",
      "Iteration 162, loss = 0.21865109\n",
      "Iteration 163, loss = 0.21834741\n",
      "Iteration 164, loss = 0.21804218\n",
      "Iteration 165, loss = 0.21776827\n",
      "Iteration 166, loss = 0.21747058\n",
      "Iteration 167, loss = 0.21716112\n",
      "Iteration 168, loss = 0.21685407\n",
      "Iteration 169, loss = 0.21657403\n",
      "Iteration 170, loss = 0.21626677\n",
      "Iteration 171, loss = 0.21597574\n",
      "Iteration 172, loss = 0.21567698\n",
      "Iteration 173, loss = 0.21539134\n",
      "Iteration 174, loss = 0.21510207\n",
      "Iteration 175, loss = 0.21480375\n",
      "Iteration 176, loss = 0.21452053\n",
      "Iteration 177, loss = 0.21422726\n",
      "Iteration 178, loss = 0.21393583\n",
      "Iteration 179, loss = 0.21366385\n",
      "Iteration 180, loss = 0.21337904\n",
      "Iteration 181, loss = 0.21307168\n",
      "Iteration 182, loss = 0.21280767\n",
      "Iteration 183, loss = 0.21252331\n",
      "Iteration 184, loss = 0.21226841\n",
      "Iteration 185, loss = 0.21197915\n",
      "Iteration 186, loss = 0.21170295\n",
      "Iteration 187, loss = 0.21144948\n",
      "Iteration 188, loss = 0.21116860\n",
      "Iteration 189, loss = 0.21088808\n",
      "Iteration 190, loss = 0.21060764\n",
      "Iteration 191, loss = 0.21037599\n",
      "Iteration 192, loss = 0.21008089\n",
      "Iteration 193, loss = 0.20980927\n",
      "Iteration 194, loss = 0.20952437\n",
      "Iteration 195, loss = 0.20925907\n",
      "Iteration 196, loss = 0.20899650\n",
      "Iteration 197, loss = 0.20873749\n",
      "Iteration 198, loss = 0.20846526\n",
      "Iteration 199, loss = 0.20822103\n",
      "Iteration 200, loss = 0.20793117\n",
      "Iteration 1, loss = 0.72893468\n",
      "Iteration 2, loss = 0.59345408\n",
      "Iteration 3, loss = 0.52125208\n",
      "Iteration 4, loss = 0.48444341\n",
      "Iteration 5, loss = 0.46179237\n",
      "Iteration 6, loss = 0.44491724\n",
      "Iteration 7, loss = 0.43123666\n",
      "Iteration 8, loss = 0.41945931\n",
      "Iteration 9, loss = 0.40899003\n",
      "Iteration 10, loss = 0.39956630\n",
      "Iteration 11, loss = 0.39090363\n",
      "Iteration 12, loss = 0.38304279\n",
      "Iteration 13, loss = 0.37576833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.36902327\n",
      "Iteration 15, loss = 0.36272223\n",
      "Iteration 16, loss = 0.35687914\n",
      "Iteration 17, loss = 0.35151520\n",
      "Iteration 18, loss = 0.34636486\n",
      "Iteration 19, loss = 0.34161578\n",
      "Iteration 20, loss = 0.33716266\n",
      "Iteration 21, loss = 0.33293989\n",
      "Iteration 22, loss = 0.32900450\n",
      "Iteration 23, loss = 0.32527878\n",
      "Iteration 24, loss = 0.32174315\n",
      "Iteration 25, loss = 0.31840113\n",
      "Iteration 26, loss = 0.31523132\n",
      "Iteration 27, loss = 0.31225524\n",
      "Iteration 28, loss = 0.30940627\n",
      "Iteration 29, loss = 0.30667651\n",
      "Iteration 30, loss = 0.30409987\n",
      "Iteration 31, loss = 0.30167117\n",
      "Iteration 32, loss = 0.29932198\n",
      "Iteration 33, loss = 0.29709615\n",
      "Iteration 34, loss = 0.29494401\n",
      "Iteration 35, loss = 0.29293420\n",
      "Iteration 36, loss = 0.29095191\n",
      "Iteration 37, loss = 0.28909129\n",
      "Iteration 38, loss = 0.28728174\n",
      "Iteration 39, loss = 0.28558573\n",
      "Iteration 40, loss = 0.28392736\n",
      "Iteration 41, loss = 0.28234066\n",
      "Iteration 42, loss = 0.28079986\n",
      "Iteration 43, loss = 0.27933755\n",
      "Iteration 44, loss = 0.27789603\n",
      "Iteration 45, loss = 0.27655744\n",
      "Iteration 46, loss = 0.27521941\n",
      "Iteration 47, loss = 0.27395959\n",
      "Iteration 48, loss = 0.27269583\n",
      "Iteration 49, loss = 0.27155836\n",
      "Iteration 50, loss = 0.27036410\n",
      "Iteration 51, loss = 0.26925624\n",
      "Iteration 52, loss = 0.26818284\n",
      "Iteration 53, loss = 0.26712358\n",
      "Iteration 54, loss = 0.26612941\n",
      "Iteration 55, loss = 0.26512032\n",
      "Iteration 56, loss = 0.26419549\n",
      "Iteration 57, loss = 0.26325549\n",
      "Iteration 58, loss = 0.26235266\n",
      "Iteration 59, loss = 0.26146200\n",
      "Iteration 60, loss = 0.26062309\n",
      "Iteration 61, loss = 0.25978129\n",
      "Iteration 62, loss = 0.25896126\n",
      "Iteration 63, loss = 0.25818213\n",
      "Iteration 64, loss = 0.25741146\n",
      "Iteration 65, loss = 0.25665081\n",
      "Iteration 66, loss = 0.25590844\n",
      "Iteration 67, loss = 0.25522626\n",
      "Iteration 68, loss = 0.25449620\n",
      "Iteration 69, loss = 0.25380908\n",
      "Iteration 70, loss = 0.25315344\n",
      "Iteration 71, loss = 0.25248970\n",
      "Iteration 72, loss = 0.25183146\n",
      "Iteration 73, loss = 0.25119526\n",
      "Iteration 74, loss = 0.25057437\n",
      "Iteration 75, loss = 0.24998828\n",
      "Iteration 76, loss = 0.24938114\n",
      "Iteration 77, loss = 0.24877760\n",
      "Iteration 78, loss = 0.24820870\n",
      "Iteration 79, loss = 0.24765569\n",
      "Iteration 80, loss = 0.24707910\n",
      "Iteration 81, loss = 0.24653124\n",
      "Iteration 82, loss = 0.24599520\n",
      "Iteration 83, loss = 0.24548250\n",
      "Iteration 84, loss = 0.24495873\n",
      "Iteration 85, loss = 0.24444118\n",
      "Iteration 86, loss = 0.24396740\n",
      "Iteration 87, loss = 0.24347436\n",
      "Iteration 88, loss = 0.24296423\n",
      "Iteration 89, loss = 0.24250493\n",
      "Iteration 90, loss = 0.24204104\n",
      "Iteration 91, loss = 0.24158138\n",
      "Iteration 92, loss = 0.24110116\n",
      "Iteration 93, loss = 0.24066591\n",
      "Iteration 94, loss = 0.24021676\n",
      "Iteration 95, loss = 0.23976493\n",
      "Iteration 96, loss = 0.23935042\n",
      "Iteration 97, loss = 0.23891884\n",
      "Iteration 98, loss = 0.23850578\n",
      "Iteration 99, loss = 0.23810690\n",
      "Iteration 100, loss = 0.23768072\n",
      "Iteration 101, loss = 0.23726411\n",
      "Iteration 102, loss = 0.23685048\n",
      "Iteration 103, loss = 0.23646581\n",
      "Iteration 104, loss = 0.23606034\n",
      "Iteration 105, loss = 0.23568130\n",
      "Iteration 106, loss = 0.23527255\n",
      "Iteration 107, loss = 0.23489355\n",
      "Iteration 108, loss = 0.23451303\n",
      "Iteration 109, loss = 0.23415078\n",
      "Iteration 110, loss = 0.23376219\n",
      "Iteration 111, loss = 0.23338269\n",
      "Iteration 112, loss = 0.23300735\n",
      "Iteration 113, loss = 0.23264360\n",
      "Iteration 114, loss = 0.23226352\n",
      "Iteration 115, loss = 0.23192806\n",
      "Iteration 116, loss = 0.23157238\n",
      "Iteration 117, loss = 0.23119733\n",
      "Iteration 118, loss = 0.23085450\n",
      "Iteration 119, loss = 0.23050794\n",
      "Iteration 120, loss = 0.23015642\n",
      "Iteration 121, loss = 0.22979639\n",
      "Iteration 122, loss = 0.22946655\n",
      "Iteration 123, loss = 0.22911173\n",
      "Iteration 124, loss = 0.22878058\n",
      "Iteration 125, loss = 0.22842794\n",
      "Iteration 126, loss = 0.22811266\n",
      "Iteration 127, loss = 0.22776656\n",
      "Iteration 128, loss = 0.22743204\n",
      "Iteration 129, loss = 0.22710209\n",
      "Iteration 130, loss = 0.22676668\n",
      "Iteration 131, loss = 0.22644967\n",
      "Iteration 132, loss = 0.22612754\n",
      "Iteration 133, loss = 0.22578710\n",
      "Iteration 134, loss = 0.22548855\n",
      "Iteration 135, loss = 0.22515319\n",
      "Iteration 136, loss = 0.22483967\n",
      "Iteration 137, loss = 0.22453697\n",
      "Iteration 138, loss = 0.22423310\n",
      "Iteration 139, loss = 0.22388804\n",
      "Iteration 140, loss = 0.22359246\n",
      "Iteration 141, loss = 0.22329571\n",
      "Iteration 142, loss = 0.22298071\n",
      "Iteration 143, loss = 0.22265477\n",
      "Iteration 144, loss = 0.22237851\n",
      "Iteration 145, loss = 0.22203109\n",
      "Iteration 146, loss = 0.22172346\n",
      "Iteration 147, loss = 0.22145075\n",
      "Iteration 148, loss = 0.22112634\n",
      "Iteration 149, loss = 0.22080033\n",
      "Iteration 150, loss = 0.22050696\n",
      "Iteration 151, loss = 0.22021741\n",
      "Iteration 152, loss = 0.21991828\n",
      "Iteration 153, loss = 0.21959702\n",
      "Iteration 154, loss = 0.21930589\n",
      "Iteration 155, loss = 0.21900327\n",
      "Iteration 156, loss = 0.21871954\n",
      "Iteration 157, loss = 0.21842563\n",
      "Iteration 158, loss = 0.21811440\n",
      "Iteration 159, loss = 0.21784542\n",
      "Iteration 160, loss = 0.21753856\n",
      "Iteration 161, loss = 0.21725110\n",
      "Iteration 162, loss = 0.21695231\n",
      "Iteration 163, loss = 0.21667254\n",
      "Iteration 164, loss = 0.21638829\n",
      "Iteration 165, loss = 0.21609941\n",
      "Iteration 166, loss = 0.21583989\n",
      "Iteration 167, loss = 0.21552909\n",
      "Iteration 168, loss = 0.21526229\n",
      "Iteration 169, loss = 0.21500236\n",
      "Iteration 170, loss = 0.21469297\n",
      "Iteration 171, loss = 0.21442786\n",
      "Iteration 172, loss = 0.21413779\n",
      "Iteration 173, loss = 0.21385666\n",
      "Iteration 174, loss = 0.21358024\n",
      "Iteration 175, loss = 0.21330568\n",
      "Iteration 176, loss = 0.21303561\n",
      "Iteration 177, loss = 0.21274642\n",
      "Iteration 178, loss = 0.21247379\n",
      "Iteration 179, loss = 0.21221991\n",
      "Iteration 180, loss = 0.21193819\n",
      "Iteration 181, loss = 0.21164953\n",
      "Iteration 182, loss = 0.21138911\n",
      "Iteration 183, loss = 0.21113201\n",
      "Iteration 184, loss = 0.21087094\n",
      "Iteration 185, loss = 0.21059218\n",
      "Iteration 186, loss = 0.21033509\n",
      "Iteration 187, loss = 0.21008675\n",
      "Iteration 188, loss = 0.20983242\n",
      "Iteration 189, loss = 0.20955037\n",
      "Iteration 190, loss = 0.20925822\n",
      "Iteration 191, loss = 0.20903181\n",
      "Iteration 192, loss = 0.20876089\n",
      "Iteration 193, loss = 0.20850156\n",
      "Iteration 194, loss = 0.20822451\n",
      "Iteration 195, loss = 0.20795698\n",
      "Iteration 196, loss = 0.20770586\n",
      "Iteration 197, loss = 0.20745022\n",
      "Iteration 198, loss = 0.20719749\n",
      "Iteration 199, loss = 0.20693841\n",
      "Iteration 200, loss = 0.20667739\n",
      "Iteration 1, loss = 0.73466796\n",
      "Iteration 2, loss = 0.60031035\n",
      "Iteration 3, loss = 0.52826222\n",
      "Iteration 4, loss = 0.49127231\n",
      "Iteration 5, loss = 0.46812641\n",
      "Iteration 6, loss = 0.45089711\n",
      "Iteration 7, loss = 0.43686770\n",
      "Iteration 8, loss = 0.42480478\n",
      "Iteration 9, loss = 0.41392412\n",
      "Iteration 10, loss = 0.40421073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39519409\n",
      "Iteration 12, loss = 0.38698513\n",
      "Iteration 13, loss = 0.37936569\n",
      "Iteration 14, loss = 0.37239618\n",
      "Iteration 15, loss = 0.36574379\n",
      "Iteration 16, loss = 0.35959780\n",
      "Iteration 17, loss = 0.35397943\n",
      "Iteration 18, loss = 0.34854622\n",
      "Iteration 19, loss = 0.34346724\n",
      "Iteration 20, loss = 0.33877395\n",
      "Iteration 21, loss = 0.33430448\n",
      "Iteration 22, loss = 0.33004932\n",
      "Iteration 23, loss = 0.32608461\n",
      "Iteration 24, loss = 0.32228521\n",
      "Iteration 25, loss = 0.31871524\n",
      "Iteration 26, loss = 0.31529099\n",
      "Iteration 27, loss = 0.31205902\n",
      "Iteration 28, loss = 0.30895420\n",
      "Iteration 29, loss = 0.30603629\n",
      "Iteration 30, loss = 0.30317435\n",
      "Iteration 31, loss = 0.30052864\n",
      "Iteration 32, loss = 0.29795150\n",
      "Iteration 33, loss = 0.29548601\n",
      "Iteration 34, loss = 0.29313654\n",
      "Iteration 35, loss = 0.29092817\n",
      "Iteration 36, loss = 0.28872589\n",
      "Iteration 37, loss = 0.28668316\n",
      "Iteration 38, loss = 0.28471139\n",
      "Iteration 39, loss = 0.28281469\n",
      "Iteration 40, loss = 0.28104778\n",
      "Iteration 41, loss = 0.27927080\n",
      "Iteration 42, loss = 0.27760096\n",
      "Iteration 43, loss = 0.27603357\n",
      "Iteration 44, loss = 0.27447098\n",
      "Iteration 45, loss = 0.27300693\n",
      "Iteration 46, loss = 0.27158380\n",
      "Iteration 47, loss = 0.27020957\n",
      "Iteration 48, loss = 0.26888751\n",
      "Iteration 49, loss = 0.26767182\n",
      "Iteration 50, loss = 0.26639560\n",
      "Iteration 51, loss = 0.26523676\n",
      "Iteration 52, loss = 0.26410391\n",
      "Iteration 53, loss = 0.26300820\n",
      "Iteration 54, loss = 0.26195758\n",
      "Iteration 55, loss = 0.26094010\n",
      "Iteration 56, loss = 0.25995197\n",
      "Iteration 57, loss = 0.25898670\n",
      "Iteration 58, loss = 0.25807363\n",
      "Iteration 59, loss = 0.25716800\n",
      "Iteration 60, loss = 0.25630555\n",
      "Iteration 61, loss = 0.25546071\n",
      "Iteration 62, loss = 0.25463255\n",
      "Iteration 63, loss = 0.25383224\n",
      "Iteration 64, loss = 0.25308173\n",
      "Iteration 65, loss = 0.25232113\n",
      "Iteration 66, loss = 0.25158801\n",
      "Iteration 67, loss = 0.25088018\n",
      "Iteration 68, loss = 0.25014958\n",
      "Iteration 69, loss = 0.24948497\n",
      "Iteration 70, loss = 0.24884500\n",
      "Iteration 71, loss = 0.24816366\n",
      "Iteration 72, loss = 0.24754021\n",
      "Iteration 73, loss = 0.24690514\n",
      "Iteration 74, loss = 0.24629926\n",
      "Iteration 75, loss = 0.24571492\n",
      "Iteration 76, loss = 0.24511443\n",
      "Iteration 77, loss = 0.24453929\n",
      "Iteration 78, loss = 0.24398284\n",
      "Iteration 79, loss = 0.24343108\n",
      "Iteration 80, loss = 0.24289318\n",
      "Iteration 81, loss = 0.24234605\n",
      "Iteration 82, loss = 0.24182595\n",
      "Iteration 83, loss = 0.24130443\n",
      "Iteration 84, loss = 0.24079584\n",
      "Iteration 85, loss = 0.24029399\n",
      "Iteration 86, loss = 0.23983398\n",
      "Iteration 87, loss = 0.23933380\n",
      "Iteration 88, loss = 0.23885575\n",
      "Iteration 89, loss = 0.23841735\n",
      "Iteration 90, loss = 0.23791877\n",
      "Iteration 91, loss = 0.23749815\n",
      "Iteration 92, loss = 0.23703474\n",
      "Iteration 93, loss = 0.23658265\n",
      "Iteration 94, loss = 0.23615519\n",
      "Iteration 95, loss = 0.23572018\n",
      "Iteration 96, loss = 0.23529412\n",
      "Iteration 97, loss = 0.23487186\n",
      "Iteration 98, loss = 0.23446877\n",
      "Iteration 99, loss = 0.23404616\n",
      "Iteration 100, loss = 0.23363306\n",
      "Iteration 101, loss = 0.23322741\n",
      "Iteration 102, loss = 0.23281742\n",
      "Iteration 103, loss = 0.23243873\n",
      "Iteration 104, loss = 0.23206682\n",
      "Iteration 105, loss = 0.23164519\n",
      "Iteration 106, loss = 0.23125510\n",
      "Iteration 107, loss = 0.23088852\n",
      "Iteration 108, loss = 0.23052293\n",
      "Iteration 109, loss = 0.23013288\n",
      "Iteration 110, loss = 0.22978921\n",
      "Iteration 111, loss = 0.22940561\n",
      "Iteration 112, loss = 0.22903926\n",
      "Iteration 113, loss = 0.22868046\n",
      "Iteration 114, loss = 0.22830775\n",
      "Iteration 115, loss = 0.22797857\n",
      "Iteration 116, loss = 0.22764334\n",
      "Iteration 117, loss = 0.22725804\n",
      "Iteration 118, loss = 0.22692837\n",
      "Iteration 119, loss = 0.22659959\n",
      "Iteration 120, loss = 0.22623563\n",
      "Iteration 121, loss = 0.22591295\n",
      "Iteration 122, loss = 0.22557212\n",
      "Iteration 123, loss = 0.22523762\n",
      "Iteration 124, loss = 0.22490650\n",
      "Iteration 125, loss = 0.22456068\n",
      "Iteration 126, loss = 0.22426002\n",
      "Iteration 127, loss = 0.22392423\n",
      "Iteration 128, loss = 0.22359627\n",
      "Iteration 129, loss = 0.22327169\n",
      "Iteration 130, loss = 0.22295901\n",
      "Iteration 131, loss = 0.22264639\n",
      "Iteration 132, loss = 0.22235842\n",
      "Iteration 133, loss = 0.22200632\n",
      "Iteration 134, loss = 0.22172227\n",
      "Iteration 135, loss = 0.22139036\n",
      "Iteration 136, loss = 0.22109440\n",
      "Iteration 137, loss = 0.22077047\n",
      "Iteration 138, loss = 0.22049804\n",
      "Iteration 139, loss = 0.22016575\n",
      "Iteration 140, loss = 0.21986844\n",
      "Iteration 141, loss = 0.21959819\n",
      "Iteration 142, loss = 0.21928169\n",
      "Iteration 143, loss = 0.21898736\n",
      "Iteration 144, loss = 0.21869746\n",
      "Iteration 145, loss = 0.21839213\n",
      "Iteration 146, loss = 0.21808515\n",
      "Iteration 147, loss = 0.21777955\n",
      "Iteration 148, loss = 0.21749945\n",
      "Iteration 149, loss = 0.21719169\n",
      "Iteration 150, loss = 0.21690473\n",
      "Iteration 151, loss = 0.21662984\n",
      "Iteration 152, loss = 0.21633872\n",
      "Iteration 153, loss = 0.21602067\n",
      "Iteration 154, loss = 0.21574586\n",
      "Iteration 155, loss = 0.21546842\n",
      "Iteration 156, loss = 0.21518309\n",
      "Iteration 157, loss = 0.21491590\n",
      "Iteration 158, loss = 0.21462396\n",
      "Iteration 159, loss = 0.21436196\n",
      "Iteration 160, loss = 0.21406524\n",
      "Iteration 161, loss = 0.21379900\n",
      "Iteration 162, loss = 0.21351573\n",
      "Iteration 163, loss = 0.21325518\n",
      "Iteration 164, loss = 0.21298679\n",
      "Iteration 165, loss = 0.21270814\n",
      "Iteration 166, loss = 0.21246291\n",
      "Iteration 167, loss = 0.21217582\n",
      "Iteration 168, loss = 0.21190404\n",
      "Iteration 169, loss = 0.21165016\n",
      "Iteration 170, loss = 0.21137641\n",
      "Iteration 171, loss = 0.21110610\n",
      "Iteration 172, loss = 0.21086216\n",
      "Iteration 173, loss = 0.21058261\n",
      "Iteration 174, loss = 0.21030888\n",
      "Iteration 175, loss = 0.21007233\n",
      "Iteration 176, loss = 0.20979416\n",
      "Iteration 177, loss = 0.20950955\n",
      "Iteration 178, loss = 0.20926547\n",
      "Iteration 179, loss = 0.20900969\n",
      "Iteration 180, loss = 0.20874737\n",
      "Iteration 181, loss = 0.20848121\n",
      "Iteration 182, loss = 0.20822089\n",
      "Iteration 183, loss = 0.20797951\n",
      "Iteration 184, loss = 0.20773290\n",
      "Iteration 185, loss = 0.20747871\n",
      "Iteration 186, loss = 0.20723448\n",
      "Iteration 187, loss = 0.20699262\n",
      "Iteration 188, loss = 0.20674368\n",
      "Iteration 189, loss = 0.20651438\n",
      "Iteration 190, loss = 0.20622007\n",
      "Iteration 191, loss = 0.20600720\n",
      "Iteration 192, loss = 0.20574940\n",
      "Iteration 193, loss = 0.20550540\n",
      "Iteration 194, loss = 0.20526467\n",
      "Iteration 195, loss = 0.20501190\n",
      "Iteration 196, loss = 0.20476971\n",
      "Iteration 197, loss = 0.20453755\n",
      "Iteration 198, loss = 0.20428089\n",
      "Iteration 199, loss = 0.20406620\n",
      "Iteration 200, loss = 0.20382559\n",
      "Iteration 1, loss = 0.61178200\n",
      "Iteration 2, loss = 0.40865526\n",
      "Iteration 3, loss = 0.33969611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.30564416\n",
      "Iteration 5, loss = 0.28502539\n",
      "Iteration 6, loss = 0.27111660\n",
      "Iteration 7, loss = 0.25981296\n",
      "Iteration 8, loss = 0.25138070\n",
      "Iteration 9, loss = 0.24372301\n",
      "Iteration 10, loss = 0.23759264\n",
      "Iteration 11, loss = 0.23106439\n",
      "Iteration 12, loss = 0.22599849\n",
      "Iteration 13, loss = 0.22056539\n",
      "Iteration 14, loss = 0.21621559\n",
      "Iteration 15, loss = 0.21151929\n",
      "Iteration 16, loss = 0.20748680\n",
      "Iteration 17, loss = 0.20373690\n",
      "Iteration 18, loss = 0.19922601\n",
      "Iteration 19, loss = 0.19537058\n",
      "Iteration 20, loss = 0.19156793\n",
      "Iteration 21, loss = 0.18796337\n",
      "Iteration 22, loss = 0.18445988\n",
      "Iteration 23, loss = 0.18137578\n",
      "Iteration 24, loss = 0.17805770\n",
      "Iteration 25, loss = 0.17501600\n",
      "Iteration 26, loss = 0.17191046\n",
      "Iteration 27, loss = 0.16862790\n",
      "Iteration 28, loss = 0.16583810\n",
      "Iteration 29, loss = 0.16291970\n",
      "Iteration 30, loss = 0.16098964\n",
      "Iteration 31, loss = 0.15775033\n",
      "Iteration 32, loss = 0.15519684\n",
      "Iteration 33, loss = 0.15328559\n",
      "Iteration 34, loss = 0.15067169\n",
      "Iteration 35, loss = 0.14778130\n",
      "Iteration 36, loss = 0.14575235\n",
      "Iteration 37, loss = 0.14368061\n",
      "Iteration 38, loss = 0.14243026\n",
      "Iteration 39, loss = 0.13996932\n",
      "Iteration 40, loss = 0.13806745\n",
      "Iteration 41, loss = 0.13630989\n",
      "Iteration 42, loss = 0.13463977\n",
      "Iteration 43, loss = 0.13269713\n",
      "Iteration 44, loss = 0.13116192\n",
      "Iteration 45, loss = 0.12969729\n",
      "Iteration 46, loss = 0.12828011\n",
      "Iteration 47, loss = 0.12657102\n",
      "Iteration 48, loss = 0.12552413\n",
      "Iteration 49, loss = 0.12390973\n",
      "Iteration 50, loss = 0.12282534\n",
      "Iteration 51, loss = 0.12107470\n",
      "Iteration 52, loss = 0.12009367\n",
      "Iteration 53, loss = 0.11870750\n",
      "Iteration 54, loss = 0.11794921\n",
      "Iteration 55, loss = 0.11632019\n",
      "Iteration 56, loss = 0.11532708\n",
      "Iteration 57, loss = 0.11400294\n",
      "Iteration 58, loss = 0.11302230\n",
      "Iteration 59, loss = 0.11187047\n",
      "Iteration 60, loss = 0.11086870\n",
      "Iteration 61, loss = 0.11017558\n",
      "Iteration 62, loss = 0.10887762\n",
      "Iteration 63, loss = 0.10824635\n",
      "Iteration 64, loss = 0.10727604\n",
      "Iteration 65, loss = 0.10575394\n",
      "Iteration 66, loss = 0.10521215\n",
      "Iteration 67, loss = 0.10407692\n",
      "Iteration 68, loss = 0.10336761\n",
      "Iteration 69, loss = 0.10243476\n",
      "Iteration 70, loss = 0.10149383\n",
      "Iteration 71, loss = 0.10138517\n",
      "Iteration 72, loss = 0.10011927\n",
      "Iteration 73, loss = 0.09886374\n",
      "Iteration 74, loss = 0.09810332\n",
      "Iteration 75, loss = 0.09739136\n",
      "Iteration 76, loss = 0.09705084\n",
      "Iteration 77, loss = 0.09568711\n",
      "Iteration 78, loss = 0.09511683\n",
      "Iteration 79, loss = 0.09446564\n",
      "Iteration 80, loss = 0.09355579\n",
      "Iteration 81, loss = 0.09319645\n",
      "Iteration 82, loss = 0.09201266\n",
      "Iteration 83, loss = 0.09090748\n",
      "Iteration 84, loss = 0.09078119\n",
      "Iteration 85, loss = 0.09025928\n",
      "Iteration 86, loss = 0.08914280\n",
      "Iteration 87, loss = 0.08846112\n",
      "Iteration 88, loss = 0.08811550\n",
      "Iteration 89, loss = 0.08777682\n",
      "Iteration 90, loss = 0.08626436\n",
      "Iteration 91, loss = 0.08581448\n",
      "Iteration 92, loss = 0.08478331\n",
      "Iteration 93, loss = 0.08449751\n",
      "Iteration 94, loss = 0.08398635\n",
      "Iteration 95, loss = 0.08419764\n",
      "Iteration 96, loss = 0.08233489\n",
      "Iteration 97, loss = 0.08165606\n",
      "Iteration 98, loss = 0.08121896\n",
      "Iteration 99, loss = 0.08061396\n",
      "Iteration 100, loss = 0.07993569\n",
      "Iteration 101, loss = 0.07912533\n",
      "Iteration 102, loss = 0.07899240\n",
      "Iteration 103, loss = 0.07810856\n",
      "Iteration 104, loss = 0.07748439\n",
      "Iteration 105, loss = 0.07729359\n",
      "Iteration 106, loss = 0.07712430\n",
      "Iteration 107, loss = 0.07563672\n",
      "Iteration 108, loss = 0.07499895\n",
      "Iteration 109, loss = 0.07507868\n",
      "Iteration 110, loss = 0.07447945\n",
      "Iteration 111, loss = 0.07375823\n",
      "Iteration 112, loss = 0.07342622\n",
      "Iteration 113, loss = 0.07273986\n",
      "Iteration 114, loss = 0.07239135\n",
      "Iteration 115, loss = 0.07255489\n",
      "Iteration 116, loss = 0.07111610\n",
      "Iteration 117, loss = 0.07022672\n",
      "Iteration 118, loss = 0.06942044\n",
      "Iteration 119, loss = 0.06924061\n",
      "Iteration 120, loss = 0.06866123\n",
      "Iteration 121, loss = 0.06844194\n",
      "Iteration 122, loss = 0.06802413\n",
      "Iteration 123, loss = 0.06692940\n",
      "Iteration 124, loss = 0.06688662\n",
      "Iteration 125, loss = 0.06582312\n",
      "Iteration 126, loss = 0.06569655\n",
      "Iteration 127, loss = 0.06521644\n",
      "Iteration 128, loss = 0.06452882\n",
      "Iteration 129, loss = 0.06464444\n",
      "Iteration 130, loss = 0.06351282\n",
      "Iteration 131, loss = 0.06326880\n",
      "Iteration 132, loss = 0.06265072\n",
      "Iteration 133, loss = 0.06176966\n",
      "Iteration 134, loss = 0.06174364\n",
      "Iteration 135, loss = 0.06108039\n",
      "Iteration 136, loss = 0.06088749\n",
      "Iteration 137, loss = 0.06043701\n",
      "Iteration 138, loss = 0.05967798\n",
      "Iteration 139, loss = 0.05918515\n",
      "Iteration 140, loss = 0.05882868\n",
      "Iteration 141, loss = 0.05855221\n",
      "Iteration 142, loss = 0.05796824\n",
      "Iteration 143, loss = 0.05742025\n",
      "Iteration 144, loss = 0.05725206\n",
      "Iteration 145, loss = 0.05694955\n",
      "Iteration 146, loss = 0.05665379\n",
      "Iteration 147, loss = 0.05531129\n",
      "Iteration 148, loss = 0.05522310\n",
      "Iteration 149, loss = 0.05480291\n",
      "Iteration 150, loss = 0.05385208\n",
      "Iteration 151, loss = 0.05403819\n",
      "Iteration 152, loss = 0.05413695\n",
      "Iteration 153, loss = 0.05292894\n",
      "Iteration 154, loss = 0.05306515\n",
      "Iteration 155, loss = 0.05237053\n",
      "Iteration 156, loss = 0.05161722\n",
      "Iteration 157, loss = 0.05153891\n",
      "Iteration 158, loss = 0.05074966\n",
      "Iteration 159, loss = 0.05074117\n",
      "Iteration 160, loss = 0.05058981\n",
      "Iteration 161, loss = 0.05034434\n",
      "Iteration 162, loss = 0.04988983\n",
      "Iteration 163, loss = 0.04892205\n",
      "Iteration 164, loss = 0.04856762\n",
      "Iteration 165, loss = 0.04862319\n",
      "Iteration 166, loss = 0.04779570\n",
      "Iteration 167, loss = 0.04773939\n",
      "Iteration 168, loss = 0.04737443\n",
      "Iteration 169, loss = 0.04714145\n",
      "Iteration 170, loss = 0.04656912\n",
      "Iteration 171, loss = 0.04649081\n",
      "Iteration 172, loss = 0.04582530\n",
      "Iteration 173, loss = 0.04535580\n",
      "Iteration 174, loss = 0.04492716\n",
      "Iteration 175, loss = 0.04483674\n",
      "Iteration 176, loss = 0.04438325\n",
      "Iteration 177, loss = 0.04396035\n",
      "Iteration 178, loss = 0.04349794\n",
      "Iteration 179, loss = 0.04289762\n",
      "Iteration 180, loss = 0.04275633\n",
      "Iteration 181, loss = 0.04216203\n",
      "Iteration 182, loss = 0.04237716\n",
      "Iteration 183, loss = 0.04249451\n",
      "Iteration 184, loss = 0.04165654\n",
      "Iteration 185, loss = 0.04120665\n",
      "Iteration 186, loss = 0.04051007\n",
      "Iteration 187, loss = 0.04052644\n",
      "Iteration 188, loss = 0.04048275\n",
      "Iteration 189, loss = 0.03965810\n",
      "Iteration 190, loss = 0.03946775\n",
      "Iteration 191, loss = 0.03931165\n",
      "Iteration 192, loss = 0.03891929\n",
      "Iteration 193, loss = 0.03864001\n",
      "Iteration 194, loss = 0.03824437\n",
      "Iteration 195, loss = 0.03805576\n",
      "Iteration 196, loss = 0.03802029\n",
      "Iteration 197, loss = 0.03797715\n",
      "Iteration 198, loss = 0.03705639\n",
      "Iteration 199, loss = 0.03714901\n",
      "Iteration 200, loss = 0.03667557\n",
      "Iteration 1, loss = 0.61264317\n",
      "Iteration 2, loss = 0.40833922\n",
      "Iteration 3, loss = 0.33834903\n",
      "Iteration 4, loss = 0.30378487\n",
      "Iteration 5, loss = 0.28289018\n",
      "Iteration 6, loss = 0.26876067\n",
      "Iteration 7, loss = 0.25746674\n",
      "Iteration 8, loss = 0.24892800\n",
      "Iteration 9, loss = 0.24128364\n",
      "Iteration 10, loss = 0.23492774\n",
      "Iteration 11, loss = 0.22853789\n",
      "Iteration 12, loss = 0.22371652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.21814242\n",
      "Iteration 14, loss = 0.21319592\n",
      "Iteration 15, loss = 0.20866385\n",
      "Iteration 16, loss = 0.20435662\n",
      "Iteration 17, loss = 0.20032369\n",
      "Iteration 18, loss = 0.19636800\n",
      "Iteration 19, loss = 0.19269990\n",
      "Iteration 20, loss = 0.18892335\n",
      "Iteration 21, loss = 0.18558164\n",
      "Iteration 22, loss = 0.18202488\n",
      "Iteration 23, loss = 0.17895786\n",
      "Iteration 24, loss = 0.17606286\n",
      "Iteration 25, loss = 0.17303217\n",
      "Iteration 26, loss = 0.16992772\n",
      "Iteration 27, loss = 0.16716374\n",
      "Iteration 28, loss = 0.16452085\n",
      "Iteration 29, loss = 0.16201464\n",
      "Iteration 30, loss = 0.15964338\n",
      "Iteration 31, loss = 0.15674766\n",
      "Iteration 32, loss = 0.15445127\n",
      "Iteration 33, loss = 0.15208565\n",
      "Iteration 34, loss = 0.15032243\n",
      "Iteration 35, loss = 0.14748638\n",
      "Iteration 36, loss = 0.14568255\n",
      "Iteration 37, loss = 0.14359594\n",
      "Iteration 38, loss = 0.14178923\n",
      "Iteration 39, loss = 0.13983468\n",
      "Iteration 40, loss = 0.13776153\n",
      "Iteration 41, loss = 0.13611923\n",
      "Iteration 42, loss = 0.13450804\n",
      "Iteration 43, loss = 0.13225557\n",
      "Iteration 44, loss = 0.13093320\n",
      "Iteration 45, loss = 0.12938098\n",
      "Iteration 46, loss = 0.12759641\n",
      "Iteration 47, loss = 0.12608139\n",
      "Iteration 48, loss = 0.12472660\n",
      "Iteration 49, loss = 0.12327183\n",
      "Iteration 50, loss = 0.12192187\n",
      "Iteration 51, loss = 0.12031877\n",
      "Iteration 52, loss = 0.11891030\n",
      "Iteration 53, loss = 0.11741274\n",
      "Iteration 54, loss = 0.11655232\n",
      "Iteration 55, loss = 0.11487351\n",
      "Iteration 56, loss = 0.11398249\n",
      "Iteration 57, loss = 0.11254936\n",
      "Iteration 58, loss = 0.11152226\n",
      "Iteration 59, loss = 0.11040542\n",
      "Iteration 60, loss = 0.10900801\n",
      "Iteration 61, loss = 0.10805230\n",
      "Iteration 62, loss = 0.10671673\n",
      "Iteration 63, loss = 0.10647315\n",
      "Iteration 64, loss = 0.10495215\n",
      "Iteration 65, loss = 0.10382984\n",
      "Iteration 66, loss = 0.10317777\n",
      "Iteration 67, loss = 0.10167008\n",
      "Iteration 68, loss = 0.10096053\n",
      "Iteration 69, loss = 0.10016603\n",
      "Iteration 70, loss = 0.09857125\n",
      "Iteration 71, loss = 0.09863003\n",
      "Iteration 72, loss = 0.09773251\n",
      "Iteration 73, loss = 0.09607211\n",
      "Iteration 74, loss = 0.09513330\n",
      "Iteration 75, loss = 0.09450133\n",
      "Iteration 76, loss = 0.09321119\n",
      "Iteration 77, loss = 0.09250441\n",
      "Iteration 78, loss = 0.09201875\n",
      "Iteration 79, loss = 0.09136779\n",
      "Iteration 80, loss = 0.09021122\n",
      "Iteration 81, loss = 0.08942798\n",
      "Iteration 82, loss = 0.08864224\n",
      "Iteration 83, loss = 0.08744675\n",
      "Iteration 84, loss = 0.08696232\n",
      "Iteration 85, loss = 0.08633966\n",
      "Iteration 86, loss = 0.08524814\n",
      "Iteration 87, loss = 0.08495856\n",
      "Iteration 88, loss = 0.08413056\n",
      "Iteration 89, loss = 0.08341199\n",
      "Iteration 90, loss = 0.08218890\n",
      "Iteration 91, loss = 0.08161621\n",
      "Iteration 92, loss = 0.08043940\n",
      "Iteration 93, loss = 0.08007804\n",
      "Iteration 94, loss = 0.07946631\n",
      "Iteration 95, loss = 0.07960786\n",
      "Iteration 96, loss = 0.07801727\n",
      "Iteration 97, loss = 0.07710966\n",
      "Iteration 98, loss = 0.07658578\n",
      "Iteration 99, loss = 0.07622739\n",
      "Iteration 100, loss = 0.07575949\n",
      "Iteration 101, loss = 0.07462274\n",
      "Iteration 102, loss = 0.07489969\n",
      "Iteration 103, loss = 0.07385981\n",
      "Iteration 104, loss = 0.07270560\n",
      "Iteration 105, loss = 0.07228918\n",
      "Iteration 106, loss = 0.07202343\n",
      "Iteration 107, loss = 0.07122886\n",
      "Iteration 108, loss = 0.07054562\n",
      "Iteration 109, loss = 0.07006268\n",
      "Iteration 110, loss = 0.06964285\n",
      "Iteration 111, loss = 0.06864485\n",
      "Iteration 112, loss = 0.06902564\n",
      "Iteration 113, loss = 0.06748758\n",
      "Iteration 114, loss = 0.06688930\n",
      "Iteration 115, loss = 0.06705448\n",
      "Iteration 116, loss = 0.06596391\n",
      "Iteration 117, loss = 0.06500087\n",
      "Iteration 118, loss = 0.06447290\n",
      "Iteration 119, loss = 0.06421666\n",
      "Iteration 120, loss = 0.06404548\n",
      "Iteration 121, loss = 0.06367043\n",
      "Iteration 122, loss = 0.06269307\n",
      "Iteration 123, loss = 0.06195342\n",
      "Iteration 124, loss = 0.06180956\n",
      "Iteration 125, loss = 0.06081742\n",
      "Iteration 126, loss = 0.06097036\n",
      "Iteration 127, loss = 0.05994155\n",
      "Iteration 128, loss = 0.05942769\n",
      "Iteration 129, loss = 0.05925416\n",
      "Iteration 130, loss = 0.05860797\n",
      "Iteration 131, loss = 0.05792337\n",
      "Iteration 132, loss = 0.05740739\n",
      "Iteration 133, loss = 0.05684168\n",
      "Iteration 134, loss = 0.05643028\n",
      "Iteration 135, loss = 0.05610259\n",
      "Iteration 136, loss = 0.05545926\n",
      "Iteration 137, loss = 0.05519648\n",
      "Iteration 138, loss = 0.05442069\n",
      "Iteration 139, loss = 0.05442714\n",
      "Iteration 140, loss = 0.05424678\n",
      "Iteration 141, loss = 0.05346997\n",
      "Iteration 142, loss = 0.05294121\n",
      "Iteration 143, loss = 0.05209012\n",
      "Iteration 144, loss = 0.05243004\n",
      "Iteration 145, loss = 0.05185101\n",
      "Iteration 146, loss = 0.05148452\n",
      "Iteration 147, loss = 0.05046319\n",
      "Iteration 148, loss = 0.05063533\n",
      "Iteration 149, loss = 0.04978075\n",
      "Iteration 150, loss = 0.04928835\n",
      "Iteration 151, loss = 0.04918945\n",
      "Iteration 152, loss = 0.04916061\n",
      "Iteration 153, loss = 0.04809840\n",
      "Iteration 154, loss = 0.04793280\n",
      "Iteration 155, loss = 0.04743618\n",
      "Iteration 156, loss = 0.04708061\n",
      "Iteration 157, loss = 0.04685296\n",
      "Iteration 158, loss = 0.04617519\n",
      "Iteration 159, loss = 0.04595409\n",
      "Iteration 160, loss = 0.04562445\n",
      "Iteration 161, loss = 0.04529640\n",
      "Iteration 162, loss = 0.04464271\n",
      "Iteration 163, loss = 0.04452484\n",
      "Iteration 164, loss = 0.04377003\n",
      "Iteration 165, loss = 0.04361059\n",
      "Iteration 166, loss = 0.04289770\n",
      "Iteration 167, loss = 0.04272576\n",
      "Iteration 168, loss = 0.04254758\n",
      "Iteration 169, loss = 0.04233104\n",
      "Iteration 170, loss = 0.04180097\n",
      "Iteration 171, loss = 0.04161654\n",
      "Iteration 172, loss = 0.04121054\n",
      "Iteration 173, loss = 0.04081498\n",
      "Iteration 174, loss = 0.04057085\n",
      "Iteration 175, loss = 0.04015835\n",
      "Iteration 176, loss = 0.03981025\n",
      "Iteration 177, loss = 0.03914189\n",
      "Iteration 178, loss = 0.03897262\n",
      "Iteration 179, loss = 0.03854726\n",
      "Iteration 180, loss = 0.03847821\n",
      "Iteration 181, loss = 0.03784923\n",
      "Iteration 182, loss = 0.03764722\n",
      "Iteration 183, loss = 0.03775552\n",
      "Iteration 184, loss = 0.03690138\n",
      "Iteration 185, loss = 0.03701714\n",
      "Iteration 186, loss = 0.03594848\n",
      "Iteration 187, loss = 0.03591565\n",
      "Iteration 188, loss = 0.03620897\n",
      "Iteration 189, loss = 0.03521290\n",
      "Iteration 190, loss = 0.03526885\n",
      "Iteration 191, loss = 0.03496823\n",
      "Iteration 192, loss = 0.03487542\n",
      "Iteration 193, loss = 0.03436144\n",
      "Iteration 194, loss = 0.03391609\n",
      "Iteration 195, loss = 0.03357753\n",
      "Iteration 196, loss = 0.03351516\n",
      "Iteration 197, loss = 0.03346195\n",
      "Iteration 198, loss = 0.03271383\n",
      "Iteration 199, loss = 0.03282878\n",
      "Iteration 200, loss = 0.03231911\n",
      "Iteration 1, loss = 0.61603689\n",
      "Iteration 2, loss = 0.41181678\n",
      "Iteration 3, loss = 0.34208453\n",
      "Iteration 4, loss = 0.30758688\n",
      "Iteration 5, loss = 0.28698856\n",
      "Iteration 6, loss = 0.27261017\n",
      "Iteration 7, loss = 0.26163808\n",
      "Iteration 8, loss = 0.25264971\n",
      "Iteration 9, loss = 0.24459064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.23796412\n",
      "Iteration 11, loss = 0.23136471\n",
      "Iteration 12, loss = 0.22620232\n",
      "Iteration 13, loss = 0.22020925\n",
      "Iteration 14, loss = 0.21491433\n",
      "Iteration 15, loss = 0.21037787\n",
      "Iteration 16, loss = 0.20554184\n",
      "Iteration 17, loss = 0.20130142\n",
      "Iteration 18, loss = 0.19711975\n",
      "Iteration 19, loss = 0.19318314\n",
      "Iteration 20, loss = 0.18893134\n",
      "Iteration 21, loss = 0.18545958\n",
      "Iteration 22, loss = 0.18190028\n",
      "Iteration 23, loss = 0.17843825\n",
      "Iteration 24, loss = 0.17539544\n",
      "Iteration 25, loss = 0.17222526\n",
      "Iteration 26, loss = 0.16885198\n",
      "Iteration 27, loss = 0.16583450\n",
      "Iteration 28, loss = 0.16292445\n",
      "Iteration 29, loss = 0.16040767\n",
      "Iteration 30, loss = 0.15768662\n",
      "Iteration 31, loss = 0.15467919\n",
      "Iteration 32, loss = 0.15208621\n",
      "Iteration 33, loss = 0.14962364\n",
      "Iteration 34, loss = 0.14743137\n",
      "Iteration 35, loss = 0.14510649\n",
      "Iteration 36, loss = 0.14278471\n",
      "Iteration 37, loss = 0.14087481\n",
      "Iteration 38, loss = 0.13900024\n",
      "Iteration 39, loss = 0.13686133\n",
      "Iteration 40, loss = 0.13491609\n",
      "Iteration 41, loss = 0.13335247\n",
      "Iteration 42, loss = 0.13162956\n",
      "Iteration 43, loss = 0.12943556\n",
      "Iteration 44, loss = 0.12813539\n",
      "Iteration 45, loss = 0.12681952\n",
      "Iteration 46, loss = 0.12536906\n",
      "Iteration 47, loss = 0.12348861\n",
      "Iteration 48, loss = 0.12202962\n",
      "Iteration 49, loss = 0.12056164\n",
      "Iteration 50, loss = 0.11922356\n",
      "Iteration 51, loss = 0.11837826\n",
      "Iteration 52, loss = 0.11700083\n",
      "Iteration 53, loss = 0.11586036\n",
      "Iteration 54, loss = 0.11479083\n",
      "Iteration 55, loss = 0.11313735\n",
      "Iteration 56, loss = 0.11219761\n",
      "Iteration 57, loss = 0.11108830\n",
      "Iteration 58, loss = 0.11010333\n",
      "Iteration 59, loss = 0.10950964\n",
      "Iteration 60, loss = 0.10827772\n",
      "Iteration 61, loss = 0.10710212\n",
      "Iteration 62, loss = 0.10595498\n",
      "Iteration 63, loss = 0.10524157\n",
      "Iteration 64, loss = 0.10396467\n",
      "Iteration 65, loss = 0.10293678\n",
      "Iteration 66, loss = 0.10262718\n",
      "Iteration 67, loss = 0.10135153\n",
      "Iteration 68, loss = 0.10089861\n",
      "Iteration 69, loss = 0.10019180\n",
      "Iteration 70, loss = 0.09906240\n",
      "Iteration 71, loss = 0.09866343\n",
      "Iteration 72, loss = 0.09764906\n",
      "Iteration 73, loss = 0.09630878\n",
      "Iteration 74, loss = 0.09543785\n",
      "Iteration 75, loss = 0.09504421\n",
      "Iteration 76, loss = 0.09450330\n",
      "Iteration 77, loss = 0.09347152\n",
      "Iteration 78, loss = 0.09286054\n",
      "Iteration 79, loss = 0.09184298\n",
      "Iteration 80, loss = 0.09109732\n",
      "Iteration 81, loss = 0.08973139\n",
      "Iteration 82, loss = 0.08978727\n",
      "Iteration 83, loss = 0.08847652\n",
      "Iteration 84, loss = 0.08797864\n",
      "Iteration 85, loss = 0.08741321\n",
      "Iteration 86, loss = 0.08657085\n",
      "Iteration 87, loss = 0.08579070\n",
      "Iteration 88, loss = 0.08555773\n",
      "Iteration 89, loss = 0.08439324\n",
      "Iteration 90, loss = 0.08402092\n",
      "Iteration 91, loss = 0.08326185\n",
      "Iteration 92, loss = 0.08235110\n",
      "Iteration 93, loss = 0.08193059\n",
      "Iteration 94, loss = 0.08148059\n",
      "Iteration 95, loss = 0.08072853\n",
      "Iteration 96, loss = 0.07964739\n",
      "Iteration 97, loss = 0.07903692\n",
      "Iteration 98, loss = 0.07844464\n",
      "Iteration 99, loss = 0.07790977\n",
      "Iteration 100, loss = 0.07747130\n",
      "Iteration 101, loss = 0.07649280\n",
      "Iteration 102, loss = 0.07671244\n",
      "Iteration 103, loss = 0.07554168\n",
      "Iteration 104, loss = 0.07485434\n",
      "Iteration 105, loss = 0.07448727\n",
      "Iteration 106, loss = 0.07381903\n",
      "Iteration 107, loss = 0.07278278\n",
      "Iteration 108, loss = 0.07240903\n",
      "Iteration 109, loss = 0.07240167\n",
      "Iteration 110, loss = 0.07150210\n",
      "Iteration 111, loss = 0.07085482\n",
      "Iteration 112, loss = 0.07029673\n",
      "Iteration 113, loss = 0.06974130\n",
      "Iteration 114, loss = 0.06888250\n",
      "Iteration 115, loss = 0.06865611\n",
      "Iteration 116, loss = 0.06815577\n",
      "Iteration 117, loss = 0.06737297\n",
      "Iteration 118, loss = 0.06675086\n",
      "Iteration 119, loss = 0.06646085\n",
      "Iteration 120, loss = 0.06601282\n",
      "Iteration 121, loss = 0.06548582\n",
      "Iteration 122, loss = 0.06469271\n",
      "Iteration 123, loss = 0.06419482\n",
      "Iteration 124, loss = 0.06495263\n",
      "Iteration 125, loss = 0.06271672\n",
      "Iteration 126, loss = 0.06287216\n",
      "Iteration 127, loss = 0.06170348\n",
      "Iteration 128, loss = 0.06157954\n",
      "Iteration 129, loss = 0.06151562\n",
      "Iteration 130, loss = 0.06062940\n",
      "Iteration 131, loss = 0.06040816\n",
      "Iteration 132, loss = 0.05965442\n",
      "Iteration 133, loss = 0.05875545\n",
      "Iteration 134, loss = 0.05889163\n",
      "Iteration 135, loss = 0.05825985\n",
      "Iteration 136, loss = 0.05785803\n",
      "Iteration 137, loss = 0.05743671\n",
      "Iteration 138, loss = 0.05687137\n",
      "Iteration 139, loss = 0.05614562\n",
      "Iteration 140, loss = 0.05619436\n",
      "Iteration 141, loss = 0.05543760\n",
      "Iteration 142, loss = 0.05471046\n",
      "Iteration 143, loss = 0.05415792\n",
      "Iteration 144, loss = 0.05437291\n",
      "Iteration 145, loss = 0.05344728\n",
      "Iteration 146, loss = 0.05375953\n",
      "Iteration 147, loss = 0.05275767\n",
      "Iteration 148, loss = 0.05251213\n",
      "Iteration 149, loss = 0.05183222\n",
      "Iteration 150, loss = 0.05167573\n",
      "Iteration 151, loss = 0.05103255\n",
      "Iteration 152, loss = 0.05102812\n",
      "Iteration 153, loss = 0.05018187\n",
      "Iteration 154, loss = 0.04962025\n",
      "Iteration 155, loss = 0.04933967\n",
      "Iteration 156, loss = 0.04892433\n",
      "Iteration 157, loss = 0.04844702\n",
      "Iteration 158, loss = 0.04824916\n",
      "Iteration 159, loss = 0.04754267\n",
      "Iteration 160, loss = 0.04757924\n",
      "Iteration 161, loss = 0.04720394\n",
      "Iteration 162, loss = 0.04671199\n",
      "Iteration 163, loss = 0.04620501\n",
      "Iteration 164, loss = 0.04553593\n",
      "Iteration 165, loss = 0.04535843\n",
      "Iteration 166, loss = 0.04517744\n",
      "Iteration 167, loss = 0.04458413\n",
      "Iteration 168, loss = 0.04450867\n",
      "Iteration 169, loss = 0.04385076\n",
      "Iteration 170, loss = 0.04352655\n",
      "Iteration 171, loss = 0.04313371\n",
      "Iteration 172, loss = 0.04333724\n",
      "Iteration 173, loss = 0.04245310\n",
      "Iteration 174, loss = 0.04237551\n",
      "Iteration 175, loss = 0.04244031\n",
      "Iteration 176, loss = 0.04134669\n",
      "Iteration 177, loss = 0.04137846\n",
      "Iteration 178, loss = 0.04097140\n",
      "Iteration 179, loss = 0.04048888\n",
      "Iteration 180, loss = 0.04023776\n",
      "Iteration 181, loss = 0.03954457\n",
      "Iteration 182, loss = 0.03974031\n",
      "Iteration 183, loss = 0.03864950\n",
      "Iteration 184, loss = 0.03850581\n",
      "Iteration 185, loss = 0.03831219\n",
      "Iteration 186, loss = 0.03763812\n",
      "Iteration 187, loss = 0.03791873\n",
      "Iteration 188, loss = 0.03797055\n",
      "Iteration 189, loss = 0.03674475\n",
      "Iteration 190, loss = 0.03684319\n",
      "Iteration 191, loss = 0.03674523\n",
      "Iteration 192, loss = 0.03644268\n",
      "Iteration 193, loss = 0.03599993\n",
      "Iteration 194, loss = 0.03539878\n",
      "Iteration 195, loss = 0.03519259\n",
      "Iteration 196, loss = 0.03498417\n",
      "Iteration 197, loss = 0.03460491\n",
      "Iteration 198, loss = 0.03435171\n",
      "Iteration 199, loss = 0.03412471\n",
      "Iteration 200, loss = 0.03384361\n",
      "Iteration 1, loss = 0.61096914\n",
      "Iteration 2, loss = 0.40827804\n",
      "Iteration 3, loss = 0.33941493\n",
      "Iteration 4, loss = 0.30506645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.28472963\n",
      "Iteration 6, loss = 0.27003333\n",
      "Iteration 7, loss = 0.25917016\n",
      "Iteration 8, loss = 0.25022302\n",
      "Iteration 9, loss = 0.24244499\n",
      "Iteration 10, loss = 0.23568113\n",
      "Iteration 11, loss = 0.22983068\n",
      "Iteration 12, loss = 0.22440417\n",
      "Iteration 13, loss = 0.21889089\n",
      "Iteration 14, loss = 0.21417813\n",
      "Iteration 15, loss = 0.20920076\n",
      "Iteration 16, loss = 0.20464580\n",
      "Iteration 17, loss = 0.20068200\n",
      "Iteration 18, loss = 0.19691113\n",
      "Iteration 19, loss = 0.19293751\n",
      "Iteration 20, loss = 0.18887067\n",
      "Iteration 21, loss = 0.18541545\n",
      "Iteration 22, loss = 0.18182989\n",
      "Iteration 23, loss = 0.17865301\n",
      "Iteration 24, loss = 0.17550694\n",
      "Iteration 25, loss = 0.17224123\n",
      "Iteration 26, loss = 0.16919741\n",
      "Iteration 27, loss = 0.16659298\n",
      "Iteration 28, loss = 0.16363294\n",
      "Iteration 29, loss = 0.16104212\n",
      "Iteration 30, loss = 0.15840704\n",
      "Iteration 31, loss = 0.15568603\n",
      "Iteration 32, loss = 0.15321116\n",
      "Iteration 33, loss = 0.15104235\n",
      "Iteration 34, loss = 0.14882924\n",
      "Iteration 35, loss = 0.14657261\n",
      "Iteration 36, loss = 0.14439652\n",
      "Iteration 37, loss = 0.14242032\n",
      "Iteration 38, loss = 0.14029441\n",
      "Iteration 39, loss = 0.13833189\n",
      "Iteration 40, loss = 0.13669305\n",
      "Iteration 41, loss = 0.13490787\n",
      "Iteration 42, loss = 0.13303371\n",
      "Iteration 43, loss = 0.13129509\n",
      "Iteration 44, loss = 0.12979205\n",
      "Iteration 45, loss = 0.12826098\n",
      "Iteration 46, loss = 0.12665160\n",
      "Iteration 47, loss = 0.12537702\n",
      "Iteration 48, loss = 0.12391266\n",
      "Iteration 49, loss = 0.12240274\n",
      "Iteration 50, loss = 0.12156320\n",
      "Iteration 51, loss = 0.11988849\n",
      "Iteration 52, loss = 0.11913398\n",
      "Iteration 53, loss = 0.11761922\n",
      "Iteration 54, loss = 0.11600044\n",
      "Iteration 55, loss = 0.11509347\n",
      "Iteration 56, loss = 0.11397442\n",
      "Iteration 57, loss = 0.11280124\n",
      "Iteration 58, loss = 0.11152826\n",
      "Iteration 59, loss = 0.11056224\n",
      "Iteration 60, loss = 0.10940557\n",
      "Iteration 61, loss = 0.10806107\n",
      "Iteration 62, loss = 0.10713668\n",
      "Iteration 63, loss = 0.10647072\n",
      "Iteration 64, loss = 0.10513803\n",
      "Iteration 65, loss = 0.10414739\n",
      "Iteration 66, loss = 0.10336299\n",
      "Iteration 67, loss = 0.10252959\n",
      "Iteration 68, loss = 0.10136751\n",
      "Iteration 69, loss = 0.10082449\n",
      "Iteration 70, loss = 0.09981332\n",
      "Iteration 71, loss = 0.09920868\n",
      "Iteration 72, loss = 0.09789525\n",
      "Iteration 73, loss = 0.09680482\n",
      "Iteration 74, loss = 0.09561331\n",
      "Iteration 75, loss = 0.09522389\n",
      "Iteration 76, loss = 0.09425283\n",
      "Iteration 77, loss = 0.09342903\n",
      "Iteration 78, loss = 0.09253446\n",
      "Iteration 79, loss = 0.09202263\n",
      "Iteration 80, loss = 0.09127072\n",
      "Iteration 81, loss = 0.08984838\n",
      "Iteration 82, loss = 0.08974192\n",
      "Iteration 83, loss = 0.08868925\n",
      "Iteration 84, loss = 0.08795116\n",
      "Iteration 85, loss = 0.08710056\n",
      "Iteration 86, loss = 0.08668400\n",
      "Iteration 87, loss = 0.08565658\n",
      "Iteration 88, loss = 0.08497791\n",
      "Iteration 89, loss = 0.08435972\n",
      "Iteration 90, loss = 0.08342301\n",
      "Iteration 91, loss = 0.08233724\n",
      "Iteration 92, loss = 0.08186960\n",
      "Iteration 93, loss = 0.08092915\n",
      "Iteration 94, loss = 0.08067000\n",
      "Iteration 95, loss = 0.07966829\n",
      "Iteration 96, loss = 0.07926309\n",
      "Iteration 97, loss = 0.07814706\n",
      "Iteration 98, loss = 0.07794077\n",
      "Iteration 99, loss = 0.07738799\n",
      "Iteration 100, loss = 0.07672308\n",
      "Iteration 101, loss = 0.07588823\n",
      "Iteration 102, loss = 0.07565607\n",
      "Iteration 103, loss = 0.07485160\n",
      "Iteration 104, loss = 0.07429632\n",
      "Iteration 105, loss = 0.07374893\n",
      "Iteration 106, loss = 0.07274713\n",
      "Iteration 107, loss = 0.07237583\n",
      "Iteration 108, loss = 0.07211831\n",
      "Iteration 109, loss = 0.07139923\n",
      "Iteration 110, loss = 0.07066915\n",
      "Iteration 111, loss = 0.06982182\n",
      "Iteration 112, loss = 0.06927475\n",
      "Iteration 113, loss = 0.06852071\n",
      "Iteration 114, loss = 0.06779834\n",
      "Iteration 115, loss = 0.06729634\n",
      "Iteration 116, loss = 0.06758950\n",
      "Iteration 117, loss = 0.06599782\n",
      "Iteration 118, loss = 0.06582859\n",
      "Iteration 119, loss = 0.06525009\n",
      "Iteration 120, loss = 0.06475382\n",
      "Iteration 121, loss = 0.06430373\n",
      "Iteration 122, loss = 0.06348316\n",
      "Iteration 123, loss = 0.06318292\n",
      "Iteration 124, loss = 0.06300751\n",
      "Iteration 125, loss = 0.06156145\n",
      "Iteration 126, loss = 0.06165256\n",
      "Iteration 127, loss = 0.06106056\n",
      "Iteration 128, loss = 0.06009621\n",
      "Iteration 129, loss = 0.06049348\n",
      "Iteration 130, loss = 0.05928344\n",
      "Iteration 131, loss = 0.05878106\n",
      "Iteration 132, loss = 0.05806337\n",
      "Iteration 133, loss = 0.05783011\n",
      "Iteration 134, loss = 0.05759120\n",
      "Iteration 135, loss = 0.05744406\n",
      "Iteration 136, loss = 0.05632800\n",
      "Iteration 137, loss = 0.05613746\n",
      "Iteration 138, loss = 0.05558276\n",
      "Iteration 139, loss = 0.05475530\n",
      "Iteration 140, loss = 0.05460721\n",
      "Iteration 141, loss = 0.05417518\n",
      "Iteration 142, loss = 0.05365534\n",
      "Iteration 143, loss = 0.05307791\n",
      "Iteration 144, loss = 0.05300298\n",
      "Iteration 145, loss = 0.05222035\n",
      "Iteration 146, loss = 0.05173774\n",
      "Iteration 147, loss = 0.05139814\n",
      "Iteration 148, loss = 0.05160922\n",
      "Iteration 149, loss = 0.05093725\n",
      "Iteration 150, loss = 0.05043694\n",
      "Iteration 151, loss = 0.05005588\n",
      "Iteration 152, loss = 0.04957131\n",
      "Iteration 153, loss = 0.04927779\n",
      "Iteration 154, loss = 0.04813304\n",
      "Iteration 155, loss = 0.04778045\n",
      "Iteration 156, loss = 0.04778040\n",
      "Iteration 157, loss = 0.04664308\n",
      "Iteration 158, loss = 0.04653692\n",
      "Iteration 159, loss = 0.04617294\n",
      "Iteration 160, loss = 0.04579641\n",
      "Iteration 161, loss = 0.04554948\n",
      "Iteration 162, loss = 0.04489952\n",
      "Iteration 163, loss = 0.04448222\n",
      "Iteration 164, loss = 0.04402882\n",
      "Iteration 165, loss = 0.04409940\n",
      "Iteration 166, loss = 0.04363106\n",
      "Iteration 167, loss = 0.04395191\n",
      "Iteration 168, loss = 0.04264124\n",
      "Iteration 169, loss = 0.04270325\n",
      "Iteration 170, loss = 0.04234891\n",
      "Iteration 171, loss = 0.04210814\n",
      "Iteration 172, loss = 0.04152899\n",
      "Iteration 173, loss = 0.04142623\n",
      "Iteration 174, loss = 0.04021804\n",
      "Iteration 175, loss = 0.04133616\n",
      "Iteration 176, loss = 0.04009894\n",
      "Iteration 177, loss = 0.03938533\n",
      "Iteration 178, loss = 0.03938045\n",
      "Iteration 179, loss = 0.03881933\n",
      "Iteration 180, loss = 0.03835633\n",
      "Iteration 181, loss = 0.03789370\n",
      "Iteration 182, loss = 0.03775242\n",
      "Iteration 183, loss = 0.03725233\n",
      "Iteration 184, loss = 0.03721706\n",
      "Iteration 185, loss = 0.03690289\n",
      "Iteration 186, loss = 0.03624860\n",
      "Iteration 187, loss = 0.03619964\n",
      "Iteration 188, loss = 0.03621514\n",
      "Iteration 189, loss = 0.03574653\n",
      "Iteration 190, loss = 0.03531475\n",
      "Iteration 191, loss = 0.03515930\n",
      "Iteration 192, loss = 0.03459595\n",
      "Iteration 193, loss = 0.03477471\n",
      "Iteration 194, loss = 0.03401162\n",
      "Iteration 195, loss = 0.03354345\n",
      "Iteration 196, loss = 0.03334074\n",
      "Iteration 197, loss = 0.03311679\n",
      "Iteration 198, loss = 0.03278323\n",
      "Iteration 199, loss = 0.03244625\n",
      "Iteration 200, loss = 0.03228196\n",
      "Iteration 1, loss = 0.61443599\n",
      "Iteration 2, loss = 0.41124507\n",
      "Iteration 3, loss = 0.34086817\n",
      "Iteration 4, loss = 0.30539251\n",
      "Iteration 5, loss = 0.28324845\n",
      "Iteration 6, loss = 0.26771142\n",
      "Iteration 7, loss = 0.25626075\n",
      "Iteration 8, loss = 0.24687524\n",
      "Iteration 9, loss = 0.23882976\n",
      "Iteration 10, loss = 0.23199458\n",
      "Iteration 11, loss = 0.22594089\n",
      "Iteration 12, loss = 0.22038276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.21532472\n",
      "Iteration 14, loss = 0.21042936\n",
      "Iteration 15, loss = 0.20563170\n",
      "Iteration 16, loss = 0.20165529\n",
      "Iteration 17, loss = 0.19777822\n",
      "Iteration 18, loss = 0.19438728\n",
      "Iteration 19, loss = 0.19022226\n",
      "Iteration 20, loss = 0.18678957\n",
      "Iteration 21, loss = 0.18346746\n",
      "Iteration 22, loss = 0.18001516\n",
      "Iteration 23, loss = 0.17730317\n",
      "Iteration 24, loss = 0.17410757\n",
      "Iteration 25, loss = 0.17087472\n",
      "Iteration 26, loss = 0.16819949\n",
      "Iteration 27, loss = 0.16584097\n",
      "Iteration 28, loss = 0.16290026\n",
      "Iteration 29, loss = 0.16063910\n",
      "Iteration 30, loss = 0.15805801\n",
      "Iteration 31, loss = 0.15554701\n",
      "Iteration 32, loss = 0.15300829\n",
      "Iteration 33, loss = 0.15109563\n",
      "Iteration 34, loss = 0.14864170\n",
      "Iteration 35, loss = 0.14639027\n",
      "Iteration 36, loss = 0.14425770\n",
      "Iteration 37, loss = 0.14251680\n",
      "Iteration 38, loss = 0.14028337\n",
      "Iteration 39, loss = 0.13868494\n",
      "Iteration 40, loss = 0.13734187\n",
      "Iteration 41, loss = 0.13512850\n",
      "Iteration 42, loss = 0.13304322\n",
      "Iteration 43, loss = 0.13124007\n",
      "Iteration 44, loss = 0.12988481\n",
      "Iteration 45, loss = 0.12846578\n",
      "Iteration 46, loss = 0.12666448\n",
      "Iteration 47, loss = 0.12545201\n",
      "Iteration 48, loss = 0.12399893\n",
      "Iteration 49, loss = 0.12239113\n",
      "Iteration 50, loss = 0.12115829\n",
      "Iteration 51, loss = 0.11998524\n",
      "Iteration 52, loss = 0.11888991\n",
      "Iteration 53, loss = 0.11720851\n",
      "Iteration 54, loss = 0.11587287\n",
      "Iteration 55, loss = 0.11485322\n",
      "Iteration 56, loss = 0.11369341\n",
      "Iteration 57, loss = 0.11236163\n",
      "Iteration 58, loss = 0.11127379\n",
      "Iteration 59, loss = 0.11034620\n",
      "Iteration 60, loss = 0.10935111\n",
      "Iteration 61, loss = 0.10814800\n",
      "Iteration 62, loss = 0.10738815\n",
      "Iteration 63, loss = 0.10623859\n",
      "Iteration 64, loss = 0.10504777\n",
      "Iteration 65, loss = 0.10413736\n",
      "Iteration 66, loss = 0.10295900\n",
      "Iteration 67, loss = 0.10221435\n",
      "Iteration 68, loss = 0.10117001\n",
      "Iteration 69, loss = 0.10050455\n",
      "Iteration 70, loss = 0.09991207\n",
      "Iteration 71, loss = 0.09854453\n",
      "Iteration 72, loss = 0.09771330\n",
      "Iteration 73, loss = 0.09668364\n",
      "Iteration 74, loss = 0.09549337\n",
      "Iteration 75, loss = 0.09516516\n",
      "Iteration 76, loss = 0.09453725\n",
      "Iteration 77, loss = 0.09361568\n",
      "Iteration 78, loss = 0.09277893\n",
      "Iteration 79, loss = 0.09165237\n",
      "Iteration 80, loss = 0.09117660\n",
      "Iteration 81, loss = 0.09009282\n",
      "Iteration 82, loss = 0.08948860\n",
      "Iteration 83, loss = 0.08850640\n",
      "Iteration 84, loss = 0.08782236\n",
      "Iteration 85, loss = 0.08708670\n",
      "Iteration 86, loss = 0.08680682\n",
      "Iteration 87, loss = 0.08543874\n",
      "Iteration 88, loss = 0.08534317\n",
      "Iteration 89, loss = 0.08433626\n",
      "Iteration 90, loss = 0.08361074\n",
      "Iteration 91, loss = 0.08303687\n",
      "Iteration 92, loss = 0.08215066\n",
      "Iteration 93, loss = 0.08127863\n",
      "Iteration 94, loss = 0.08131432\n",
      "Iteration 95, loss = 0.08011283\n",
      "Iteration 96, loss = 0.07985154\n",
      "Iteration 97, loss = 0.07869085\n",
      "Iteration 98, loss = 0.07813965\n",
      "Iteration 99, loss = 0.07759829\n",
      "Iteration 100, loss = 0.07722759\n",
      "Iteration 101, loss = 0.07634828\n",
      "Iteration 102, loss = 0.07589902\n",
      "Iteration 103, loss = 0.07523982\n",
      "Iteration 104, loss = 0.07440472\n",
      "Iteration 105, loss = 0.07401384\n",
      "Iteration 106, loss = 0.07345761\n",
      "Iteration 107, loss = 0.07290434\n",
      "Iteration 108, loss = 0.07333445\n",
      "Iteration 109, loss = 0.07189508\n",
      "Iteration 110, loss = 0.07112936\n",
      "Iteration 111, loss = 0.07060112\n",
      "Iteration 112, loss = 0.06998613\n",
      "Iteration 113, loss = 0.06952714\n",
      "Iteration 114, loss = 0.06866650\n",
      "Iteration 115, loss = 0.06821419\n",
      "Iteration 116, loss = 0.06802012\n",
      "Iteration 117, loss = 0.06690022\n",
      "Iteration 118, loss = 0.06673080\n",
      "Iteration 119, loss = 0.06577783\n",
      "Iteration 120, loss = 0.06540917\n",
      "Iteration 121, loss = 0.06564986\n",
      "Iteration 122, loss = 0.06441439\n",
      "Iteration 123, loss = 0.06422903\n",
      "Iteration 124, loss = 0.06350931\n",
      "Iteration 125, loss = 0.06252964\n",
      "Iteration 126, loss = 0.06231430\n",
      "Iteration 127, loss = 0.06184115\n",
      "Iteration 128, loss = 0.06075727\n",
      "Iteration 129, loss = 0.06119931\n",
      "Iteration 130, loss = 0.06002592\n",
      "Iteration 131, loss = 0.05960538\n",
      "Iteration 132, loss = 0.05892782\n",
      "Iteration 133, loss = 0.05895952\n",
      "Iteration 134, loss = 0.05840843\n",
      "Iteration 135, loss = 0.05875908\n",
      "Iteration 136, loss = 0.05741961\n",
      "Iteration 137, loss = 0.05683561\n",
      "Iteration 138, loss = 0.05646130\n",
      "Iteration 139, loss = 0.05605222\n",
      "Iteration 140, loss = 0.05567342\n",
      "Iteration 141, loss = 0.05474126\n",
      "Iteration 142, loss = 0.05451551\n",
      "Iteration 143, loss = 0.05404908\n",
      "Iteration 144, loss = 0.05411944\n",
      "Iteration 145, loss = 0.05318320\n",
      "Iteration 146, loss = 0.05254322\n",
      "Iteration 147, loss = 0.05212208\n",
      "Iteration 148, loss = 0.05201717\n",
      "Iteration 149, loss = 0.05184073\n",
      "Iteration 150, loss = 0.05121949\n",
      "Iteration 151, loss = 0.05111667\n",
      "Iteration 152, loss = 0.05069461\n",
      "Iteration 153, loss = 0.05044699\n",
      "Iteration 154, loss = 0.04950020\n",
      "Iteration 155, loss = 0.04874022\n",
      "Iteration 156, loss = 0.04884293\n",
      "Iteration 157, loss = 0.04788966\n",
      "Iteration 158, loss = 0.04753196\n",
      "Iteration 159, loss = 0.04708561\n",
      "Iteration 160, loss = 0.04645616\n",
      "Iteration 161, loss = 0.04633816\n",
      "Iteration 162, loss = 0.04647625\n",
      "Iteration 163, loss = 0.04574133\n",
      "Iteration 164, loss = 0.04534456\n",
      "Iteration 165, loss = 0.04535816\n",
      "Iteration 166, loss = 0.04473205\n",
      "Iteration 167, loss = 0.04461526\n",
      "Iteration 168, loss = 0.04375577\n",
      "Iteration 169, loss = 0.04350152\n",
      "Iteration 170, loss = 0.04346499\n",
      "Iteration 171, loss = 0.04307188\n",
      "Iteration 172, loss = 0.04228218\n",
      "Iteration 173, loss = 0.04223306\n",
      "Iteration 174, loss = 0.04130428\n",
      "Iteration 175, loss = 0.04185617\n",
      "Iteration 176, loss = 0.04143577\n",
      "Iteration 177, loss = 0.04072763\n",
      "Iteration 178, loss = 0.04015934\n",
      "Iteration 179, loss = 0.03952326\n",
      "Iteration 180, loss = 0.03951312\n",
      "Iteration 181, loss = 0.03895394\n",
      "Iteration 182, loss = 0.03869973\n",
      "Iteration 183, loss = 0.03832793\n",
      "Iteration 184, loss = 0.03832007\n",
      "Iteration 185, loss = 0.03821863\n",
      "Iteration 186, loss = 0.03739880\n",
      "Iteration 187, loss = 0.03712704\n",
      "Iteration 188, loss = 0.03665415\n",
      "Iteration 189, loss = 0.03677228\n",
      "Iteration 190, loss = 0.03632805\n",
      "Iteration 191, loss = 0.03591643\n",
      "Iteration 192, loss = 0.03559389\n",
      "Iteration 193, loss = 0.03560692\n",
      "Iteration 194, loss = 0.03511067\n",
      "Iteration 195, loss = 0.03471964\n",
      "Iteration 196, loss = 0.03457461\n",
      "Iteration 197, loss = 0.03431713\n",
      "Iteration 198, loss = 0.03370291\n",
      "Iteration 199, loss = 0.03361809\n",
      "Iteration 200, loss = 0.03345449\n",
      "Iteration 1, loss = 0.53120254\n",
      "Iteration 2, loss = 0.40621748\n",
      "Iteration 3, loss = 0.37549391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.35262423\n",
      "Iteration 5, loss = 0.33383351\n",
      "Iteration 6, loss = 0.31854520\n",
      "Iteration 7, loss = 0.30663145\n",
      "Iteration 8, loss = 0.29716529\n",
      "Iteration 9, loss = 0.28949803\n",
      "Iteration 10, loss = 0.28308891\n",
      "Iteration 11, loss = 0.27825692\n",
      "Iteration 12, loss = 0.27412484\n",
      "Iteration 13, loss = 0.27033310\n",
      "Iteration 14, loss = 0.26726385\n",
      "Iteration 15, loss = 0.26449640\n",
      "Iteration 16, loss = 0.26175104\n",
      "Iteration 17, loss = 0.25937443\n",
      "Iteration 18, loss = 0.25724295\n",
      "Iteration 19, loss = 0.25530189\n",
      "Iteration 20, loss = 0.25335252\n",
      "Iteration 21, loss = 0.25153684\n",
      "Iteration 22, loss = 0.24969517\n",
      "Iteration 23, loss = 0.24788081\n",
      "Iteration 24, loss = 0.24667321\n",
      "Iteration 25, loss = 0.24511165\n",
      "Iteration 26, loss = 0.24349503\n",
      "Iteration 27, loss = 0.24222762\n",
      "Iteration 28, loss = 0.24076800\n",
      "Iteration 29, loss = 0.23987642\n",
      "Iteration 30, loss = 0.23888720\n",
      "Iteration 31, loss = 0.23779260\n",
      "Iteration 32, loss = 0.23669127\n",
      "Iteration 33, loss = 0.23618878\n",
      "Iteration 34, loss = 0.23522094\n",
      "Iteration 35, loss = 0.23455494\n",
      "Iteration 36, loss = 0.23364362\n",
      "Iteration 37, loss = 0.23326451\n",
      "Iteration 38, loss = 0.23256643\n",
      "Iteration 39, loss = 0.23235828\n",
      "Iteration 40, loss = 0.23189961\n",
      "Iteration 41, loss = 0.23122912\n",
      "Iteration 42, loss = 0.23116914\n",
      "Iteration 43, loss = 0.23068813\n",
      "Iteration 44, loss = 0.23083868\n",
      "Iteration 45, loss = 0.23040784\n",
      "Iteration 46, loss = 0.22996782\n",
      "Iteration 47, loss = 0.23012443\n",
      "Iteration 48, loss = 0.22950611\n",
      "Iteration 49, loss = 0.22921315\n",
      "Iteration 50, loss = 0.22854986\n",
      "Iteration 51, loss = 0.22889543\n",
      "Iteration 52, loss = 0.22830748\n",
      "Iteration 53, loss = 0.22799797\n",
      "Iteration 54, loss = 0.22807234\n",
      "Iteration 55, loss = 0.22763994\n",
      "Iteration 56, loss = 0.22718434\n",
      "Iteration 57, loss = 0.22732549\n",
      "Iteration 58, loss = 0.22637532\n",
      "Iteration 59, loss = 0.22615698\n",
      "Iteration 60, loss = 0.22563516\n",
      "Iteration 61, loss = 0.22540027\n",
      "Iteration 62, loss = 0.22495669\n",
      "Iteration 63, loss = 0.22461183\n",
      "Iteration 64, loss = 0.22419592\n",
      "Iteration 65, loss = 0.22395768\n",
      "Iteration 66, loss = 0.22327105\n",
      "Iteration 67, loss = 0.22325072\n",
      "Iteration 68, loss = 0.22226910\n",
      "Iteration 69, loss = 0.22173488\n",
      "Iteration 70, loss = 0.22123528\n",
      "Iteration 71, loss = 0.22059990\n",
      "Iteration 72, loss = 0.22030843\n",
      "Iteration 73, loss = 0.21969474\n",
      "Iteration 74, loss = 0.21929548\n",
      "Iteration 75, loss = 0.21909449\n",
      "Iteration 76, loss = 0.21771001\n",
      "Iteration 77, loss = 0.21710987\n",
      "Iteration 78, loss = 0.21651135\n",
      "Iteration 79, loss = 0.21599368\n",
      "Iteration 80, loss = 0.21525236\n",
      "Iteration 81, loss = 0.21437984\n",
      "Iteration 82, loss = 0.21361733\n",
      "Iteration 83, loss = 0.21283710\n",
      "Iteration 84, loss = 0.21221004\n",
      "Iteration 85, loss = 0.21147761\n",
      "Iteration 86, loss = 0.21045337\n",
      "Iteration 87, loss = 0.20979191\n",
      "Iteration 88, loss = 0.20897454\n",
      "Iteration 89, loss = 0.20843715\n",
      "Iteration 90, loss = 0.20758949\n",
      "Iteration 91, loss = 0.20636733\n",
      "Iteration 92, loss = 0.20571670\n",
      "Iteration 93, loss = 0.20456372\n",
      "Iteration 94, loss = 0.20378674\n",
      "Iteration 95, loss = 0.20297514\n",
      "Iteration 96, loss = 0.20191542\n",
      "Iteration 97, loss = 0.20097807\n",
      "Iteration 98, loss = 0.20014893\n",
      "Iteration 99, loss = 0.19944094\n",
      "Iteration 100, loss = 0.19864704\n",
      "Iteration 101, loss = 0.19738568\n",
      "Iteration 102, loss = 0.19698935\n",
      "Iteration 103, loss = 0.19581577\n",
      "Iteration 104, loss = 0.19497141\n",
      "Iteration 105, loss = 0.19403428\n",
      "Iteration 106, loss = 0.19312395\n",
      "Iteration 107, loss = 0.19196188\n",
      "Iteration 108, loss = 0.19124294\n",
      "Iteration 109, loss = 0.19017620\n",
      "Iteration 110, loss = 0.18925630\n",
      "Iteration 111, loss = 0.18849617\n",
      "Iteration 112, loss = 0.18781525\n",
      "Iteration 113, loss = 0.18677656\n",
      "Iteration 114, loss = 0.18596082\n",
      "Iteration 115, loss = 0.18499610\n",
      "Iteration 116, loss = 0.18418931\n",
      "Iteration 117, loss = 0.18342778\n",
      "Iteration 118, loss = 0.18284572\n",
      "Iteration 119, loss = 0.18188387\n",
      "Iteration 120, loss = 0.18174113\n",
      "Iteration 121, loss = 0.18031546\n",
      "Iteration 122, loss = 0.17987289\n",
      "Iteration 123, loss = 0.17858377\n",
      "Iteration 124, loss = 0.17776824\n",
      "Iteration 125, loss = 0.17718807\n",
      "Iteration 126, loss = 0.17652176\n",
      "Iteration 127, loss = 0.17559049\n",
      "Iteration 128, loss = 0.17513979\n",
      "Iteration 129, loss = 0.17446088\n",
      "Iteration 130, loss = 0.17398725\n",
      "Iteration 131, loss = 0.17301905\n",
      "Iteration 132, loss = 0.17236416\n",
      "Iteration 133, loss = 0.17162805\n",
      "Iteration 134, loss = 0.17112785\n",
      "Iteration 135, loss = 0.17046569\n",
      "Iteration 136, loss = 0.16993390\n",
      "Iteration 137, loss = 0.16908778\n",
      "Iteration 138, loss = 0.16846282\n",
      "Iteration 139, loss = 0.16775153\n",
      "Iteration 140, loss = 0.16778349\n",
      "Iteration 141, loss = 0.16665813\n",
      "Iteration 142, loss = 0.16587722\n",
      "Iteration 143, loss = 0.16608301\n",
      "Iteration 144, loss = 0.16483484\n",
      "Iteration 145, loss = 0.16459579\n",
      "Iteration 146, loss = 0.16406475\n",
      "Iteration 147, loss = 0.16332470\n",
      "Iteration 148, loss = 0.16287765\n",
      "Iteration 149, loss = 0.16213568\n",
      "Iteration 150, loss = 0.16172960\n",
      "Iteration 151, loss = 0.16088247\n",
      "Iteration 152, loss = 0.16046270\n",
      "Iteration 153, loss = 0.16014385\n",
      "Iteration 154, loss = 0.15990747\n",
      "Iteration 155, loss = 0.15906613\n",
      "Iteration 156, loss = 0.15826894\n",
      "Iteration 157, loss = 0.15778558\n",
      "Iteration 158, loss = 0.15726597\n",
      "Iteration 159, loss = 0.15708066\n",
      "Iteration 160, loss = 0.15649682\n",
      "Iteration 161, loss = 0.15562149\n",
      "Iteration 162, loss = 0.15513673\n",
      "Iteration 163, loss = 0.15475716\n",
      "Iteration 164, loss = 0.15395492\n",
      "Iteration 165, loss = 0.15353346\n",
      "Iteration 166, loss = 0.15303226\n",
      "Iteration 167, loss = 0.15241976\n",
      "Iteration 168, loss = 0.15216070\n",
      "Iteration 169, loss = 0.15163858\n",
      "Iteration 170, loss = 0.15086841\n",
      "Iteration 171, loss = 0.15084493\n",
      "Iteration 172, loss = 0.14991009\n",
      "Iteration 173, loss = 0.14947079\n",
      "Iteration 174, loss = 0.14873555\n",
      "Iteration 175, loss = 0.14823505\n",
      "Iteration 176, loss = 0.14785396\n",
      "Iteration 177, loss = 0.14746532\n",
      "Iteration 178, loss = 0.14705312\n",
      "Iteration 179, loss = 0.14623314\n",
      "Iteration 180, loss = 0.14567469\n",
      "Iteration 181, loss = 0.14563478\n",
      "Iteration 182, loss = 0.14492905\n",
      "Iteration 183, loss = 0.14489073\n",
      "Iteration 184, loss = 0.14412131\n",
      "Iteration 185, loss = 0.14332776\n",
      "Iteration 186, loss = 0.14271269\n",
      "Iteration 187, loss = 0.14220437\n",
      "Iteration 188, loss = 0.14184975\n",
      "Iteration 189, loss = 0.14120320\n",
      "Iteration 190, loss = 0.14067213\n",
      "Iteration 191, loss = 0.14033319\n",
      "Iteration 192, loss = 0.13970411\n",
      "Iteration 193, loss = 0.13910170\n",
      "Iteration 194, loss = 0.13853973\n",
      "Iteration 195, loss = 0.13838494\n",
      "Iteration 196, loss = 0.13768956\n",
      "Iteration 197, loss = 0.13707566\n",
      "Iteration 198, loss = 0.13681767\n",
      "Iteration 199, loss = 0.13631092\n",
      "Iteration 200, loss = 0.13570798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Applied MLP Neural Network Classifier model to 'missing' bank churn X and y training data\n",
    "mlpmodelparams_missing , mlpmodelScore_missing, mlpmodelTime_missing = mlpmodel(X_train_missing, y_train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate MLP Neural Network Classifier Model Build Time with GridSearchCV : 695.39 seconds\n",
      "MLP Neural Network Classifier Model Best Parameters given Missing Dataset: {'activation': 'logistic', 'alpha': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'power_t': 0.1, 'solver': 'adam'}\n",
      "MLP Neural Network Classifier Model Best Parameter Score given Missing Dataset: 0.928042328042328\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate MLP Neural Network Classifier Model Build Time with GridSearchCV : {round(mlpmodelTime_missing , 2)} seconds\")\n",
    "print(f\"MLP Neural Network Classifier Model Best Parameters given Missing Dataset: {mlpmodelparams_missing}\")\n",
    "print(f\"MLP Neural Network Classifier Model Best Parameter Score given Missing Dataset: {mlpmodelScore_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9337721102426985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.81      0.73      0.76       360\n",
      " not_churned       0.95      0.97      0.96      2071\n",
      "\n",
      "    accuracy                           0.93      2431\n",
      "   macro avg       0.88      0.85      0.86      2431\n",
      "weighted avg       0.93      0.93      0.93      2431\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for MLP Neural Network model model for X_train_missing and y_train_missing\n",
    "mlpmodel_missing = MLPClassifier(hidden_layer_sizes = (100,), activation='logistic', alpha=0.0001, beta_1=0.9, beta_2=0.999, learning_rate='constant', learning_rate_init =0.001, power_t=0.1, solver='adam', random_state=random_state)\n",
    "mlpmodel_missing.fit(X_train_missing, y_train_missing.values.ravel())\n",
    "y_pred_mlpmodel_missing = mlpmodel_missing.predict(X_test_missing)\n",
    "\n",
    "print(accuracy_score(y_test_missing, y_pred_mlpmodel_missing))\n",
    "print(classification_report(y_test_missing, y_pred_mlpmodel_missing, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76838965\n",
      "Iteration 2, loss = 0.67322147\n",
      "Iteration 3, loss = 0.58455563\n",
      "Iteration 4, loss = 0.51830839\n",
      "Iteration 5, loss = 0.47067279\n",
      "Iteration 6, loss = 0.43473387\n",
      "Iteration 7, loss = 0.40788960\n",
      "Iteration 8, loss = 0.38676616\n",
      "Iteration 9, loss = 0.37013198\n",
      "Iteration 10, loss = 0.35659617\n",
      "Iteration 11, loss = 0.34547694\n",
      "Iteration 12, loss = 0.33597431\n",
      "Iteration 13, loss = 0.32823223\n",
      "Iteration 14, loss = 0.32137986\n",
      "Iteration 15, loss = 0.31549914\n",
      "Iteration 16, loss = 0.31036968\n",
      "Iteration 17, loss = 0.30579954\n",
      "Iteration 18, loss = 0.30183204\n",
      "Iteration 19, loss = 0.29816244\n",
      "Iteration 20, loss = 0.29490583\n",
      "Iteration 21, loss = 0.29207313\n",
      "Iteration 22, loss = 0.28940235\n",
      "Iteration 23, loss = 0.28693011\n",
      "Iteration 24, loss = 0.28472920\n",
      "Iteration 25, loss = 0.28272471\n",
      "Iteration 26, loss = 0.28080601\n",
      "Iteration 27, loss = 0.27907884\n",
      "Iteration 28, loss = 0.27750287\n",
      "Iteration 29, loss = 0.27593986\n",
      "Iteration 30, loss = 0.27455294\n",
      "Iteration 31, loss = 0.27321613\n",
      "Iteration 32, loss = 0.27201176\n",
      "Iteration 33, loss = 0.27084198\n",
      "Iteration 34, loss = 0.26975883\n",
      "Iteration 35, loss = 0.26876750\n",
      "Iteration 36, loss = 0.26778725\n",
      "Iteration 37, loss = 0.26678487\n",
      "Iteration 38, loss = 0.26596016\n",
      "Iteration 39, loss = 0.26509941\n",
      "Iteration 40, loss = 0.26431628\n",
      "Iteration 41, loss = 0.26357850\n",
      "Iteration 42, loss = 0.26285598\n",
      "Iteration 43, loss = 0.26212337\n",
      "Iteration 44, loss = 0.26149126\n",
      "Iteration 45, loss = 0.26087443\n",
      "Iteration 46, loss = 0.26023170\n",
      "Iteration 47, loss = 0.25972476\n",
      "Iteration 48, loss = 0.25912478\n",
      "Iteration 49, loss = 0.25858811\n",
      "Iteration 50, loss = 0.25805123\n",
      "Iteration 51, loss = 0.25755809\n",
      "Iteration 52, loss = 0.25712229\n",
      "Iteration 53, loss = 0.25661345\n",
      "Iteration 54, loss = 0.25620052\n",
      "Iteration 55, loss = 0.25574567\n",
      "Iteration 56, loss = 0.25533246\n",
      "Iteration 57, loss = 0.25498470\n",
      "Iteration 58, loss = 0.25456208\n",
      "Iteration 59, loss = 0.25416851\n",
      "Iteration 60, loss = 0.25382125\n",
      "Iteration 61, loss = 0.25346378\n",
      "Iteration 62, loss = 0.25312268\n",
      "Iteration 63, loss = 0.25280868\n",
      "Iteration 64, loss = 0.25247887\n",
      "Iteration 65, loss = 0.25214812\n",
      "Iteration 66, loss = 0.25188412\n",
      "Iteration 67, loss = 0.25153139\n",
      "Iteration 68, loss = 0.25122737\n",
      "Iteration 69, loss = 0.25099495\n",
      "Iteration 70, loss = 0.25066628\n",
      "Iteration 71, loss = 0.25041672\n",
      "Iteration 72, loss = 0.25015523\n",
      "Iteration 73, loss = 0.24988474\n",
      "Iteration 74, loss = 0.24963614\n",
      "Iteration 75, loss = 0.24937469\n",
      "Iteration 76, loss = 0.24916378\n",
      "Iteration 77, loss = 0.24895604\n",
      "Iteration 78, loss = 0.24870007\n",
      "Iteration 79, loss = 0.24845004\n",
      "Iteration 80, loss = 0.24824100\n",
      "Iteration 81, loss = 0.24802990\n",
      "Iteration 82, loss = 0.24785912\n",
      "Iteration 83, loss = 0.24756558\n",
      "Iteration 84, loss = 0.24738389\n",
      "Iteration 85, loss = 0.24717876\n",
      "Iteration 86, loss = 0.24699610\n",
      "Iteration 87, loss = 0.24680416\n",
      "Iteration 88, loss = 0.24660469\n",
      "Iteration 89, loss = 0.24640648\n",
      "Iteration 90, loss = 0.24625637\n",
      "Iteration 91, loss = 0.24605652\n",
      "Iteration 92, loss = 0.24591722\n",
      "Iteration 93, loss = 0.24569506\n",
      "Iteration 94, loss = 0.24554549\n",
      "Iteration 95, loss = 0.24537602\n",
      "Iteration 96, loss = 0.24521974\n",
      "Iteration 97, loss = 0.24505169\n",
      "Iteration 98, loss = 0.24488407\n",
      "Iteration 99, loss = 0.24472131\n",
      "Iteration 100, loss = 0.24464193\n",
      "Iteration 101, loss = 0.24446220\n",
      "Iteration 102, loss = 0.24426143\n",
      "Iteration 103, loss = 0.24414671\n",
      "Iteration 104, loss = 0.24401795\n",
      "Iteration 105, loss = 0.24388174\n",
      "Iteration 106, loss = 0.24372599\n",
      "Iteration 107, loss = 0.24360303\n",
      "Iteration 108, loss = 0.24346761\n",
      "Iteration 109, loss = 0.24334066\n",
      "Iteration 110, loss = 0.24324197\n",
      "Iteration 111, loss = 0.24309748\n",
      "Iteration 112, loss = 0.24294622\n",
      "Iteration 113, loss = 0.24285583\n",
      "Iteration 114, loss = 0.24271774\n",
      "Iteration 115, loss = 0.24264930\n",
      "Iteration 116, loss = 0.24253338\n",
      "Iteration 117, loss = 0.24242884\n",
      "Iteration 118, loss = 0.24225069\n",
      "Iteration 119, loss = 0.24218209\n",
      "Iteration 120, loss = 0.24207441\n",
      "Iteration 121, loss = 0.24195359\n",
      "Iteration 122, loss = 0.24187229\n",
      "Iteration 123, loss = 0.24175782\n",
      "Iteration 124, loss = 0.24164909\n",
      "Iteration 125, loss = 0.24157255\n",
      "Iteration 126, loss = 0.24146991\n",
      "Iteration 127, loss = 0.24137943\n",
      "Iteration 128, loss = 0.24128509\n",
      "Iteration 129, loss = 0.24120646\n",
      "Iteration 130, loss = 0.24109183\n",
      "Iteration 131, loss = 0.24102271\n",
      "Iteration 132, loss = 0.24095683\n",
      "Iteration 133, loss = 0.24085652\n",
      "Iteration 134, loss = 0.24077423\n",
      "Iteration 135, loss = 0.24068966\n",
      "Iteration 136, loss = 0.24067414\n",
      "Iteration 137, loss = 0.24054808\n",
      "Iteration 138, loss = 0.24044678\n",
      "Iteration 139, loss = 0.24039368\n",
      "Iteration 140, loss = 0.24030921\n",
      "Iteration 141, loss = 0.24021759\n",
      "Iteration 142, loss = 0.24016053\n",
      "Iteration 143, loss = 0.24010399\n",
      "Iteration 144, loss = 0.24002391\n",
      "Iteration 145, loss = 0.23995621\n",
      "Iteration 146, loss = 0.23988403\n",
      "Iteration 147, loss = 0.23983703\n",
      "Iteration 148, loss = 0.23979439\n",
      "Iteration 149, loss = 0.23968224\n",
      "Iteration 150, loss = 0.23961946\n",
      "Iteration 151, loss = 0.23958603\n",
      "Iteration 152, loss = 0.23953980\n",
      "Iteration 153, loss = 0.23948641\n",
      "Iteration 154, loss = 0.23939828\n",
      "Iteration 155, loss = 0.23936098\n",
      "Iteration 156, loss = 0.23928269\n",
      "Iteration 157, loss = 0.23923103\n",
      "Iteration 158, loss = 0.23915980\n",
      "Iteration 159, loss = 0.23913325\n",
      "Iteration 160, loss = 0.23911405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76739801\n",
      "Iteration 2, loss = 0.67431107\n",
      "Iteration 3, loss = 0.58643882\n",
      "Iteration 4, loss = 0.52116738\n",
      "Iteration 5, loss = 0.47334060\n",
      "Iteration 6, loss = 0.43718487\n",
      "Iteration 7, loss = 0.41004918\n",
      "Iteration 8, loss = 0.38857306\n",
      "Iteration 9, loss = 0.37157304\n",
      "Iteration 10, loss = 0.35770321\n",
      "Iteration 11, loss = 0.34609169\n",
      "Iteration 12, loss = 0.33629650\n",
      "Iteration 13, loss = 0.32811238\n",
      "Iteration 14, loss = 0.32088115\n",
      "Iteration 15, loss = 0.31472670\n",
      "Iteration 16, loss = 0.30909596\n",
      "Iteration 17, loss = 0.30434647\n",
      "Iteration 18, loss = 0.30008115\n",
      "Iteration 19, loss = 0.29618054\n",
      "Iteration 20, loss = 0.29280246\n",
      "Iteration 21, loss = 0.28978137\n",
      "Iteration 22, loss = 0.28704282\n",
      "Iteration 23, loss = 0.28457616\n",
      "Iteration 24, loss = 0.28235153\n",
      "Iteration 25, loss = 0.28032719\n",
      "Iteration 26, loss = 0.27847760\n",
      "Iteration 27, loss = 0.27676216\n",
      "Iteration 28, loss = 0.27521743\n",
      "Iteration 29, loss = 0.27370001\n",
      "Iteration 30, loss = 0.27238923\n",
      "Iteration 31, loss = 0.27108032\n",
      "Iteration 32, loss = 0.26992102\n",
      "Iteration 33, loss = 0.26884035\n",
      "Iteration 34, loss = 0.26782254\n",
      "Iteration 35, loss = 0.26682495\n",
      "Iteration 36, loss = 0.26588112\n",
      "Iteration 37, loss = 0.26499375\n",
      "Iteration 38, loss = 0.26418645\n",
      "Iteration 39, loss = 0.26340974\n",
      "Iteration 40, loss = 0.26267635\n",
      "Iteration 41, loss = 0.26195284\n",
      "Iteration 42, loss = 0.26128560\n",
      "Iteration 43, loss = 0.26061065\n",
      "Iteration 44, loss = 0.26000095\n",
      "Iteration 45, loss = 0.25941422\n",
      "Iteration 46, loss = 0.25886964\n",
      "Iteration 47, loss = 0.25833924\n",
      "Iteration 48, loss = 0.25779649\n",
      "Iteration 49, loss = 0.25726891\n",
      "Iteration 50, loss = 0.25676971\n",
      "Iteration 51, loss = 0.25629368\n",
      "Iteration 52, loss = 0.25585892\n",
      "Iteration 53, loss = 0.25540314\n",
      "Iteration 54, loss = 0.25500575\n",
      "Iteration 55, loss = 0.25457851\n",
      "Iteration 56, loss = 0.25416274\n",
      "Iteration 57, loss = 0.25380913\n",
      "Iteration 58, loss = 0.25347852\n",
      "Iteration 59, loss = 0.25306061\n",
      "Iteration 60, loss = 0.25270123\n",
      "Iteration 61, loss = 0.25236575\n",
      "Iteration 62, loss = 0.25203838\n",
      "Iteration 63, loss = 0.25169882\n",
      "Iteration 64, loss = 0.25137510\n",
      "Iteration 65, loss = 0.25105267\n",
      "Iteration 66, loss = 0.25081060\n",
      "Iteration 67, loss = 0.25045088\n",
      "Iteration 68, loss = 0.25018085\n",
      "Iteration 69, loss = 0.24990054\n",
      "Iteration 70, loss = 0.24961190\n",
      "Iteration 71, loss = 0.24935291\n",
      "Iteration 72, loss = 0.24913417\n",
      "Iteration 73, loss = 0.24882219\n",
      "Iteration 74, loss = 0.24858413\n",
      "Iteration 75, loss = 0.24835502\n",
      "Iteration 76, loss = 0.24809970\n",
      "Iteration 77, loss = 0.24788500\n",
      "Iteration 78, loss = 0.24765788\n",
      "Iteration 79, loss = 0.24744395\n",
      "Iteration 80, loss = 0.24718259\n",
      "Iteration 81, loss = 0.24695307\n",
      "Iteration 82, loss = 0.24680432\n",
      "Iteration 83, loss = 0.24654825\n",
      "Iteration 84, loss = 0.24634236\n",
      "Iteration 85, loss = 0.24614578\n",
      "Iteration 86, loss = 0.24593491\n",
      "Iteration 87, loss = 0.24576959\n",
      "Iteration 88, loss = 0.24556901\n",
      "Iteration 89, loss = 0.24539204\n",
      "Iteration 90, loss = 0.24521504\n",
      "Iteration 91, loss = 0.24501097\n",
      "Iteration 92, loss = 0.24484619\n",
      "Iteration 93, loss = 0.24465379\n",
      "Iteration 94, loss = 0.24448124\n",
      "Iteration 95, loss = 0.24432996\n",
      "Iteration 96, loss = 0.24414720\n",
      "Iteration 97, loss = 0.24400605\n",
      "Iteration 98, loss = 0.24385550\n",
      "Iteration 99, loss = 0.24367760\n",
      "Iteration 100, loss = 0.24356228\n",
      "Iteration 101, loss = 0.24337928\n",
      "Iteration 102, loss = 0.24321246\n",
      "Iteration 103, loss = 0.24306893\n",
      "Iteration 104, loss = 0.24293726\n",
      "Iteration 105, loss = 0.24283993\n",
      "Iteration 106, loss = 0.24266476\n",
      "Iteration 107, loss = 0.24254191\n",
      "Iteration 108, loss = 0.24240433\n",
      "Iteration 109, loss = 0.24227637\n",
      "Iteration 110, loss = 0.24213268\n",
      "Iteration 111, loss = 0.24201833\n",
      "Iteration 112, loss = 0.24183963\n",
      "Iteration 113, loss = 0.24178785\n",
      "Iteration 114, loss = 0.24165105\n",
      "Iteration 115, loss = 0.24154825\n",
      "Iteration 116, loss = 0.24141167\n",
      "Iteration 117, loss = 0.24129785\n",
      "Iteration 118, loss = 0.24117065\n",
      "Iteration 119, loss = 0.24108245\n",
      "Iteration 120, loss = 0.24098565\n",
      "Iteration 121, loss = 0.24083410\n",
      "Iteration 122, loss = 0.24076572\n",
      "Iteration 123, loss = 0.24063355\n",
      "Iteration 124, loss = 0.24052387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 125, loss = 0.24046640\n",
      "Iteration 126, loss = 0.24032624\n",
      "Iteration 127, loss = 0.24025366\n",
      "Iteration 128, loss = 0.24013751\n",
      "Iteration 129, loss = 0.24006297\n",
      "Iteration 130, loss = 0.23996342\n",
      "Iteration 131, loss = 0.23990849\n",
      "Iteration 132, loss = 0.23977984\n",
      "Iteration 133, loss = 0.23973126\n",
      "Iteration 134, loss = 0.23963262\n",
      "Iteration 135, loss = 0.23953997\n",
      "Iteration 136, loss = 0.23943590\n",
      "Iteration 137, loss = 0.23939161\n",
      "Iteration 138, loss = 0.23928970\n",
      "Iteration 139, loss = 0.23920325\n",
      "Iteration 140, loss = 0.23909787\n",
      "Iteration 141, loss = 0.23906753\n",
      "Iteration 142, loss = 0.23898596\n",
      "Iteration 143, loss = 0.23895715\n",
      "Iteration 144, loss = 0.23884283\n",
      "Iteration 145, loss = 0.23878083\n",
      "Iteration 146, loss = 0.23869583\n",
      "Iteration 147, loss = 0.23863663\n",
      "Iteration 148, loss = 0.23858843\n",
      "Iteration 149, loss = 0.23847160\n",
      "Iteration 150, loss = 0.23844147\n",
      "Iteration 151, loss = 0.23837355\n",
      "Iteration 152, loss = 0.23832720\n",
      "Iteration 153, loss = 0.23825902\n",
      "Iteration 154, loss = 0.23815860\n",
      "Iteration 155, loss = 0.23812542\n",
      "Iteration 156, loss = 0.23805120\n",
      "Iteration 157, loss = 0.23799440\n",
      "Iteration 158, loss = 0.23793336\n",
      "Iteration 159, loss = 0.23791520\n",
      "Iteration 160, loss = 0.23787021\n",
      "Iteration 161, loss = 0.23776064\n",
      "Iteration 162, loss = 0.23773745\n",
      "Iteration 163, loss = 0.23764623\n",
      "Iteration 164, loss = 0.23761176\n",
      "Iteration 165, loss = 0.23755471\n",
      "Iteration 166, loss = 0.23750380\n",
      "Iteration 167, loss = 0.23745857\n",
      "Iteration 168, loss = 0.23740148\n",
      "Iteration 169, loss = 0.23737810\n",
      "Iteration 170, loss = 0.23734994\n",
      "Iteration 171, loss = 0.23729629\n",
      "Iteration 172, loss = 0.23724823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77162242\n",
      "Iteration 2, loss = 0.67748617\n",
      "Iteration 3, loss = 0.58794400\n",
      "Iteration 4, loss = 0.52254080\n",
      "Iteration 5, loss = 0.47424964\n",
      "Iteration 6, loss = 0.43810525\n",
      "Iteration 7, loss = 0.41112077\n",
      "Iteration 8, loss = 0.39000980\n",
      "Iteration 9, loss = 0.37303759\n",
      "Iteration 10, loss = 0.35936038\n",
      "Iteration 11, loss = 0.34800461\n",
      "Iteration 12, loss = 0.33826141\n",
      "Iteration 13, loss = 0.33030194\n",
      "Iteration 14, loss = 0.32318889\n",
      "Iteration 15, loss = 0.31712944\n",
      "Iteration 16, loss = 0.31161325\n",
      "Iteration 17, loss = 0.30698921\n",
      "Iteration 18, loss = 0.30281375\n",
      "Iteration 19, loss = 0.29895039\n",
      "Iteration 20, loss = 0.29566227\n",
      "Iteration 21, loss = 0.29272317\n",
      "Iteration 22, loss = 0.29004910\n",
      "Iteration 23, loss = 0.28763821\n",
      "Iteration 24, loss = 0.28548240\n",
      "Iteration 25, loss = 0.28348886\n",
      "Iteration 26, loss = 0.28169077\n",
      "Iteration 27, loss = 0.28002608\n",
      "Iteration 28, loss = 0.27850813\n",
      "Iteration 29, loss = 0.27707138\n",
      "Iteration 30, loss = 0.27577074\n",
      "Iteration 31, loss = 0.27455160\n",
      "Iteration 32, loss = 0.27341106\n",
      "Iteration 33, loss = 0.27237194\n",
      "Iteration 34, loss = 0.27137795\n",
      "Iteration 35, loss = 0.27042952\n",
      "Iteration 36, loss = 0.26955236\n",
      "Iteration 37, loss = 0.26869121\n",
      "Iteration 38, loss = 0.26792249\n",
      "Iteration 39, loss = 0.26715706\n",
      "Iteration 40, loss = 0.26644204\n",
      "Iteration 41, loss = 0.26579624\n",
      "Iteration 42, loss = 0.26515180\n",
      "Iteration 43, loss = 0.26451052\n",
      "Iteration 44, loss = 0.26392944\n",
      "Iteration 45, loss = 0.26334874\n",
      "Iteration 46, loss = 0.26285923\n",
      "Iteration 47, loss = 0.26229824\n",
      "Iteration 48, loss = 0.26183149\n",
      "Iteration 49, loss = 0.26132680\n",
      "Iteration 50, loss = 0.26089601\n",
      "Iteration 51, loss = 0.26043373\n",
      "Iteration 52, loss = 0.25999001\n",
      "Iteration 53, loss = 0.25959219\n",
      "Iteration 54, loss = 0.25924199\n",
      "Iteration 55, loss = 0.25877991\n",
      "Iteration 56, loss = 0.25840747\n",
      "Iteration 57, loss = 0.25810189\n",
      "Iteration 58, loss = 0.25774067\n",
      "Iteration 59, loss = 0.25738061\n",
      "Iteration 60, loss = 0.25705716\n",
      "Iteration 61, loss = 0.25674988\n",
      "Iteration 62, loss = 0.25643373\n",
      "Iteration 63, loss = 0.25611206\n",
      "Iteration 64, loss = 0.25581203\n",
      "Iteration 65, loss = 0.25553383\n",
      "Iteration 66, loss = 0.25531087\n",
      "Iteration 67, loss = 0.25497498\n",
      "Iteration 68, loss = 0.25471798\n",
      "Iteration 69, loss = 0.25444013\n",
      "Iteration 70, loss = 0.25418458\n",
      "Iteration 71, loss = 0.25397418\n",
      "Iteration 72, loss = 0.25373106\n",
      "Iteration 73, loss = 0.25346929\n",
      "Iteration 74, loss = 0.25326488\n",
      "Iteration 75, loss = 0.25304513\n",
      "Iteration 76, loss = 0.25277710\n",
      "Iteration 77, loss = 0.25257649\n",
      "Iteration 78, loss = 0.25235960\n",
      "Iteration 79, loss = 0.25213905\n",
      "Iteration 80, loss = 0.25195508\n",
      "Iteration 81, loss = 0.25173276\n",
      "Iteration 82, loss = 0.25157554\n",
      "Iteration 83, loss = 0.25135097\n",
      "Iteration 84, loss = 0.25114059\n",
      "Iteration 85, loss = 0.25099414\n",
      "Iteration 86, loss = 0.25078438\n",
      "Iteration 87, loss = 0.25063867\n",
      "Iteration 88, loss = 0.25048078\n",
      "Iteration 89, loss = 0.25030966\n",
      "Iteration 90, loss = 0.25013444\n",
      "Iteration 91, loss = 0.24995540\n",
      "Iteration 92, loss = 0.24978648\n",
      "Iteration 93, loss = 0.24960802\n",
      "Iteration 94, loss = 0.24945377\n",
      "Iteration 95, loss = 0.24929919\n",
      "Iteration 96, loss = 0.24915854\n",
      "Iteration 97, loss = 0.24901201\n",
      "Iteration 98, loss = 0.24887977\n",
      "Iteration 99, loss = 0.24870531\n",
      "Iteration 100, loss = 0.24860670\n",
      "Iteration 101, loss = 0.24847814\n",
      "Iteration 102, loss = 0.24831321\n",
      "Iteration 103, loss = 0.24816944\n",
      "Iteration 104, loss = 0.24804513\n",
      "Iteration 105, loss = 0.24791983\n",
      "Iteration 106, loss = 0.24778205\n",
      "Iteration 107, loss = 0.24771180\n",
      "Iteration 108, loss = 0.24755563\n",
      "Iteration 109, loss = 0.24744664\n",
      "Iteration 110, loss = 0.24731147\n",
      "Iteration 111, loss = 0.24720130\n",
      "Iteration 112, loss = 0.24708820\n",
      "Iteration 113, loss = 0.24700698\n",
      "Iteration 114, loss = 0.24685607\n",
      "Iteration 115, loss = 0.24673173\n",
      "Iteration 116, loss = 0.24663488\n",
      "Iteration 117, loss = 0.24653201\n",
      "Iteration 118, loss = 0.24642329\n",
      "Iteration 119, loss = 0.24635669\n",
      "Iteration 120, loss = 0.24624392\n",
      "Iteration 121, loss = 0.24610883\n",
      "Iteration 122, loss = 0.24606603\n",
      "Iteration 123, loss = 0.24593718\n",
      "Iteration 124, loss = 0.24581064\n",
      "Iteration 125, loss = 0.24579858\n",
      "Iteration 126, loss = 0.24566322\n",
      "Iteration 127, loss = 0.24561003\n",
      "Iteration 128, loss = 0.24548411\n",
      "Iteration 129, loss = 0.24540366\n",
      "Iteration 130, loss = 0.24530654\n",
      "Iteration 131, loss = 0.24526129\n",
      "Iteration 132, loss = 0.24516503\n",
      "Iteration 133, loss = 0.24507217\n",
      "Iteration 134, loss = 0.24498773\n",
      "Iteration 135, loss = 0.24490699\n",
      "Iteration 136, loss = 0.24483122\n",
      "Iteration 137, loss = 0.24475472\n",
      "Iteration 138, loss = 0.24469674\n",
      "Iteration 139, loss = 0.24462868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77396419\n",
      "Iteration 2, loss = 0.67924684\n",
      "Iteration 3, loss = 0.59064307\n",
      "Iteration 4, loss = 0.52431238\n",
      "Iteration 5, loss = 0.47595306\n",
      "Iteration 6, loss = 0.44023040\n",
      "Iteration 7, loss = 0.41301107\n",
      "Iteration 8, loss = 0.39152446\n",
      "Iteration 9, loss = 0.37451441\n",
      "Iteration 10, loss = 0.36061268\n",
      "Iteration 11, loss = 0.34913668\n",
      "Iteration 12, loss = 0.33930013\n",
      "Iteration 13, loss = 0.33111863\n",
      "Iteration 14, loss = 0.32388238\n",
      "Iteration 15, loss = 0.31753440\n",
      "Iteration 16, loss = 0.31209198\n",
      "Iteration 17, loss = 0.30720011\n",
      "Iteration 18, loss = 0.30283838\n",
      "Iteration 19, loss = 0.29898381\n",
      "Iteration 20, loss = 0.29554890\n",
      "Iteration 21, loss = 0.29247935\n",
      "Iteration 22, loss = 0.28968691\n",
      "Iteration 23, loss = 0.28717232\n",
      "Iteration 24, loss = 0.28490107\n",
      "Iteration 25, loss = 0.28287012\n",
      "Iteration 26, loss = 0.28092559\n",
      "Iteration 27, loss = 0.27921937\n",
      "Iteration 28, loss = 0.27761195\n",
      "Iteration 29, loss = 0.27613296\n",
      "Iteration 30, loss = 0.27467660\n",
      "Iteration 31, loss = 0.27343148\n",
      "Iteration 32, loss = 0.27216714\n",
      "Iteration 33, loss = 0.27106651\n",
      "Iteration 34, loss = 0.26997822\n",
      "Iteration 35, loss = 0.26897984\n",
      "Iteration 36, loss = 0.26801564\n",
      "Iteration 37, loss = 0.26707399\n",
      "Iteration 38, loss = 0.26629388\n",
      "Iteration 39, loss = 0.26543506\n",
      "Iteration 40, loss = 0.26465681\n",
      "Iteration 41, loss = 0.26392981\n",
      "Iteration 42, loss = 0.26321110\n",
      "Iteration 43, loss = 0.26253895\n",
      "Iteration 44, loss = 0.26191423\n",
      "Iteration 45, loss = 0.26125555\n",
      "Iteration 46, loss = 0.26070656\n",
      "Iteration 47, loss = 0.26012163\n",
      "Iteration 48, loss = 0.25952397\n",
      "Iteration 49, loss = 0.25904259\n",
      "Iteration 50, loss = 0.25850955\n",
      "Iteration 51, loss = 0.25801200\n",
      "Iteration 52, loss = 0.25755328\n",
      "Iteration 53, loss = 0.25707023\n",
      "Iteration 54, loss = 0.25661626\n",
      "Iteration 55, loss = 0.25614743\n",
      "Iteration 56, loss = 0.25575895\n",
      "Iteration 57, loss = 0.25534280\n",
      "Iteration 58, loss = 0.25497255\n",
      "Iteration 59, loss = 0.25455672\n",
      "Iteration 60, loss = 0.25418875\n",
      "Iteration 61, loss = 0.25384627\n",
      "Iteration 62, loss = 0.25345335\n",
      "Iteration 63, loss = 0.25312663\n",
      "Iteration 64, loss = 0.25277019\n",
      "Iteration 65, loss = 0.25247189\n",
      "Iteration 66, loss = 0.25211189\n",
      "Iteration 67, loss = 0.25181295\n",
      "Iteration 68, loss = 0.25151116\n",
      "Iteration 69, loss = 0.25119809\n",
      "Iteration 70, loss = 0.25091411\n",
      "Iteration 71, loss = 0.25064498\n",
      "Iteration 72, loss = 0.25039069\n",
      "Iteration 73, loss = 0.25007126\n",
      "Iteration 74, loss = 0.24979351\n",
      "Iteration 75, loss = 0.24957042\n",
      "Iteration 76, loss = 0.24931327\n",
      "Iteration 77, loss = 0.24901884\n",
      "Iteration 78, loss = 0.24880539\n",
      "Iteration 79, loss = 0.24855575\n",
      "Iteration 80, loss = 0.24829465\n",
      "Iteration 81, loss = 0.24809591\n",
      "Iteration 82, loss = 0.24794553\n",
      "Iteration 83, loss = 0.24762585\n",
      "Iteration 84, loss = 0.24743288\n",
      "Iteration 85, loss = 0.24717629\n",
      "Iteration 86, loss = 0.24700888\n",
      "Iteration 87, loss = 0.24678323\n",
      "Iteration 88, loss = 0.24657337\n",
      "Iteration 89, loss = 0.24637415\n",
      "Iteration 90, loss = 0.24617821\n",
      "Iteration 91, loss = 0.24597776\n",
      "Iteration 92, loss = 0.24583481\n",
      "Iteration 93, loss = 0.24558635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 0.24541253\n",
      "Iteration 95, loss = 0.24523283\n",
      "Iteration 96, loss = 0.24510355\n",
      "Iteration 97, loss = 0.24490499\n",
      "Iteration 98, loss = 0.24469130\n",
      "Iteration 99, loss = 0.24454513\n",
      "Iteration 100, loss = 0.24443525\n",
      "Iteration 101, loss = 0.24422617\n",
      "Iteration 102, loss = 0.24409427\n",
      "Iteration 103, loss = 0.24395753\n",
      "Iteration 104, loss = 0.24373743\n",
      "Iteration 105, loss = 0.24359755\n",
      "Iteration 106, loss = 0.24345566\n",
      "Iteration 107, loss = 0.24331012\n",
      "Iteration 108, loss = 0.24316011\n",
      "Iteration 109, loss = 0.24301564\n",
      "Iteration 110, loss = 0.24289101\n",
      "Iteration 111, loss = 0.24280170\n",
      "Iteration 112, loss = 0.24263017\n",
      "Iteration 113, loss = 0.24248945\n",
      "Iteration 114, loss = 0.24236798\n",
      "Iteration 115, loss = 0.24224255\n",
      "Iteration 116, loss = 0.24211180\n",
      "Iteration 117, loss = 0.24198972\n",
      "Iteration 118, loss = 0.24183951\n",
      "Iteration 119, loss = 0.24177791\n",
      "Iteration 120, loss = 0.24164347\n",
      "Iteration 121, loss = 0.24153338\n",
      "Iteration 122, loss = 0.24140723\n",
      "Iteration 123, loss = 0.24132034\n",
      "Iteration 124, loss = 0.24122659\n",
      "Iteration 125, loss = 0.24109011\n",
      "Iteration 126, loss = 0.24097734\n",
      "Iteration 127, loss = 0.24088642\n",
      "Iteration 128, loss = 0.24079304\n",
      "Iteration 129, loss = 0.24068130\n",
      "Iteration 130, loss = 0.24064704\n",
      "Iteration 131, loss = 0.24049707\n",
      "Iteration 132, loss = 0.24040226\n",
      "Iteration 133, loss = 0.24032465\n",
      "Iteration 134, loss = 0.24031278\n",
      "Iteration 135, loss = 0.24010542\n",
      "Iteration 136, loss = 0.24003390\n",
      "Iteration 137, loss = 0.23996522\n",
      "Iteration 138, loss = 0.23985934\n",
      "Iteration 139, loss = 0.23977507\n",
      "Iteration 140, loss = 0.23971878\n",
      "Iteration 141, loss = 0.23964631\n",
      "Iteration 142, loss = 0.23953032\n",
      "Iteration 143, loss = 0.23948983\n",
      "Iteration 144, loss = 0.23943618\n",
      "Iteration 145, loss = 0.23932395\n",
      "Iteration 146, loss = 0.23924791\n",
      "Iteration 147, loss = 0.23919469\n",
      "Iteration 148, loss = 0.23912268\n",
      "Iteration 149, loss = 0.23905749\n",
      "Iteration 150, loss = 0.23897354\n",
      "Iteration 151, loss = 0.23890340\n",
      "Iteration 152, loss = 0.23893190\n",
      "Iteration 153, loss = 0.23878225\n",
      "Iteration 154, loss = 0.23871877\n",
      "Iteration 155, loss = 0.23866763\n",
      "Iteration 156, loss = 0.23860036\n",
      "Iteration 157, loss = 0.23856370\n",
      "Iteration 158, loss = 0.23845097\n",
      "Iteration 159, loss = 0.23843491\n",
      "Iteration 160, loss = 0.23833677\n",
      "Iteration 161, loss = 0.23831034\n",
      "Iteration 162, loss = 0.23827378\n",
      "Iteration 163, loss = 0.23818179\n",
      "Iteration 164, loss = 0.23815517\n",
      "Iteration 165, loss = 0.23808845\n",
      "Iteration 166, loss = 0.23806241\n",
      "Iteration 167, loss = 0.23799862\n",
      "Iteration 168, loss = 0.23793844\n",
      "Iteration 169, loss = 0.23793060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77548385\n",
      "Iteration 2, loss = 0.67807468\n",
      "Iteration 3, loss = 0.58731450\n",
      "Iteration 4, loss = 0.52011004\n",
      "Iteration 5, loss = 0.47158008\n",
      "Iteration 6, loss = 0.43570167\n",
      "Iteration 7, loss = 0.40896748\n",
      "Iteration 8, loss = 0.38771395\n",
      "Iteration 9, loss = 0.37125583\n",
      "Iteration 10, loss = 0.35795663\n",
      "Iteration 11, loss = 0.34689467\n",
      "Iteration 12, loss = 0.33758150\n",
      "Iteration 13, loss = 0.32985023\n",
      "Iteration 14, loss = 0.32303259\n",
      "Iteration 15, loss = 0.31717184\n",
      "Iteration 16, loss = 0.31201987\n",
      "Iteration 17, loss = 0.30752697\n",
      "Iteration 18, loss = 0.30350440\n",
      "Iteration 19, loss = 0.29989328\n",
      "Iteration 20, loss = 0.29661687\n",
      "Iteration 21, loss = 0.29379735\n",
      "Iteration 22, loss = 0.29108433\n",
      "Iteration 23, loss = 0.28858655\n",
      "Iteration 24, loss = 0.28641814\n",
      "Iteration 25, loss = 0.28439403\n",
      "Iteration 26, loss = 0.28250781\n",
      "Iteration 27, loss = 0.28080955\n",
      "Iteration 28, loss = 0.27923430\n",
      "Iteration 29, loss = 0.27781864\n",
      "Iteration 30, loss = 0.27638342\n",
      "Iteration 31, loss = 0.27516244\n",
      "Iteration 32, loss = 0.27397449\n",
      "Iteration 33, loss = 0.27294068\n",
      "Iteration 34, loss = 0.27189941\n",
      "Iteration 35, loss = 0.27095951\n",
      "Iteration 36, loss = 0.27006737\n",
      "Iteration 37, loss = 0.26923233\n",
      "Iteration 38, loss = 0.26842920\n",
      "Iteration 39, loss = 0.26768071\n",
      "Iteration 40, loss = 0.26694640\n",
      "Iteration 41, loss = 0.26626385\n",
      "Iteration 42, loss = 0.26564205\n",
      "Iteration 43, loss = 0.26501697\n",
      "Iteration 44, loss = 0.26445481\n",
      "Iteration 45, loss = 0.26386410\n",
      "Iteration 46, loss = 0.26335200\n",
      "Iteration 47, loss = 0.26285148\n",
      "Iteration 48, loss = 0.26230126\n",
      "Iteration 49, loss = 0.26188691\n",
      "Iteration 50, loss = 0.26140155\n",
      "Iteration 51, loss = 0.26102416\n",
      "Iteration 52, loss = 0.26057559\n",
      "Iteration 53, loss = 0.26013972\n",
      "Iteration 54, loss = 0.25971011\n",
      "Iteration 55, loss = 0.25932332\n",
      "Iteration 56, loss = 0.25896541\n",
      "Iteration 57, loss = 0.25862431\n",
      "Iteration 58, loss = 0.25828943\n",
      "Iteration 59, loss = 0.25792633\n",
      "Iteration 60, loss = 0.25760133\n",
      "Iteration 61, loss = 0.25729476\n",
      "Iteration 62, loss = 0.25695324\n",
      "Iteration 63, loss = 0.25668572\n",
      "Iteration 64, loss = 0.25636169\n",
      "Iteration 65, loss = 0.25612510\n",
      "Iteration 66, loss = 0.25581963\n",
      "Iteration 67, loss = 0.25555290\n",
      "Iteration 68, loss = 0.25527289\n",
      "Iteration 69, loss = 0.25502030\n",
      "Iteration 70, loss = 0.25479090\n",
      "Iteration 71, loss = 0.25453746\n",
      "Iteration 72, loss = 0.25428596\n",
      "Iteration 73, loss = 0.25402547\n",
      "Iteration 74, loss = 0.25381118\n",
      "Iteration 75, loss = 0.25357496\n",
      "Iteration 76, loss = 0.25338521\n",
      "Iteration 77, loss = 0.25316984\n",
      "Iteration 78, loss = 0.25297858\n",
      "Iteration 79, loss = 0.25273164\n",
      "Iteration 80, loss = 0.25255430\n",
      "Iteration 81, loss = 0.25235874\n",
      "Iteration 82, loss = 0.25222109\n",
      "Iteration 83, loss = 0.25198306\n",
      "Iteration 84, loss = 0.25177482\n",
      "Iteration 85, loss = 0.25153783\n",
      "Iteration 86, loss = 0.25141728\n",
      "Iteration 87, loss = 0.25122483\n",
      "Iteration 88, loss = 0.25104647\n",
      "Iteration 89, loss = 0.25087372\n",
      "Iteration 90, loss = 0.25069418\n",
      "Iteration 91, loss = 0.25055126\n",
      "Iteration 92, loss = 0.25039163\n",
      "Iteration 93, loss = 0.25020523\n",
      "Iteration 94, loss = 0.25007704\n",
      "Iteration 95, loss = 0.24993050\n",
      "Iteration 96, loss = 0.24985388\n",
      "Iteration 97, loss = 0.24962590\n",
      "Iteration 98, loss = 0.24948744\n",
      "Iteration 99, loss = 0.24933451\n",
      "Iteration 100, loss = 0.24921940\n",
      "Iteration 101, loss = 0.24909656\n",
      "Iteration 102, loss = 0.24895208\n",
      "Iteration 103, loss = 0.24883832\n",
      "Iteration 104, loss = 0.24866179\n",
      "Iteration 105, loss = 0.24854293\n",
      "Iteration 106, loss = 0.24841147\n",
      "Iteration 107, loss = 0.24829223\n",
      "Iteration 108, loss = 0.24821452\n",
      "Iteration 109, loss = 0.24805869\n",
      "Iteration 110, loss = 0.24795457\n",
      "Iteration 111, loss = 0.24787893\n",
      "Iteration 112, loss = 0.24773596\n",
      "Iteration 113, loss = 0.24762642\n",
      "Iteration 114, loss = 0.24750009\n",
      "Iteration 115, loss = 0.24740648\n",
      "Iteration 116, loss = 0.24730307\n",
      "Iteration 117, loss = 0.24719439\n",
      "Iteration 118, loss = 0.24706762\n",
      "Iteration 119, loss = 0.24701157\n",
      "Iteration 120, loss = 0.24688978\n",
      "Iteration 121, loss = 0.24681654\n",
      "Iteration 122, loss = 0.24673610\n",
      "Iteration 123, loss = 0.24662898\n",
      "Iteration 124, loss = 0.24655636\n",
      "Iteration 125, loss = 0.24643610\n",
      "Iteration 126, loss = 0.24636794\n",
      "Iteration 127, loss = 0.24627333\n",
      "Iteration 128, loss = 0.24619894\n",
      "Iteration 129, loss = 0.24611070\n",
      "Iteration 130, loss = 0.24606641\n",
      "Iteration 131, loss = 0.24593192\n",
      "Iteration 132, loss = 0.24586200\n",
      "Iteration 133, loss = 0.24579692\n",
      "Iteration 134, loss = 0.24576567\n",
      "Iteration 135, loss = 0.24562661\n",
      "Iteration 136, loss = 0.24556396\n",
      "Iteration 137, loss = 0.24547074\n",
      "Iteration 138, loss = 0.24542161\n",
      "Iteration 139, loss = 0.24534559\n",
      "Iteration 140, loss = 0.24531433\n",
      "Iteration 141, loss = 0.24521140\n",
      "Iteration 142, loss = 0.24515810\n",
      "Iteration 143, loss = 0.24511259\n",
      "Iteration 144, loss = 0.24504666\n",
      "Iteration 145, loss = 0.24496538\n",
      "Iteration 146, loss = 0.24488905\n",
      "Iteration 147, loss = 0.24486735\n",
      "Iteration 148, loss = 0.24480829\n",
      "Iteration 149, loss = 0.24473339\n",
      "Iteration 150, loss = 0.24470319\n",
      "Iteration 151, loss = 0.24463695\n",
      "Iteration 152, loss = 0.24462325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67584268\n",
      "Iteration 2, loss = 0.50356848\n",
      "Iteration 3, loss = 0.41796032\n",
      "Iteration 4, loss = 0.36516432\n",
      "Iteration 5, loss = 0.32934988\n",
      "Iteration 6, loss = 0.30499434\n",
      "Iteration 7, loss = 0.28882327\n",
      "Iteration 8, loss = 0.27676495\n",
      "Iteration 9, loss = 0.26783581\n",
      "Iteration 10, loss = 0.26169172\n",
      "Iteration 11, loss = 0.25661112\n",
      "Iteration 12, loss = 0.25235950\n",
      "Iteration 13, loss = 0.24993012\n",
      "Iteration 14, loss = 0.24755039\n",
      "Iteration 15, loss = 0.24558002\n",
      "Iteration 16, loss = 0.24437925\n",
      "Iteration 17, loss = 0.24256278\n",
      "Iteration 18, loss = 0.24210405\n",
      "Iteration 19, loss = 0.24123513\n",
      "Iteration 20, loss = 0.24104741\n",
      "Iteration 21, loss = 0.23974902\n",
      "Iteration 22, loss = 0.24047266\n",
      "Iteration 23, loss = 0.23925001\n",
      "Iteration 24, loss = 0.23890113\n",
      "Iteration 25, loss = 0.23896817\n",
      "Iteration 26, loss = 0.23892780\n",
      "Iteration 27, loss = 0.23876636\n",
      "Iteration 28, loss = 0.23918162\n",
      "Iteration 29, loss = 0.23799657\n",
      "Iteration 30, loss = 0.23874357\n",
      "Iteration 31, loss = 0.23833593\n",
      "Iteration 32, loss = 0.23813490\n",
      "Iteration 33, loss = 0.23769096\n",
      "Iteration 34, loss = 0.23812259\n",
      "Iteration 35, loss = 0.23897020\n",
      "Iteration 36, loss = 0.23887180\n",
      "Iteration 37, loss = 0.23792157\n",
      "Iteration 38, loss = 0.23805495\n",
      "Iteration 39, loss = 0.23817928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.23904807\n",
      "Iteration 41, loss = 0.23839498\n",
      "Iteration 42, loss = 0.23850699\n",
      "Iteration 43, loss = 0.23806148\n",
      "Iteration 44, loss = 0.23856890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67304770\n",
      "Iteration 2, loss = 0.50433898\n",
      "Iteration 3, loss = 0.41731498\n",
      "Iteration 4, loss = 0.36232805\n",
      "Iteration 5, loss = 0.32588992\n",
      "Iteration 6, loss = 0.30189743\n",
      "Iteration 7, loss = 0.28579252\n",
      "Iteration 8, loss = 0.27383832\n",
      "Iteration 9, loss = 0.26546460\n",
      "Iteration 10, loss = 0.25957136\n",
      "Iteration 11, loss = 0.25457682\n",
      "Iteration 12, loss = 0.25067952\n",
      "Iteration 13, loss = 0.24806590\n",
      "Iteration 14, loss = 0.24559105\n",
      "Iteration 15, loss = 0.24353725\n",
      "Iteration 16, loss = 0.24229416\n",
      "Iteration 17, loss = 0.24155053\n",
      "Iteration 18, loss = 0.24035573\n",
      "Iteration 19, loss = 0.23948445\n",
      "Iteration 20, loss = 0.23873145\n",
      "Iteration 21, loss = 0.23836684\n",
      "Iteration 22, loss = 0.23835335\n",
      "Iteration 23, loss = 0.23780432\n",
      "Iteration 24, loss = 0.23749213\n",
      "Iteration 25, loss = 0.23716467\n",
      "Iteration 26, loss = 0.23710940\n",
      "Iteration 27, loss = 0.23707918\n",
      "Iteration 28, loss = 0.23737830\n",
      "Iteration 29, loss = 0.23682540\n",
      "Iteration 30, loss = 0.23672156\n",
      "Iteration 31, loss = 0.23657747\n",
      "Iteration 32, loss = 0.23650168\n",
      "Iteration 33, loss = 0.23633356\n",
      "Iteration 34, loss = 0.23717706\n",
      "Iteration 35, loss = 0.23703393\n",
      "Iteration 36, loss = 0.23712044\n",
      "Iteration 37, loss = 0.23624404\n",
      "Iteration 38, loss = 0.23613027\n",
      "Iteration 39, loss = 0.23651676\n",
      "Iteration 40, loss = 0.23710180\n",
      "Iteration 41, loss = 0.23698546\n",
      "Iteration 42, loss = 0.23674054\n",
      "Iteration 43, loss = 0.23643111\n",
      "Iteration 44, loss = 0.23663126\n",
      "Iteration 45, loss = 0.23687257\n",
      "Iteration 46, loss = 0.23716954\n",
      "Iteration 47, loss = 0.23700543\n",
      "Iteration 48, loss = 0.23713369\n",
      "Iteration 49, loss = 0.23760620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67284065\n",
      "Iteration 2, loss = 0.50489963\n",
      "Iteration 3, loss = 0.41812080\n",
      "Iteration 4, loss = 0.36405662\n",
      "Iteration 5, loss = 0.32844317\n",
      "Iteration 6, loss = 0.30477212\n",
      "Iteration 7, loss = 0.28890108\n",
      "Iteration 8, loss = 0.27812306\n",
      "Iteration 9, loss = 0.26911723\n",
      "Iteration 10, loss = 0.26329326\n",
      "Iteration 11, loss = 0.25892378\n",
      "Iteration 12, loss = 0.25501057\n",
      "Iteration 13, loss = 0.25311961\n",
      "Iteration 14, loss = 0.25070309\n",
      "Iteration 15, loss = 0.24864851\n",
      "Iteration 16, loss = 0.24735704\n",
      "Iteration 17, loss = 0.24713883\n",
      "Iteration 18, loss = 0.24601234\n",
      "Iteration 19, loss = 0.24483677\n",
      "Iteration 20, loss = 0.24420844\n",
      "Iteration 21, loss = 0.24416709\n",
      "Iteration 22, loss = 0.24459275\n",
      "Iteration 23, loss = 0.24383251\n",
      "Iteration 24, loss = 0.24311701\n",
      "Iteration 25, loss = 0.24281650\n",
      "Iteration 26, loss = 0.24310332\n",
      "Iteration 27, loss = 0.24268075\n",
      "Iteration 28, loss = 0.24294886\n",
      "Iteration 29, loss = 0.24222970\n",
      "Iteration 30, loss = 0.24204471\n",
      "Iteration 31, loss = 0.24236660\n",
      "Iteration 32, loss = 0.24286692\n",
      "Iteration 33, loss = 0.24230108\n",
      "Iteration 34, loss = 0.24256923\n",
      "Iteration 35, loss = 0.24222695\n",
      "Iteration 36, loss = 0.24290887\n",
      "Iteration 37, loss = 0.24265029\n",
      "Iteration 38, loss = 0.24244820\n",
      "Iteration 39, loss = 0.24196619\n",
      "Iteration 40, loss = 0.24249528\n",
      "Iteration 41, loss = 0.24298945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67557537\n",
      "Iteration 2, loss = 0.50804544\n",
      "Iteration 3, loss = 0.42171332\n",
      "Iteration 4, loss = 0.36468111\n",
      "Iteration 5, loss = 0.32903431\n",
      "Iteration 6, loss = 0.30455427\n",
      "Iteration 7, loss = 0.28782259\n",
      "Iteration 8, loss = 0.27590425\n",
      "Iteration 9, loss = 0.26794082\n",
      "Iteration 10, loss = 0.26118288\n",
      "Iteration 11, loss = 0.25729124\n",
      "Iteration 12, loss = 0.25269552\n",
      "Iteration 13, loss = 0.24941476\n",
      "Iteration 14, loss = 0.24721220\n",
      "Iteration 15, loss = 0.24482975\n",
      "Iteration 16, loss = 0.24323419\n",
      "Iteration 17, loss = 0.24220643\n",
      "Iteration 18, loss = 0.24111162\n",
      "Iteration 19, loss = 0.24041337\n",
      "Iteration 20, loss = 0.23971726\n",
      "Iteration 21, loss = 0.23996502\n",
      "Iteration 22, loss = 0.23860419\n",
      "Iteration 23, loss = 0.23836725\n",
      "Iteration 24, loss = 0.23807318\n",
      "Iteration 25, loss = 0.23815569\n",
      "Iteration 26, loss = 0.23741541\n",
      "Iteration 27, loss = 0.23828291\n",
      "Iteration 28, loss = 0.23790439\n",
      "Iteration 29, loss = 0.23779996\n",
      "Iteration 30, loss = 0.23725230\n",
      "Iteration 31, loss = 0.23737151\n",
      "Iteration 32, loss = 0.23741870\n",
      "Iteration 33, loss = 0.23708025\n",
      "Iteration 34, loss = 0.23751223\n",
      "Iteration 35, loss = 0.23730693\n",
      "Iteration 36, loss = 0.23677387\n",
      "Iteration 37, loss = 0.23713107\n",
      "Iteration 38, loss = 0.23685588\n",
      "Iteration 39, loss = 0.23727170\n",
      "Iteration 40, loss = 0.23805314\n",
      "Iteration 41, loss = 0.23701625\n",
      "Iteration 42, loss = 0.23702888\n",
      "Iteration 43, loss = 0.23748100\n",
      "Iteration 44, loss = 0.23789063\n",
      "Iteration 45, loss = 0.23681322\n",
      "Iteration 46, loss = 0.23783346\n",
      "Iteration 47, loss = 0.23700935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67057719\n",
      "Iteration 2, loss = 0.50300760\n",
      "Iteration 3, loss = 0.41851634\n",
      "Iteration 4, loss = 0.36458728\n",
      "Iteration 5, loss = 0.33050103\n",
      "Iteration 6, loss = 0.30557872\n",
      "Iteration 7, loss = 0.28965490\n",
      "Iteration 8, loss = 0.27790313\n",
      "Iteration 9, loss = 0.27063619\n",
      "Iteration 10, loss = 0.26497443\n",
      "Iteration 11, loss = 0.26134365\n",
      "Iteration 12, loss = 0.25711132\n",
      "Iteration 13, loss = 0.25447959\n",
      "Iteration 14, loss = 0.25170837\n",
      "Iteration 15, loss = 0.24985401\n",
      "Iteration 16, loss = 0.24866262\n",
      "Iteration 17, loss = 0.24759565\n",
      "Iteration 18, loss = 0.24691964\n",
      "Iteration 19, loss = 0.24573491\n",
      "Iteration 20, loss = 0.24524781\n",
      "Iteration 21, loss = 0.24514489\n",
      "Iteration 22, loss = 0.24494210\n",
      "Iteration 23, loss = 0.24452743\n",
      "Iteration 24, loss = 0.24396117\n",
      "Iteration 25, loss = 0.24379740\n",
      "Iteration 26, loss = 0.24373939\n",
      "Iteration 27, loss = 0.24444494\n",
      "Iteration 28, loss = 0.24354618\n",
      "Iteration 29, loss = 0.24473712\n",
      "Iteration 30, loss = 0.24425876\n",
      "Iteration 31, loss = 0.24347558\n",
      "Iteration 32, loss = 0.24357085\n",
      "Iteration 33, loss = 0.24317451\n",
      "Iteration 34, loss = 0.24410284\n",
      "Iteration 35, loss = 0.24379442\n",
      "Iteration 36, loss = 0.24311469\n",
      "Iteration 37, loss = 0.24409784\n",
      "Iteration 38, loss = 0.24309928\n",
      "Iteration 39, loss = 0.24337930\n",
      "Iteration 40, loss = 0.24372134\n",
      "Iteration 41, loss = 0.24301622\n",
      "Iteration 42, loss = 0.24346717\n",
      "Iteration 43, loss = 0.24352875\n",
      "Iteration 44, loss = 0.24394637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76838965\n",
      "Iteration 2, loss = 0.67322147\n",
      "Iteration 3, loss = 0.58455563\n",
      "Iteration 4, loss = 0.51830839\n",
      "Iteration 5, loss = 0.47067279\n",
      "Iteration 6, loss = 0.43473387\n",
      "Iteration 7, loss = 0.40788960\n",
      "Iteration 8, loss = 0.38676616\n",
      "Iteration 9, loss = 0.37013198\n",
      "Iteration 10, loss = 0.35659617\n",
      "Iteration 11, loss = 0.34547694\n",
      "Iteration 12, loss = 0.33597431\n",
      "Iteration 13, loss = 0.32823223\n",
      "Iteration 14, loss = 0.32137986\n",
      "Iteration 15, loss = 0.31549914\n",
      "Iteration 16, loss = 0.31036968\n",
      "Iteration 17, loss = 0.30579954\n",
      "Iteration 18, loss = 0.30183204\n",
      "Iteration 19, loss = 0.29816244\n",
      "Iteration 20, loss = 0.29490583\n",
      "Iteration 21, loss = 0.29207313\n",
      "Iteration 22, loss = 0.28940235\n",
      "Iteration 23, loss = 0.28693011\n",
      "Iteration 24, loss = 0.28472920\n",
      "Iteration 25, loss = 0.28272471\n",
      "Iteration 26, loss = 0.28080601\n",
      "Iteration 27, loss = 0.27907884\n",
      "Iteration 28, loss = 0.27750287\n",
      "Iteration 29, loss = 0.27593986\n",
      "Iteration 30, loss = 0.27455294\n",
      "Iteration 31, loss = 0.27321613\n",
      "Iteration 32, loss = 0.27201176\n",
      "Iteration 33, loss = 0.27084198\n",
      "Iteration 34, loss = 0.26975883\n",
      "Iteration 35, loss = 0.26876750\n",
      "Iteration 36, loss = 0.26778725\n",
      "Iteration 37, loss = 0.26678487\n",
      "Iteration 38, loss = 0.26596016\n",
      "Iteration 39, loss = 0.26509941\n",
      "Iteration 40, loss = 0.26431628\n",
      "Iteration 41, loss = 0.26357850\n",
      "Iteration 42, loss = 0.26285598\n",
      "Iteration 43, loss = 0.26212337\n",
      "Iteration 44, loss = 0.26149126\n",
      "Iteration 45, loss = 0.26087443\n",
      "Iteration 46, loss = 0.26023170\n",
      "Iteration 47, loss = 0.25972476\n",
      "Iteration 48, loss = 0.25912478\n",
      "Iteration 49, loss = 0.25858811\n",
      "Iteration 50, loss = 0.25805123\n",
      "Iteration 51, loss = 0.25755809\n",
      "Iteration 52, loss = 0.25712229\n",
      "Iteration 53, loss = 0.25661345\n",
      "Iteration 54, loss = 0.25620052\n",
      "Iteration 55, loss = 0.25574567\n",
      "Iteration 56, loss = 0.25533246\n",
      "Iteration 57, loss = 0.25498470\n",
      "Iteration 58, loss = 0.25456208\n",
      "Iteration 59, loss = 0.25416851\n",
      "Iteration 60, loss = 0.25382125\n",
      "Iteration 61, loss = 0.25346378\n",
      "Iteration 62, loss = 0.25312268\n",
      "Iteration 63, loss = 0.25280868\n",
      "Iteration 64, loss = 0.25247887\n",
      "Iteration 65, loss = 0.25214812\n",
      "Iteration 66, loss = 0.25188412\n",
      "Iteration 67, loss = 0.25153139\n",
      "Iteration 68, loss = 0.25122737\n",
      "Iteration 69, loss = 0.25099495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.25066628\n",
      "Iteration 71, loss = 0.25041672\n",
      "Iteration 72, loss = 0.25015523\n",
      "Iteration 73, loss = 0.24988474\n",
      "Iteration 74, loss = 0.24963614\n",
      "Iteration 75, loss = 0.24937469\n",
      "Iteration 76, loss = 0.24916378\n",
      "Iteration 77, loss = 0.24895604\n",
      "Iteration 78, loss = 0.24870007\n",
      "Iteration 79, loss = 0.24845004\n",
      "Iteration 80, loss = 0.24824100\n",
      "Iteration 81, loss = 0.24802990\n",
      "Iteration 82, loss = 0.24785912\n",
      "Iteration 83, loss = 0.24756558\n",
      "Iteration 84, loss = 0.24738389\n",
      "Iteration 85, loss = 0.24717876\n",
      "Iteration 86, loss = 0.24699610\n",
      "Iteration 87, loss = 0.24680416\n",
      "Iteration 88, loss = 0.24660469\n",
      "Iteration 89, loss = 0.24640648\n",
      "Iteration 90, loss = 0.24625637\n",
      "Iteration 91, loss = 0.24605652\n",
      "Iteration 92, loss = 0.24591722\n",
      "Iteration 93, loss = 0.24569506\n",
      "Iteration 94, loss = 0.24554549\n",
      "Iteration 95, loss = 0.24537602\n",
      "Iteration 96, loss = 0.24521974\n",
      "Iteration 97, loss = 0.24505169\n",
      "Iteration 98, loss = 0.24488407\n",
      "Iteration 99, loss = 0.24472131\n",
      "Iteration 100, loss = 0.24464193\n",
      "Iteration 101, loss = 0.24446220\n",
      "Iteration 102, loss = 0.24426143\n",
      "Iteration 103, loss = 0.24414671\n",
      "Iteration 104, loss = 0.24401795\n",
      "Iteration 105, loss = 0.24388174\n",
      "Iteration 106, loss = 0.24372599\n",
      "Iteration 107, loss = 0.24360303\n",
      "Iteration 108, loss = 0.24346761\n",
      "Iteration 109, loss = 0.24334066\n",
      "Iteration 110, loss = 0.24324197\n",
      "Iteration 111, loss = 0.24309748\n",
      "Iteration 112, loss = 0.24294622\n",
      "Iteration 113, loss = 0.24285583\n",
      "Iteration 114, loss = 0.24271774\n",
      "Iteration 115, loss = 0.24264930\n",
      "Iteration 116, loss = 0.24253338\n",
      "Iteration 117, loss = 0.24242884\n",
      "Iteration 118, loss = 0.24225069\n",
      "Iteration 119, loss = 0.24218209\n",
      "Iteration 120, loss = 0.24207441\n",
      "Iteration 121, loss = 0.24195359\n",
      "Iteration 122, loss = 0.24187229\n",
      "Iteration 123, loss = 0.24175782\n",
      "Iteration 124, loss = 0.24164909\n",
      "Iteration 125, loss = 0.24157255\n",
      "Iteration 126, loss = 0.24146991\n",
      "Iteration 127, loss = 0.24137943\n",
      "Iteration 128, loss = 0.24128509\n",
      "Iteration 129, loss = 0.24120646\n",
      "Iteration 130, loss = 0.24109183\n",
      "Iteration 131, loss = 0.24102271\n",
      "Iteration 132, loss = 0.24095683\n",
      "Iteration 133, loss = 0.24085652\n",
      "Iteration 134, loss = 0.24077423\n",
      "Iteration 135, loss = 0.24068966\n",
      "Iteration 136, loss = 0.24067414\n",
      "Iteration 137, loss = 0.24054808\n",
      "Iteration 138, loss = 0.24044678\n",
      "Iteration 139, loss = 0.24039368\n",
      "Iteration 140, loss = 0.24030921\n",
      "Iteration 141, loss = 0.24021759\n",
      "Iteration 142, loss = 0.24016053\n",
      "Iteration 143, loss = 0.24010399\n",
      "Iteration 144, loss = 0.24002391\n",
      "Iteration 145, loss = 0.23995621\n",
      "Iteration 146, loss = 0.23988403\n",
      "Iteration 147, loss = 0.23983703\n",
      "Iteration 148, loss = 0.23979439\n",
      "Iteration 149, loss = 0.23968224\n",
      "Iteration 150, loss = 0.23961946\n",
      "Iteration 151, loss = 0.23958603\n",
      "Iteration 152, loss = 0.23953980\n",
      "Iteration 153, loss = 0.23948641\n",
      "Iteration 154, loss = 0.23939828\n",
      "Iteration 155, loss = 0.23936098\n",
      "Iteration 156, loss = 0.23928269\n",
      "Iteration 157, loss = 0.23923103\n",
      "Iteration 158, loss = 0.23915980\n",
      "Iteration 159, loss = 0.23913325\n",
      "Iteration 160, loss = 0.23911405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76739801\n",
      "Iteration 2, loss = 0.67431107\n",
      "Iteration 3, loss = 0.58643882\n",
      "Iteration 4, loss = 0.52116738\n",
      "Iteration 5, loss = 0.47334060\n",
      "Iteration 6, loss = 0.43718487\n",
      "Iteration 7, loss = 0.41004918\n",
      "Iteration 8, loss = 0.38857306\n",
      "Iteration 9, loss = 0.37157304\n",
      "Iteration 10, loss = 0.35770321\n",
      "Iteration 11, loss = 0.34609169\n",
      "Iteration 12, loss = 0.33629650\n",
      "Iteration 13, loss = 0.32811238\n",
      "Iteration 14, loss = 0.32088115\n",
      "Iteration 15, loss = 0.31472670\n",
      "Iteration 16, loss = 0.30909596\n",
      "Iteration 17, loss = 0.30434647\n",
      "Iteration 18, loss = 0.30008115\n",
      "Iteration 19, loss = 0.29618054\n",
      "Iteration 20, loss = 0.29280246\n",
      "Iteration 21, loss = 0.28978137\n",
      "Iteration 22, loss = 0.28704282\n",
      "Iteration 23, loss = 0.28457616\n",
      "Iteration 24, loss = 0.28235153\n",
      "Iteration 25, loss = 0.28032719\n",
      "Iteration 26, loss = 0.27847760\n",
      "Iteration 27, loss = 0.27676216\n",
      "Iteration 28, loss = 0.27521743\n",
      "Iteration 29, loss = 0.27370001\n",
      "Iteration 30, loss = 0.27238923\n",
      "Iteration 31, loss = 0.27108032\n",
      "Iteration 32, loss = 0.26992102\n",
      "Iteration 33, loss = 0.26884035\n",
      "Iteration 34, loss = 0.26782254\n",
      "Iteration 35, loss = 0.26682495\n",
      "Iteration 36, loss = 0.26588112\n",
      "Iteration 37, loss = 0.26499375\n",
      "Iteration 38, loss = 0.26418645\n",
      "Iteration 39, loss = 0.26340974\n",
      "Iteration 40, loss = 0.26267635\n",
      "Iteration 41, loss = 0.26195284\n",
      "Iteration 42, loss = 0.26128560\n",
      "Iteration 43, loss = 0.26061065\n",
      "Iteration 44, loss = 0.26000095\n",
      "Iteration 45, loss = 0.25941422\n",
      "Iteration 46, loss = 0.25886964\n",
      "Iteration 47, loss = 0.25833924\n",
      "Iteration 48, loss = 0.25779649\n",
      "Iteration 49, loss = 0.25726891\n",
      "Iteration 50, loss = 0.25676971\n",
      "Iteration 51, loss = 0.25629368\n",
      "Iteration 52, loss = 0.25585892\n",
      "Iteration 53, loss = 0.25540314\n",
      "Iteration 54, loss = 0.25500575\n",
      "Iteration 55, loss = 0.25457851\n",
      "Iteration 56, loss = 0.25416274\n",
      "Iteration 57, loss = 0.25380913\n",
      "Iteration 58, loss = 0.25347852\n",
      "Iteration 59, loss = 0.25306061\n",
      "Iteration 60, loss = 0.25270123\n",
      "Iteration 61, loss = 0.25236575\n",
      "Iteration 62, loss = 0.25203838\n",
      "Iteration 63, loss = 0.25169882\n",
      "Iteration 64, loss = 0.25137510\n",
      "Iteration 65, loss = 0.25105267\n",
      "Iteration 66, loss = 0.25081060\n",
      "Iteration 67, loss = 0.25045088\n",
      "Iteration 68, loss = 0.25018085\n",
      "Iteration 69, loss = 0.24990054\n",
      "Iteration 70, loss = 0.24961190\n",
      "Iteration 71, loss = 0.24935291\n",
      "Iteration 72, loss = 0.24913417\n",
      "Iteration 73, loss = 0.24882219\n",
      "Iteration 74, loss = 0.24858413\n",
      "Iteration 75, loss = 0.24835502\n",
      "Iteration 76, loss = 0.24809970\n",
      "Iteration 77, loss = 0.24788500\n",
      "Iteration 78, loss = 0.24765788\n",
      "Iteration 79, loss = 0.24744395\n",
      "Iteration 80, loss = 0.24718259\n",
      "Iteration 81, loss = 0.24695307\n",
      "Iteration 82, loss = 0.24680432\n",
      "Iteration 83, loss = 0.24654825\n",
      "Iteration 84, loss = 0.24634236\n",
      "Iteration 85, loss = 0.24614578\n",
      "Iteration 86, loss = 0.24593491\n",
      "Iteration 87, loss = 0.24576959\n",
      "Iteration 88, loss = 0.24556901\n",
      "Iteration 89, loss = 0.24539204\n",
      "Iteration 90, loss = 0.24521504\n",
      "Iteration 91, loss = 0.24501097\n",
      "Iteration 92, loss = 0.24484619\n",
      "Iteration 93, loss = 0.24465379\n",
      "Iteration 94, loss = 0.24448124\n",
      "Iteration 95, loss = 0.24432996\n",
      "Iteration 96, loss = 0.24414720\n",
      "Iteration 97, loss = 0.24400605\n",
      "Iteration 98, loss = 0.24385550\n",
      "Iteration 99, loss = 0.24367760\n",
      "Iteration 100, loss = 0.24356228\n",
      "Iteration 101, loss = 0.24337928\n",
      "Iteration 102, loss = 0.24321246\n",
      "Iteration 103, loss = 0.24306893\n",
      "Iteration 104, loss = 0.24293726\n",
      "Iteration 105, loss = 0.24283993\n",
      "Iteration 106, loss = 0.24266476\n",
      "Iteration 107, loss = 0.24254191\n",
      "Iteration 108, loss = 0.24240433\n",
      "Iteration 109, loss = 0.24227637\n",
      "Iteration 110, loss = 0.24213268\n",
      "Iteration 111, loss = 0.24201833\n",
      "Iteration 112, loss = 0.24183963\n",
      "Iteration 113, loss = 0.24178785\n",
      "Iteration 114, loss = 0.24165105\n",
      "Iteration 115, loss = 0.24154825\n",
      "Iteration 116, loss = 0.24141167\n",
      "Iteration 117, loss = 0.24129785\n",
      "Iteration 118, loss = 0.24117065\n",
      "Iteration 119, loss = 0.24108245\n",
      "Iteration 120, loss = 0.24098565\n",
      "Iteration 121, loss = 0.24083410\n",
      "Iteration 122, loss = 0.24076572\n",
      "Iteration 123, loss = 0.24063355\n",
      "Iteration 124, loss = 0.24052387\n",
      "Iteration 125, loss = 0.24046640\n",
      "Iteration 126, loss = 0.24032624\n",
      "Iteration 127, loss = 0.24025366\n",
      "Iteration 128, loss = 0.24013751\n",
      "Iteration 129, loss = 0.24006297\n",
      "Iteration 130, loss = 0.23996342\n",
      "Iteration 131, loss = 0.23990849\n",
      "Iteration 132, loss = 0.23977984\n",
      "Iteration 133, loss = 0.23973126\n",
      "Iteration 134, loss = 0.23963262\n",
      "Iteration 135, loss = 0.23953997\n",
      "Iteration 136, loss = 0.23943590\n",
      "Iteration 137, loss = 0.23939161\n",
      "Iteration 138, loss = 0.23928970\n",
      "Iteration 139, loss = 0.23920325\n",
      "Iteration 140, loss = 0.23909787\n",
      "Iteration 141, loss = 0.23906753\n",
      "Iteration 142, loss = 0.23898596\n",
      "Iteration 143, loss = 0.23895715\n",
      "Iteration 144, loss = 0.23884283\n",
      "Iteration 145, loss = 0.23878083\n",
      "Iteration 146, loss = 0.23869583\n",
      "Iteration 147, loss = 0.23863663\n",
      "Iteration 148, loss = 0.23858843\n",
      "Iteration 149, loss = 0.23847160\n",
      "Iteration 150, loss = 0.23844147\n",
      "Iteration 151, loss = 0.23837355\n",
      "Iteration 152, loss = 0.23832720\n",
      "Iteration 153, loss = 0.23825902\n",
      "Iteration 154, loss = 0.23815860\n",
      "Iteration 155, loss = 0.23812542\n",
      "Iteration 156, loss = 0.23805120\n",
      "Iteration 157, loss = 0.23799440\n",
      "Iteration 158, loss = 0.23793336\n",
      "Iteration 159, loss = 0.23791520\n",
      "Iteration 160, loss = 0.23787021\n",
      "Iteration 161, loss = 0.23776064\n",
      "Iteration 162, loss = 0.23773745\n",
      "Iteration 163, loss = 0.23764623\n",
      "Iteration 164, loss = 0.23761176\n",
      "Iteration 165, loss = 0.23755471\n",
      "Iteration 166, loss = 0.23750380\n",
      "Iteration 167, loss = 0.23745857\n",
      "Iteration 168, loss = 0.23740148\n",
      "Iteration 169, loss = 0.23737810\n",
      "Iteration 170, loss = 0.23734994\n",
      "Iteration 171, loss = 0.23729629\n",
      "Iteration 172, loss = 0.23724823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77162242\n",
      "Iteration 2, loss = 0.67748617\n",
      "Iteration 3, loss = 0.58794400\n",
      "Iteration 4, loss = 0.52254080\n",
      "Iteration 5, loss = 0.47424964\n",
      "Iteration 6, loss = 0.43810525\n",
      "Iteration 7, loss = 0.41112077\n",
      "Iteration 8, loss = 0.39000980\n",
      "Iteration 9, loss = 0.37303759\n",
      "Iteration 10, loss = 0.35936038\n",
      "Iteration 11, loss = 0.34800461\n",
      "Iteration 12, loss = 0.33826141\n",
      "Iteration 13, loss = 0.33030194\n",
      "Iteration 14, loss = 0.32318889\n",
      "Iteration 15, loss = 0.31712944\n",
      "Iteration 16, loss = 0.31161325\n",
      "Iteration 17, loss = 0.30698921\n",
      "Iteration 18, loss = 0.30281375\n",
      "Iteration 19, loss = 0.29895039\n",
      "Iteration 20, loss = 0.29566227\n",
      "Iteration 21, loss = 0.29272317\n",
      "Iteration 22, loss = 0.29004910\n",
      "Iteration 23, loss = 0.28763821\n",
      "Iteration 24, loss = 0.28548240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.28348886\n",
      "Iteration 26, loss = 0.28169077\n",
      "Iteration 27, loss = 0.28002608\n",
      "Iteration 28, loss = 0.27850813\n",
      "Iteration 29, loss = 0.27707138\n",
      "Iteration 30, loss = 0.27577074\n",
      "Iteration 31, loss = 0.27455160\n",
      "Iteration 32, loss = 0.27341106\n",
      "Iteration 33, loss = 0.27237194\n",
      "Iteration 34, loss = 0.27137795\n",
      "Iteration 35, loss = 0.27042952\n",
      "Iteration 36, loss = 0.26955236\n",
      "Iteration 37, loss = 0.26869121\n",
      "Iteration 38, loss = 0.26792249\n",
      "Iteration 39, loss = 0.26715706\n",
      "Iteration 40, loss = 0.26644204\n",
      "Iteration 41, loss = 0.26579624\n",
      "Iteration 42, loss = 0.26515180\n",
      "Iteration 43, loss = 0.26451052\n",
      "Iteration 44, loss = 0.26392944\n",
      "Iteration 45, loss = 0.26334874\n",
      "Iteration 46, loss = 0.26285923\n",
      "Iteration 47, loss = 0.26229824\n",
      "Iteration 48, loss = 0.26183149\n",
      "Iteration 49, loss = 0.26132680\n",
      "Iteration 50, loss = 0.26089601\n",
      "Iteration 51, loss = 0.26043373\n",
      "Iteration 52, loss = 0.25999001\n",
      "Iteration 53, loss = 0.25959219\n",
      "Iteration 54, loss = 0.25924199\n",
      "Iteration 55, loss = 0.25877991\n",
      "Iteration 56, loss = 0.25840747\n",
      "Iteration 57, loss = 0.25810189\n",
      "Iteration 58, loss = 0.25774067\n",
      "Iteration 59, loss = 0.25738061\n",
      "Iteration 60, loss = 0.25705716\n",
      "Iteration 61, loss = 0.25674988\n",
      "Iteration 62, loss = 0.25643373\n",
      "Iteration 63, loss = 0.25611206\n",
      "Iteration 64, loss = 0.25581203\n",
      "Iteration 65, loss = 0.25553383\n",
      "Iteration 66, loss = 0.25531087\n",
      "Iteration 67, loss = 0.25497498\n",
      "Iteration 68, loss = 0.25471798\n",
      "Iteration 69, loss = 0.25444013\n",
      "Iteration 70, loss = 0.25418458\n",
      "Iteration 71, loss = 0.25397418\n",
      "Iteration 72, loss = 0.25373106\n",
      "Iteration 73, loss = 0.25346929\n",
      "Iteration 74, loss = 0.25326488\n",
      "Iteration 75, loss = 0.25304513\n",
      "Iteration 76, loss = 0.25277710\n",
      "Iteration 77, loss = 0.25257649\n",
      "Iteration 78, loss = 0.25235960\n",
      "Iteration 79, loss = 0.25213905\n",
      "Iteration 80, loss = 0.25195508\n",
      "Iteration 81, loss = 0.25173276\n",
      "Iteration 82, loss = 0.25157554\n",
      "Iteration 83, loss = 0.25135097\n",
      "Iteration 84, loss = 0.25114059\n",
      "Iteration 85, loss = 0.25099414\n",
      "Iteration 86, loss = 0.25078438\n",
      "Iteration 87, loss = 0.25063867\n",
      "Iteration 88, loss = 0.25048078\n",
      "Iteration 89, loss = 0.25030966\n",
      "Iteration 90, loss = 0.25013444\n",
      "Iteration 91, loss = 0.24995540\n",
      "Iteration 92, loss = 0.24978648\n",
      "Iteration 93, loss = 0.24960802\n",
      "Iteration 94, loss = 0.24945377\n",
      "Iteration 95, loss = 0.24929919\n",
      "Iteration 96, loss = 0.24915854\n",
      "Iteration 97, loss = 0.24901201\n",
      "Iteration 98, loss = 0.24887977\n",
      "Iteration 99, loss = 0.24870531\n",
      "Iteration 100, loss = 0.24860670\n",
      "Iteration 101, loss = 0.24847814\n",
      "Iteration 102, loss = 0.24831321\n",
      "Iteration 103, loss = 0.24816944\n",
      "Iteration 104, loss = 0.24804513\n",
      "Iteration 105, loss = 0.24791983\n",
      "Iteration 106, loss = 0.24778205\n",
      "Iteration 107, loss = 0.24771180\n",
      "Iteration 108, loss = 0.24755563\n",
      "Iteration 109, loss = 0.24744664\n",
      "Iteration 110, loss = 0.24731147\n",
      "Iteration 111, loss = 0.24720130\n",
      "Iteration 112, loss = 0.24708820\n",
      "Iteration 113, loss = 0.24700698\n",
      "Iteration 114, loss = 0.24685607\n",
      "Iteration 115, loss = 0.24673173\n",
      "Iteration 116, loss = 0.24663488\n",
      "Iteration 117, loss = 0.24653201\n",
      "Iteration 118, loss = 0.24642329\n",
      "Iteration 119, loss = 0.24635669\n",
      "Iteration 120, loss = 0.24624392\n",
      "Iteration 121, loss = 0.24610883\n",
      "Iteration 122, loss = 0.24606603\n",
      "Iteration 123, loss = 0.24593718\n",
      "Iteration 124, loss = 0.24581064\n",
      "Iteration 125, loss = 0.24579858\n",
      "Iteration 126, loss = 0.24566322\n",
      "Iteration 127, loss = 0.24561003\n",
      "Iteration 128, loss = 0.24548411\n",
      "Iteration 129, loss = 0.24540366\n",
      "Iteration 130, loss = 0.24530654\n",
      "Iteration 131, loss = 0.24526129\n",
      "Iteration 132, loss = 0.24516503\n",
      "Iteration 133, loss = 0.24507217\n",
      "Iteration 134, loss = 0.24498773\n",
      "Iteration 135, loss = 0.24490699\n",
      "Iteration 136, loss = 0.24483122\n",
      "Iteration 137, loss = 0.24475472\n",
      "Iteration 138, loss = 0.24469674\n",
      "Iteration 139, loss = 0.24462868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77396419\n",
      "Iteration 2, loss = 0.67924684\n",
      "Iteration 3, loss = 0.59064307\n",
      "Iteration 4, loss = 0.52431238\n",
      "Iteration 5, loss = 0.47595306\n",
      "Iteration 6, loss = 0.44023040\n",
      "Iteration 7, loss = 0.41301107\n",
      "Iteration 8, loss = 0.39152446\n",
      "Iteration 9, loss = 0.37451441\n",
      "Iteration 10, loss = 0.36061268\n",
      "Iteration 11, loss = 0.34913668\n",
      "Iteration 12, loss = 0.33930013\n",
      "Iteration 13, loss = 0.33111863\n",
      "Iteration 14, loss = 0.32388238\n",
      "Iteration 15, loss = 0.31753440\n",
      "Iteration 16, loss = 0.31209198\n",
      "Iteration 17, loss = 0.30720011\n",
      "Iteration 18, loss = 0.30283838\n",
      "Iteration 19, loss = 0.29898381\n",
      "Iteration 20, loss = 0.29554890\n",
      "Iteration 21, loss = 0.29247935\n",
      "Iteration 22, loss = 0.28968691\n",
      "Iteration 23, loss = 0.28717232\n",
      "Iteration 24, loss = 0.28490107\n",
      "Iteration 25, loss = 0.28287012\n",
      "Iteration 26, loss = 0.28092559\n",
      "Iteration 27, loss = 0.27921937\n",
      "Iteration 28, loss = 0.27761195\n",
      "Iteration 29, loss = 0.27613296\n",
      "Iteration 30, loss = 0.27467660\n",
      "Iteration 31, loss = 0.27343148\n",
      "Iteration 32, loss = 0.27216714\n",
      "Iteration 33, loss = 0.27106651\n",
      "Iteration 34, loss = 0.26997822\n",
      "Iteration 35, loss = 0.26897984\n",
      "Iteration 36, loss = 0.26801564\n",
      "Iteration 37, loss = 0.26707399\n",
      "Iteration 38, loss = 0.26629388\n",
      "Iteration 39, loss = 0.26543506\n",
      "Iteration 40, loss = 0.26465681\n",
      "Iteration 41, loss = 0.26392981\n",
      "Iteration 42, loss = 0.26321110\n",
      "Iteration 43, loss = 0.26253895\n",
      "Iteration 44, loss = 0.26191423\n",
      "Iteration 45, loss = 0.26125555\n",
      "Iteration 46, loss = 0.26070656\n",
      "Iteration 47, loss = 0.26012163\n",
      "Iteration 48, loss = 0.25952397\n",
      "Iteration 49, loss = 0.25904259\n",
      "Iteration 50, loss = 0.25850955\n",
      "Iteration 51, loss = 0.25801200\n",
      "Iteration 52, loss = 0.25755328\n",
      "Iteration 53, loss = 0.25707023\n",
      "Iteration 54, loss = 0.25661626\n",
      "Iteration 55, loss = 0.25614743\n",
      "Iteration 56, loss = 0.25575895\n",
      "Iteration 57, loss = 0.25534280\n",
      "Iteration 58, loss = 0.25497255\n",
      "Iteration 59, loss = 0.25455672\n",
      "Iteration 60, loss = 0.25418875\n",
      "Iteration 61, loss = 0.25384627\n",
      "Iteration 62, loss = 0.25345335\n",
      "Iteration 63, loss = 0.25312663\n",
      "Iteration 64, loss = 0.25277019\n",
      "Iteration 65, loss = 0.25247189\n",
      "Iteration 66, loss = 0.25211189\n",
      "Iteration 67, loss = 0.25181295\n",
      "Iteration 68, loss = 0.25151116\n",
      "Iteration 69, loss = 0.25119809\n",
      "Iteration 70, loss = 0.25091411\n",
      "Iteration 71, loss = 0.25064498\n",
      "Iteration 72, loss = 0.25039069\n",
      "Iteration 73, loss = 0.25007126\n",
      "Iteration 74, loss = 0.24979351\n",
      "Iteration 75, loss = 0.24957042\n",
      "Iteration 76, loss = 0.24931327\n",
      "Iteration 77, loss = 0.24901884\n",
      "Iteration 78, loss = 0.24880539\n",
      "Iteration 79, loss = 0.24855575\n",
      "Iteration 80, loss = 0.24829465\n",
      "Iteration 81, loss = 0.24809591\n",
      "Iteration 82, loss = 0.24794553\n",
      "Iteration 83, loss = 0.24762585\n",
      "Iteration 84, loss = 0.24743288\n",
      "Iteration 85, loss = 0.24717629\n",
      "Iteration 86, loss = 0.24700888\n",
      "Iteration 87, loss = 0.24678323\n",
      "Iteration 88, loss = 0.24657337\n",
      "Iteration 89, loss = 0.24637415\n",
      "Iteration 90, loss = 0.24617821\n",
      "Iteration 91, loss = 0.24597776\n",
      "Iteration 92, loss = 0.24583481\n",
      "Iteration 93, loss = 0.24558635\n",
      "Iteration 94, loss = 0.24541253\n",
      "Iteration 95, loss = 0.24523283\n",
      "Iteration 96, loss = 0.24510355\n",
      "Iteration 97, loss = 0.24490499\n",
      "Iteration 98, loss = 0.24469130\n",
      "Iteration 99, loss = 0.24454513\n",
      "Iteration 100, loss = 0.24443525\n",
      "Iteration 101, loss = 0.24422617\n",
      "Iteration 102, loss = 0.24409427\n",
      "Iteration 103, loss = 0.24395753\n",
      "Iteration 104, loss = 0.24373743\n",
      "Iteration 105, loss = 0.24359755\n",
      "Iteration 106, loss = 0.24345566\n",
      "Iteration 107, loss = 0.24331012\n",
      "Iteration 108, loss = 0.24316011\n",
      "Iteration 109, loss = 0.24301564\n",
      "Iteration 110, loss = 0.24289101\n",
      "Iteration 111, loss = 0.24280170\n",
      "Iteration 112, loss = 0.24263017\n",
      "Iteration 113, loss = 0.24248945\n",
      "Iteration 114, loss = 0.24236798\n",
      "Iteration 115, loss = 0.24224255\n",
      "Iteration 116, loss = 0.24211180\n",
      "Iteration 117, loss = 0.24198972\n",
      "Iteration 118, loss = 0.24183951\n",
      "Iteration 119, loss = 0.24177791\n",
      "Iteration 120, loss = 0.24164347\n",
      "Iteration 121, loss = 0.24153338\n",
      "Iteration 122, loss = 0.24140723\n",
      "Iteration 123, loss = 0.24132034\n",
      "Iteration 124, loss = 0.24122659\n",
      "Iteration 125, loss = 0.24109011\n",
      "Iteration 126, loss = 0.24097734\n",
      "Iteration 127, loss = 0.24088642\n",
      "Iteration 128, loss = 0.24079304\n",
      "Iteration 129, loss = 0.24068130\n",
      "Iteration 130, loss = 0.24064704\n",
      "Iteration 131, loss = 0.24049707\n",
      "Iteration 132, loss = 0.24040226\n",
      "Iteration 133, loss = 0.24032465\n",
      "Iteration 134, loss = 0.24031278\n",
      "Iteration 135, loss = 0.24010542\n",
      "Iteration 136, loss = 0.24003390\n",
      "Iteration 137, loss = 0.23996522\n",
      "Iteration 138, loss = 0.23985934\n",
      "Iteration 139, loss = 0.23977507\n",
      "Iteration 140, loss = 0.23971878\n",
      "Iteration 141, loss = 0.23964631\n",
      "Iteration 142, loss = 0.23953032\n",
      "Iteration 143, loss = 0.23948983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 144, loss = 0.23943618\n",
      "Iteration 145, loss = 0.23932395\n",
      "Iteration 146, loss = 0.23924791\n",
      "Iteration 147, loss = 0.23919469\n",
      "Iteration 148, loss = 0.23912268\n",
      "Iteration 149, loss = 0.23905749\n",
      "Iteration 150, loss = 0.23897354\n",
      "Iteration 151, loss = 0.23890340\n",
      "Iteration 152, loss = 0.23893190\n",
      "Iteration 153, loss = 0.23878225\n",
      "Iteration 154, loss = 0.23871877\n",
      "Iteration 155, loss = 0.23866763\n",
      "Iteration 156, loss = 0.23860036\n",
      "Iteration 157, loss = 0.23856370\n",
      "Iteration 158, loss = 0.23845097\n",
      "Iteration 159, loss = 0.23843491\n",
      "Iteration 160, loss = 0.23833677\n",
      "Iteration 161, loss = 0.23831034\n",
      "Iteration 162, loss = 0.23827378\n",
      "Iteration 163, loss = 0.23818179\n",
      "Iteration 164, loss = 0.23815517\n",
      "Iteration 165, loss = 0.23808845\n",
      "Iteration 166, loss = 0.23806241\n",
      "Iteration 167, loss = 0.23799862\n",
      "Iteration 168, loss = 0.23793844\n",
      "Iteration 169, loss = 0.23793060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77548385\n",
      "Iteration 2, loss = 0.67807468\n",
      "Iteration 3, loss = 0.58731450\n",
      "Iteration 4, loss = 0.52011004\n",
      "Iteration 5, loss = 0.47158008\n",
      "Iteration 6, loss = 0.43570167\n",
      "Iteration 7, loss = 0.40896748\n",
      "Iteration 8, loss = 0.38771395\n",
      "Iteration 9, loss = 0.37125583\n",
      "Iteration 10, loss = 0.35795663\n",
      "Iteration 11, loss = 0.34689467\n",
      "Iteration 12, loss = 0.33758150\n",
      "Iteration 13, loss = 0.32985023\n",
      "Iteration 14, loss = 0.32303259\n",
      "Iteration 15, loss = 0.31717184\n",
      "Iteration 16, loss = 0.31201987\n",
      "Iteration 17, loss = 0.30752697\n",
      "Iteration 18, loss = 0.30350440\n",
      "Iteration 19, loss = 0.29989328\n",
      "Iteration 20, loss = 0.29661687\n",
      "Iteration 21, loss = 0.29379735\n",
      "Iteration 22, loss = 0.29108433\n",
      "Iteration 23, loss = 0.28858655\n",
      "Iteration 24, loss = 0.28641814\n",
      "Iteration 25, loss = 0.28439403\n",
      "Iteration 26, loss = 0.28250781\n",
      "Iteration 27, loss = 0.28080955\n",
      "Iteration 28, loss = 0.27923430\n",
      "Iteration 29, loss = 0.27781864\n",
      "Iteration 30, loss = 0.27638342\n",
      "Iteration 31, loss = 0.27516244\n",
      "Iteration 32, loss = 0.27397449\n",
      "Iteration 33, loss = 0.27294068\n",
      "Iteration 34, loss = 0.27189941\n",
      "Iteration 35, loss = 0.27095951\n",
      "Iteration 36, loss = 0.27006737\n",
      "Iteration 37, loss = 0.26923233\n",
      "Iteration 38, loss = 0.26842920\n",
      "Iteration 39, loss = 0.26768071\n",
      "Iteration 40, loss = 0.26694640\n",
      "Iteration 41, loss = 0.26626385\n",
      "Iteration 42, loss = 0.26564205\n",
      "Iteration 43, loss = 0.26501697\n",
      "Iteration 44, loss = 0.26445481\n",
      "Iteration 45, loss = 0.26386410\n",
      "Iteration 46, loss = 0.26335200\n",
      "Iteration 47, loss = 0.26285148\n",
      "Iteration 48, loss = 0.26230126\n",
      "Iteration 49, loss = 0.26188691\n",
      "Iteration 50, loss = 0.26140155\n",
      "Iteration 51, loss = 0.26102416\n",
      "Iteration 52, loss = 0.26057559\n",
      "Iteration 53, loss = 0.26013972\n",
      "Iteration 54, loss = 0.25971011\n",
      "Iteration 55, loss = 0.25932332\n",
      "Iteration 56, loss = 0.25896541\n",
      "Iteration 57, loss = 0.25862431\n",
      "Iteration 58, loss = 0.25828943\n",
      "Iteration 59, loss = 0.25792633\n",
      "Iteration 60, loss = 0.25760133\n",
      "Iteration 61, loss = 0.25729476\n",
      "Iteration 62, loss = 0.25695324\n",
      "Iteration 63, loss = 0.25668572\n",
      "Iteration 64, loss = 0.25636169\n",
      "Iteration 65, loss = 0.25612510\n",
      "Iteration 66, loss = 0.25581963\n",
      "Iteration 67, loss = 0.25555290\n",
      "Iteration 68, loss = 0.25527289\n",
      "Iteration 69, loss = 0.25502030\n",
      "Iteration 70, loss = 0.25479090\n",
      "Iteration 71, loss = 0.25453746\n",
      "Iteration 72, loss = 0.25428596\n",
      "Iteration 73, loss = 0.25402547\n",
      "Iteration 74, loss = 0.25381118\n",
      "Iteration 75, loss = 0.25357496\n",
      "Iteration 76, loss = 0.25338521\n",
      "Iteration 77, loss = 0.25316984\n",
      "Iteration 78, loss = 0.25297858\n",
      "Iteration 79, loss = 0.25273164\n",
      "Iteration 80, loss = 0.25255430\n",
      "Iteration 81, loss = 0.25235874\n",
      "Iteration 82, loss = 0.25222109\n",
      "Iteration 83, loss = 0.25198306\n",
      "Iteration 84, loss = 0.25177482\n",
      "Iteration 85, loss = 0.25153783\n",
      "Iteration 86, loss = 0.25141728\n",
      "Iteration 87, loss = 0.25122483\n",
      "Iteration 88, loss = 0.25104647\n",
      "Iteration 89, loss = 0.25087372\n",
      "Iteration 90, loss = 0.25069418\n",
      "Iteration 91, loss = 0.25055126\n",
      "Iteration 92, loss = 0.25039163\n",
      "Iteration 93, loss = 0.25020523\n",
      "Iteration 94, loss = 0.25007704\n",
      "Iteration 95, loss = 0.24993050\n",
      "Iteration 96, loss = 0.24985388\n",
      "Iteration 97, loss = 0.24962590\n",
      "Iteration 98, loss = 0.24948744\n",
      "Iteration 99, loss = 0.24933451\n",
      "Iteration 100, loss = 0.24921940\n",
      "Iteration 101, loss = 0.24909656\n",
      "Iteration 102, loss = 0.24895208\n",
      "Iteration 103, loss = 0.24883832\n",
      "Iteration 104, loss = 0.24866179\n",
      "Iteration 105, loss = 0.24854293\n",
      "Iteration 106, loss = 0.24841147\n",
      "Iteration 107, loss = 0.24829223\n",
      "Iteration 108, loss = 0.24821452\n",
      "Iteration 109, loss = 0.24805869\n",
      "Iteration 110, loss = 0.24795457\n",
      "Iteration 111, loss = 0.24787893\n",
      "Iteration 112, loss = 0.24773596\n",
      "Iteration 113, loss = 0.24762642\n",
      "Iteration 114, loss = 0.24750009\n",
      "Iteration 115, loss = 0.24740648\n",
      "Iteration 116, loss = 0.24730307\n",
      "Iteration 117, loss = 0.24719439\n",
      "Iteration 118, loss = 0.24706762\n",
      "Iteration 119, loss = 0.24701157\n",
      "Iteration 120, loss = 0.24688978\n",
      "Iteration 121, loss = 0.24681654\n",
      "Iteration 122, loss = 0.24673610\n",
      "Iteration 123, loss = 0.24662898\n",
      "Iteration 124, loss = 0.24655636\n",
      "Iteration 125, loss = 0.24643610\n",
      "Iteration 126, loss = 0.24636794\n",
      "Iteration 127, loss = 0.24627333\n",
      "Iteration 128, loss = 0.24619894\n",
      "Iteration 129, loss = 0.24611070\n",
      "Iteration 130, loss = 0.24606641\n",
      "Iteration 131, loss = 0.24593192\n",
      "Iteration 132, loss = 0.24586200\n",
      "Iteration 133, loss = 0.24579692\n",
      "Iteration 134, loss = 0.24576567\n",
      "Iteration 135, loss = 0.24562661\n",
      "Iteration 136, loss = 0.24556396\n",
      "Iteration 137, loss = 0.24547074\n",
      "Iteration 138, loss = 0.24542161\n",
      "Iteration 139, loss = 0.24534559\n",
      "Iteration 140, loss = 0.24531433\n",
      "Iteration 141, loss = 0.24521140\n",
      "Iteration 142, loss = 0.24515810\n",
      "Iteration 143, loss = 0.24511259\n",
      "Iteration 144, loss = 0.24504666\n",
      "Iteration 145, loss = 0.24496538\n",
      "Iteration 146, loss = 0.24488905\n",
      "Iteration 147, loss = 0.24486735\n",
      "Iteration 148, loss = 0.24480829\n",
      "Iteration 149, loss = 0.24473339\n",
      "Iteration 150, loss = 0.24470319\n",
      "Iteration 151, loss = 0.24463695\n",
      "Iteration 152, loss = 0.24462325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67584268\n",
      "Iteration 2, loss = 0.50356848\n",
      "Iteration 3, loss = 0.41796032\n",
      "Iteration 4, loss = 0.36516432\n",
      "Iteration 5, loss = 0.32934988\n",
      "Iteration 6, loss = 0.30499434\n",
      "Iteration 7, loss = 0.28882327\n",
      "Iteration 8, loss = 0.27676495\n",
      "Iteration 9, loss = 0.26783581\n",
      "Iteration 10, loss = 0.26169172\n",
      "Iteration 11, loss = 0.25661112\n",
      "Iteration 12, loss = 0.25235950\n",
      "Iteration 13, loss = 0.24993012\n",
      "Iteration 14, loss = 0.24755039\n",
      "Iteration 15, loss = 0.24558002\n",
      "Iteration 16, loss = 0.24437925\n",
      "Iteration 17, loss = 0.24256278\n",
      "Iteration 18, loss = 0.24210405\n",
      "Iteration 19, loss = 0.24123513\n",
      "Iteration 20, loss = 0.24104741\n",
      "Iteration 21, loss = 0.23974902\n",
      "Iteration 22, loss = 0.24047266\n",
      "Iteration 23, loss = 0.23925001\n",
      "Iteration 24, loss = 0.23890113\n",
      "Iteration 25, loss = 0.23896817\n",
      "Iteration 26, loss = 0.23892780\n",
      "Iteration 27, loss = 0.23876636\n",
      "Iteration 28, loss = 0.23918162\n",
      "Iteration 29, loss = 0.23799657\n",
      "Iteration 30, loss = 0.23874357\n",
      "Iteration 31, loss = 0.23833593\n",
      "Iteration 32, loss = 0.23813490\n",
      "Iteration 33, loss = 0.23769096\n",
      "Iteration 34, loss = 0.23812259\n",
      "Iteration 35, loss = 0.23897020\n",
      "Iteration 36, loss = 0.23887180\n",
      "Iteration 37, loss = 0.23792157\n",
      "Iteration 38, loss = 0.23805495\n",
      "Iteration 39, loss = 0.23817928\n",
      "Iteration 40, loss = 0.23904807\n",
      "Iteration 41, loss = 0.23839498\n",
      "Iteration 42, loss = 0.23850699\n",
      "Iteration 43, loss = 0.23806148\n",
      "Iteration 44, loss = 0.23856890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67304770\n",
      "Iteration 2, loss = 0.50433898\n",
      "Iteration 3, loss = 0.41731498\n",
      "Iteration 4, loss = 0.36232805\n",
      "Iteration 5, loss = 0.32588992\n",
      "Iteration 6, loss = 0.30189743\n",
      "Iteration 7, loss = 0.28579252\n",
      "Iteration 8, loss = 0.27383832\n",
      "Iteration 9, loss = 0.26546460\n",
      "Iteration 10, loss = 0.25957136\n",
      "Iteration 11, loss = 0.25457682\n",
      "Iteration 12, loss = 0.25067952\n",
      "Iteration 13, loss = 0.24806590\n",
      "Iteration 14, loss = 0.24559105\n",
      "Iteration 15, loss = 0.24353725\n",
      "Iteration 16, loss = 0.24229416\n",
      "Iteration 17, loss = 0.24155053\n",
      "Iteration 18, loss = 0.24035573\n",
      "Iteration 19, loss = 0.23948445\n",
      "Iteration 20, loss = 0.23873145\n",
      "Iteration 21, loss = 0.23836684\n",
      "Iteration 22, loss = 0.23835335\n",
      "Iteration 23, loss = 0.23780432\n",
      "Iteration 24, loss = 0.23749213\n",
      "Iteration 25, loss = 0.23716467\n",
      "Iteration 26, loss = 0.23710940\n",
      "Iteration 27, loss = 0.23707918\n",
      "Iteration 28, loss = 0.23737830\n",
      "Iteration 29, loss = 0.23682540\n",
      "Iteration 30, loss = 0.23672156\n",
      "Iteration 31, loss = 0.23657747\n",
      "Iteration 32, loss = 0.23650168\n",
      "Iteration 33, loss = 0.23633356\n",
      "Iteration 34, loss = 0.23717706\n",
      "Iteration 35, loss = 0.23703393\n",
      "Iteration 36, loss = 0.23712044\n",
      "Iteration 37, loss = 0.23624404\n",
      "Iteration 38, loss = 0.23613027\n",
      "Iteration 39, loss = 0.23651676\n",
      "Iteration 40, loss = 0.23710180\n",
      "Iteration 41, loss = 0.23698546\n",
      "Iteration 42, loss = 0.23674054\n",
      "Iteration 43, loss = 0.23643111\n",
      "Iteration 44, loss = 0.23663126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.23687257\n",
      "Iteration 46, loss = 0.23716954\n",
      "Iteration 47, loss = 0.23700543\n",
      "Iteration 48, loss = 0.23713369\n",
      "Iteration 49, loss = 0.23760620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67284065\n",
      "Iteration 2, loss = 0.50489963\n",
      "Iteration 3, loss = 0.41812080\n",
      "Iteration 4, loss = 0.36405662\n",
      "Iteration 5, loss = 0.32844317\n",
      "Iteration 6, loss = 0.30477212\n",
      "Iteration 7, loss = 0.28890108\n",
      "Iteration 8, loss = 0.27812306\n",
      "Iteration 9, loss = 0.26911723\n",
      "Iteration 10, loss = 0.26329326\n",
      "Iteration 11, loss = 0.25892378\n",
      "Iteration 12, loss = 0.25501057\n",
      "Iteration 13, loss = 0.25311961\n",
      "Iteration 14, loss = 0.25070309\n",
      "Iteration 15, loss = 0.24864851\n",
      "Iteration 16, loss = 0.24735704\n",
      "Iteration 17, loss = 0.24713883\n",
      "Iteration 18, loss = 0.24601234\n",
      "Iteration 19, loss = 0.24483677\n",
      "Iteration 20, loss = 0.24420844\n",
      "Iteration 21, loss = 0.24416709\n",
      "Iteration 22, loss = 0.24459275\n",
      "Iteration 23, loss = 0.24383251\n",
      "Iteration 24, loss = 0.24311701\n",
      "Iteration 25, loss = 0.24281650\n",
      "Iteration 26, loss = 0.24310332\n",
      "Iteration 27, loss = 0.24268075\n",
      "Iteration 28, loss = 0.24294886\n",
      "Iteration 29, loss = 0.24222970\n",
      "Iteration 30, loss = 0.24204471\n",
      "Iteration 31, loss = 0.24236660\n",
      "Iteration 32, loss = 0.24286692\n",
      "Iteration 33, loss = 0.24230108\n",
      "Iteration 34, loss = 0.24256923\n",
      "Iteration 35, loss = 0.24222695\n",
      "Iteration 36, loss = 0.24290887\n",
      "Iteration 37, loss = 0.24265029\n",
      "Iteration 38, loss = 0.24244820\n",
      "Iteration 39, loss = 0.24196619\n",
      "Iteration 40, loss = 0.24249528\n",
      "Iteration 41, loss = 0.24298945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67557537\n",
      "Iteration 2, loss = 0.50804544\n",
      "Iteration 3, loss = 0.42171332\n",
      "Iteration 4, loss = 0.36468111\n",
      "Iteration 5, loss = 0.32903431\n",
      "Iteration 6, loss = 0.30455427\n",
      "Iteration 7, loss = 0.28782259\n",
      "Iteration 8, loss = 0.27590425\n",
      "Iteration 9, loss = 0.26794082\n",
      "Iteration 10, loss = 0.26118288\n",
      "Iteration 11, loss = 0.25729124\n",
      "Iteration 12, loss = 0.25269552\n",
      "Iteration 13, loss = 0.24941476\n",
      "Iteration 14, loss = 0.24721220\n",
      "Iteration 15, loss = 0.24482975\n",
      "Iteration 16, loss = 0.24323419\n",
      "Iteration 17, loss = 0.24220643\n",
      "Iteration 18, loss = 0.24111162\n",
      "Iteration 19, loss = 0.24041337\n",
      "Iteration 20, loss = 0.23971726\n",
      "Iteration 21, loss = 0.23996502\n",
      "Iteration 22, loss = 0.23860419\n",
      "Iteration 23, loss = 0.23836725\n",
      "Iteration 24, loss = 0.23807318\n",
      "Iteration 25, loss = 0.23815569\n",
      "Iteration 26, loss = 0.23741541\n",
      "Iteration 27, loss = 0.23828291\n",
      "Iteration 28, loss = 0.23790439\n",
      "Iteration 29, loss = 0.23779996\n",
      "Iteration 30, loss = 0.23725230\n",
      "Iteration 31, loss = 0.23737151\n",
      "Iteration 32, loss = 0.23741870\n",
      "Iteration 33, loss = 0.23708025\n",
      "Iteration 34, loss = 0.23751223\n",
      "Iteration 35, loss = 0.23730693\n",
      "Iteration 36, loss = 0.23677387\n",
      "Iteration 37, loss = 0.23713107\n",
      "Iteration 38, loss = 0.23685588\n",
      "Iteration 39, loss = 0.23727170\n",
      "Iteration 40, loss = 0.23805314\n",
      "Iteration 41, loss = 0.23701625\n",
      "Iteration 42, loss = 0.23702888\n",
      "Iteration 43, loss = 0.23748100\n",
      "Iteration 44, loss = 0.23789063\n",
      "Iteration 45, loss = 0.23681322\n",
      "Iteration 46, loss = 0.23783346\n",
      "Iteration 47, loss = 0.23700935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67057719\n",
      "Iteration 2, loss = 0.50300760\n",
      "Iteration 3, loss = 0.41851634\n",
      "Iteration 4, loss = 0.36458728\n",
      "Iteration 5, loss = 0.33050103\n",
      "Iteration 6, loss = 0.30557872\n",
      "Iteration 7, loss = 0.28965490\n",
      "Iteration 8, loss = 0.27790313\n",
      "Iteration 9, loss = 0.27063619\n",
      "Iteration 10, loss = 0.26497443\n",
      "Iteration 11, loss = 0.26134365\n",
      "Iteration 12, loss = 0.25711132\n",
      "Iteration 13, loss = 0.25447959\n",
      "Iteration 14, loss = 0.25170837\n",
      "Iteration 15, loss = 0.24985401\n",
      "Iteration 16, loss = 0.24866262\n",
      "Iteration 17, loss = 0.24759565\n",
      "Iteration 18, loss = 0.24691964\n",
      "Iteration 19, loss = 0.24573491\n",
      "Iteration 20, loss = 0.24524781\n",
      "Iteration 21, loss = 0.24514489\n",
      "Iteration 22, loss = 0.24494210\n",
      "Iteration 23, loss = 0.24452743\n",
      "Iteration 24, loss = 0.24396117\n",
      "Iteration 25, loss = 0.24379740\n",
      "Iteration 26, loss = 0.24373939\n",
      "Iteration 27, loss = 0.24444494\n",
      "Iteration 28, loss = 0.24354618\n",
      "Iteration 29, loss = 0.24473712\n",
      "Iteration 30, loss = 0.24425876\n",
      "Iteration 31, loss = 0.24347558\n",
      "Iteration 32, loss = 0.24357085\n",
      "Iteration 33, loss = 0.24317451\n",
      "Iteration 34, loss = 0.24410284\n",
      "Iteration 35, loss = 0.24379442\n",
      "Iteration 36, loss = 0.24311469\n",
      "Iteration 37, loss = 0.24409784\n",
      "Iteration 38, loss = 0.24309928\n",
      "Iteration 39, loss = 0.24337930\n",
      "Iteration 40, loss = 0.24372134\n",
      "Iteration 41, loss = 0.24301622\n",
      "Iteration 42, loss = 0.24346717\n",
      "Iteration 43, loss = 0.24352875\n",
      "Iteration 44, loss = 0.24394637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76838965\n",
      "Iteration 2, loss = 0.67322147\n",
      "Iteration 3, loss = 0.58455563\n",
      "Iteration 4, loss = 0.51830839\n",
      "Iteration 5, loss = 0.47067279\n",
      "Iteration 6, loss = 0.43473387\n",
      "Iteration 7, loss = 0.40788960\n",
      "Iteration 8, loss = 0.38676616\n",
      "Iteration 9, loss = 0.37013198\n",
      "Iteration 10, loss = 0.35659617\n",
      "Iteration 11, loss = 0.34547694\n",
      "Iteration 12, loss = 0.33597431\n",
      "Iteration 13, loss = 0.32823223\n",
      "Iteration 14, loss = 0.32137986\n",
      "Iteration 15, loss = 0.31549914\n",
      "Iteration 16, loss = 0.31036968\n",
      "Iteration 17, loss = 0.30579954\n",
      "Iteration 18, loss = 0.30183204\n",
      "Iteration 19, loss = 0.29816244\n",
      "Iteration 20, loss = 0.29490583\n",
      "Iteration 21, loss = 0.29207313\n",
      "Iteration 22, loss = 0.28940235\n",
      "Iteration 23, loss = 0.28693011\n",
      "Iteration 24, loss = 0.28472920\n",
      "Iteration 25, loss = 0.28272471\n",
      "Iteration 26, loss = 0.28080601\n",
      "Iteration 27, loss = 0.27907884\n",
      "Iteration 28, loss = 0.27750287\n",
      "Iteration 29, loss = 0.27593986\n",
      "Iteration 30, loss = 0.27455294\n",
      "Iteration 31, loss = 0.27321613\n",
      "Iteration 32, loss = 0.27201176\n",
      "Iteration 33, loss = 0.27084198\n",
      "Iteration 34, loss = 0.26975883\n",
      "Iteration 35, loss = 0.26876750\n",
      "Iteration 36, loss = 0.26778725\n",
      "Iteration 37, loss = 0.26678487\n",
      "Iteration 38, loss = 0.26596016\n",
      "Iteration 39, loss = 0.26509941\n",
      "Iteration 40, loss = 0.26431628\n",
      "Iteration 41, loss = 0.26357850\n",
      "Iteration 42, loss = 0.26285598\n",
      "Iteration 43, loss = 0.26212337\n",
      "Iteration 44, loss = 0.26149126\n",
      "Iteration 45, loss = 0.26087443\n",
      "Iteration 46, loss = 0.26023170\n",
      "Iteration 47, loss = 0.25972476\n",
      "Iteration 48, loss = 0.25912478\n",
      "Iteration 49, loss = 0.25858811\n",
      "Iteration 50, loss = 0.25805123\n",
      "Iteration 51, loss = 0.25755809\n",
      "Iteration 52, loss = 0.25712229\n",
      "Iteration 53, loss = 0.25661345\n",
      "Iteration 54, loss = 0.25620052\n",
      "Iteration 55, loss = 0.25574567\n",
      "Iteration 56, loss = 0.25533246\n",
      "Iteration 57, loss = 0.25498470\n",
      "Iteration 58, loss = 0.25456208\n",
      "Iteration 59, loss = 0.25416851\n",
      "Iteration 60, loss = 0.25382125\n",
      "Iteration 61, loss = 0.25346378\n",
      "Iteration 62, loss = 0.25312268\n",
      "Iteration 63, loss = 0.25280868\n",
      "Iteration 64, loss = 0.25247887\n",
      "Iteration 65, loss = 0.25214812\n",
      "Iteration 66, loss = 0.25188412\n",
      "Iteration 67, loss = 0.25153139\n",
      "Iteration 68, loss = 0.25122737\n",
      "Iteration 69, loss = 0.25099495\n",
      "Iteration 70, loss = 0.25066628\n",
      "Iteration 71, loss = 0.25041672\n",
      "Iteration 72, loss = 0.25015523\n",
      "Iteration 73, loss = 0.24988474\n",
      "Iteration 74, loss = 0.24963614\n",
      "Iteration 75, loss = 0.24937469\n",
      "Iteration 76, loss = 0.24916378\n",
      "Iteration 77, loss = 0.24895604\n",
      "Iteration 78, loss = 0.24870007\n",
      "Iteration 79, loss = 0.24845004\n",
      "Iteration 80, loss = 0.24824100\n",
      "Iteration 81, loss = 0.24802990\n",
      "Iteration 82, loss = 0.24785912\n",
      "Iteration 83, loss = 0.24756558\n",
      "Iteration 84, loss = 0.24738389\n",
      "Iteration 85, loss = 0.24717876\n",
      "Iteration 86, loss = 0.24699610\n",
      "Iteration 87, loss = 0.24680416\n",
      "Iteration 88, loss = 0.24660469\n",
      "Iteration 89, loss = 0.24640648\n",
      "Iteration 90, loss = 0.24625637\n",
      "Iteration 91, loss = 0.24605652\n",
      "Iteration 92, loss = 0.24591722\n",
      "Iteration 93, loss = 0.24569506\n",
      "Iteration 94, loss = 0.24554549\n",
      "Iteration 95, loss = 0.24537602\n",
      "Iteration 96, loss = 0.24521974\n",
      "Iteration 97, loss = 0.24505169\n",
      "Iteration 98, loss = 0.24488407\n",
      "Iteration 99, loss = 0.24472131\n",
      "Iteration 100, loss = 0.24464193\n",
      "Iteration 101, loss = 0.24446220\n",
      "Iteration 102, loss = 0.24426143\n",
      "Iteration 103, loss = 0.24414671\n",
      "Iteration 104, loss = 0.24401795\n",
      "Iteration 105, loss = 0.24388174\n",
      "Iteration 106, loss = 0.24372599\n",
      "Iteration 107, loss = 0.24360303\n",
      "Iteration 108, loss = 0.24346761\n",
      "Iteration 109, loss = 0.24334066\n",
      "Iteration 110, loss = 0.24324197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 111, loss = 0.24309748\n",
      "Iteration 112, loss = 0.24294622\n",
      "Iteration 113, loss = 0.24285583\n",
      "Iteration 114, loss = 0.24271774\n",
      "Iteration 115, loss = 0.24264930\n",
      "Iteration 116, loss = 0.24253338\n",
      "Iteration 117, loss = 0.24242884\n",
      "Iteration 118, loss = 0.24225069\n",
      "Iteration 119, loss = 0.24218209\n",
      "Iteration 120, loss = 0.24207441\n",
      "Iteration 121, loss = 0.24195359\n",
      "Iteration 122, loss = 0.24187229\n",
      "Iteration 123, loss = 0.24175782\n",
      "Iteration 124, loss = 0.24164909\n",
      "Iteration 125, loss = 0.24157255\n",
      "Iteration 126, loss = 0.24146991\n",
      "Iteration 127, loss = 0.24137943\n",
      "Iteration 128, loss = 0.24128509\n",
      "Iteration 129, loss = 0.24120646\n",
      "Iteration 130, loss = 0.24109183\n",
      "Iteration 131, loss = 0.24102271\n",
      "Iteration 132, loss = 0.24095683\n",
      "Iteration 133, loss = 0.24085652\n",
      "Iteration 134, loss = 0.24077423\n",
      "Iteration 135, loss = 0.24068966\n",
      "Iteration 136, loss = 0.24067414\n",
      "Iteration 137, loss = 0.24054808\n",
      "Iteration 138, loss = 0.24044678\n",
      "Iteration 139, loss = 0.24039368\n",
      "Iteration 140, loss = 0.24030921\n",
      "Iteration 141, loss = 0.24021759\n",
      "Iteration 142, loss = 0.24016053\n",
      "Iteration 143, loss = 0.24010399\n",
      "Iteration 144, loss = 0.24002391\n",
      "Iteration 145, loss = 0.23995621\n",
      "Iteration 146, loss = 0.23988403\n",
      "Iteration 147, loss = 0.23983703\n",
      "Iteration 148, loss = 0.23979439\n",
      "Iteration 149, loss = 0.23968224\n",
      "Iteration 150, loss = 0.23961946\n",
      "Iteration 151, loss = 0.23958603\n",
      "Iteration 152, loss = 0.23953980\n",
      "Iteration 153, loss = 0.23948641\n",
      "Iteration 154, loss = 0.23939828\n",
      "Iteration 155, loss = 0.23936098\n",
      "Iteration 156, loss = 0.23928269\n",
      "Iteration 157, loss = 0.23923103\n",
      "Iteration 158, loss = 0.23915980\n",
      "Iteration 159, loss = 0.23913325\n",
      "Iteration 160, loss = 0.23911405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76739801\n",
      "Iteration 2, loss = 0.67431107\n",
      "Iteration 3, loss = 0.58643882\n",
      "Iteration 4, loss = 0.52116738\n",
      "Iteration 5, loss = 0.47334060\n",
      "Iteration 6, loss = 0.43718487\n",
      "Iteration 7, loss = 0.41004918\n",
      "Iteration 8, loss = 0.38857306\n",
      "Iteration 9, loss = 0.37157304\n",
      "Iteration 10, loss = 0.35770321\n",
      "Iteration 11, loss = 0.34609169\n",
      "Iteration 12, loss = 0.33629650\n",
      "Iteration 13, loss = 0.32811238\n",
      "Iteration 14, loss = 0.32088115\n",
      "Iteration 15, loss = 0.31472670\n",
      "Iteration 16, loss = 0.30909596\n",
      "Iteration 17, loss = 0.30434647\n",
      "Iteration 18, loss = 0.30008115\n",
      "Iteration 19, loss = 0.29618054\n",
      "Iteration 20, loss = 0.29280246\n",
      "Iteration 21, loss = 0.28978137\n",
      "Iteration 22, loss = 0.28704282\n",
      "Iteration 23, loss = 0.28457616\n",
      "Iteration 24, loss = 0.28235153\n",
      "Iteration 25, loss = 0.28032719\n",
      "Iteration 26, loss = 0.27847760\n",
      "Iteration 27, loss = 0.27676216\n",
      "Iteration 28, loss = 0.27521743\n",
      "Iteration 29, loss = 0.27370001\n",
      "Iteration 30, loss = 0.27238923\n",
      "Iteration 31, loss = 0.27108032\n",
      "Iteration 32, loss = 0.26992102\n",
      "Iteration 33, loss = 0.26884035\n",
      "Iteration 34, loss = 0.26782254\n",
      "Iteration 35, loss = 0.26682495\n",
      "Iteration 36, loss = 0.26588112\n",
      "Iteration 37, loss = 0.26499375\n",
      "Iteration 38, loss = 0.26418645\n",
      "Iteration 39, loss = 0.26340974\n",
      "Iteration 40, loss = 0.26267635\n",
      "Iteration 41, loss = 0.26195284\n",
      "Iteration 42, loss = 0.26128560\n",
      "Iteration 43, loss = 0.26061065\n",
      "Iteration 44, loss = 0.26000095\n",
      "Iteration 45, loss = 0.25941422\n",
      "Iteration 46, loss = 0.25886964\n",
      "Iteration 47, loss = 0.25833924\n",
      "Iteration 48, loss = 0.25779649\n",
      "Iteration 49, loss = 0.25726891\n",
      "Iteration 50, loss = 0.25676971\n",
      "Iteration 51, loss = 0.25629368\n",
      "Iteration 52, loss = 0.25585892\n",
      "Iteration 53, loss = 0.25540314\n",
      "Iteration 54, loss = 0.25500575\n",
      "Iteration 55, loss = 0.25457851\n",
      "Iteration 56, loss = 0.25416274\n",
      "Iteration 57, loss = 0.25380913\n",
      "Iteration 58, loss = 0.25347852\n",
      "Iteration 59, loss = 0.25306061\n",
      "Iteration 60, loss = 0.25270123\n",
      "Iteration 61, loss = 0.25236575\n",
      "Iteration 62, loss = 0.25203838\n",
      "Iteration 63, loss = 0.25169882\n",
      "Iteration 64, loss = 0.25137510\n",
      "Iteration 65, loss = 0.25105267\n",
      "Iteration 66, loss = 0.25081060\n",
      "Iteration 67, loss = 0.25045088\n",
      "Iteration 68, loss = 0.25018085\n",
      "Iteration 69, loss = 0.24990054\n",
      "Iteration 70, loss = 0.24961190\n",
      "Iteration 71, loss = 0.24935291\n",
      "Iteration 72, loss = 0.24913417\n",
      "Iteration 73, loss = 0.24882219\n",
      "Iteration 74, loss = 0.24858413\n",
      "Iteration 75, loss = 0.24835502\n",
      "Iteration 76, loss = 0.24809970\n",
      "Iteration 77, loss = 0.24788500\n",
      "Iteration 78, loss = 0.24765788\n",
      "Iteration 79, loss = 0.24744395\n",
      "Iteration 80, loss = 0.24718259\n",
      "Iteration 81, loss = 0.24695307\n",
      "Iteration 82, loss = 0.24680432\n",
      "Iteration 83, loss = 0.24654825\n",
      "Iteration 84, loss = 0.24634236\n",
      "Iteration 85, loss = 0.24614578\n",
      "Iteration 86, loss = 0.24593491\n",
      "Iteration 87, loss = 0.24576959\n",
      "Iteration 88, loss = 0.24556901\n",
      "Iteration 89, loss = 0.24539204\n",
      "Iteration 90, loss = 0.24521504\n",
      "Iteration 91, loss = 0.24501097\n",
      "Iteration 92, loss = 0.24484619\n",
      "Iteration 93, loss = 0.24465379\n",
      "Iteration 94, loss = 0.24448124\n",
      "Iteration 95, loss = 0.24432996\n",
      "Iteration 96, loss = 0.24414720\n",
      "Iteration 97, loss = 0.24400605\n",
      "Iteration 98, loss = 0.24385550\n",
      "Iteration 99, loss = 0.24367760\n",
      "Iteration 100, loss = 0.24356228\n",
      "Iteration 101, loss = 0.24337928\n",
      "Iteration 102, loss = 0.24321246\n",
      "Iteration 103, loss = 0.24306893\n",
      "Iteration 104, loss = 0.24293726\n",
      "Iteration 105, loss = 0.24283993\n",
      "Iteration 106, loss = 0.24266476\n",
      "Iteration 107, loss = 0.24254191\n",
      "Iteration 108, loss = 0.24240433\n",
      "Iteration 109, loss = 0.24227637\n",
      "Iteration 110, loss = 0.24213268\n",
      "Iteration 111, loss = 0.24201833\n",
      "Iteration 112, loss = 0.24183963\n",
      "Iteration 113, loss = 0.24178785\n",
      "Iteration 114, loss = 0.24165105\n",
      "Iteration 115, loss = 0.24154825\n",
      "Iteration 116, loss = 0.24141167\n",
      "Iteration 117, loss = 0.24129785\n",
      "Iteration 118, loss = 0.24117065\n",
      "Iteration 119, loss = 0.24108245\n",
      "Iteration 120, loss = 0.24098565\n",
      "Iteration 121, loss = 0.24083410\n",
      "Iteration 122, loss = 0.24076572\n",
      "Iteration 123, loss = 0.24063355\n",
      "Iteration 124, loss = 0.24052387\n",
      "Iteration 125, loss = 0.24046640\n",
      "Iteration 126, loss = 0.24032624\n",
      "Iteration 127, loss = 0.24025366\n",
      "Iteration 128, loss = 0.24013751\n",
      "Iteration 129, loss = 0.24006297\n",
      "Iteration 130, loss = 0.23996342\n",
      "Iteration 131, loss = 0.23990849\n",
      "Iteration 132, loss = 0.23977984\n",
      "Iteration 133, loss = 0.23973126\n",
      "Iteration 134, loss = 0.23963262\n",
      "Iteration 135, loss = 0.23953997\n",
      "Iteration 136, loss = 0.23943590\n",
      "Iteration 137, loss = 0.23939161\n",
      "Iteration 138, loss = 0.23928970\n",
      "Iteration 139, loss = 0.23920325\n",
      "Iteration 140, loss = 0.23909787\n",
      "Iteration 141, loss = 0.23906753\n",
      "Iteration 142, loss = 0.23898596\n",
      "Iteration 143, loss = 0.23895715\n",
      "Iteration 144, loss = 0.23884283\n",
      "Iteration 145, loss = 0.23878083\n",
      "Iteration 146, loss = 0.23869583\n",
      "Iteration 147, loss = 0.23863663\n",
      "Iteration 148, loss = 0.23858843\n",
      "Iteration 149, loss = 0.23847160\n",
      "Iteration 150, loss = 0.23844147\n",
      "Iteration 151, loss = 0.23837355\n",
      "Iteration 152, loss = 0.23832720\n",
      "Iteration 153, loss = 0.23825902\n",
      "Iteration 154, loss = 0.23815860\n",
      "Iteration 155, loss = 0.23812542\n",
      "Iteration 156, loss = 0.23805120\n",
      "Iteration 157, loss = 0.23799440\n",
      "Iteration 158, loss = 0.23793336\n",
      "Iteration 159, loss = 0.23791520\n",
      "Iteration 160, loss = 0.23787021\n",
      "Iteration 161, loss = 0.23776064\n",
      "Iteration 162, loss = 0.23773745\n",
      "Iteration 163, loss = 0.23764623\n",
      "Iteration 164, loss = 0.23761176\n",
      "Iteration 165, loss = 0.23755471\n",
      "Iteration 166, loss = 0.23750380\n",
      "Iteration 167, loss = 0.23745857\n",
      "Iteration 168, loss = 0.23740148\n",
      "Iteration 169, loss = 0.23737810\n",
      "Iteration 170, loss = 0.23734994\n",
      "Iteration 171, loss = 0.23729629\n",
      "Iteration 172, loss = 0.23724823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77162242\n",
      "Iteration 2, loss = 0.67748617\n",
      "Iteration 3, loss = 0.58794400\n",
      "Iteration 4, loss = 0.52254080\n",
      "Iteration 5, loss = 0.47424964\n",
      "Iteration 6, loss = 0.43810525\n",
      "Iteration 7, loss = 0.41112077\n",
      "Iteration 8, loss = 0.39000980\n",
      "Iteration 9, loss = 0.37303759\n",
      "Iteration 10, loss = 0.35936038\n",
      "Iteration 11, loss = 0.34800461\n",
      "Iteration 12, loss = 0.33826141\n",
      "Iteration 13, loss = 0.33030194\n",
      "Iteration 14, loss = 0.32318889\n",
      "Iteration 15, loss = 0.31712944\n",
      "Iteration 16, loss = 0.31161325\n",
      "Iteration 17, loss = 0.30698921\n",
      "Iteration 18, loss = 0.30281375\n",
      "Iteration 19, loss = 0.29895039\n",
      "Iteration 20, loss = 0.29566227\n",
      "Iteration 21, loss = 0.29272317\n",
      "Iteration 22, loss = 0.29004910\n",
      "Iteration 23, loss = 0.28763821\n",
      "Iteration 24, loss = 0.28548240\n",
      "Iteration 25, loss = 0.28348886\n",
      "Iteration 26, loss = 0.28169077\n",
      "Iteration 27, loss = 0.28002608\n",
      "Iteration 28, loss = 0.27850813\n",
      "Iteration 29, loss = 0.27707138\n",
      "Iteration 30, loss = 0.27577074\n",
      "Iteration 31, loss = 0.27455160\n",
      "Iteration 32, loss = 0.27341106\n",
      "Iteration 33, loss = 0.27237194\n",
      "Iteration 34, loss = 0.27137795\n",
      "Iteration 35, loss = 0.27042952\n",
      "Iteration 36, loss = 0.26955236\n",
      "Iteration 37, loss = 0.26869121\n",
      "Iteration 38, loss = 0.26792249\n",
      "Iteration 39, loss = 0.26715706\n",
      "Iteration 40, loss = 0.26644204\n",
      "Iteration 41, loss = 0.26579624\n",
      "Iteration 42, loss = 0.26515180\n",
      "Iteration 43, loss = 0.26451052\n",
      "Iteration 44, loss = 0.26392944\n",
      "Iteration 45, loss = 0.26334874\n",
      "Iteration 46, loss = 0.26285923\n",
      "Iteration 47, loss = 0.26229824\n",
      "Iteration 48, loss = 0.26183149\n",
      "Iteration 49, loss = 0.26132680\n",
      "Iteration 50, loss = 0.26089601\n",
      "Iteration 51, loss = 0.26043373\n",
      "Iteration 52, loss = 0.25999001\n",
      "Iteration 53, loss = 0.25959219\n",
      "Iteration 54, loss = 0.25924199\n",
      "Iteration 55, loss = 0.25877991\n",
      "Iteration 56, loss = 0.25840747\n",
      "Iteration 57, loss = 0.25810189\n",
      "Iteration 58, loss = 0.25774067\n",
      "Iteration 59, loss = 0.25738061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.25705716\n",
      "Iteration 61, loss = 0.25674988\n",
      "Iteration 62, loss = 0.25643373\n",
      "Iteration 63, loss = 0.25611206\n",
      "Iteration 64, loss = 0.25581203\n",
      "Iteration 65, loss = 0.25553383\n",
      "Iteration 66, loss = 0.25531087\n",
      "Iteration 67, loss = 0.25497498\n",
      "Iteration 68, loss = 0.25471798\n",
      "Iteration 69, loss = 0.25444013\n",
      "Iteration 70, loss = 0.25418458\n",
      "Iteration 71, loss = 0.25397418\n",
      "Iteration 72, loss = 0.25373106\n",
      "Iteration 73, loss = 0.25346929\n",
      "Iteration 74, loss = 0.25326488\n",
      "Iteration 75, loss = 0.25304513\n",
      "Iteration 76, loss = 0.25277710\n",
      "Iteration 77, loss = 0.25257649\n",
      "Iteration 78, loss = 0.25235960\n",
      "Iteration 79, loss = 0.25213905\n",
      "Iteration 80, loss = 0.25195508\n",
      "Iteration 81, loss = 0.25173276\n",
      "Iteration 82, loss = 0.25157554\n",
      "Iteration 83, loss = 0.25135097\n",
      "Iteration 84, loss = 0.25114059\n",
      "Iteration 85, loss = 0.25099414\n",
      "Iteration 86, loss = 0.25078438\n",
      "Iteration 87, loss = 0.25063867\n",
      "Iteration 88, loss = 0.25048078\n",
      "Iteration 89, loss = 0.25030966\n",
      "Iteration 90, loss = 0.25013444\n",
      "Iteration 91, loss = 0.24995540\n",
      "Iteration 92, loss = 0.24978648\n",
      "Iteration 93, loss = 0.24960802\n",
      "Iteration 94, loss = 0.24945377\n",
      "Iteration 95, loss = 0.24929919\n",
      "Iteration 96, loss = 0.24915854\n",
      "Iteration 97, loss = 0.24901201\n",
      "Iteration 98, loss = 0.24887977\n",
      "Iteration 99, loss = 0.24870531\n",
      "Iteration 100, loss = 0.24860670\n",
      "Iteration 101, loss = 0.24847814\n",
      "Iteration 102, loss = 0.24831321\n",
      "Iteration 103, loss = 0.24816944\n",
      "Iteration 104, loss = 0.24804513\n",
      "Iteration 105, loss = 0.24791983\n",
      "Iteration 106, loss = 0.24778205\n",
      "Iteration 107, loss = 0.24771180\n",
      "Iteration 108, loss = 0.24755563\n",
      "Iteration 109, loss = 0.24744664\n",
      "Iteration 110, loss = 0.24731147\n",
      "Iteration 111, loss = 0.24720130\n",
      "Iteration 112, loss = 0.24708820\n",
      "Iteration 113, loss = 0.24700698\n",
      "Iteration 114, loss = 0.24685607\n",
      "Iteration 115, loss = 0.24673173\n",
      "Iteration 116, loss = 0.24663488\n",
      "Iteration 117, loss = 0.24653201\n",
      "Iteration 118, loss = 0.24642329\n",
      "Iteration 119, loss = 0.24635669\n",
      "Iteration 120, loss = 0.24624392\n",
      "Iteration 121, loss = 0.24610883\n",
      "Iteration 122, loss = 0.24606603\n",
      "Iteration 123, loss = 0.24593718\n",
      "Iteration 124, loss = 0.24581064\n",
      "Iteration 125, loss = 0.24579858\n",
      "Iteration 126, loss = 0.24566322\n",
      "Iteration 127, loss = 0.24561003\n",
      "Iteration 128, loss = 0.24548411\n",
      "Iteration 129, loss = 0.24540366\n",
      "Iteration 130, loss = 0.24530654\n",
      "Iteration 131, loss = 0.24526129\n",
      "Iteration 132, loss = 0.24516503\n",
      "Iteration 133, loss = 0.24507217\n",
      "Iteration 134, loss = 0.24498773\n",
      "Iteration 135, loss = 0.24490699\n",
      "Iteration 136, loss = 0.24483122\n",
      "Iteration 137, loss = 0.24475472\n",
      "Iteration 138, loss = 0.24469674\n",
      "Iteration 139, loss = 0.24462868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77396419\n",
      "Iteration 2, loss = 0.67924684\n",
      "Iteration 3, loss = 0.59064307\n",
      "Iteration 4, loss = 0.52431238\n",
      "Iteration 5, loss = 0.47595306\n",
      "Iteration 6, loss = 0.44023040\n",
      "Iteration 7, loss = 0.41301107\n",
      "Iteration 8, loss = 0.39152446\n",
      "Iteration 9, loss = 0.37451441\n",
      "Iteration 10, loss = 0.36061268\n",
      "Iteration 11, loss = 0.34913668\n",
      "Iteration 12, loss = 0.33930013\n",
      "Iteration 13, loss = 0.33111863\n",
      "Iteration 14, loss = 0.32388238\n",
      "Iteration 15, loss = 0.31753440\n",
      "Iteration 16, loss = 0.31209198\n",
      "Iteration 17, loss = 0.30720011\n",
      "Iteration 18, loss = 0.30283838\n",
      "Iteration 19, loss = 0.29898381\n",
      "Iteration 20, loss = 0.29554890\n",
      "Iteration 21, loss = 0.29247935\n",
      "Iteration 22, loss = 0.28968691\n",
      "Iteration 23, loss = 0.28717232\n",
      "Iteration 24, loss = 0.28490107\n",
      "Iteration 25, loss = 0.28287012\n",
      "Iteration 26, loss = 0.28092559\n",
      "Iteration 27, loss = 0.27921937\n",
      "Iteration 28, loss = 0.27761195\n",
      "Iteration 29, loss = 0.27613296\n",
      "Iteration 30, loss = 0.27467660\n",
      "Iteration 31, loss = 0.27343148\n",
      "Iteration 32, loss = 0.27216714\n",
      "Iteration 33, loss = 0.27106651\n",
      "Iteration 34, loss = 0.26997822\n",
      "Iteration 35, loss = 0.26897984\n",
      "Iteration 36, loss = 0.26801564\n",
      "Iteration 37, loss = 0.26707399\n",
      "Iteration 38, loss = 0.26629388\n",
      "Iteration 39, loss = 0.26543506\n",
      "Iteration 40, loss = 0.26465681\n",
      "Iteration 41, loss = 0.26392981\n",
      "Iteration 42, loss = 0.26321110\n",
      "Iteration 43, loss = 0.26253895\n",
      "Iteration 44, loss = 0.26191423\n",
      "Iteration 45, loss = 0.26125555\n",
      "Iteration 46, loss = 0.26070656\n",
      "Iteration 47, loss = 0.26012163\n",
      "Iteration 48, loss = 0.25952397\n",
      "Iteration 49, loss = 0.25904259\n",
      "Iteration 50, loss = 0.25850955\n",
      "Iteration 51, loss = 0.25801200\n",
      "Iteration 52, loss = 0.25755328\n",
      "Iteration 53, loss = 0.25707023\n",
      "Iteration 54, loss = 0.25661626\n",
      "Iteration 55, loss = 0.25614743\n",
      "Iteration 56, loss = 0.25575895\n",
      "Iteration 57, loss = 0.25534280\n",
      "Iteration 58, loss = 0.25497255\n",
      "Iteration 59, loss = 0.25455672\n",
      "Iteration 60, loss = 0.25418875\n",
      "Iteration 61, loss = 0.25384627\n",
      "Iteration 62, loss = 0.25345335\n",
      "Iteration 63, loss = 0.25312663\n",
      "Iteration 64, loss = 0.25277019\n",
      "Iteration 65, loss = 0.25247189\n",
      "Iteration 66, loss = 0.25211189\n",
      "Iteration 67, loss = 0.25181295\n",
      "Iteration 68, loss = 0.25151116\n",
      "Iteration 69, loss = 0.25119809\n",
      "Iteration 70, loss = 0.25091411\n",
      "Iteration 71, loss = 0.25064498\n",
      "Iteration 72, loss = 0.25039069\n",
      "Iteration 73, loss = 0.25007126\n",
      "Iteration 74, loss = 0.24979351\n",
      "Iteration 75, loss = 0.24957042\n",
      "Iteration 76, loss = 0.24931327\n",
      "Iteration 77, loss = 0.24901884\n",
      "Iteration 78, loss = 0.24880539\n",
      "Iteration 79, loss = 0.24855575\n",
      "Iteration 80, loss = 0.24829465\n",
      "Iteration 81, loss = 0.24809591\n",
      "Iteration 82, loss = 0.24794553\n",
      "Iteration 83, loss = 0.24762585\n",
      "Iteration 84, loss = 0.24743288\n",
      "Iteration 85, loss = 0.24717629\n",
      "Iteration 86, loss = 0.24700888\n",
      "Iteration 87, loss = 0.24678323\n",
      "Iteration 88, loss = 0.24657337\n",
      "Iteration 89, loss = 0.24637415\n",
      "Iteration 90, loss = 0.24617821\n",
      "Iteration 91, loss = 0.24597776\n",
      "Iteration 92, loss = 0.24583481\n",
      "Iteration 93, loss = 0.24558635\n",
      "Iteration 94, loss = 0.24541253\n",
      "Iteration 95, loss = 0.24523283\n",
      "Iteration 96, loss = 0.24510355\n",
      "Iteration 97, loss = 0.24490499\n",
      "Iteration 98, loss = 0.24469130\n",
      "Iteration 99, loss = 0.24454513\n",
      "Iteration 100, loss = 0.24443525\n",
      "Iteration 101, loss = 0.24422617\n",
      "Iteration 102, loss = 0.24409427\n",
      "Iteration 103, loss = 0.24395753\n",
      "Iteration 104, loss = 0.24373743\n",
      "Iteration 105, loss = 0.24359755\n",
      "Iteration 106, loss = 0.24345566\n",
      "Iteration 107, loss = 0.24331012\n",
      "Iteration 108, loss = 0.24316011\n",
      "Iteration 109, loss = 0.24301564\n",
      "Iteration 110, loss = 0.24289101\n",
      "Iteration 111, loss = 0.24280170\n",
      "Iteration 112, loss = 0.24263017\n",
      "Iteration 113, loss = 0.24248945\n",
      "Iteration 114, loss = 0.24236798\n",
      "Iteration 115, loss = 0.24224255\n",
      "Iteration 116, loss = 0.24211180\n",
      "Iteration 117, loss = 0.24198972\n",
      "Iteration 118, loss = 0.24183951\n",
      "Iteration 119, loss = 0.24177791\n",
      "Iteration 120, loss = 0.24164347\n",
      "Iteration 121, loss = 0.24153338\n",
      "Iteration 122, loss = 0.24140723\n",
      "Iteration 123, loss = 0.24132034\n",
      "Iteration 124, loss = 0.24122659\n",
      "Iteration 125, loss = 0.24109011\n",
      "Iteration 126, loss = 0.24097734\n",
      "Iteration 127, loss = 0.24088642\n",
      "Iteration 128, loss = 0.24079304\n",
      "Iteration 129, loss = 0.24068130\n",
      "Iteration 130, loss = 0.24064704\n",
      "Iteration 131, loss = 0.24049707\n",
      "Iteration 132, loss = 0.24040226\n",
      "Iteration 133, loss = 0.24032465\n",
      "Iteration 134, loss = 0.24031278\n",
      "Iteration 135, loss = 0.24010542\n",
      "Iteration 136, loss = 0.24003390\n",
      "Iteration 137, loss = 0.23996522\n",
      "Iteration 138, loss = 0.23985934\n",
      "Iteration 139, loss = 0.23977507\n",
      "Iteration 140, loss = 0.23971878\n",
      "Iteration 141, loss = 0.23964631\n",
      "Iteration 142, loss = 0.23953032\n",
      "Iteration 143, loss = 0.23948983\n",
      "Iteration 144, loss = 0.23943618\n",
      "Iteration 145, loss = 0.23932395\n",
      "Iteration 146, loss = 0.23924791\n",
      "Iteration 147, loss = 0.23919469\n",
      "Iteration 148, loss = 0.23912268\n",
      "Iteration 149, loss = 0.23905749\n",
      "Iteration 150, loss = 0.23897354\n",
      "Iteration 151, loss = 0.23890340\n",
      "Iteration 152, loss = 0.23893190\n",
      "Iteration 153, loss = 0.23878225\n",
      "Iteration 154, loss = 0.23871877\n",
      "Iteration 155, loss = 0.23866763\n",
      "Iteration 156, loss = 0.23860036\n",
      "Iteration 157, loss = 0.23856370\n",
      "Iteration 158, loss = 0.23845097\n",
      "Iteration 159, loss = 0.23843491\n",
      "Iteration 160, loss = 0.23833677\n",
      "Iteration 161, loss = 0.23831034\n",
      "Iteration 162, loss = 0.23827378\n",
      "Iteration 163, loss = 0.23818179\n",
      "Iteration 164, loss = 0.23815517\n",
      "Iteration 165, loss = 0.23808845\n",
      "Iteration 166, loss = 0.23806241\n",
      "Iteration 167, loss = 0.23799862\n",
      "Iteration 168, loss = 0.23793844\n",
      "Iteration 169, loss = 0.23793060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77548385\n",
      "Iteration 2, loss = 0.67807468\n",
      "Iteration 3, loss = 0.58731450\n",
      "Iteration 4, loss = 0.52011004\n",
      "Iteration 5, loss = 0.47158008\n",
      "Iteration 6, loss = 0.43570167\n",
      "Iteration 7, loss = 0.40896748\n",
      "Iteration 8, loss = 0.38771395\n",
      "Iteration 9, loss = 0.37125583\n",
      "Iteration 10, loss = 0.35795663\n",
      "Iteration 11, loss = 0.34689467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.33758150\n",
      "Iteration 13, loss = 0.32985023\n",
      "Iteration 14, loss = 0.32303259\n",
      "Iteration 15, loss = 0.31717184\n",
      "Iteration 16, loss = 0.31201987\n",
      "Iteration 17, loss = 0.30752697\n",
      "Iteration 18, loss = 0.30350440\n",
      "Iteration 19, loss = 0.29989328\n",
      "Iteration 20, loss = 0.29661687\n",
      "Iteration 21, loss = 0.29379735\n",
      "Iteration 22, loss = 0.29108433\n",
      "Iteration 23, loss = 0.28858655\n",
      "Iteration 24, loss = 0.28641814\n",
      "Iteration 25, loss = 0.28439403\n",
      "Iteration 26, loss = 0.28250781\n",
      "Iteration 27, loss = 0.28080955\n",
      "Iteration 28, loss = 0.27923430\n",
      "Iteration 29, loss = 0.27781864\n",
      "Iteration 30, loss = 0.27638342\n",
      "Iteration 31, loss = 0.27516244\n",
      "Iteration 32, loss = 0.27397449\n",
      "Iteration 33, loss = 0.27294068\n",
      "Iteration 34, loss = 0.27189941\n",
      "Iteration 35, loss = 0.27095951\n",
      "Iteration 36, loss = 0.27006737\n",
      "Iteration 37, loss = 0.26923233\n",
      "Iteration 38, loss = 0.26842920\n",
      "Iteration 39, loss = 0.26768071\n",
      "Iteration 40, loss = 0.26694640\n",
      "Iteration 41, loss = 0.26626385\n",
      "Iteration 42, loss = 0.26564205\n",
      "Iteration 43, loss = 0.26501697\n",
      "Iteration 44, loss = 0.26445481\n",
      "Iteration 45, loss = 0.26386410\n",
      "Iteration 46, loss = 0.26335200\n",
      "Iteration 47, loss = 0.26285148\n",
      "Iteration 48, loss = 0.26230126\n",
      "Iteration 49, loss = 0.26188691\n",
      "Iteration 50, loss = 0.26140155\n",
      "Iteration 51, loss = 0.26102416\n",
      "Iteration 52, loss = 0.26057559\n",
      "Iteration 53, loss = 0.26013972\n",
      "Iteration 54, loss = 0.25971011\n",
      "Iteration 55, loss = 0.25932332\n",
      "Iteration 56, loss = 0.25896541\n",
      "Iteration 57, loss = 0.25862431\n",
      "Iteration 58, loss = 0.25828943\n",
      "Iteration 59, loss = 0.25792633\n",
      "Iteration 60, loss = 0.25760133\n",
      "Iteration 61, loss = 0.25729476\n",
      "Iteration 62, loss = 0.25695324\n",
      "Iteration 63, loss = 0.25668572\n",
      "Iteration 64, loss = 0.25636169\n",
      "Iteration 65, loss = 0.25612510\n",
      "Iteration 66, loss = 0.25581963\n",
      "Iteration 67, loss = 0.25555290\n",
      "Iteration 68, loss = 0.25527289\n",
      "Iteration 69, loss = 0.25502030\n",
      "Iteration 70, loss = 0.25479090\n",
      "Iteration 71, loss = 0.25453746\n",
      "Iteration 72, loss = 0.25428596\n",
      "Iteration 73, loss = 0.25402547\n",
      "Iteration 74, loss = 0.25381118\n",
      "Iteration 75, loss = 0.25357496\n",
      "Iteration 76, loss = 0.25338521\n",
      "Iteration 77, loss = 0.25316984\n",
      "Iteration 78, loss = 0.25297858\n",
      "Iteration 79, loss = 0.25273164\n",
      "Iteration 80, loss = 0.25255430\n",
      "Iteration 81, loss = 0.25235874\n",
      "Iteration 82, loss = 0.25222109\n",
      "Iteration 83, loss = 0.25198306\n",
      "Iteration 84, loss = 0.25177482\n",
      "Iteration 85, loss = 0.25153783\n",
      "Iteration 86, loss = 0.25141728\n",
      "Iteration 87, loss = 0.25122483\n",
      "Iteration 88, loss = 0.25104647\n",
      "Iteration 89, loss = 0.25087372\n",
      "Iteration 90, loss = 0.25069418\n",
      "Iteration 91, loss = 0.25055126\n",
      "Iteration 92, loss = 0.25039163\n",
      "Iteration 93, loss = 0.25020523\n",
      "Iteration 94, loss = 0.25007704\n",
      "Iteration 95, loss = 0.24993050\n",
      "Iteration 96, loss = 0.24985388\n",
      "Iteration 97, loss = 0.24962590\n",
      "Iteration 98, loss = 0.24948744\n",
      "Iteration 99, loss = 0.24933451\n",
      "Iteration 100, loss = 0.24921940\n",
      "Iteration 101, loss = 0.24909656\n",
      "Iteration 102, loss = 0.24895208\n",
      "Iteration 103, loss = 0.24883832\n",
      "Iteration 104, loss = 0.24866179\n",
      "Iteration 105, loss = 0.24854293\n",
      "Iteration 106, loss = 0.24841147\n",
      "Iteration 107, loss = 0.24829223\n",
      "Iteration 108, loss = 0.24821452\n",
      "Iteration 109, loss = 0.24805869\n",
      "Iteration 110, loss = 0.24795457\n",
      "Iteration 111, loss = 0.24787893\n",
      "Iteration 112, loss = 0.24773596\n",
      "Iteration 113, loss = 0.24762642\n",
      "Iteration 114, loss = 0.24750009\n",
      "Iteration 115, loss = 0.24740648\n",
      "Iteration 116, loss = 0.24730307\n",
      "Iteration 117, loss = 0.24719439\n",
      "Iteration 118, loss = 0.24706762\n",
      "Iteration 119, loss = 0.24701157\n",
      "Iteration 120, loss = 0.24688978\n",
      "Iteration 121, loss = 0.24681654\n",
      "Iteration 122, loss = 0.24673610\n",
      "Iteration 123, loss = 0.24662898\n",
      "Iteration 124, loss = 0.24655636\n",
      "Iteration 125, loss = 0.24643610\n",
      "Iteration 126, loss = 0.24636794\n",
      "Iteration 127, loss = 0.24627333\n",
      "Iteration 128, loss = 0.24619894\n",
      "Iteration 129, loss = 0.24611070\n",
      "Iteration 130, loss = 0.24606641\n",
      "Iteration 131, loss = 0.24593192\n",
      "Iteration 132, loss = 0.24586200\n",
      "Iteration 133, loss = 0.24579692\n",
      "Iteration 134, loss = 0.24576567\n",
      "Iteration 135, loss = 0.24562661\n",
      "Iteration 136, loss = 0.24556396\n",
      "Iteration 137, loss = 0.24547074\n",
      "Iteration 138, loss = 0.24542161\n",
      "Iteration 139, loss = 0.24534559\n",
      "Iteration 140, loss = 0.24531433\n",
      "Iteration 141, loss = 0.24521140\n",
      "Iteration 142, loss = 0.24515810\n",
      "Iteration 143, loss = 0.24511259\n",
      "Iteration 144, loss = 0.24504666\n",
      "Iteration 145, loss = 0.24496538\n",
      "Iteration 146, loss = 0.24488905\n",
      "Iteration 147, loss = 0.24486735\n",
      "Iteration 148, loss = 0.24480829\n",
      "Iteration 149, loss = 0.24473339\n",
      "Iteration 150, loss = 0.24470319\n",
      "Iteration 151, loss = 0.24463695\n",
      "Iteration 152, loss = 0.24462325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67584268\n",
      "Iteration 2, loss = 0.50356848\n",
      "Iteration 3, loss = 0.41796032\n",
      "Iteration 4, loss = 0.36516432\n",
      "Iteration 5, loss = 0.32934988\n",
      "Iteration 6, loss = 0.30499434\n",
      "Iteration 7, loss = 0.28882327\n",
      "Iteration 8, loss = 0.27676495\n",
      "Iteration 9, loss = 0.26783581\n",
      "Iteration 10, loss = 0.26169172\n",
      "Iteration 11, loss = 0.25661112\n",
      "Iteration 12, loss = 0.25235950\n",
      "Iteration 13, loss = 0.24993012\n",
      "Iteration 14, loss = 0.24755039\n",
      "Iteration 15, loss = 0.24558002\n",
      "Iteration 16, loss = 0.24437925\n",
      "Iteration 17, loss = 0.24256278\n",
      "Iteration 18, loss = 0.24210405\n",
      "Iteration 19, loss = 0.24123513\n",
      "Iteration 20, loss = 0.24104741\n",
      "Iteration 21, loss = 0.23974902\n",
      "Iteration 22, loss = 0.24047266\n",
      "Iteration 23, loss = 0.23925001\n",
      "Iteration 24, loss = 0.23890113\n",
      "Iteration 25, loss = 0.23896817\n",
      "Iteration 26, loss = 0.23892780\n",
      "Iteration 27, loss = 0.23876636\n",
      "Iteration 28, loss = 0.23918162\n",
      "Iteration 29, loss = 0.23799657\n",
      "Iteration 30, loss = 0.23874357\n",
      "Iteration 31, loss = 0.23833593\n",
      "Iteration 32, loss = 0.23813490\n",
      "Iteration 33, loss = 0.23769096\n",
      "Iteration 34, loss = 0.23812259\n",
      "Iteration 35, loss = 0.23897020\n",
      "Iteration 36, loss = 0.23887180\n",
      "Iteration 37, loss = 0.23792157\n",
      "Iteration 38, loss = 0.23805495\n",
      "Iteration 39, loss = 0.23817928\n",
      "Iteration 40, loss = 0.23904807\n",
      "Iteration 41, loss = 0.23839498\n",
      "Iteration 42, loss = 0.23850699\n",
      "Iteration 43, loss = 0.23806148\n",
      "Iteration 44, loss = 0.23856890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67304770\n",
      "Iteration 2, loss = 0.50433898\n",
      "Iteration 3, loss = 0.41731498\n",
      "Iteration 4, loss = 0.36232805\n",
      "Iteration 5, loss = 0.32588992\n",
      "Iteration 6, loss = 0.30189743\n",
      "Iteration 7, loss = 0.28579252\n",
      "Iteration 8, loss = 0.27383832\n",
      "Iteration 9, loss = 0.26546460\n",
      "Iteration 10, loss = 0.25957136\n",
      "Iteration 11, loss = 0.25457682\n",
      "Iteration 12, loss = 0.25067952\n",
      "Iteration 13, loss = 0.24806590\n",
      "Iteration 14, loss = 0.24559105\n",
      "Iteration 15, loss = 0.24353725\n",
      "Iteration 16, loss = 0.24229416\n",
      "Iteration 17, loss = 0.24155053\n",
      "Iteration 18, loss = 0.24035573\n",
      "Iteration 19, loss = 0.23948445\n",
      "Iteration 20, loss = 0.23873145\n",
      "Iteration 21, loss = 0.23836684\n",
      "Iteration 22, loss = 0.23835335\n",
      "Iteration 23, loss = 0.23780432\n",
      "Iteration 24, loss = 0.23749213\n",
      "Iteration 25, loss = 0.23716467\n",
      "Iteration 26, loss = 0.23710940\n",
      "Iteration 27, loss = 0.23707918\n",
      "Iteration 28, loss = 0.23737830\n",
      "Iteration 29, loss = 0.23682540\n",
      "Iteration 30, loss = 0.23672156\n",
      "Iteration 31, loss = 0.23657747\n",
      "Iteration 32, loss = 0.23650168\n",
      "Iteration 33, loss = 0.23633356\n",
      "Iteration 34, loss = 0.23717706\n",
      "Iteration 35, loss = 0.23703393\n",
      "Iteration 36, loss = 0.23712044\n",
      "Iteration 37, loss = 0.23624404\n",
      "Iteration 38, loss = 0.23613027\n",
      "Iteration 39, loss = 0.23651676\n",
      "Iteration 40, loss = 0.23710180\n",
      "Iteration 41, loss = 0.23698546\n",
      "Iteration 42, loss = 0.23674054\n",
      "Iteration 43, loss = 0.23643111\n",
      "Iteration 44, loss = 0.23663126\n",
      "Iteration 45, loss = 0.23687257\n",
      "Iteration 46, loss = 0.23716954\n",
      "Iteration 47, loss = 0.23700543\n",
      "Iteration 48, loss = 0.23713369\n",
      "Iteration 49, loss = 0.23760620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67284065\n",
      "Iteration 2, loss = 0.50489963\n",
      "Iteration 3, loss = 0.41812080\n",
      "Iteration 4, loss = 0.36405662\n",
      "Iteration 5, loss = 0.32844317\n",
      "Iteration 6, loss = 0.30477212\n",
      "Iteration 7, loss = 0.28890108\n",
      "Iteration 8, loss = 0.27812306\n",
      "Iteration 9, loss = 0.26911723\n",
      "Iteration 10, loss = 0.26329326\n",
      "Iteration 11, loss = 0.25892378\n",
      "Iteration 12, loss = 0.25501057\n",
      "Iteration 13, loss = 0.25311961\n",
      "Iteration 14, loss = 0.25070309\n",
      "Iteration 15, loss = 0.24864851\n",
      "Iteration 16, loss = 0.24735704\n",
      "Iteration 17, loss = 0.24713883\n",
      "Iteration 18, loss = 0.24601234\n",
      "Iteration 19, loss = 0.24483677\n",
      "Iteration 20, loss = 0.24420844\n",
      "Iteration 21, loss = 0.24416709\n",
      "Iteration 22, loss = 0.24459275\n",
      "Iteration 23, loss = 0.24383251\n",
      "Iteration 24, loss = 0.24311701\n",
      "Iteration 25, loss = 0.24281650\n",
      "Iteration 26, loss = 0.24310332\n",
      "Iteration 27, loss = 0.24268075\n",
      "Iteration 28, loss = 0.24294886\n",
      "Iteration 29, loss = 0.24222970\n",
      "Iteration 30, loss = 0.24204471\n",
      "Iteration 31, loss = 0.24236660\n",
      "Iteration 32, loss = 0.24286692\n",
      "Iteration 33, loss = 0.24230108\n",
      "Iteration 34, loss = 0.24256923\n",
      "Iteration 35, loss = 0.24222695\n",
      "Iteration 36, loss = 0.24290887\n",
      "Iteration 37, loss = 0.24265029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.24244820\n",
      "Iteration 39, loss = 0.24196619\n",
      "Iteration 40, loss = 0.24249528\n",
      "Iteration 41, loss = 0.24298945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67557537\n",
      "Iteration 2, loss = 0.50804544\n",
      "Iteration 3, loss = 0.42171332\n",
      "Iteration 4, loss = 0.36468111\n",
      "Iteration 5, loss = 0.32903431\n",
      "Iteration 6, loss = 0.30455427\n",
      "Iteration 7, loss = 0.28782259\n",
      "Iteration 8, loss = 0.27590425\n",
      "Iteration 9, loss = 0.26794082\n",
      "Iteration 10, loss = 0.26118288\n",
      "Iteration 11, loss = 0.25729124\n",
      "Iteration 12, loss = 0.25269552\n",
      "Iteration 13, loss = 0.24941476\n",
      "Iteration 14, loss = 0.24721220\n",
      "Iteration 15, loss = 0.24482975\n",
      "Iteration 16, loss = 0.24323419\n",
      "Iteration 17, loss = 0.24220643\n",
      "Iteration 18, loss = 0.24111162\n",
      "Iteration 19, loss = 0.24041337\n",
      "Iteration 20, loss = 0.23971726\n",
      "Iteration 21, loss = 0.23996502\n",
      "Iteration 22, loss = 0.23860419\n",
      "Iteration 23, loss = 0.23836725\n",
      "Iteration 24, loss = 0.23807318\n",
      "Iteration 25, loss = 0.23815569\n",
      "Iteration 26, loss = 0.23741541\n",
      "Iteration 27, loss = 0.23828291\n",
      "Iteration 28, loss = 0.23790439\n",
      "Iteration 29, loss = 0.23779996\n",
      "Iteration 30, loss = 0.23725230\n",
      "Iteration 31, loss = 0.23737151\n",
      "Iteration 32, loss = 0.23741870\n",
      "Iteration 33, loss = 0.23708025\n",
      "Iteration 34, loss = 0.23751223\n",
      "Iteration 35, loss = 0.23730693\n",
      "Iteration 36, loss = 0.23677387\n",
      "Iteration 37, loss = 0.23713107\n",
      "Iteration 38, loss = 0.23685588\n",
      "Iteration 39, loss = 0.23727170\n",
      "Iteration 40, loss = 0.23805314\n",
      "Iteration 41, loss = 0.23701625\n",
      "Iteration 42, loss = 0.23702888\n",
      "Iteration 43, loss = 0.23748100\n",
      "Iteration 44, loss = 0.23789063\n",
      "Iteration 45, loss = 0.23681322\n",
      "Iteration 46, loss = 0.23783346\n",
      "Iteration 47, loss = 0.23700935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67057719\n",
      "Iteration 2, loss = 0.50300760\n",
      "Iteration 3, loss = 0.41851634\n",
      "Iteration 4, loss = 0.36458728\n",
      "Iteration 5, loss = 0.33050103\n",
      "Iteration 6, loss = 0.30557872\n",
      "Iteration 7, loss = 0.28965490\n",
      "Iteration 8, loss = 0.27790313\n",
      "Iteration 9, loss = 0.27063619\n",
      "Iteration 10, loss = 0.26497443\n",
      "Iteration 11, loss = 0.26134365\n",
      "Iteration 12, loss = 0.25711132\n",
      "Iteration 13, loss = 0.25447959\n",
      "Iteration 14, loss = 0.25170837\n",
      "Iteration 15, loss = 0.24985401\n",
      "Iteration 16, loss = 0.24866262\n",
      "Iteration 17, loss = 0.24759565\n",
      "Iteration 18, loss = 0.24691964\n",
      "Iteration 19, loss = 0.24573491\n",
      "Iteration 20, loss = 0.24524781\n",
      "Iteration 21, loss = 0.24514489\n",
      "Iteration 22, loss = 0.24494210\n",
      "Iteration 23, loss = 0.24452743\n",
      "Iteration 24, loss = 0.24396117\n",
      "Iteration 25, loss = 0.24379740\n",
      "Iteration 26, loss = 0.24373939\n",
      "Iteration 27, loss = 0.24444494\n",
      "Iteration 28, loss = 0.24354618\n",
      "Iteration 29, loss = 0.24473712\n",
      "Iteration 30, loss = 0.24425876\n",
      "Iteration 31, loss = 0.24347558\n",
      "Iteration 32, loss = 0.24357085\n",
      "Iteration 33, loss = 0.24317451\n",
      "Iteration 34, loss = 0.24410284\n",
      "Iteration 35, loss = 0.24379442\n",
      "Iteration 36, loss = 0.24311469\n",
      "Iteration 37, loss = 0.24409784\n",
      "Iteration 38, loss = 0.24309928\n",
      "Iteration 39, loss = 0.24337930\n",
      "Iteration 40, loss = 0.24372134\n",
      "Iteration 41, loss = 0.24301622\n",
      "Iteration 42, loss = 0.24346717\n",
      "Iteration 43, loss = 0.24352875\n",
      "Iteration 44, loss = 0.24394637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70810519\n",
      "Iteration 2, loss = 0.48936615\n",
      "Iteration 3, loss = 0.44522553\n",
      "Iteration 4, loss = 0.44164346\n",
      "Iteration 5, loss = 0.44109047\n",
      "Iteration 6, loss = 0.44030706\n",
      "Iteration 7, loss = 0.43952365\n",
      "Iteration 8, loss = 0.43880571\n",
      "Iteration 9, loss = 0.43808335\n",
      "Iteration 10, loss = 0.43738486\n",
      "Iteration 11, loss = 0.43664688\n",
      "Iteration 12, loss = 0.43594581\n",
      "Iteration 13, loss = 0.43522431\n",
      "Iteration 14, loss = 0.43450689\n",
      "Iteration 15, loss = 0.43379917\n",
      "Iteration 16, loss = 0.43309511\n",
      "Iteration 17, loss = 0.43240442\n",
      "Iteration 18, loss = 0.43170659\n",
      "Iteration 19, loss = 0.43096392\n",
      "Iteration 20, loss = 0.43033072\n",
      "Iteration 21, loss = 0.42957575\n",
      "Iteration 22, loss = 0.42887539\n",
      "Iteration 23, loss = 0.42818533\n",
      "Iteration 24, loss = 0.42750098\n",
      "Iteration 25, loss = 0.42678601\n",
      "Iteration 26, loss = 0.42611365\n",
      "Iteration 27, loss = 0.42539940\n",
      "Iteration 28, loss = 0.42469418\n",
      "Iteration 29, loss = 0.42398237\n",
      "Iteration 30, loss = 0.42327339\n",
      "Iteration 31, loss = 0.42258642\n",
      "Iteration 32, loss = 0.42189389\n",
      "Iteration 33, loss = 0.42119977\n",
      "Iteration 34, loss = 0.42048652\n",
      "Iteration 35, loss = 0.41978878\n",
      "Iteration 36, loss = 0.41913269\n",
      "Iteration 37, loss = 0.41836062\n",
      "Iteration 38, loss = 0.41764791\n",
      "Iteration 39, loss = 0.41695020\n",
      "Iteration 40, loss = 0.41624017\n",
      "Iteration 41, loss = 0.41551520\n",
      "Iteration 42, loss = 0.41484958\n",
      "Iteration 43, loss = 0.41407944\n",
      "Iteration 44, loss = 0.41339468\n",
      "Iteration 45, loss = 0.41268239\n",
      "Iteration 46, loss = 0.41197300\n",
      "Iteration 47, loss = 0.41124169\n",
      "Iteration 48, loss = 0.41050382\n",
      "Iteration 49, loss = 0.40980239\n",
      "Iteration 50, loss = 0.40905809\n",
      "Iteration 51, loss = 0.40835375\n",
      "Iteration 52, loss = 0.40765366\n",
      "Iteration 53, loss = 0.40684651\n",
      "Iteration 54, loss = 0.40609465\n",
      "Iteration 55, loss = 0.40536162\n",
      "Iteration 56, loss = 0.40464385\n",
      "Iteration 57, loss = 0.40389596\n",
      "Iteration 58, loss = 0.40314803\n",
      "Iteration 59, loss = 0.40239326\n",
      "Iteration 60, loss = 0.40168597\n",
      "Iteration 61, loss = 0.40090798\n",
      "Iteration 62, loss = 0.40015546\n",
      "Iteration 63, loss = 0.39940327\n",
      "Iteration 64, loss = 0.39862110\n",
      "Iteration 65, loss = 0.39788664\n",
      "Iteration 66, loss = 0.39711145\n",
      "Iteration 67, loss = 0.39633584\n",
      "Iteration 68, loss = 0.39558879\n",
      "Iteration 69, loss = 0.39482400\n",
      "Iteration 70, loss = 0.39409820\n",
      "Iteration 71, loss = 0.39328784\n",
      "Iteration 72, loss = 0.39251689\n",
      "Iteration 73, loss = 0.39173376\n",
      "Iteration 74, loss = 0.39094587\n",
      "Iteration 75, loss = 0.39017900\n",
      "Iteration 76, loss = 0.38938261\n",
      "Iteration 77, loss = 0.38862957\n",
      "Iteration 78, loss = 0.38782375\n",
      "Iteration 79, loss = 0.38704566\n",
      "Iteration 80, loss = 0.38626155\n",
      "Iteration 81, loss = 0.38546913\n",
      "Iteration 82, loss = 0.38470265\n",
      "Iteration 83, loss = 0.38387959\n",
      "Iteration 84, loss = 0.38310150\n",
      "Iteration 85, loss = 0.38232541\n",
      "Iteration 86, loss = 0.38158338\n",
      "Iteration 87, loss = 0.38072124\n",
      "Iteration 88, loss = 0.37995243\n",
      "Iteration 89, loss = 0.37915025\n",
      "Iteration 90, loss = 0.37834563\n",
      "Iteration 91, loss = 0.37756389\n",
      "Iteration 92, loss = 0.37676679\n",
      "Iteration 93, loss = 0.37601537\n",
      "Iteration 94, loss = 0.37516550\n",
      "Iteration 95, loss = 0.37435270\n",
      "Iteration 96, loss = 0.37357040\n",
      "Iteration 97, loss = 0.37275512\n",
      "Iteration 98, loss = 0.37194935\n",
      "Iteration 99, loss = 0.37116149\n",
      "Iteration 100, loss = 0.37037429\n",
      "Iteration 101, loss = 0.36956083\n",
      "Iteration 102, loss = 0.36878565\n",
      "Iteration 103, loss = 0.36797331\n",
      "Iteration 104, loss = 0.36720702\n",
      "Iteration 105, loss = 0.36639733\n",
      "Iteration 106, loss = 0.36560218\n",
      "Iteration 107, loss = 0.36482950\n",
      "Iteration 108, loss = 0.36400745\n",
      "Iteration 109, loss = 0.36324302\n",
      "Iteration 110, loss = 0.36250227\n",
      "Iteration 111, loss = 0.36165384\n",
      "Iteration 112, loss = 0.36088807\n",
      "Iteration 113, loss = 0.36009370\n",
      "Iteration 114, loss = 0.35931435\n",
      "Iteration 115, loss = 0.35856737\n",
      "Iteration 116, loss = 0.35775177\n",
      "Iteration 117, loss = 0.35698719\n",
      "Iteration 118, loss = 0.35622620\n",
      "Iteration 119, loss = 0.35545808\n",
      "Iteration 120, loss = 0.35467172\n",
      "Iteration 121, loss = 0.35393089\n",
      "Iteration 122, loss = 0.35318918\n",
      "Iteration 123, loss = 0.35237915\n",
      "Iteration 124, loss = 0.35162900\n",
      "Iteration 125, loss = 0.35087555\n",
      "Iteration 126, loss = 0.35014181\n",
      "Iteration 127, loss = 0.34938203\n",
      "Iteration 128, loss = 0.34863830\n",
      "Iteration 129, loss = 0.34790237\n",
      "Iteration 130, loss = 0.34717266\n",
      "Iteration 131, loss = 0.34646616\n",
      "Iteration 132, loss = 0.34570985\n",
      "Iteration 133, loss = 0.34497348\n",
      "Iteration 134, loss = 0.34426095\n",
      "Iteration 135, loss = 0.34353315\n",
      "Iteration 136, loss = 0.34285028\n",
      "Iteration 137, loss = 0.34211999\n",
      "Iteration 138, loss = 0.34141339\n",
      "Iteration 139, loss = 0.34072892\n",
      "Iteration 140, loss = 0.34000282\n",
      "Iteration 141, loss = 0.33930710\n",
      "Iteration 142, loss = 0.33864138\n",
      "Iteration 143, loss = 0.33792869\n",
      "Iteration 144, loss = 0.33725969\n",
      "Iteration 145, loss = 0.33657735\n",
      "Iteration 146, loss = 0.33591784\n",
      "Iteration 147, loss = 0.33525781\n",
      "Iteration 148, loss = 0.33458013\n",
      "Iteration 149, loss = 0.33392690\n",
      "Iteration 150, loss = 0.33325414\n",
      "Iteration 151, loss = 0.33263717\n",
      "Iteration 152, loss = 0.33193473\n",
      "Iteration 153, loss = 0.33133214\n",
      "Iteration 154, loss = 0.33066795\n",
      "Iteration 155, loss = 0.33005553\n",
      "Iteration 156, loss = 0.32942189\n",
      "Iteration 157, loss = 0.32880744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 158, loss = 0.32817784\n",
      "Iteration 159, loss = 0.32759122\n",
      "Iteration 160, loss = 0.32697436\n",
      "Iteration 161, loss = 0.32636636\n",
      "Iteration 162, loss = 0.32576781\n",
      "Iteration 163, loss = 0.32515442\n",
      "Iteration 164, loss = 0.32457154\n",
      "Iteration 165, loss = 0.32400888\n",
      "Iteration 166, loss = 0.32341481\n",
      "Iteration 167, loss = 0.32288640\n",
      "Iteration 168, loss = 0.32226949\n",
      "Iteration 169, loss = 0.32172678\n",
      "Iteration 170, loss = 0.32114548\n",
      "Iteration 171, loss = 0.32058613\n",
      "Iteration 172, loss = 0.32003365\n",
      "Iteration 173, loss = 0.31950716\n",
      "Iteration 174, loss = 0.31895309\n",
      "Iteration 175, loss = 0.31841902\n",
      "Iteration 176, loss = 0.31788300\n",
      "Iteration 177, loss = 0.31736919\n",
      "Iteration 178, loss = 0.31687585\n",
      "Iteration 179, loss = 0.31634695\n",
      "Iteration 180, loss = 0.31582379\n",
      "Iteration 181, loss = 0.31531451\n",
      "Iteration 182, loss = 0.31487207\n",
      "Iteration 183, loss = 0.31431140\n",
      "Iteration 184, loss = 0.31383799\n",
      "Iteration 185, loss = 0.31331557\n",
      "Iteration 186, loss = 0.31283345\n",
      "Iteration 187, loss = 0.31236429\n",
      "Iteration 188, loss = 0.31189682\n",
      "Iteration 189, loss = 0.31142550\n",
      "Iteration 190, loss = 0.31094789\n",
      "Iteration 191, loss = 0.31049065\n",
      "Iteration 192, loss = 0.31004848\n",
      "Iteration 193, loss = 0.30956992\n",
      "Iteration 194, loss = 0.30914677\n",
      "Iteration 195, loss = 0.30869609\n",
      "Iteration 196, loss = 0.30825090\n",
      "Iteration 197, loss = 0.30781508\n",
      "Iteration 198, loss = 0.30739044\n",
      "Iteration 199, loss = 0.30696428\n",
      "Iteration 200, loss = 0.30653407\n",
      "Iteration 1, loss = 0.70792320\n",
      "Iteration 2, loss = 0.49004319\n",
      "Iteration 3, loss = 0.44566063\n",
      "Iteration 4, loss = 0.44227696\n",
      "Iteration 5, loss = 0.44157085\n",
      "Iteration 6, loss = 0.44078474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.43998309\n",
      "Iteration 8, loss = 0.43925104\n",
      "Iteration 9, loss = 0.43848377\n",
      "Iteration 10, loss = 0.43777981\n",
      "Iteration 11, loss = 0.43701496\n",
      "Iteration 12, loss = 0.43629936\n",
      "Iteration 13, loss = 0.43556204\n",
      "Iteration 14, loss = 0.43480714\n",
      "Iteration 15, loss = 0.43410090\n",
      "Iteration 16, loss = 0.43336660\n",
      "Iteration 17, loss = 0.43268116\n",
      "Iteration 18, loss = 0.43196438\n",
      "Iteration 19, loss = 0.43121835\n",
      "Iteration 20, loss = 0.43049402\n",
      "Iteration 21, loss = 0.42976202\n",
      "Iteration 22, loss = 0.42902296\n",
      "Iteration 23, loss = 0.42830130\n",
      "Iteration 24, loss = 0.42761266\n",
      "Iteration 25, loss = 0.42686611\n",
      "Iteration 26, loss = 0.42615081\n",
      "Iteration 27, loss = 0.42545326\n",
      "Iteration 28, loss = 0.42473653\n",
      "Iteration 29, loss = 0.42396825\n",
      "Iteration 30, loss = 0.42327485\n",
      "Iteration 31, loss = 0.42253485\n",
      "Iteration 32, loss = 0.42180093\n",
      "Iteration 33, loss = 0.42111937\n",
      "Iteration 34, loss = 0.42035761\n",
      "Iteration 35, loss = 0.41966900\n",
      "Iteration 36, loss = 0.41891197\n",
      "Iteration 37, loss = 0.41819118\n",
      "Iteration 38, loss = 0.41744328\n",
      "Iteration 39, loss = 0.41671517\n",
      "Iteration 40, loss = 0.41598267\n",
      "Iteration 41, loss = 0.41527218\n",
      "Iteration 42, loss = 0.41452525\n",
      "Iteration 43, loss = 0.41377175\n",
      "Iteration 44, loss = 0.41307095\n",
      "Iteration 45, loss = 0.41230452\n",
      "Iteration 46, loss = 0.41161836\n",
      "Iteration 47, loss = 0.41083556\n",
      "Iteration 48, loss = 0.41006499\n",
      "Iteration 49, loss = 0.40937395\n",
      "Iteration 50, loss = 0.40858545\n",
      "Iteration 51, loss = 0.40780266\n",
      "Iteration 52, loss = 0.40710813\n",
      "Iteration 53, loss = 0.40628725\n",
      "Iteration 54, loss = 0.40551154\n",
      "Iteration 55, loss = 0.40475509\n",
      "Iteration 56, loss = 0.40400932\n",
      "Iteration 57, loss = 0.40323224\n",
      "Iteration 58, loss = 0.40245879\n",
      "Iteration 59, loss = 0.40169593\n",
      "Iteration 60, loss = 0.40093096\n",
      "Iteration 61, loss = 0.40014393\n",
      "Iteration 62, loss = 0.39938649\n",
      "Iteration 63, loss = 0.39857681\n",
      "Iteration 64, loss = 0.39781189\n",
      "Iteration 65, loss = 0.39701529\n",
      "Iteration 66, loss = 0.39622464\n",
      "Iteration 67, loss = 0.39543513\n",
      "Iteration 68, loss = 0.39464606\n",
      "Iteration 69, loss = 0.39385452\n",
      "Iteration 70, loss = 0.39308236\n",
      "Iteration 71, loss = 0.39225567\n",
      "Iteration 72, loss = 0.39148161\n",
      "Iteration 73, loss = 0.39066671\n",
      "Iteration 74, loss = 0.38984277\n",
      "Iteration 75, loss = 0.38905648\n",
      "Iteration 76, loss = 0.38824413\n",
      "Iteration 77, loss = 0.38744898\n",
      "Iteration 78, loss = 0.38662315\n",
      "Iteration 79, loss = 0.38585006\n",
      "Iteration 80, loss = 0.38501529\n",
      "Iteration 81, loss = 0.38419540\n",
      "Iteration 82, loss = 0.38336858\n",
      "Iteration 83, loss = 0.38255664\n",
      "Iteration 84, loss = 0.38175095\n",
      "Iteration 85, loss = 0.38093637\n",
      "Iteration 86, loss = 0.38015027\n",
      "Iteration 87, loss = 0.37929357\n",
      "Iteration 88, loss = 0.37848593\n",
      "Iteration 89, loss = 0.37765578\n",
      "Iteration 90, loss = 0.37688087\n",
      "Iteration 91, loss = 0.37602597\n",
      "Iteration 92, loss = 0.37519533\n",
      "Iteration 93, loss = 0.37440313\n",
      "Iteration 94, loss = 0.37357195\n",
      "Iteration 95, loss = 0.37272776\n",
      "Iteration 96, loss = 0.37190033\n",
      "Iteration 97, loss = 0.37112128\n",
      "Iteration 98, loss = 0.37031758\n",
      "Iteration 99, loss = 0.36944085\n",
      "Iteration 100, loss = 0.36863726\n",
      "Iteration 101, loss = 0.36780411\n",
      "Iteration 102, loss = 0.36699020\n",
      "Iteration 103, loss = 0.36618004\n",
      "Iteration 104, loss = 0.36535772\n",
      "Iteration 105, loss = 0.36454497\n",
      "Iteration 106, loss = 0.36372105\n",
      "Iteration 107, loss = 0.36291709\n",
      "Iteration 108, loss = 0.36210543\n",
      "Iteration 109, loss = 0.36130152\n",
      "Iteration 110, loss = 0.36053876\n",
      "Iteration 111, loss = 0.35968585\n",
      "Iteration 112, loss = 0.35889865\n",
      "Iteration 113, loss = 0.35809234\n",
      "Iteration 114, loss = 0.35729679\n",
      "Iteration 115, loss = 0.35649335\n",
      "Iteration 116, loss = 0.35569303\n",
      "Iteration 117, loss = 0.35491266\n",
      "Iteration 118, loss = 0.35413416\n",
      "Iteration 119, loss = 0.35334090\n",
      "Iteration 120, loss = 0.35256633\n",
      "Iteration 121, loss = 0.35183231\n",
      "Iteration 122, loss = 0.35097867\n",
      "Iteration 123, loss = 0.35023829\n",
      "Iteration 124, loss = 0.34944912\n",
      "Iteration 125, loss = 0.34870164\n",
      "Iteration 126, loss = 0.34791503\n",
      "Iteration 127, loss = 0.34715270\n",
      "Iteration 128, loss = 0.34641357\n",
      "Iteration 129, loss = 0.34565514\n",
      "Iteration 130, loss = 0.34490447\n",
      "Iteration 131, loss = 0.34417566\n",
      "Iteration 132, loss = 0.34342123\n",
      "Iteration 133, loss = 0.34267718\n",
      "Iteration 134, loss = 0.34195538\n",
      "Iteration 135, loss = 0.34123096\n",
      "Iteration 136, loss = 0.34049577\n",
      "Iteration 137, loss = 0.33979039\n",
      "Iteration 138, loss = 0.33908010\n",
      "Iteration 139, loss = 0.33835777\n",
      "Iteration 140, loss = 0.33764163\n",
      "Iteration 141, loss = 0.33693406\n",
      "Iteration 142, loss = 0.33625235\n",
      "Iteration 143, loss = 0.33555024\n",
      "Iteration 144, loss = 0.33485112\n",
      "Iteration 145, loss = 0.33417640\n",
      "Iteration 146, loss = 0.33351234\n",
      "Iteration 147, loss = 0.33284196\n",
      "Iteration 148, loss = 0.33217217\n",
      "Iteration 149, loss = 0.33149002\n",
      "Iteration 150, loss = 0.33083576\n",
      "Iteration 151, loss = 0.33017067\n",
      "Iteration 152, loss = 0.32953113\n",
      "Iteration 153, loss = 0.32889316\n",
      "Iteration 154, loss = 0.32822166\n",
      "Iteration 155, loss = 0.32759405\n",
      "Iteration 156, loss = 0.32699171\n",
      "Iteration 157, loss = 0.32635037\n",
      "Iteration 158, loss = 0.32575029\n",
      "Iteration 159, loss = 0.32510111\n",
      "Iteration 160, loss = 0.32449404\n",
      "Iteration 161, loss = 0.32391585\n",
      "Iteration 162, loss = 0.32329834\n",
      "Iteration 163, loss = 0.32270951\n",
      "Iteration 164, loss = 0.32211563\n",
      "Iteration 165, loss = 0.32153544\n",
      "Iteration 166, loss = 0.32096725\n",
      "Iteration 167, loss = 0.32040362\n",
      "Iteration 168, loss = 0.31980585\n",
      "Iteration 169, loss = 0.31927100\n",
      "Iteration 170, loss = 0.31869213\n",
      "Iteration 171, loss = 0.31813278\n",
      "Iteration 172, loss = 0.31758386\n",
      "Iteration 173, loss = 0.31704317\n",
      "Iteration 174, loss = 0.31650416\n",
      "Iteration 175, loss = 0.31596986\n",
      "Iteration 176, loss = 0.31544244\n",
      "Iteration 177, loss = 0.31492310\n",
      "Iteration 178, loss = 0.31443222\n",
      "Iteration 179, loss = 0.31387816\n",
      "Iteration 180, loss = 0.31340560\n",
      "Iteration 181, loss = 0.31289814\n",
      "Iteration 182, loss = 0.31246777\n",
      "Iteration 183, loss = 0.31189118\n",
      "Iteration 184, loss = 0.31142746\n",
      "Iteration 185, loss = 0.31090680\n",
      "Iteration 186, loss = 0.31045801\n",
      "Iteration 187, loss = 0.30996544\n",
      "Iteration 188, loss = 0.30951526\n",
      "Iteration 189, loss = 0.30903499\n",
      "Iteration 190, loss = 0.30858317\n",
      "Iteration 191, loss = 0.30810266\n",
      "Iteration 192, loss = 0.30766170\n",
      "Iteration 193, loss = 0.30722292\n",
      "Iteration 194, loss = 0.30676161\n",
      "Iteration 195, loss = 0.30634076\n",
      "Iteration 196, loss = 0.30589683\n",
      "Iteration 197, loss = 0.30547012\n",
      "Iteration 198, loss = 0.30505831\n",
      "Iteration 199, loss = 0.30462259\n",
      "Iteration 200, loss = 0.30421254\n",
      "Iteration 1, loss = 0.70782552\n",
      "Iteration 2, loss = 0.48933998\n",
      "Iteration 3, loss = 0.44485435\n",
      "Iteration 4, loss = 0.44199818\n",
      "Iteration 5, loss = 0.44124079\n",
      "Iteration 6, loss = 0.44041811\n",
      "Iteration 7, loss = 0.43962721\n",
      "Iteration 8, loss = 0.43887081\n",
      "Iteration 9, loss = 0.43811179\n",
      "Iteration 10, loss = 0.43739480\n",
      "Iteration 11, loss = 0.43665954\n",
      "Iteration 12, loss = 0.43587453\n",
      "Iteration 13, loss = 0.43519886\n",
      "Iteration 14, loss = 0.43441848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.43366969\n",
      "Iteration 16, loss = 0.43294152\n",
      "Iteration 17, loss = 0.43224149\n",
      "Iteration 18, loss = 0.43154489\n",
      "Iteration 19, loss = 0.43081115\n",
      "Iteration 20, loss = 0.43003784\n",
      "Iteration 21, loss = 0.42929855\n",
      "Iteration 22, loss = 0.42858726\n",
      "Iteration 23, loss = 0.42786489\n",
      "Iteration 24, loss = 0.42717778\n",
      "Iteration 25, loss = 0.42640512\n",
      "Iteration 26, loss = 0.42569442\n",
      "Iteration 27, loss = 0.42498836\n",
      "Iteration 28, loss = 0.42428294\n",
      "Iteration 29, loss = 0.42351208\n",
      "Iteration 30, loss = 0.42278417\n",
      "Iteration 31, loss = 0.42209988\n",
      "Iteration 32, loss = 0.42132362\n",
      "Iteration 33, loss = 0.42064316\n",
      "Iteration 34, loss = 0.41987128\n",
      "Iteration 35, loss = 0.41920639\n",
      "Iteration 36, loss = 0.41842735\n",
      "Iteration 37, loss = 0.41771201\n",
      "Iteration 38, loss = 0.41695844\n",
      "Iteration 39, loss = 0.41621722\n",
      "Iteration 40, loss = 0.41547933\n",
      "Iteration 41, loss = 0.41476300\n",
      "Iteration 42, loss = 0.41405142\n",
      "Iteration 43, loss = 0.41325973\n",
      "Iteration 44, loss = 0.41253282\n",
      "Iteration 45, loss = 0.41182462\n",
      "Iteration 46, loss = 0.41111687\n",
      "Iteration 47, loss = 0.41031844\n",
      "Iteration 48, loss = 0.40954794\n",
      "Iteration 49, loss = 0.40880717\n",
      "Iteration 50, loss = 0.40803530\n",
      "Iteration 51, loss = 0.40727505\n",
      "Iteration 52, loss = 0.40653608\n",
      "Iteration 53, loss = 0.40579026\n",
      "Iteration 54, loss = 0.40500005\n",
      "Iteration 55, loss = 0.40424884\n",
      "Iteration 56, loss = 0.40347347\n",
      "Iteration 57, loss = 0.40272410\n",
      "Iteration 58, loss = 0.40194307\n",
      "Iteration 59, loss = 0.40117789\n",
      "Iteration 60, loss = 0.40042204\n",
      "Iteration 61, loss = 0.39962498\n",
      "Iteration 62, loss = 0.39885691\n",
      "Iteration 63, loss = 0.39808037\n",
      "Iteration 64, loss = 0.39729948\n",
      "Iteration 65, loss = 0.39650730\n",
      "Iteration 66, loss = 0.39572262\n",
      "Iteration 67, loss = 0.39495060\n",
      "Iteration 68, loss = 0.39416096\n",
      "Iteration 69, loss = 0.39337250\n",
      "Iteration 70, loss = 0.39257350\n",
      "Iteration 71, loss = 0.39176970\n",
      "Iteration 72, loss = 0.39099046\n",
      "Iteration 73, loss = 0.39017604\n",
      "Iteration 74, loss = 0.38939171\n",
      "Iteration 75, loss = 0.38862326\n",
      "Iteration 76, loss = 0.38777766\n",
      "Iteration 77, loss = 0.38699020\n",
      "Iteration 78, loss = 0.38618076\n",
      "Iteration 79, loss = 0.38540498\n",
      "Iteration 80, loss = 0.38459678\n",
      "Iteration 81, loss = 0.38377750\n",
      "Iteration 82, loss = 0.38296756\n",
      "Iteration 83, loss = 0.38215854\n",
      "Iteration 84, loss = 0.38135314\n",
      "Iteration 85, loss = 0.38054581\n",
      "Iteration 86, loss = 0.37971623\n",
      "Iteration 87, loss = 0.37894357\n",
      "Iteration 88, loss = 0.37811903\n",
      "Iteration 89, loss = 0.37730007\n",
      "Iteration 90, loss = 0.37652658\n",
      "Iteration 91, loss = 0.37568542\n",
      "Iteration 92, loss = 0.37484734\n",
      "Iteration 93, loss = 0.37405357\n",
      "Iteration 94, loss = 0.37323990\n",
      "Iteration 95, loss = 0.37242907\n",
      "Iteration 96, loss = 0.37161921\n",
      "Iteration 97, loss = 0.37083932\n",
      "Iteration 98, loss = 0.37008054\n",
      "Iteration 99, loss = 0.36919640\n",
      "Iteration 100, loss = 0.36840497\n",
      "Iteration 101, loss = 0.36757927\n",
      "Iteration 102, loss = 0.36678407\n",
      "Iteration 103, loss = 0.36597017\n",
      "Iteration 104, loss = 0.36518088\n",
      "Iteration 105, loss = 0.36437024\n",
      "Iteration 106, loss = 0.36358522\n",
      "Iteration 107, loss = 0.36280521\n",
      "Iteration 108, loss = 0.36196514\n",
      "Iteration 109, loss = 0.36117847\n",
      "Iteration 110, loss = 0.36041638\n",
      "Iteration 111, loss = 0.35960143\n",
      "Iteration 112, loss = 0.35881564\n",
      "Iteration 113, loss = 0.35802132\n",
      "Iteration 114, loss = 0.35724741\n",
      "Iteration 115, loss = 0.35643764\n",
      "Iteration 116, loss = 0.35570347\n",
      "Iteration 117, loss = 0.35489226\n",
      "Iteration 118, loss = 0.35415168\n",
      "Iteration 119, loss = 0.35336945\n",
      "Iteration 120, loss = 0.35266868\n",
      "Iteration 121, loss = 0.35184208\n",
      "Iteration 122, loss = 0.35107164\n",
      "Iteration 123, loss = 0.35031679\n",
      "Iteration 124, loss = 0.34956036\n",
      "Iteration 125, loss = 0.34881593\n",
      "Iteration 126, loss = 0.34805521\n",
      "Iteration 127, loss = 0.34730814\n",
      "Iteration 128, loss = 0.34659391\n",
      "Iteration 129, loss = 0.34584060\n",
      "Iteration 130, loss = 0.34510917\n",
      "Iteration 131, loss = 0.34441218\n",
      "Iteration 132, loss = 0.34365081\n",
      "Iteration 133, loss = 0.34292857\n",
      "Iteration 134, loss = 0.34221261\n",
      "Iteration 135, loss = 0.34151431\n",
      "Iteration 136, loss = 0.34080162\n",
      "Iteration 137, loss = 0.34011560\n",
      "Iteration 138, loss = 0.33949953\n",
      "Iteration 139, loss = 0.33872418\n",
      "Iteration 140, loss = 0.33804543\n",
      "Iteration 141, loss = 0.33737227\n",
      "Iteration 142, loss = 0.33665331\n",
      "Iteration 143, loss = 0.33597867\n",
      "Iteration 144, loss = 0.33530214\n",
      "Iteration 145, loss = 0.33464925\n",
      "Iteration 146, loss = 0.33399040\n",
      "Iteration 147, loss = 0.33332261\n",
      "Iteration 148, loss = 0.33266972\n",
      "Iteration 149, loss = 0.33203501\n",
      "Iteration 150, loss = 0.33138516\n",
      "Iteration 151, loss = 0.33074460\n",
      "Iteration 152, loss = 0.33011132\n",
      "Iteration 153, loss = 0.32952771\n",
      "Iteration 154, loss = 0.32884330\n",
      "Iteration 155, loss = 0.32825352\n",
      "Iteration 156, loss = 0.32763286\n",
      "Iteration 157, loss = 0.32702776\n",
      "Iteration 158, loss = 0.32643282\n",
      "Iteration 159, loss = 0.32582119\n",
      "Iteration 160, loss = 0.32525442\n",
      "Iteration 161, loss = 0.32464609\n",
      "Iteration 162, loss = 0.32405787\n",
      "Iteration 163, loss = 0.32349133\n",
      "Iteration 164, loss = 0.32289573\n",
      "Iteration 165, loss = 0.32234585\n",
      "Iteration 166, loss = 0.32182735\n",
      "Iteration 167, loss = 0.32123710\n",
      "Iteration 168, loss = 0.32066301\n",
      "Iteration 169, loss = 0.32013862\n",
      "Iteration 170, loss = 0.31958432\n",
      "Iteration 171, loss = 0.31906128\n",
      "Iteration 172, loss = 0.31851581\n",
      "Iteration 173, loss = 0.31800394\n",
      "Iteration 174, loss = 0.31749002\n",
      "Iteration 175, loss = 0.31694321\n",
      "Iteration 176, loss = 0.31642982\n",
      "Iteration 177, loss = 0.31593924\n",
      "Iteration 178, loss = 0.31543146\n",
      "Iteration 179, loss = 0.31492794\n",
      "Iteration 180, loss = 0.31448288\n",
      "Iteration 181, loss = 0.31397576\n",
      "Iteration 182, loss = 0.31348798\n",
      "Iteration 183, loss = 0.31297907\n",
      "Iteration 184, loss = 0.31251605\n",
      "Iteration 185, loss = 0.31204445\n",
      "Iteration 186, loss = 0.31163984\n",
      "Iteration 187, loss = 0.31114673\n",
      "Iteration 188, loss = 0.31071029\n",
      "Iteration 189, loss = 0.31024720\n",
      "Iteration 190, loss = 0.30977304\n",
      "Iteration 191, loss = 0.30933935\n",
      "Iteration 192, loss = 0.30890104\n",
      "Iteration 193, loss = 0.30849537\n",
      "Iteration 194, loss = 0.30803742\n",
      "Iteration 195, loss = 0.30764789\n",
      "Iteration 196, loss = 0.30719246\n",
      "Iteration 197, loss = 0.30679352\n",
      "Iteration 198, loss = 0.30641103\n",
      "Iteration 199, loss = 0.30596090\n",
      "Iteration 200, loss = 0.30555675\n",
      "Iteration 1, loss = 0.70928992\n",
      "Iteration 2, loss = 0.48814853\n",
      "Iteration 3, loss = 0.44587314\n",
      "Iteration 4, loss = 0.44229369\n",
      "Iteration 5, loss = 0.44157319\n",
      "Iteration 6, loss = 0.44094109\n",
      "Iteration 7, loss = 0.44003295\n",
      "Iteration 8, loss = 0.43931151\n",
      "Iteration 9, loss = 0.43871837\n",
      "Iteration 10, loss = 0.43785023\n",
      "Iteration 11, loss = 0.43715376\n",
      "Iteration 12, loss = 0.43643176\n",
      "Iteration 13, loss = 0.43572918\n",
      "Iteration 14, loss = 0.43505466\n",
      "Iteration 15, loss = 0.43428715\n",
      "Iteration 16, loss = 0.43360630\n",
      "Iteration 17, loss = 0.43289121\n",
      "Iteration 18, loss = 0.43220344\n",
      "Iteration 19, loss = 0.43147213\n",
      "Iteration 20, loss = 0.43075974\n",
      "Iteration 21, loss = 0.43005872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.42938437\n",
      "Iteration 23, loss = 0.42866506\n",
      "Iteration 24, loss = 0.42794978\n",
      "Iteration 25, loss = 0.42726743\n",
      "Iteration 26, loss = 0.42655149\n",
      "Iteration 27, loss = 0.42586653\n",
      "Iteration 28, loss = 0.42516909\n",
      "Iteration 29, loss = 0.42445191\n",
      "Iteration 30, loss = 0.42373462\n",
      "Iteration 31, loss = 0.42308256\n",
      "Iteration 32, loss = 0.42238233\n",
      "Iteration 33, loss = 0.42168684\n",
      "Iteration 34, loss = 0.42092221\n",
      "Iteration 35, loss = 0.42023021\n",
      "Iteration 36, loss = 0.41953189\n",
      "Iteration 37, loss = 0.41882119\n",
      "Iteration 38, loss = 0.41815511\n",
      "Iteration 39, loss = 0.41741290\n",
      "Iteration 40, loss = 0.41671177\n",
      "Iteration 41, loss = 0.41597821\n",
      "Iteration 42, loss = 0.41527605\n",
      "Iteration 43, loss = 0.41454958\n",
      "Iteration 44, loss = 0.41382540\n",
      "Iteration 45, loss = 0.41308123\n",
      "Iteration 46, loss = 0.41238927\n",
      "Iteration 47, loss = 0.41179104\n",
      "Iteration 48, loss = 0.41096735\n",
      "Iteration 49, loss = 0.41023744\n",
      "Iteration 50, loss = 0.40945537\n",
      "Iteration 51, loss = 0.40872784\n",
      "Iteration 52, loss = 0.40802173\n",
      "Iteration 53, loss = 0.40724934\n",
      "Iteration 54, loss = 0.40652717\n",
      "Iteration 55, loss = 0.40577750\n",
      "Iteration 56, loss = 0.40506770\n",
      "Iteration 57, loss = 0.40427981\n",
      "Iteration 58, loss = 0.40353823\n",
      "Iteration 59, loss = 0.40280588\n",
      "Iteration 60, loss = 0.40202716\n",
      "Iteration 61, loss = 0.40134781\n",
      "Iteration 62, loss = 0.40053166\n",
      "Iteration 63, loss = 0.39979818\n",
      "Iteration 64, loss = 0.39902060\n",
      "Iteration 65, loss = 0.39833956\n",
      "Iteration 66, loss = 0.39747740\n",
      "Iteration 67, loss = 0.39673755\n",
      "Iteration 68, loss = 0.39597509\n",
      "Iteration 69, loss = 0.39518444\n",
      "Iteration 70, loss = 0.39446358\n",
      "Iteration 71, loss = 0.39362880\n",
      "Iteration 72, loss = 0.39288405\n",
      "Iteration 73, loss = 0.39209561\n",
      "Iteration 74, loss = 0.39133847\n",
      "Iteration 75, loss = 0.39054769\n",
      "Iteration 76, loss = 0.38974803\n",
      "Iteration 77, loss = 0.38896481\n",
      "Iteration 78, loss = 0.38818068\n",
      "Iteration 79, loss = 0.38738614\n",
      "Iteration 80, loss = 0.38659002\n",
      "Iteration 81, loss = 0.38580894\n",
      "Iteration 82, loss = 0.38508664\n",
      "Iteration 83, loss = 0.38424440\n",
      "Iteration 84, loss = 0.38343721\n",
      "Iteration 85, loss = 0.38265301\n",
      "Iteration 86, loss = 0.38186224\n",
      "Iteration 87, loss = 0.38107004\n",
      "Iteration 88, loss = 0.38026071\n",
      "Iteration 89, loss = 0.37945412\n",
      "Iteration 90, loss = 0.37864553\n",
      "Iteration 91, loss = 0.37788154\n",
      "Iteration 92, loss = 0.37705636\n",
      "Iteration 93, loss = 0.37624470\n",
      "Iteration 94, loss = 0.37548632\n",
      "Iteration 95, loss = 0.37464197\n",
      "Iteration 96, loss = 0.37385950\n",
      "Iteration 97, loss = 0.37310072\n",
      "Iteration 98, loss = 0.37225274\n",
      "Iteration 99, loss = 0.37146427\n",
      "Iteration 100, loss = 0.37065804\n",
      "Iteration 101, loss = 0.36987535\n",
      "Iteration 102, loss = 0.36904647\n",
      "Iteration 103, loss = 0.36829878\n",
      "Iteration 104, loss = 0.36745876\n",
      "Iteration 105, loss = 0.36665976\n",
      "Iteration 106, loss = 0.36588915\n",
      "Iteration 107, loss = 0.36506897\n",
      "Iteration 108, loss = 0.36429400\n",
      "Iteration 109, loss = 0.36348008\n",
      "Iteration 110, loss = 0.36270679\n",
      "Iteration 111, loss = 0.36191555\n",
      "Iteration 112, loss = 0.36112559\n",
      "Iteration 113, loss = 0.36033717\n",
      "Iteration 114, loss = 0.35960793\n",
      "Iteration 115, loss = 0.35883451\n",
      "Iteration 116, loss = 0.35799835\n",
      "Iteration 117, loss = 0.35722384\n",
      "Iteration 118, loss = 0.35645329\n",
      "Iteration 119, loss = 0.35567632\n",
      "Iteration 120, loss = 0.35490998\n",
      "Iteration 121, loss = 0.35414917\n",
      "Iteration 122, loss = 0.35338339\n",
      "Iteration 123, loss = 0.35263669\n",
      "Iteration 124, loss = 0.35186918\n",
      "Iteration 125, loss = 0.35110697\n",
      "Iteration 126, loss = 0.35035592\n",
      "Iteration 127, loss = 0.34961989\n",
      "Iteration 128, loss = 0.34886143\n",
      "Iteration 129, loss = 0.34810384\n",
      "Iteration 130, loss = 0.34741603\n",
      "Iteration 131, loss = 0.34666471\n",
      "Iteration 132, loss = 0.34592042\n",
      "Iteration 133, loss = 0.34521582\n",
      "Iteration 134, loss = 0.34448765\n",
      "Iteration 135, loss = 0.34377308\n",
      "Iteration 136, loss = 0.34301542\n",
      "Iteration 137, loss = 0.34233286\n",
      "Iteration 138, loss = 0.34159746\n",
      "Iteration 139, loss = 0.34094575\n",
      "Iteration 140, loss = 0.34022937\n",
      "Iteration 141, loss = 0.33951914\n",
      "Iteration 142, loss = 0.33881841\n",
      "Iteration 143, loss = 0.33816420\n",
      "Iteration 144, loss = 0.33749453\n",
      "Iteration 145, loss = 0.33677017\n",
      "Iteration 146, loss = 0.33611944\n",
      "Iteration 147, loss = 0.33542118\n",
      "Iteration 148, loss = 0.33478596\n",
      "Iteration 149, loss = 0.33410006\n",
      "Iteration 150, loss = 0.33346948\n",
      "Iteration 151, loss = 0.33285188\n",
      "Iteration 152, loss = 0.33214828\n",
      "Iteration 153, loss = 0.33153318\n",
      "Iteration 154, loss = 0.33087699\n",
      "Iteration 155, loss = 0.33025926\n",
      "Iteration 156, loss = 0.32961587\n",
      "Iteration 157, loss = 0.32900226\n",
      "Iteration 158, loss = 0.32838274\n",
      "Iteration 159, loss = 0.32780017\n",
      "Iteration 160, loss = 0.32715573\n",
      "Iteration 161, loss = 0.32655856\n",
      "Iteration 162, loss = 0.32600311\n",
      "Iteration 163, loss = 0.32535380\n",
      "Iteration 164, loss = 0.32476816\n",
      "Iteration 165, loss = 0.32422552\n",
      "Iteration 166, loss = 0.32367121\n",
      "Iteration 167, loss = 0.32305205\n",
      "Iteration 168, loss = 0.32248305\n",
      "Iteration 169, loss = 0.32190623\n",
      "Iteration 170, loss = 0.32136653\n",
      "Iteration 171, loss = 0.32080528\n",
      "Iteration 172, loss = 0.32025687\n",
      "Iteration 173, loss = 0.31974321\n",
      "Iteration 174, loss = 0.31918578\n",
      "Iteration 175, loss = 0.31865074\n",
      "Iteration 176, loss = 0.31812949\n",
      "Iteration 177, loss = 0.31758591\n",
      "Iteration 178, loss = 0.31705481\n",
      "Iteration 179, loss = 0.31654521\n",
      "Iteration 180, loss = 0.31604030\n",
      "Iteration 181, loss = 0.31560388\n",
      "Iteration 182, loss = 0.31504266\n",
      "Iteration 183, loss = 0.31453214\n",
      "Iteration 184, loss = 0.31409379\n",
      "Iteration 185, loss = 0.31356406\n",
      "Iteration 186, loss = 0.31308485\n",
      "Iteration 187, loss = 0.31261580\n",
      "Iteration 188, loss = 0.31217890\n",
      "Iteration 189, loss = 0.31165047\n",
      "Iteration 190, loss = 0.31132365\n",
      "Iteration 191, loss = 0.31081411\n",
      "Iteration 192, loss = 0.31034187\n",
      "Iteration 193, loss = 0.30985256\n",
      "Iteration 194, loss = 0.30940142\n",
      "Iteration 195, loss = 0.30895786\n",
      "Iteration 196, loss = 0.30852132\n",
      "Iteration 197, loss = 0.30809149\n",
      "Iteration 198, loss = 0.30767413\n",
      "Iteration 199, loss = 0.30724158\n",
      "Iteration 200, loss = 0.30687068\n",
      "Iteration 1, loss = 0.70951243\n",
      "Iteration 2, loss = 0.48863949\n",
      "Iteration 3, loss = 0.44541185\n",
      "Iteration 4, loss = 0.44225827\n",
      "Iteration 5, loss = 0.44169612\n",
      "Iteration 6, loss = 0.44106989\n",
      "Iteration 7, loss = 0.44020513\n",
      "Iteration 8, loss = 0.43946797\n",
      "Iteration 9, loss = 0.43880519\n",
      "Iteration 10, loss = 0.43800326\n",
      "Iteration 11, loss = 0.43731916\n",
      "Iteration 12, loss = 0.43656089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.43589551\n",
      "Iteration 14, loss = 0.43516092\n",
      "Iteration 15, loss = 0.43441098\n",
      "Iteration 16, loss = 0.43372531\n",
      "Iteration 17, loss = 0.43301471\n",
      "Iteration 18, loss = 0.43230206\n",
      "Iteration 19, loss = 0.43159763\n",
      "Iteration 20, loss = 0.43089582\n",
      "Iteration 21, loss = 0.43018302\n",
      "Iteration 22, loss = 0.42956293\n",
      "Iteration 23, loss = 0.42881451\n",
      "Iteration 24, loss = 0.42809063\n",
      "Iteration 25, loss = 0.42740329\n",
      "Iteration 26, loss = 0.42668036\n",
      "Iteration 27, loss = 0.42598272\n",
      "Iteration 28, loss = 0.42527863\n",
      "Iteration 29, loss = 0.42459002\n",
      "Iteration 30, loss = 0.42386207\n",
      "Iteration 31, loss = 0.42317716\n",
      "Iteration 32, loss = 0.42248434\n",
      "Iteration 33, loss = 0.42180196\n",
      "Iteration 34, loss = 0.42106009\n",
      "Iteration 35, loss = 0.42035742\n",
      "Iteration 36, loss = 0.41967573\n",
      "Iteration 37, loss = 0.41896222\n",
      "Iteration 38, loss = 0.41827855\n",
      "Iteration 39, loss = 0.41755311\n",
      "Iteration 40, loss = 0.41679486\n",
      "Iteration 41, loss = 0.41609801\n",
      "Iteration 42, loss = 0.41538004\n",
      "Iteration 43, loss = 0.41466958\n",
      "Iteration 44, loss = 0.41395592\n",
      "Iteration 45, loss = 0.41323067\n",
      "Iteration 46, loss = 0.41254700\n",
      "Iteration 47, loss = 0.41195293\n",
      "Iteration 48, loss = 0.41106922\n",
      "Iteration 49, loss = 0.41037030\n",
      "Iteration 50, loss = 0.40958359\n",
      "Iteration 51, loss = 0.40886119\n",
      "Iteration 52, loss = 0.40817217\n",
      "Iteration 53, loss = 0.40740015\n",
      "Iteration 54, loss = 0.40668128\n",
      "Iteration 55, loss = 0.40591561\n",
      "Iteration 56, loss = 0.40521351\n",
      "Iteration 57, loss = 0.40442696\n",
      "Iteration 58, loss = 0.40368960\n",
      "Iteration 59, loss = 0.40295454\n",
      "Iteration 60, loss = 0.40219861\n",
      "Iteration 61, loss = 0.40144074\n",
      "Iteration 62, loss = 0.40068120\n",
      "Iteration 63, loss = 0.39996778\n",
      "Iteration 64, loss = 0.39917998\n",
      "Iteration 65, loss = 0.39847525\n",
      "Iteration 66, loss = 0.39766895\n",
      "Iteration 67, loss = 0.39690513\n",
      "Iteration 68, loss = 0.39612845\n",
      "Iteration 69, loss = 0.39536076\n",
      "Iteration 70, loss = 0.39461186\n",
      "Iteration 71, loss = 0.39384871\n",
      "Iteration 72, loss = 0.39304806\n",
      "Iteration 73, loss = 0.39228875\n",
      "Iteration 74, loss = 0.39150510\n",
      "Iteration 75, loss = 0.39074436\n",
      "Iteration 76, loss = 0.38994071\n",
      "Iteration 77, loss = 0.38915733\n",
      "Iteration 78, loss = 0.38836962\n",
      "Iteration 79, loss = 0.38757144\n",
      "Iteration 80, loss = 0.38678682\n",
      "Iteration 81, loss = 0.38601665\n",
      "Iteration 82, loss = 0.38524311\n",
      "Iteration 83, loss = 0.38445995\n",
      "Iteration 84, loss = 0.38370358\n",
      "Iteration 85, loss = 0.38283567\n",
      "Iteration 86, loss = 0.38209098\n",
      "Iteration 87, loss = 0.38128373\n",
      "Iteration 88, loss = 0.38048433\n",
      "Iteration 89, loss = 0.37967632\n",
      "Iteration 90, loss = 0.37887183\n",
      "Iteration 91, loss = 0.37810806\n",
      "Iteration 92, loss = 0.37728349\n",
      "Iteration 93, loss = 0.37652670\n",
      "Iteration 94, loss = 0.37570834\n",
      "Iteration 95, loss = 0.37489975\n",
      "Iteration 96, loss = 0.37410240\n",
      "Iteration 97, loss = 0.37332496\n",
      "Iteration 98, loss = 0.37252153\n",
      "Iteration 99, loss = 0.37172518\n",
      "Iteration 100, loss = 0.37092913\n",
      "Iteration 101, loss = 0.37014112\n",
      "Iteration 102, loss = 0.36934133\n",
      "Iteration 103, loss = 0.36859670\n",
      "Iteration 104, loss = 0.36773702\n",
      "Iteration 105, loss = 0.36694826\n",
      "Iteration 106, loss = 0.36619331\n",
      "Iteration 107, loss = 0.36539688\n",
      "Iteration 108, loss = 0.36460304\n",
      "Iteration 109, loss = 0.36380354\n",
      "Iteration 110, loss = 0.36302500\n",
      "Iteration 111, loss = 0.36225328\n",
      "Iteration 112, loss = 0.36146750\n",
      "Iteration 113, loss = 0.36069272\n",
      "Iteration 114, loss = 0.35990797\n",
      "Iteration 115, loss = 0.35913066\n",
      "Iteration 116, loss = 0.35836425\n",
      "Iteration 117, loss = 0.35758171\n",
      "Iteration 118, loss = 0.35679751\n",
      "Iteration 119, loss = 0.35605113\n",
      "Iteration 120, loss = 0.35528121\n",
      "Iteration 121, loss = 0.35452849\n",
      "Iteration 122, loss = 0.35377079\n",
      "Iteration 123, loss = 0.35301352\n",
      "Iteration 124, loss = 0.35226430\n",
      "Iteration 125, loss = 0.35154260\n",
      "Iteration 126, loss = 0.35081989\n",
      "Iteration 127, loss = 0.35003185\n",
      "Iteration 128, loss = 0.34927163\n",
      "Iteration 129, loss = 0.34853138\n",
      "Iteration 130, loss = 0.34784097\n",
      "Iteration 131, loss = 0.34707308\n",
      "Iteration 132, loss = 0.34638501\n",
      "Iteration 133, loss = 0.34563040\n",
      "Iteration 134, loss = 0.34492427\n",
      "Iteration 135, loss = 0.34420300\n",
      "Iteration 136, loss = 0.34350023\n",
      "Iteration 137, loss = 0.34278915\n",
      "Iteration 138, loss = 0.34207926\n",
      "Iteration 139, loss = 0.34139406\n",
      "Iteration 140, loss = 0.34070011\n",
      "Iteration 141, loss = 0.34000271\n",
      "Iteration 142, loss = 0.33934755\n",
      "Iteration 143, loss = 0.33865476\n",
      "Iteration 144, loss = 0.33797786\n",
      "Iteration 145, loss = 0.33728804\n",
      "Iteration 146, loss = 0.33661727\n",
      "Iteration 147, loss = 0.33594484\n",
      "Iteration 148, loss = 0.33531879\n",
      "Iteration 149, loss = 0.33463917\n",
      "Iteration 150, loss = 0.33401452\n",
      "Iteration 151, loss = 0.33345953\n",
      "Iteration 152, loss = 0.33269564\n",
      "Iteration 153, loss = 0.33209345\n",
      "Iteration 154, loss = 0.33146507\n",
      "Iteration 155, loss = 0.33081551\n",
      "Iteration 156, loss = 0.33019516\n",
      "Iteration 157, loss = 0.32958229\n",
      "Iteration 158, loss = 0.32898209\n",
      "Iteration 159, loss = 0.32837399\n",
      "Iteration 160, loss = 0.32776066\n",
      "Iteration 161, loss = 0.32719575\n",
      "Iteration 162, loss = 0.32661394\n",
      "Iteration 163, loss = 0.32597095\n",
      "Iteration 164, loss = 0.32540609\n",
      "Iteration 165, loss = 0.32485289\n",
      "Iteration 166, loss = 0.32425601\n",
      "Iteration 167, loss = 0.32369598\n",
      "Iteration 168, loss = 0.32314370\n",
      "Iteration 169, loss = 0.32257200\n",
      "Iteration 170, loss = 0.32203303\n",
      "Iteration 171, loss = 0.32151798\n",
      "Iteration 172, loss = 0.32093871\n",
      "Iteration 173, loss = 0.32044892\n",
      "Iteration 174, loss = 0.31994051\n",
      "Iteration 175, loss = 0.31936974\n",
      "Iteration 176, loss = 0.31881147\n",
      "Iteration 177, loss = 0.31834573\n",
      "Iteration 178, loss = 0.31778515\n",
      "Iteration 179, loss = 0.31727892\n",
      "Iteration 180, loss = 0.31678900\n",
      "Iteration 181, loss = 0.31630501\n",
      "Iteration 182, loss = 0.31579220\n",
      "Iteration 183, loss = 0.31529401\n",
      "Iteration 184, loss = 0.31487520\n",
      "Iteration 185, loss = 0.31435005\n",
      "Iteration 186, loss = 0.31386615\n",
      "Iteration 187, loss = 0.31340391\n",
      "Iteration 188, loss = 0.31292846\n",
      "Iteration 189, loss = 0.31246016\n",
      "Iteration 190, loss = 0.31209946\n",
      "Iteration 191, loss = 0.31162657\n",
      "Iteration 192, loss = 0.31114665\n",
      "Iteration 193, loss = 0.31068869\n",
      "Iteration 194, loss = 0.31025065\n",
      "Iteration 195, loss = 0.30979352\n",
      "Iteration 196, loss = 0.30936664\n",
      "Iteration 197, loss = 0.30894942\n",
      "Iteration 198, loss = 0.30852844\n",
      "Iteration 199, loss = 0.30811539\n",
      "Iteration 200, loss = 0.30772157\n",
      "Iteration 1, loss = 0.68975465\n",
      "Iteration 2, loss = 0.48836598\n",
      "Iteration 3, loss = 0.41557887\n",
      "Iteration 4, loss = 0.39063160\n",
      "Iteration 5, loss = 0.37608027\n",
      "Iteration 6, loss = 0.36390155\n",
      "Iteration 7, loss = 0.35329455\n",
      "Iteration 8, loss = 0.34394750\n",
      "Iteration 9, loss = 0.33551455\n",
      "Iteration 10, loss = 0.32807508\n",
      "Iteration 11, loss = 0.32152240\n",
      "Iteration 12, loss = 0.31564259\n",
      "Iteration 13, loss = 0.31053298\n",
      "Iteration 14, loss = 0.30593462\n",
      "Iteration 15, loss = 0.30162462\n",
      "Iteration 16, loss = 0.29807238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.29466139\n",
      "Iteration 18, loss = 0.29174924\n",
      "Iteration 19, loss = 0.28895254\n",
      "Iteration 20, loss = 0.28656546\n",
      "Iteration 21, loss = 0.28425093\n",
      "Iteration 22, loss = 0.28227695\n",
      "Iteration 23, loss = 0.28021849\n",
      "Iteration 24, loss = 0.27832114\n",
      "Iteration 25, loss = 0.27668710\n",
      "Iteration 26, loss = 0.27510271\n",
      "Iteration 27, loss = 0.27356097\n",
      "Iteration 28, loss = 0.27209061\n",
      "Iteration 29, loss = 0.27064618\n",
      "Iteration 30, loss = 0.26929982\n",
      "Iteration 31, loss = 0.26797408\n",
      "Iteration 32, loss = 0.26675318\n",
      "Iteration 33, loss = 0.26557292\n",
      "Iteration 34, loss = 0.26442489\n",
      "Iteration 35, loss = 0.26339552\n",
      "Iteration 36, loss = 0.26237654\n",
      "Iteration 37, loss = 0.26090898\n",
      "Iteration 38, loss = 0.25991750\n",
      "Iteration 39, loss = 0.25882663\n",
      "Iteration 40, loss = 0.25791242\n",
      "Iteration 41, loss = 0.25681358\n",
      "Iteration 42, loss = 0.25606194\n",
      "Iteration 43, loss = 0.25480365\n",
      "Iteration 44, loss = 0.25394671\n",
      "Iteration 45, loss = 0.25311682\n",
      "Iteration 46, loss = 0.25209171\n",
      "Iteration 47, loss = 0.25150928\n",
      "Iteration 48, loss = 0.25040373\n",
      "Iteration 49, loss = 0.24957675\n",
      "Iteration 50, loss = 0.24862666\n",
      "Iteration 51, loss = 0.24796307\n",
      "Iteration 52, loss = 0.24736855\n",
      "Iteration 53, loss = 0.24641172\n",
      "Iteration 54, loss = 0.24568414\n",
      "Iteration 55, loss = 0.24505062\n",
      "Iteration 56, loss = 0.24433218\n",
      "Iteration 57, loss = 0.24390348\n",
      "Iteration 58, loss = 0.24319989\n",
      "Iteration 59, loss = 0.24251306\n",
      "Iteration 60, loss = 0.24210421\n",
      "Iteration 61, loss = 0.24146451\n",
      "Iteration 62, loss = 0.24100609\n",
      "Iteration 63, loss = 0.24066626\n",
      "Iteration 64, loss = 0.24005230\n",
      "Iteration 65, loss = 0.23955356\n",
      "Iteration 66, loss = 0.23934324\n",
      "Iteration 67, loss = 0.23866275\n",
      "Iteration 68, loss = 0.23824765\n",
      "Iteration 69, loss = 0.23815472\n",
      "Iteration 70, loss = 0.23753175\n",
      "Iteration 71, loss = 0.23748687\n",
      "Iteration 72, loss = 0.23695954\n",
      "Iteration 73, loss = 0.23658943\n",
      "Iteration 74, loss = 0.23619916\n",
      "Iteration 75, loss = 0.23578577\n",
      "Iteration 76, loss = 0.23568875\n",
      "Iteration 77, loss = 0.23556755\n",
      "Iteration 78, loss = 0.23512379\n",
      "Iteration 79, loss = 0.23482117\n",
      "Iteration 80, loss = 0.23450324\n",
      "Iteration 81, loss = 0.23423730\n",
      "Iteration 82, loss = 0.23431093\n",
      "Iteration 83, loss = 0.23346529\n",
      "Iteration 84, loss = 0.23341917\n",
      "Iteration 85, loss = 0.23319165\n",
      "Iteration 86, loss = 0.23318066\n",
      "Iteration 87, loss = 0.23259621\n",
      "Iteration 88, loss = 0.23245502\n",
      "Iteration 89, loss = 0.23197886\n",
      "Iteration 90, loss = 0.23196561\n",
      "Iteration 91, loss = 0.23149631\n",
      "Iteration 92, loss = 0.23146108\n",
      "Iteration 93, loss = 0.23098058\n",
      "Iteration 94, loss = 0.23071351\n",
      "Iteration 95, loss = 0.23033918\n",
      "Iteration 96, loss = 0.23012863\n",
      "Iteration 97, loss = 0.22983073\n",
      "Iteration 98, loss = 0.22932986\n",
      "Iteration 99, loss = 0.22904558\n",
      "Iteration 100, loss = 0.22899693\n",
      "Iteration 101, loss = 0.22863858\n",
      "Iteration 102, loss = 0.22805604\n",
      "Iteration 103, loss = 0.22781439\n",
      "Iteration 104, loss = 0.22761901\n",
      "Iteration 105, loss = 0.22711450\n",
      "Iteration 106, loss = 0.22680055\n",
      "Iteration 107, loss = 0.22664140\n",
      "Iteration 108, loss = 0.22609673\n",
      "Iteration 109, loss = 0.22563714\n",
      "Iteration 110, loss = 0.22560886\n",
      "Iteration 111, loss = 0.22494351\n",
      "Iteration 112, loss = 0.22455258\n",
      "Iteration 113, loss = 0.22422029\n",
      "Iteration 114, loss = 0.22366702\n",
      "Iteration 115, loss = 0.22350547\n",
      "Iteration 116, loss = 0.22304208\n",
      "Iteration 117, loss = 0.22266332\n",
      "Iteration 118, loss = 0.22198823\n",
      "Iteration 119, loss = 0.22176599\n",
      "Iteration 120, loss = 0.22126434\n",
      "Iteration 121, loss = 0.22082565\n",
      "Iteration 122, loss = 0.22041719\n",
      "Iteration 123, loss = 0.21979251\n",
      "Iteration 124, loss = 0.21943033\n",
      "Iteration 125, loss = 0.21908481\n",
      "Iteration 126, loss = 0.21858113\n",
      "Iteration 127, loss = 0.21800698\n",
      "Iteration 128, loss = 0.21750389\n",
      "Iteration 129, loss = 0.21717246\n",
      "Iteration 130, loss = 0.21657470\n",
      "Iteration 131, loss = 0.21624621\n",
      "Iteration 132, loss = 0.21581850\n",
      "Iteration 133, loss = 0.21511735\n",
      "Iteration 134, loss = 0.21473162\n",
      "Iteration 135, loss = 0.21413633\n",
      "Iteration 136, loss = 0.21428339\n",
      "Iteration 137, loss = 0.21310858\n",
      "Iteration 138, loss = 0.21255978\n",
      "Iteration 139, loss = 0.21247246\n",
      "Iteration 140, loss = 0.21152839\n",
      "Iteration 141, loss = 0.21099415\n",
      "Iteration 142, loss = 0.21066143\n",
      "Iteration 143, loss = 0.21006147\n",
      "Iteration 144, loss = 0.20938199\n",
      "Iteration 145, loss = 0.20889006\n",
      "Iteration 146, loss = 0.20838016\n",
      "Iteration 147, loss = 0.20792375\n",
      "Iteration 148, loss = 0.20739714\n",
      "Iteration 149, loss = 0.20667757\n",
      "Iteration 150, loss = 0.20618974\n",
      "Iteration 151, loss = 0.20569071\n",
      "Iteration 152, loss = 0.20509354\n",
      "Iteration 153, loss = 0.20462683\n",
      "Iteration 154, loss = 0.20384395\n",
      "Iteration 155, loss = 0.20347589\n",
      "Iteration 156, loss = 0.20278303\n",
      "Iteration 157, loss = 0.20226716\n",
      "Iteration 158, loss = 0.20178061\n",
      "Iteration 159, loss = 0.20131401\n",
      "Iteration 160, loss = 0.20081696\n",
      "Iteration 161, loss = 0.19997733\n",
      "Iteration 162, loss = 0.19945655\n",
      "Iteration 163, loss = 0.19869112\n",
      "Iteration 164, loss = 0.19818151\n",
      "Iteration 165, loss = 0.19771607\n",
      "Iteration 166, loss = 0.19716424\n",
      "Iteration 167, loss = 0.19684431\n",
      "Iteration 168, loss = 0.19585485\n",
      "Iteration 169, loss = 0.19553956\n",
      "Iteration 170, loss = 0.19481645\n",
      "Iteration 171, loss = 0.19418210\n",
      "Iteration 172, loss = 0.19350556\n",
      "Iteration 173, loss = 0.19292553\n",
      "Iteration 174, loss = 0.19249483\n",
      "Iteration 175, loss = 0.19168049\n",
      "Iteration 176, loss = 0.19129733\n",
      "Iteration 177, loss = 0.19079825\n",
      "Iteration 178, loss = 0.19016816\n",
      "Iteration 179, loss = 0.18974191\n",
      "Iteration 180, loss = 0.18894085\n",
      "Iteration 181, loss = 0.18818000\n",
      "Iteration 182, loss = 0.18791547\n",
      "Iteration 183, loss = 0.18712817\n",
      "Iteration 184, loss = 0.18666261\n",
      "Iteration 185, loss = 0.18611759\n",
      "Iteration 186, loss = 0.18547436\n",
      "Iteration 187, loss = 0.18481853\n",
      "Iteration 188, loss = 0.18423536\n",
      "Iteration 189, loss = 0.18375272\n",
      "Iteration 190, loss = 0.18319573\n",
      "Iteration 191, loss = 0.18228787\n",
      "Iteration 192, loss = 0.18197195\n",
      "Iteration 193, loss = 0.18126951\n",
      "Iteration 194, loss = 0.18090411\n",
      "Iteration 195, loss = 0.18022092\n",
      "Iteration 196, loss = 0.17968414\n",
      "Iteration 197, loss = 0.17918094\n",
      "Iteration 198, loss = 0.17867037\n",
      "Iteration 199, loss = 0.17815868\n",
      "Iteration 200, loss = 0.17739607\n",
      "Iteration 1, loss = 0.68947204\n",
      "Iteration 2, loss = 0.48862988\n",
      "Iteration 3, loss = 0.41525731\n",
      "Iteration 4, loss = 0.39028092\n",
      "Iteration 5, loss = 0.37534252\n",
      "Iteration 6, loss = 0.36293483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.35237258\n",
      "Iteration 8, loss = 0.34296606\n",
      "Iteration 9, loss = 0.33451955\n",
      "Iteration 10, loss = 0.32725328\n",
      "Iteration 11, loss = 0.32059690\n",
      "Iteration 12, loss = 0.31482047\n",
      "Iteration 13, loss = 0.30967880\n",
      "Iteration 14, loss = 0.30502520\n",
      "Iteration 15, loss = 0.30079610\n",
      "Iteration 16, loss = 0.29715346\n",
      "Iteration 17, loss = 0.29390329\n",
      "Iteration 18, loss = 0.29081878\n",
      "Iteration 19, loss = 0.28812573\n",
      "Iteration 20, loss = 0.28550483\n",
      "Iteration 21, loss = 0.28328897\n",
      "Iteration 22, loss = 0.28118309\n",
      "Iteration 23, loss = 0.27919679\n",
      "Iteration 24, loss = 0.27734224\n",
      "Iteration 25, loss = 0.27561236\n",
      "Iteration 26, loss = 0.27408949\n",
      "Iteration 27, loss = 0.27251475\n",
      "Iteration 28, loss = 0.27110130\n",
      "Iteration 29, loss = 0.26965506\n",
      "Iteration 30, loss = 0.26832066\n",
      "Iteration 31, loss = 0.26698603\n",
      "Iteration 32, loss = 0.26582727\n",
      "Iteration 33, loss = 0.26472430\n",
      "Iteration 34, loss = 0.26356610\n",
      "Iteration 35, loss = 0.26249198\n",
      "Iteration 36, loss = 0.26124123\n",
      "Iteration 37, loss = 0.26013638\n",
      "Iteration 38, loss = 0.25905868\n",
      "Iteration 39, loss = 0.25805276\n",
      "Iteration 40, loss = 0.25713282\n",
      "Iteration 41, loss = 0.25610381\n",
      "Iteration 42, loss = 0.25514737\n",
      "Iteration 43, loss = 0.25414268\n",
      "Iteration 44, loss = 0.25324654\n",
      "Iteration 45, loss = 0.25238451\n",
      "Iteration 46, loss = 0.25167560\n",
      "Iteration 47, loss = 0.25080734\n",
      "Iteration 48, loss = 0.24985737\n",
      "Iteration 49, loss = 0.24901675\n",
      "Iteration 50, loss = 0.24804049\n",
      "Iteration 51, loss = 0.24732584\n",
      "Iteration 52, loss = 0.24673521\n",
      "Iteration 53, loss = 0.24576801\n",
      "Iteration 54, loss = 0.24517434\n",
      "Iteration 55, loss = 0.24450812\n",
      "Iteration 56, loss = 0.24373022\n",
      "Iteration 57, loss = 0.24325047\n",
      "Iteration 58, loss = 0.24278900\n",
      "Iteration 59, loss = 0.24199068\n",
      "Iteration 60, loss = 0.24152680\n",
      "Iteration 61, loss = 0.24093628\n",
      "Iteration 62, loss = 0.24048933\n",
      "Iteration 63, loss = 0.23991427\n",
      "Iteration 64, loss = 0.23943239\n",
      "Iteration 65, loss = 0.23886300\n",
      "Iteration 66, loss = 0.23867928\n",
      "Iteration 67, loss = 0.23803284\n",
      "Iteration 68, loss = 0.23773727\n",
      "Iteration 69, loss = 0.23734482\n",
      "Iteration 70, loss = 0.23695230\n",
      "Iteration 71, loss = 0.23672622\n",
      "Iteration 72, loss = 0.23647003\n",
      "Iteration 73, loss = 0.23595173\n",
      "Iteration 74, loss = 0.23565680\n",
      "Iteration 75, loss = 0.23547128\n",
      "Iteration 76, loss = 0.23517205\n",
      "Iteration 77, loss = 0.23500315\n",
      "Iteration 78, loss = 0.23476598\n",
      "Iteration 79, loss = 0.23469427\n",
      "Iteration 80, loss = 0.23426250\n",
      "Iteration 81, loss = 0.23384555\n",
      "Iteration 82, loss = 0.23392544\n",
      "Iteration 83, loss = 0.23345288\n",
      "Iteration 84, loss = 0.23326055\n",
      "Iteration 85, loss = 0.23312968\n",
      "Iteration 86, loss = 0.23299448\n",
      "Iteration 87, loss = 0.23272646\n",
      "Iteration 88, loss = 0.23258382\n",
      "Iteration 89, loss = 0.23226061\n",
      "Iteration 90, loss = 0.23238758\n",
      "Iteration 91, loss = 0.23190887\n",
      "Iteration 92, loss = 0.23159959\n",
      "Iteration 93, loss = 0.23139012\n",
      "Iteration 94, loss = 0.23120856\n",
      "Iteration 95, loss = 0.23098586\n",
      "Iteration 96, loss = 0.23066610\n",
      "Iteration 97, loss = 0.23077314\n",
      "Iteration 98, loss = 0.23047699\n",
      "Iteration 99, loss = 0.23005495\n",
      "Iteration 100, loss = 0.23004733\n",
      "Iteration 101, loss = 0.22962817\n",
      "Iteration 102, loss = 0.22930341\n",
      "Iteration 103, loss = 0.22918897\n",
      "Iteration 104, loss = 0.22893671\n",
      "Iteration 105, loss = 0.22878946\n",
      "Iteration 106, loss = 0.22851671\n",
      "Iteration 107, loss = 0.22841024\n",
      "Iteration 108, loss = 0.22798020\n",
      "Iteration 109, loss = 0.22766249\n",
      "Iteration 110, loss = 0.22748241\n",
      "Iteration 111, loss = 0.22708495\n",
      "Iteration 112, loss = 0.22675171\n",
      "Iteration 113, loss = 0.22661665\n",
      "Iteration 114, loss = 0.22626156\n",
      "Iteration 115, loss = 0.22606437\n",
      "Iteration 116, loss = 0.22562192\n",
      "Iteration 117, loss = 0.22529888\n",
      "Iteration 118, loss = 0.22486574\n",
      "Iteration 119, loss = 0.22464423\n",
      "Iteration 120, loss = 0.22459233\n",
      "Iteration 121, loss = 0.22383203\n",
      "Iteration 122, loss = 0.22372508\n",
      "Iteration 123, loss = 0.22320280\n",
      "Iteration 124, loss = 0.22300120\n",
      "Iteration 125, loss = 0.22286004\n",
      "Iteration 126, loss = 0.22214318\n",
      "Iteration 127, loss = 0.22186218\n",
      "Iteration 128, loss = 0.22132724\n",
      "Iteration 129, loss = 0.22112566\n",
      "Iteration 130, loss = 0.22065871\n",
      "Iteration 131, loss = 0.22055277\n",
      "Iteration 132, loss = 0.21996067\n",
      "Iteration 133, loss = 0.21964932\n",
      "Iteration 134, loss = 0.21922160\n",
      "Iteration 135, loss = 0.21874121\n",
      "Iteration 136, loss = 0.21824277\n",
      "Iteration 137, loss = 0.21816661\n",
      "Iteration 138, loss = 0.21748049\n",
      "Iteration 139, loss = 0.21719788\n",
      "Iteration 140, loss = 0.21637497\n",
      "Iteration 141, loss = 0.21625618\n",
      "Iteration 142, loss = 0.21588576\n",
      "Iteration 143, loss = 0.21552528\n",
      "Iteration 144, loss = 0.21488559\n",
      "Iteration 145, loss = 0.21451015\n",
      "Iteration 146, loss = 0.21401680\n",
      "Iteration 147, loss = 0.21356081\n",
      "Iteration 148, loss = 0.21331372\n",
      "Iteration 149, loss = 0.21264869\n",
      "Iteration 150, loss = 0.21229449\n",
      "Iteration 151, loss = 0.21161365\n",
      "Iteration 152, loss = 0.21123809\n",
      "Iteration 153, loss = 0.21072557\n",
      "Iteration 154, loss = 0.21007134\n",
      "Iteration 155, loss = 0.20968805\n",
      "Iteration 156, loss = 0.20913456\n",
      "Iteration 157, loss = 0.20854963\n",
      "Iteration 158, loss = 0.20836538\n",
      "Iteration 159, loss = 0.20778990\n",
      "Iteration 160, loss = 0.20735527\n",
      "Iteration 161, loss = 0.20666159\n",
      "Iteration 162, loss = 0.20619621\n",
      "Iteration 163, loss = 0.20575365\n",
      "Iteration 164, loss = 0.20517396\n",
      "Iteration 165, loss = 0.20447923\n",
      "Iteration 166, loss = 0.20403583\n",
      "Iteration 167, loss = 0.20364205\n",
      "Iteration 168, loss = 0.20303492\n",
      "Iteration 169, loss = 0.20275778\n",
      "Iteration 170, loss = 0.20209348\n",
      "Iteration 171, loss = 0.20150869\n",
      "Iteration 172, loss = 0.20094215\n",
      "Iteration 173, loss = 0.20036873\n",
      "Iteration 174, loss = 0.19986879\n",
      "Iteration 175, loss = 0.19924363\n",
      "Iteration 176, loss = 0.19873878\n",
      "Iteration 177, loss = 0.19824938\n",
      "Iteration 178, loss = 0.19768177\n",
      "Iteration 179, loss = 0.19732446\n",
      "Iteration 180, loss = 0.19693849\n",
      "Iteration 181, loss = 0.19612455\n",
      "Iteration 182, loss = 0.19585628\n",
      "Iteration 183, loss = 0.19513806\n",
      "Iteration 184, loss = 0.19454585\n",
      "Iteration 185, loss = 0.19399807\n",
      "Iteration 186, loss = 0.19346659\n",
      "Iteration 187, loss = 0.19285513\n",
      "Iteration 188, loss = 0.19223795\n",
      "Iteration 189, loss = 0.19166651\n",
      "Iteration 190, loss = 0.19133009\n",
      "Iteration 191, loss = 0.19050495\n",
      "Iteration 192, loss = 0.18995706\n",
      "Iteration 193, loss = 0.18949654\n",
      "Iteration 194, loss = 0.18887898\n",
      "Iteration 195, loss = 0.18840418\n",
      "Iteration 196, loss = 0.18781685\n",
      "Iteration 197, loss = 0.18736913\n",
      "Iteration 198, loss = 0.18677583\n",
      "Iteration 199, loss = 0.18621008\n",
      "Iteration 200, loss = 0.18549447\n",
      "Iteration 1, loss = 0.68840086\n",
      "Iteration 2, loss = 0.48756835\n",
      "Iteration 3, loss = 0.41426843\n",
      "Iteration 4, loss = 0.39000071\n",
      "Iteration 5, loss = 0.37509824\n",
      "Iteration 6, loss = 0.36278993\n",
      "Iteration 7, loss = 0.35220539\n",
      "Iteration 8, loss = 0.34285013\n",
      "Iteration 9, loss = 0.33435979\n",
      "Iteration 10, loss = 0.32717173\n",
      "Iteration 11, loss = 0.32066066\n",
      "Iteration 12, loss = 0.31476750\n",
      "Iteration 13, loss = 0.30997194\n",
      "Iteration 14, loss = 0.30532555\n",
      "Iteration 15, loss = 0.30116122\n",
      "Iteration 16, loss = 0.29763672\n",
      "Iteration 17, loss = 0.29458842\n",
      "Iteration 18, loss = 0.29166445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.28909337\n",
      "Iteration 20, loss = 0.28658948\n",
      "Iteration 21, loss = 0.28453136\n",
      "Iteration 22, loss = 0.28270433\n",
      "Iteration 23, loss = 0.28072895\n",
      "Iteration 24, loss = 0.27904593\n",
      "Iteration 25, loss = 0.27741068\n",
      "Iteration 26, loss = 0.27598263\n",
      "Iteration 27, loss = 0.27457745\n",
      "Iteration 28, loss = 0.27328161\n",
      "Iteration 29, loss = 0.27201141\n",
      "Iteration 30, loss = 0.27074682\n",
      "Iteration 31, loss = 0.26962774\n",
      "Iteration 32, loss = 0.26859217\n",
      "Iteration 33, loss = 0.26750748\n",
      "Iteration 34, loss = 0.26642492\n",
      "Iteration 35, loss = 0.26542752\n",
      "Iteration 36, loss = 0.26438269\n",
      "Iteration 37, loss = 0.26341793\n",
      "Iteration 38, loss = 0.26244930\n",
      "Iteration 39, loss = 0.26145011\n",
      "Iteration 40, loss = 0.26055875\n",
      "Iteration 41, loss = 0.25977581\n",
      "Iteration 42, loss = 0.25890945\n",
      "Iteration 43, loss = 0.25785737\n",
      "Iteration 44, loss = 0.25713349\n",
      "Iteration 45, loss = 0.25629149\n",
      "Iteration 46, loss = 0.25566991\n",
      "Iteration 47, loss = 0.25474050\n",
      "Iteration 48, loss = 0.25401122\n",
      "Iteration 49, loss = 0.25318785\n",
      "Iteration 50, loss = 0.25249690\n",
      "Iteration 51, loss = 0.25174424\n",
      "Iteration 52, loss = 0.25104028\n",
      "Iteration 53, loss = 0.25038311\n",
      "Iteration 54, loss = 0.24985755\n",
      "Iteration 55, loss = 0.24905195\n",
      "Iteration 56, loss = 0.24835431\n",
      "Iteration 57, loss = 0.24803845\n",
      "Iteration 58, loss = 0.24743149\n",
      "Iteration 59, loss = 0.24681835\n",
      "Iteration 60, loss = 0.24639348\n",
      "Iteration 61, loss = 0.24580877\n",
      "Iteration 62, loss = 0.24535530\n",
      "Iteration 63, loss = 0.24481487\n",
      "Iteration 64, loss = 0.24431575\n",
      "Iteration 65, loss = 0.24392088\n",
      "Iteration 66, loss = 0.24375312\n",
      "Iteration 67, loss = 0.24313248\n",
      "Iteration 68, loss = 0.24280780\n",
      "Iteration 69, loss = 0.24235207\n",
      "Iteration 70, loss = 0.24195108\n",
      "Iteration 71, loss = 0.24179309\n",
      "Iteration 72, loss = 0.24141354\n",
      "Iteration 73, loss = 0.24100362\n",
      "Iteration 74, loss = 0.24081936\n",
      "Iteration 75, loss = 0.24061360\n",
      "Iteration 76, loss = 0.24008699\n",
      "Iteration 77, loss = 0.23992080\n",
      "Iteration 78, loss = 0.23958720\n",
      "Iteration 79, loss = 0.23936734\n",
      "Iteration 80, loss = 0.23916517\n",
      "Iteration 81, loss = 0.23876016\n",
      "Iteration 82, loss = 0.23872918\n",
      "Iteration 83, loss = 0.23834621\n",
      "Iteration 84, loss = 0.23795327\n",
      "Iteration 85, loss = 0.23793210\n",
      "Iteration 86, loss = 0.23760893\n",
      "Iteration 87, loss = 0.23753218\n",
      "Iteration 88, loss = 0.23739990\n",
      "Iteration 89, loss = 0.23706403\n",
      "Iteration 90, loss = 0.23702183\n",
      "Iteration 91, loss = 0.23674587\n",
      "Iteration 92, loss = 0.23618940\n",
      "Iteration 93, loss = 0.23594436\n",
      "Iteration 94, loss = 0.23564693\n",
      "Iteration 95, loss = 0.23549038\n",
      "Iteration 96, loss = 0.23526633\n",
      "Iteration 97, loss = 0.23515472\n",
      "Iteration 98, loss = 0.23481572\n",
      "Iteration 99, loss = 0.23444366\n",
      "Iteration 100, loss = 0.23444093\n",
      "Iteration 101, loss = 0.23410264\n",
      "Iteration 102, loss = 0.23368398\n",
      "Iteration 103, loss = 0.23344801\n",
      "Iteration 104, loss = 0.23317273\n",
      "Iteration 105, loss = 0.23287103\n",
      "Iteration 106, loss = 0.23285563\n",
      "Iteration 107, loss = 0.23328667\n",
      "Iteration 108, loss = 0.23211262\n",
      "Iteration 109, loss = 0.23195712\n",
      "Iteration 110, loss = 0.23167447\n",
      "Iteration 111, loss = 0.23115180\n",
      "Iteration 112, loss = 0.23109873\n",
      "Iteration 113, loss = 0.23072345\n",
      "Iteration 114, loss = 0.23032170\n",
      "Iteration 115, loss = 0.22989798\n",
      "Iteration 116, loss = 0.22962733\n",
      "Iteration 117, loss = 0.22924463\n",
      "Iteration 118, loss = 0.22895746\n",
      "Iteration 119, loss = 0.22869966\n",
      "Iteration 120, loss = 0.22856656\n",
      "Iteration 121, loss = 0.22793841\n",
      "Iteration 122, loss = 0.22779949\n",
      "Iteration 123, loss = 0.22721876\n",
      "Iteration 124, loss = 0.22682176\n",
      "Iteration 125, loss = 0.22682659\n",
      "Iteration 126, loss = 0.22615396\n",
      "Iteration 127, loss = 0.22589848\n",
      "Iteration 128, loss = 0.22543046\n",
      "Iteration 129, loss = 0.22509034\n",
      "Iteration 130, loss = 0.22452113\n",
      "Iteration 131, loss = 0.22450935\n",
      "Iteration 132, loss = 0.22393295\n",
      "Iteration 133, loss = 0.22347176\n",
      "Iteration 134, loss = 0.22307656\n",
      "Iteration 135, loss = 0.22264842\n",
      "Iteration 136, loss = 0.22229591\n",
      "Iteration 137, loss = 0.22191859\n",
      "Iteration 138, loss = 0.22168494\n",
      "Iteration 139, loss = 0.22124045\n",
      "Iteration 140, loss = 0.22063889\n",
      "Iteration 141, loss = 0.22057858\n",
      "Iteration 142, loss = 0.22019824\n",
      "Iteration 143, loss = 0.21950936\n",
      "Iteration 144, loss = 0.21907926\n",
      "Iteration 145, loss = 0.21870551\n",
      "Iteration 146, loss = 0.21797308\n",
      "Iteration 147, loss = 0.21776585\n",
      "Iteration 148, loss = 0.21733648\n",
      "Iteration 149, loss = 0.21691906\n",
      "Iteration 150, loss = 0.21651525\n",
      "Iteration 151, loss = 0.21589572\n",
      "Iteration 152, loss = 0.21549847\n",
      "Iteration 153, loss = 0.21506014\n",
      "Iteration 154, loss = 0.21442221\n",
      "Iteration 155, loss = 0.21423348\n",
      "Iteration 156, loss = 0.21353982\n",
      "Iteration 157, loss = 0.21320660\n",
      "Iteration 158, loss = 0.21290261\n",
      "Iteration 159, loss = 0.21239943\n",
      "Iteration 160, loss = 0.21205423\n",
      "Iteration 161, loss = 0.21121086\n",
      "Iteration 162, loss = 0.21064909\n",
      "Iteration 163, loss = 0.21052346\n",
      "Iteration 164, loss = 0.20988130\n",
      "Iteration 165, loss = 0.20930197\n",
      "Iteration 166, loss = 0.20886673\n",
      "Iteration 167, loss = 0.20831942\n",
      "Iteration 168, loss = 0.20774541\n",
      "Iteration 169, loss = 0.20744714\n",
      "Iteration 170, loss = 0.20677528\n",
      "Iteration 171, loss = 0.20629285\n",
      "Iteration 172, loss = 0.20577189\n",
      "Iteration 173, loss = 0.20550260\n",
      "Iteration 174, loss = 0.20490067\n",
      "Iteration 175, loss = 0.20414481\n",
      "Iteration 176, loss = 0.20363259\n",
      "Iteration 177, loss = 0.20327724\n",
      "Iteration 178, loss = 0.20295125\n",
      "Iteration 179, loss = 0.20229458\n",
      "Iteration 180, loss = 0.20203967\n",
      "Iteration 181, loss = 0.20126614\n",
      "Iteration 182, loss = 0.20070700\n",
      "Iteration 183, loss = 0.20007513\n",
      "Iteration 184, loss = 0.19968606\n",
      "Iteration 185, loss = 0.19908624\n",
      "Iteration 186, loss = 0.19878665\n",
      "Iteration 187, loss = 0.19813528\n",
      "Iteration 188, loss = 0.19749931\n",
      "Iteration 189, loss = 0.19710624\n",
      "Iteration 190, loss = 0.19641384\n",
      "Iteration 191, loss = 0.19575833\n",
      "Iteration 192, loss = 0.19528671\n",
      "Iteration 193, loss = 0.19467515\n",
      "Iteration 194, loss = 0.19431746\n",
      "Iteration 195, loss = 0.19367540\n",
      "Iteration 196, loss = 0.19300635\n",
      "Iteration 197, loss = 0.19257870\n",
      "Iteration 198, loss = 0.19212054\n",
      "Iteration 199, loss = 0.19169583\n",
      "Iteration 200, loss = 0.19097293\n",
      "Iteration 1, loss = 0.69036692\n",
      "Iteration 2, loss = 0.48773065\n",
      "Iteration 3, loss = 0.41593754\n",
      "Iteration 4, loss = 0.39088116\n",
      "Iteration 5, loss = 0.37652241\n",
      "Iteration 6, loss = 0.36489177\n",
      "Iteration 7, loss = 0.35408609\n",
      "Iteration 8, loss = 0.34483602\n",
      "Iteration 9, loss = 0.33685464\n",
      "Iteration 10, loss = 0.32933333\n",
      "Iteration 11, loss = 0.32295098\n",
      "Iteration 12, loss = 0.31694628\n",
      "Iteration 13, loss = 0.31193467\n",
      "Iteration 14, loss = 0.30723734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.30310490\n",
      "Iteration 16, loss = 0.29936971\n",
      "Iteration 17, loss = 0.29608273\n",
      "Iteration 18, loss = 0.29302381\n",
      "Iteration 19, loss = 0.29025765\n",
      "Iteration 20, loss = 0.28772037\n",
      "Iteration 21, loss = 0.28539482\n",
      "Iteration 22, loss = 0.28325657\n",
      "Iteration 23, loss = 0.28122942\n",
      "Iteration 24, loss = 0.27927274\n",
      "Iteration 25, loss = 0.27755793\n",
      "Iteration 26, loss = 0.27575502\n",
      "Iteration 27, loss = 0.27430863\n",
      "Iteration 28, loss = 0.27274782\n",
      "Iteration 29, loss = 0.27127362\n",
      "Iteration 30, loss = 0.26979665\n",
      "Iteration 31, loss = 0.26844623\n",
      "Iteration 32, loss = 0.26713921\n",
      "Iteration 33, loss = 0.26588924\n",
      "Iteration 34, loss = 0.26457885\n",
      "Iteration 35, loss = 0.26336211\n",
      "Iteration 36, loss = 0.26213088\n",
      "Iteration 37, loss = 0.26091898\n",
      "Iteration 38, loss = 0.26000127\n",
      "Iteration 39, loss = 0.25881327\n",
      "Iteration 40, loss = 0.25777830\n",
      "Iteration 41, loss = 0.25655918\n",
      "Iteration 42, loss = 0.25559715\n",
      "Iteration 43, loss = 0.25469695\n",
      "Iteration 44, loss = 0.25369492\n",
      "Iteration 45, loss = 0.25259238\n",
      "Iteration 46, loss = 0.25193732\n",
      "Iteration 47, loss = 0.25092619\n",
      "Iteration 48, loss = 0.24985294\n",
      "Iteration 49, loss = 0.24912240\n",
      "Iteration 50, loss = 0.24822362\n",
      "Iteration 51, loss = 0.24733979\n",
      "Iteration 52, loss = 0.24670317\n",
      "Iteration 53, loss = 0.24576692\n",
      "Iteration 54, loss = 0.24513297\n",
      "Iteration 55, loss = 0.24422161\n",
      "Iteration 56, loss = 0.24364742\n",
      "Iteration 57, loss = 0.24301246\n",
      "Iteration 58, loss = 0.24240252\n",
      "Iteration 59, loss = 0.24181317\n",
      "Iteration 60, loss = 0.24124646\n",
      "Iteration 61, loss = 0.24080736\n",
      "Iteration 62, loss = 0.24014446\n",
      "Iteration 63, loss = 0.23970920\n",
      "Iteration 64, loss = 0.23922764\n",
      "Iteration 65, loss = 0.23898510\n",
      "Iteration 66, loss = 0.23807043\n",
      "Iteration 67, loss = 0.23785374\n",
      "Iteration 68, loss = 0.23739197\n",
      "Iteration 69, loss = 0.23698555\n",
      "Iteration 70, loss = 0.23669093\n",
      "Iteration 71, loss = 0.23633144\n",
      "Iteration 72, loss = 0.23608490\n",
      "Iteration 73, loss = 0.23563523\n",
      "Iteration 74, loss = 0.23532711\n",
      "Iteration 75, loss = 0.23532467\n",
      "Iteration 76, loss = 0.23478879\n",
      "Iteration 77, loss = 0.23430993\n",
      "Iteration 78, loss = 0.23419004\n",
      "Iteration 79, loss = 0.23387755\n",
      "Iteration 80, loss = 0.23352152\n",
      "Iteration 81, loss = 0.23338248\n",
      "Iteration 82, loss = 0.23396522\n",
      "Iteration 83, loss = 0.23305617\n",
      "Iteration 84, loss = 0.23262492\n",
      "Iteration 85, loss = 0.23219116\n",
      "Iteration 86, loss = 0.23236319\n",
      "Iteration 87, loss = 0.23177149\n",
      "Iteration 88, loss = 0.23167546\n",
      "Iteration 89, loss = 0.23131273\n",
      "Iteration 90, loss = 0.23102429\n",
      "Iteration 91, loss = 0.23060896\n",
      "Iteration 92, loss = 0.23059579\n",
      "Iteration 93, loss = 0.23003191\n",
      "Iteration 94, loss = 0.22990910\n",
      "Iteration 95, loss = 0.22954130\n",
      "Iteration 96, loss = 0.22961552\n",
      "Iteration 97, loss = 0.22933400\n",
      "Iteration 98, loss = 0.22855921\n",
      "Iteration 99, loss = 0.22849163\n",
      "Iteration 100, loss = 0.22846738\n",
      "Iteration 101, loss = 0.22805303\n",
      "Iteration 102, loss = 0.22769039\n",
      "Iteration 103, loss = 0.22737790\n",
      "Iteration 104, loss = 0.22673726\n",
      "Iteration 105, loss = 0.22650404\n",
      "Iteration 106, loss = 0.22617252\n",
      "Iteration 107, loss = 0.22590242\n",
      "Iteration 108, loss = 0.22546211\n",
      "Iteration 109, loss = 0.22506188\n",
      "Iteration 110, loss = 0.22478477\n",
      "Iteration 111, loss = 0.22466373\n",
      "Iteration 112, loss = 0.22410652\n",
      "Iteration 113, loss = 0.22362263\n",
      "Iteration 114, loss = 0.22393938\n",
      "Iteration 115, loss = 0.22309422\n",
      "Iteration 116, loss = 0.22255856\n",
      "Iteration 117, loss = 0.22202146\n",
      "Iteration 118, loss = 0.22148725\n",
      "Iteration 119, loss = 0.22129987\n",
      "Iteration 120, loss = 0.22082882\n",
      "Iteration 121, loss = 0.22040302\n",
      "Iteration 122, loss = 0.21994249\n",
      "Iteration 123, loss = 0.21953968\n",
      "Iteration 124, loss = 0.21911559\n",
      "Iteration 125, loss = 0.21863801\n",
      "Iteration 126, loss = 0.21804316\n",
      "Iteration 127, loss = 0.21765150\n",
      "Iteration 128, loss = 0.21723687\n",
      "Iteration 129, loss = 0.21666887\n",
      "Iteration 130, loss = 0.21665193\n",
      "Iteration 131, loss = 0.21577868\n",
      "Iteration 132, loss = 0.21520236\n",
      "Iteration 133, loss = 0.21473979\n",
      "Iteration 134, loss = 0.21458947\n",
      "Iteration 135, loss = 0.21345146\n",
      "Iteration 136, loss = 0.21315034\n",
      "Iteration 137, loss = 0.21279856\n",
      "Iteration 138, loss = 0.21207525\n",
      "Iteration 139, loss = 0.21156348\n",
      "Iteration 140, loss = 0.21136793\n",
      "Iteration 141, loss = 0.21040201\n",
      "Iteration 142, loss = 0.20987614\n",
      "Iteration 143, loss = 0.20960129\n",
      "Iteration 144, loss = 0.20888558\n",
      "Iteration 145, loss = 0.20838839\n",
      "Iteration 146, loss = 0.20770593\n",
      "Iteration 147, loss = 0.20717106\n",
      "Iteration 148, loss = 0.20663337\n",
      "Iteration 149, loss = 0.20607629\n",
      "Iteration 150, loss = 0.20546337\n",
      "Iteration 151, loss = 0.20499639\n",
      "Iteration 152, loss = 0.20487629\n",
      "Iteration 153, loss = 0.20380010\n",
      "Iteration 154, loss = 0.20315565\n",
      "Iteration 155, loss = 0.20261599\n",
      "Iteration 156, loss = 0.20198136\n",
      "Iteration 157, loss = 0.20153354\n",
      "Iteration 158, loss = 0.20065378\n",
      "Iteration 159, loss = 0.20034724\n",
      "Iteration 160, loss = 0.19956331\n",
      "Iteration 161, loss = 0.19909733\n",
      "Iteration 162, loss = 0.19892916\n",
      "Iteration 163, loss = 0.19797957\n",
      "Iteration 164, loss = 0.19744146\n",
      "Iteration 165, loss = 0.19691893\n",
      "Iteration 166, loss = 0.19654670\n",
      "Iteration 167, loss = 0.19565680\n",
      "Iteration 168, loss = 0.19522987\n",
      "Iteration 169, loss = 0.19455050\n",
      "Iteration 170, loss = 0.19386564\n",
      "Iteration 171, loss = 0.19329091\n",
      "Iteration 172, loss = 0.19283497\n",
      "Iteration 173, loss = 0.19216302\n",
      "Iteration 174, loss = 0.19145415\n",
      "Iteration 175, loss = 0.19132439\n",
      "Iteration 176, loss = 0.19056424\n",
      "Iteration 177, loss = 0.18975634\n",
      "Iteration 178, loss = 0.18950897\n",
      "Iteration 179, loss = 0.18866537\n",
      "Iteration 180, loss = 0.18812096\n",
      "Iteration 181, loss = 0.18832506\n",
      "Iteration 182, loss = 0.18684856\n",
      "Iteration 183, loss = 0.18669180\n",
      "Iteration 184, loss = 0.18598174\n",
      "Iteration 185, loss = 0.18536317\n",
      "Iteration 186, loss = 0.18483878\n",
      "Iteration 187, loss = 0.18430094\n",
      "Iteration 188, loss = 0.18399016\n",
      "Iteration 189, loss = 0.18303945\n",
      "Iteration 190, loss = 0.18306297\n",
      "Iteration 191, loss = 0.18314700\n",
      "Iteration 192, loss = 0.18213513\n",
      "Iteration 193, loss = 0.18094324\n",
      "Iteration 194, loss = 0.18069813\n",
      "Iteration 195, loss = 0.18031055\n",
      "Iteration 196, loss = 0.17949669\n",
      "Iteration 197, loss = 0.17895803\n",
      "Iteration 198, loss = 0.17832177\n",
      "Iteration 199, loss = 0.17778203\n",
      "Iteration 200, loss = 0.17778029\n",
      "Iteration 1, loss = 0.69118921\n",
      "Iteration 2, loss = 0.48799379\n",
      "Iteration 3, loss = 0.41504811\n",
      "Iteration 4, loss = 0.39030737\n",
      "Iteration 5, loss = 0.37620307\n",
      "Iteration 6, loss = 0.36439277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.35386807\n",
      "Iteration 8, loss = 0.34461792\n",
      "Iteration 9, loss = 0.33647702\n",
      "Iteration 10, loss = 0.32930211\n",
      "Iteration 11, loss = 0.32286058\n",
      "Iteration 12, loss = 0.31691345\n",
      "Iteration 13, loss = 0.31207950\n",
      "Iteration 14, loss = 0.30741836\n",
      "Iteration 15, loss = 0.30338309\n",
      "Iteration 16, loss = 0.29975564\n",
      "Iteration 17, loss = 0.29658503\n",
      "Iteration 18, loss = 0.29368370\n",
      "Iteration 19, loss = 0.29101928\n",
      "Iteration 20, loss = 0.28868472\n",
      "Iteration 21, loss = 0.28660584\n",
      "Iteration 22, loss = 0.28474807\n",
      "Iteration 23, loss = 0.28274006\n",
      "Iteration 24, loss = 0.28104690\n",
      "Iteration 25, loss = 0.27941747\n",
      "Iteration 26, loss = 0.27793230\n",
      "Iteration 27, loss = 0.27661438\n",
      "Iteration 28, loss = 0.27513331\n",
      "Iteration 29, loss = 0.27400748\n",
      "Iteration 30, loss = 0.27263071\n",
      "Iteration 31, loss = 0.27135183\n",
      "Iteration 32, loss = 0.27021436\n",
      "Iteration 33, loss = 0.26912907\n",
      "Iteration 34, loss = 0.26809986\n",
      "Iteration 35, loss = 0.26695258\n",
      "Iteration 36, loss = 0.26597713\n",
      "Iteration 37, loss = 0.26499840\n",
      "Iteration 38, loss = 0.26392152\n",
      "Iteration 39, loss = 0.26307799\n",
      "Iteration 40, loss = 0.26191459\n",
      "Iteration 41, loss = 0.26087057\n",
      "Iteration 42, loss = 0.26005342\n",
      "Iteration 43, loss = 0.25920633\n",
      "Iteration 44, loss = 0.25837496\n",
      "Iteration 45, loss = 0.25751481\n",
      "Iteration 46, loss = 0.25683645\n",
      "Iteration 47, loss = 0.25597529\n",
      "Iteration 48, loss = 0.25486454\n",
      "Iteration 49, loss = 0.25440167\n",
      "Iteration 50, loss = 0.25347003\n",
      "Iteration 51, loss = 0.25296749\n",
      "Iteration 52, loss = 0.25226367\n",
      "Iteration 53, loss = 0.25139348\n",
      "Iteration 54, loss = 0.25067558\n",
      "Iteration 55, loss = 0.25001642\n",
      "Iteration 56, loss = 0.24939132\n",
      "Iteration 57, loss = 0.24889178\n",
      "Iteration 58, loss = 0.24837785\n",
      "Iteration 59, loss = 0.24780688\n",
      "Iteration 60, loss = 0.24729588\n",
      "Iteration 61, loss = 0.24683617\n",
      "Iteration 62, loss = 0.24621830\n",
      "Iteration 63, loss = 0.24586855\n",
      "Iteration 64, loss = 0.24540774\n",
      "Iteration 65, loss = 0.24531083\n",
      "Iteration 66, loss = 0.24459887\n",
      "Iteration 67, loss = 0.24433754\n",
      "Iteration 68, loss = 0.24388537\n",
      "Iteration 69, loss = 0.24358579\n",
      "Iteration 70, loss = 0.24330427\n",
      "Iteration 71, loss = 0.24302854\n",
      "Iteration 72, loss = 0.24249375\n",
      "Iteration 73, loss = 0.24228840\n",
      "Iteration 74, loss = 0.24201100\n",
      "Iteration 75, loss = 0.24176155\n",
      "Iteration 76, loss = 0.24144204\n",
      "Iteration 77, loss = 0.24125054\n",
      "Iteration 78, loss = 0.24106302\n",
      "Iteration 79, loss = 0.24065746\n",
      "Iteration 80, loss = 0.24057729\n",
      "Iteration 81, loss = 0.24029515\n",
      "Iteration 82, loss = 0.24036901\n",
      "Iteration 83, loss = 0.24007637\n",
      "Iteration 84, loss = 0.23960930\n",
      "Iteration 85, loss = 0.23906347\n",
      "Iteration 86, loss = 0.23922565\n",
      "Iteration 87, loss = 0.23877600\n",
      "Iteration 88, loss = 0.23863221\n",
      "Iteration 89, loss = 0.23830953\n",
      "Iteration 90, loss = 0.23805885\n",
      "Iteration 91, loss = 0.23790855\n",
      "Iteration 92, loss = 0.23763368\n",
      "Iteration 93, loss = 0.23734068\n",
      "Iteration 94, loss = 0.23722048\n",
      "Iteration 95, loss = 0.23703374\n",
      "Iteration 96, loss = 0.23707591\n",
      "Iteration 97, loss = 0.23648602\n",
      "Iteration 98, loss = 0.23614754\n",
      "Iteration 99, loss = 0.23592373\n",
      "Iteration 100, loss = 0.23578892\n",
      "Iteration 101, loss = 0.23585021\n",
      "Iteration 102, loss = 0.23549096\n",
      "Iteration 103, loss = 0.23486710\n",
      "Iteration 104, loss = 0.23441954\n",
      "Iteration 105, loss = 0.23428927\n",
      "Iteration 106, loss = 0.23396805\n",
      "Iteration 107, loss = 0.23372888\n",
      "Iteration 108, loss = 0.23338285\n",
      "Iteration 109, loss = 0.23299476\n",
      "Iteration 110, loss = 0.23270212\n",
      "Iteration 111, loss = 0.23255749\n",
      "Iteration 112, loss = 0.23209645\n",
      "Iteration 113, loss = 0.23178684\n",
      "Iteration 114, loss = 0.23157421\n",
      "Iteration 115, loss = 0.23098249\n",
      "Iteration 116, loss = 0.23080481\n",
      "Iteration 117, loss = 0.23026174\n",
      "Iteration 118, loss = 0.22978880\n",
      "Iteration 119, loss = 0.22958599\n",
      "Iteration 120, loss = 0.22909280\n",
      "Iteration 121, loss = 0.22881809\n",
      "Iteration 122, loss = 0.22845896\n",
      "Iteration 123, loss = 0.22811396\n",
      "Iteration 124, loss = 0.22763961\n",
      "Iteration 125, loss = 0.22724104\n",
      "Iteration 126, loss = 0.22708805\n",
      "Iteration 127, loss = 0.22647031\n",
      "Iteration 128, loss = 0.22599340\n",
      "Iteration 129, loss = 0.22550158\n",
      "Iteration 130, loss = 0.22529329\n",
      "Iteration 131, loss = 0.22450357\n",
      "Iteration 132, loss = 0.22432596\n",
      "Iteration 133, loss = 0.22361257\n",
      "Iteration 134, loss = 0.22344507\n",
      "Iteration 135, loss = 0.22265498\n",
      "Iteration 136, loss = 0.22240845\n",
      "Iteration 137, loss = 0.22170257\n",
      "Iteration 138, loss = 0.22132604\n",
      "Iteration 139, loss = 0.22081828\n",
      "Iteration 140, loss = 0.22057292\n",
      "Iteration 141, loss = 0.21974391\n",
      "Iteration 142, loss = 0.21946698\n",
      "Iteration 143, loss = 0.21909439\n",
      "Iteration 144, loss = 0.21836497\n",
      "Iteration 145, loss = 0.21793510\n",
      "Iteration 146, loss = 0.21725574\n",
      "Iteration 147, loss = 0.21681361\n",
      "Iteration 148, loss = 0.21638189\n",
      "Iteration 149, loss = 0.21576025\n",
      "Iteration 150, loss = 0.21543255\n",
      "Iteration 151, loss = 0.21526873\n",
      "Iteration 152, loss = 0.21484103\n",
      "Iteration 153, loss = 0.21348700\n",
      "Iteration 154, loss = 0.21342252\n",
      "Iteration 155, loss = 0.21243403\n",
      "Iteration 156, loss = 0.21187814\n",
      "Iteration 157, loss = 0.21144864\n",
      "Iteration 158, loss = 0.21083789\n",
      "Iteration 159, loss = 0.21031512\n",
      "Iteration 160, loss = 0.20960599\n",
      "Iteration 161, loss = 0.20928902\n",
      "Iteration 162, loss = 0.20908542\n",
      "Iteration 163, loss = 0.20810159\n",
      "Iteration 164, loss = 0.20747404\n",
      "Iteration 165, loss = 0.20703775\n",
      "Iteration 166, loss = 0.20610855\n",
      "Iteration 167, loss = 0.20562506\n",
      "Iteration 168, loss = 0.20518848\n",
      "Iteration 169, loss = 0.20456416\n",
      "Iteration 170, loss = 0.20386925\n",
      "Iteration 171, loss = 0.20350637\n",
      "Iteration 172, loss = 0.20276251\n",
      "Iteration 173, loss = 0.20232039\n",
      "Iteration 174, loss = 0.20154939\n",
      "Iteration 175, loss = 0.20150482\n",
      "Iteration 176, loss = 0.20045770\n",
      "Iteration 177, loss = 0.19984471\n",
      "Iteration 178, loss = 0.19934223\n",
      "Iteration 179, loss = 0.19851297\n",
      "Iteration 180, loss = 0.19803359\n",
      "Iteration 181, loss = 0.19765901\n",
      "Iteration 182, loss = 0.19669598\n",
      "Iteration 183, loss = 0.19640214\n",
      "Iteration 184, loss = 0.19560484\n",
      "Iteration 185, loss = 0.19492170\n",
      "Iteration 186, loss = 0.19448547\n",
      "Iteration 187, loss = 0.19395393\n",
      "Iteration 188, loss = 0.19309344\n",
      "Iteration 189, loss = 0.19248910\n",
      "Iteration 190, loss = 0.19233110\n",
      "Iteration 191, loss = 0.19223414\n",
      "Iteration 192, loss = 0.19152930\n",
      "Iteration 193, loss = 0.19008065\n",
      "Iteration 194, loss = 0.18985411\n",
      "Iteration 195, loss = 0.18935751\n",
      "Iteration 196, loss = 0.18843006\n",
      "Iteration 197, loss = 0.18785551\n",
      "Iteration 198, loss = 0.18724516\n",
      "Iteration 199, loss = 0.18652389\n",
      "Iteration 200, loss = 0.18621455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70810519\n",
      "Iteration 2, loss = 0.48936615\n",
      "Iteration 3, loss = 0.44522553\n",
      "Iteration 4, loss = 0.44164346\n",
      "Iteration 5, loss = 0.44109047\n",
      "Iteration 6, loss = 0.44030706\n",
      "Iteration 7, loss = 0.43952365\n",
      "Iteration 8, loss = 0.43880571\n",
      "Iteration 9, loss = 0.43808335\n",
      "Iteration 10, loss = 0.43738486\n",
      "Iteration 11, loss = 0.43664688\n",
      "Iteration 12, loss = 0.43594581\n",
      "Iteration 13, loss = 0.43522431\n",
      "Iteration 14, loss = 0.43450689\n",
      "Iteration 15, loss = 0.43379917\n",
      "Iteration 16, loss = 0.43309511\n",
      "Iteration 17, loss = 0.43240442\n",
      "Iteration 18, loss = 0.43170659\n",
      "Iteration 19, loss = 0.43096392\n",
      "Iteration 20, loss = 0.43033072\n",
      "Iteration 21, loss = 0.42957575\n",
      "Iteration 22, loss = 0.42887539\n",
      "Iteration 23, loss = 0.42818533\n",
      "Iteration 24, loss = 0.42750098\n",
      "Iteration 25, loss = 0.42678601\n",
      "Iteration 26, loss = 0.42611365\n",
      "Iteration 27, loss = 0.42539940\n",
      "Iteration 28, loss = 0.42469418\n",
      "Iteration 29, loss = 0.42398237\n",
      "Iteration 30, loss = 0.42327339\n",
      "Iteration 31, loss = 0.42258642\n",
      "Iteration 32, loss = 0.42189389\n",
      "Iteration 33, loss = 0.42119977\n",
      "Iteration 34, loss = 0.42048652\n",
      "Iteration 35, loss = 0.41978878\n",
      "Iteration 36, loss = 0.41913269\n",
      "Iteration 37, loss = 0.41836062\n",
      "Iteration 38, loss = 0.41764791\n",
      "Iteration 39, loss = 0.41695020\n",
      "Iteration 40, loss = 0.41624017\n",
      "Iteration 41, loss = 0.41551520\n",
      "Iteration 42, loss = 0.41484958\n",
      "Iteration 43, loss = 0.41407944\n",
      "Iteration 44, loss = 0.41339468\n",
      "Iteration 45, loss = 0.41268239\n",
      "Iteration 46, loss = 0.41197300\n",
      "Iteration 47, loss = 0.41124169\n",
      "Iteration 48, loss = 0.41050382\n",
      "Iteration 49, loss = 0.40980239\n",
      "Iteration 50, loss = 0.40905809\n",
      "Iteration 51, loss = 0.40835375\n",
      "Iteration 52, loss = 0.40765366\n",
      "Iteration 53, loss = 0.40684651\n",
      "Iteration 54, loss = 0.40609465\n",
      "Iteration 55, loss = 0.40536162\n",
      "Iteration 56, loss = 0.40464385\n",
      "Iteration 57, loss = 0.40389596\n",
      "Iteration 58, loss = 0.40314803\n",
      "Iteration 59, loss = 0.40239326\n",
      "Iteration 60, loss = 0.40168597\n",
      "Iteration 61, loss = 0.40090798\n",
      "Iteration 62, loss = 0.40015546\n",
      "Iteration 63, loss = 0.39940327\n",
      "Iteration 64, loss = 0.39862110\n",
      "Iteration 65, loss = 0.39788664\n",
      "Iteration 66, loss = 0.39711145\n",
      "Iteration 67, loss = 0.39633584\n",
      "Iteration 68, loss = 0.39558879\n",
      "Iteration 69, loss = 0.39482400\n",
      "Iteration 70, loss = 0.39409820\n",
      "Iteration 71, loss = 0.39328784\n",
      "Iteration 72, loss = 0.39251689\n",
      "Iteration 73, loss = 0.39173376\n",
      "Iteration 74, loss = 0.39094587\n",
      "Iteration 75, loss = 0.39017900\n",
      "Iteration 76, loss = 0.38938261\n",
      "Iteration 77, loss = 0.38862957\n",
      "Iteration 78, loss = 0.38782375\n",
      "Iteration 79, loss = 0.38704566\n",
      "Iteration 80, loss = 0.38626155\n",
      "Iteration 81, loss = 0.38546913\n",
      "Iteration 82, loss = 0.38470265\n",
      "Iteration 83, loss = 0.38387959\n",
      "Iteration 84, loss = 0.38310150\n",
      "Iteration 85, loss = 0.38232541\n",
      "Iteration 86, loss = 0.38158338\n",
      "Iteration 87, loss = 0.38072124\n",
      "Iteration 88, loss = 0.37995243\n",
      "Iteration 89, loss = 0.37915025\n",
      "Iteration 90, loss = 0.37834563\n",
      "Iteration 91, loss = 0.37756389\n",
      "Iteration 92, loss = 0.37676679\n",
      "Iteration 93, loss = 0.37601537\n",
      "Iteration 94, loss = 0.37516550\n",
      "Iteration 95, loss = 0.37435270\n",
      "Iteration 96, loss = 0.37357040\n",
      "Iteration 97, loss = 0.37275512\n",
      "Iteration 98, loss = 0.37194935\n",
      "Iteration 99, loss = 0.37116149\n",
      "Iteration 100, loss = 0.37037429\n",
      "Iteration 101, loss = 0.36956083\n",
      "Iteration 102, loss = 0.36878565\n",
      "Iteration 103, loss = 0.36797331\n",
      "Iteration 104, loss = 0.36720702\n",
      "Iteration 105, loss = 0.36639733\n",
      "Iteration 106, loss = 0.36560218\n",
      "Iteration 107, loss = 0.36482950\n",
      "Iteration 108, loss = 0.36400745\n",
      "Iteration 109, loss = 0.36324302\n",
      "Iteration 110, loss = 0.36250227\n",
      "Iteration 111, loss = 0.36165384\n",
      "Iteration 112, loss = 0.36088807\n",
      "Iteration 113, loss = 0.36009370\n",
      "Iteration 114, loss = 0.35931435\n",
      "Iteration 115, loss = 0.35856737\n",
      "Iteration 116, loss = 0.35775177\n",
      "Iteration 117, loss = 0.35698719\n",
      "Iteration 118, loss = 0.35622620\n",
      "Iteration 119, loss = 0.35545808\n",
      "Iteration 120, loss = 0.35467172\n",
      "Iteration 121, loss = 0.35393089\n",
      "Iteration 122, loss = 0.35318918\n",
      "Iteration 123, loss = 0.35237915\n",
      "Iteration 124, loss = 0.35162900\n",
      "Iteration 125, loss = 0.35087555\n",
      "Iteration 126, loss = 0.35014181\n",
      "Iteration 127, loss = 0.34938203\n",
      "Iteration 128, loss = 0.34863830\n",
      "Iteration 129, loss = 0.34790237\n",
      "Iteration 130, loss = 0.34717266\n",
      "Iteration 131, loss = 0.34646616\n",
      "Iteration 132, loss = 0.34570985\n",
      "Iteration 133, loss = 0.34497348\n",
      "Iteration 134, loss = 0.34426095\n",
      "Iteration 135, loss = 0.34353315\n",
      "Iteration 136, loss = 0.34285028\n",
      "Iteration 137, loss = 0.34211999\n",
      "Iteration 138, loss = 0.34141339\n",
      "Iteration 139, loss = 0.34072892\n",
      "Iteration 140, loss = 0.34000282\n",
      "Iteration 141, loss = 0.33930710\n",
      "Iteration 142, loss = 0.33864138\n",
      "Iteration 143, loss = 0.33792869\n",
      "Iteration 144, loss = 0.33725969\n",
      "Iteration 145, loss = 0.33657735\n",
      "Iteration 146, loss = 0.33591784\n",
      "Iteration 147, loss = 0.33525781\n",
      "Iteration 148, loss = 0.33458013\n",
      "Iteration 149, loss = 0.33392690\n",
      "Iteration 150, loss = 0.33325414\n",
      "Iteration 151, loss = 0.33263717\n",
      "Iteration 152, loss = 0.33193473\n",
      "Iteration 153, loss = 0.33133214\n",
      "Iteration 154, loss = 0.33066795\n",
      "Iteration 155, loss = 0.33005553\n",
      "Iteration 156, loss = 0.32942189\n",
      "Iteration 157, loss = 0.32880744\n",
      "Iteration 158, loss = 0.32817784\n",
      "Iteration 159, loss = 0.32759122\n",
      "Iteration 160, loss = 0.32697436\n",
      "Iteration 161, loss = 0.32636636\n",
      "Iteration 162, loss = 0.32576781\n",
      "Iteration 163, loss = 0.32515442\n",
      "Iteration 164, loss = 0.32457154\n",
      "Iteration 165, loss = 0.32400888\n",
      "Iteration 166, loss = 0.32341481\n",
      "Iteration 167, loss = 0.32288640\n",
      "Iteration 168, loss = 0.32226949\n",
      "Iteration 169, loss = 0.32172678\n",
      "Iteration 170, loss = 0.32114548\n",
      "Iteration 171, loss = 0.32058613\n",
      "Iteration 172, loss = 0.32003365\n",
      "Iteration 173, loss = 0.31950716\n",
      "Iteration 174, loss = 0.31895309\n",
      "Iteration 175, loss = 0.31841902\n",
      "Iteration 176, loss = 0.31788300\n",
      "Iteration 177, loss = 0.31736919\n",
      "Iteration 178, loss = 0.31687585\n",
      "Iteration 179, loss = 0.31634695\n",
      "Iteration 180, loss = 0.31582379\n",
      "Iteration 181, loss = 0.31531451\n",
      "Iteration 182, loss = 0.31487207\n",
      "Iteration 183, loss = 0.31431140\n",
      "Iteration 184, loss = 0.31383799\n",
      "Iteration 185, loss = 0.31331557\n",
      "Iteration 186, loss = 0.31283345\n",
      "Iteration 187, loss = 0.31236429\n",
      "Iteration 188, loss = 0.31189682\n",
      "Iteration 189, loss = 0.31142550\n",
      "Iteration 190, loss = 0.31094789\n",
      "Iteration 191, loss = 0.31049065\n",
      "Iteration 192, loss = 0.31004848\n",
      "Iteration 193, loss = 0.30956992\n",
      "Iteration 194, loss = 0.30914677\n",
      "Iteration 195, loss = 0.30869609\n",
      "Iteration 196, loss = 0.30825090\n",
      "Iteration 197, loss = 0.30781508\n",
      "Iteration 198, loss = 0.30739044\n",
      "Iteration 199, loss = 0.30696428\n",
      "Iteration 200, loss = 0.30653407\n",
      "Iteration 1, loss = 0.70792320\n",
      "Iteration 2, loss = 0.49004319\n",
      "Iteration 3, loss = 0.44566063\n",
      "Iteration 4, loss = 0.44227696\n",
      "Iteration 5, loss = 0.44157085\n",
      "Iteration 6, loss = 0.44078474\n",
      "Iteration 7, loss = 0.43998309\n",
      "Iteration 8, loss = 0.43925104\n",
      "Iteration 9, loss = 0.43848377\n",
      "Iteration 10, loss = 0.43777981\n",
      "Iteration 11, loss = 0.43701496\n",
      "Iteration 12, loss = 0.43629936\n",
      "Iteration 13, loss = 0.43556204\n",
      "Iteration 14, loss = 0.43480714\n",
      "Iteration 15, loss = 0.43410090\n",
      "Iteration 16, loss = 0.43336660\n",
      "Iteration 17, loss = 0.43268116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.43196438\n",
      "Iteration 19, loss = 0.43121835\n",
      "Iteration 20, loss = 0.43049402\n",
      "Iteration 21, loss = 0.42976202\n",
      "Iteration 22, loss = 0.42902296\n",
      "Iteration 23, loss = 0.42830130\n",
      "Iteration 24, loss = 0.42761266\n",
      "Iteration 25, loss = 0.42686611\n",
      "Iteration 26, loss = 0.42615081\n",
      "Iteration 27, loss = 0.42545326\n",
      "Iteration 28, loss = 0.42473653\n",
      "Iteration 29, loss = 0.42396825\n",
      "Iteration 30, loss = 0.42327485\n",
      "Iteration 31, loss = 0.42253485\n",
      "Iteration 32, loss = 0.42180093\n",
      "Iteration 33, loss = 0.42111937\n",
      "Iteration 34, loss = 0.42035761\n",
      "Iteration 35, loss = 0.41966900\n",
      "Iteration 36, loss = 0.41891197\n",
      "Iteration 37, loss = 0.41819118\n",
      "Iteration 38, loss = 0.41744328\n",
      "Iteration 39, loss = 0.41671517\n",
      "Iteration 40, loss = 0.41598267\n",
      "Iteration 41, loss = 0.41527218\n",
      "Iteration 42, loss = 0.41452525\n",
      "Iteration 43, loss = 0.41377175\n",
      "Iteration 44, loss = 0.41307095\n",
      "Iteration 45, loss = 0.41230452\n",
      "Iteration 46, loss = 0.41161836\n",
      "Iteration 47, loss = 0.41083556\n",
      "Iteration 48, loss = 0.41006499\n",
      "Iteration 49, loss = 0.40937395\n",
      "Iteration 50, loss = 0.40858545\n",
      "Iteration 51, loss = 0.40780266\n",
      "Iteration 52, loss = 0.40710813\n",
      "Iteration 53, loss = 0.40628725\n",
      "Iteration 54, loss = 0.40551154\n",
      "Iteration 55, loss = 0.40475509\n",
      "Iteration 56, loss = 0.40400932\n",
      "Iteration 57, loss = 0.40323224\n",
      "Iteration 58, loss = 0.40245879\n",
      "Iteration 59, loss = 0.40169593\n",
      "Iteration 60, loss = 0.40093096\n",
      "Iteration 61, loss = 0.40014393\n",
      "Iteration 62, loss = 0.39938649\n",
      "Iteration 63, loss = 0.39857681\n",
      "Iteration 64, loss = 0.39781189\n",
      "Iteration 65, loss = 0.39701529\n",
      "Iteration 66, loss = 0.39622464\n",
      "Iteration 67, loss = 0.39543513\n",
      "Iteration 68, loss = 0.39464606\n",
      "Iteration 69, loss = 0.39385452\n",
      "Iteration 70, loss = 0.39308236\n",
      "Iteration 71, loss = 0.39225567\n",
      "Iteration 72, loss = 0.39148161\n",
      "Iteration 73, loss = 0.39066671\n",
      "Iteration 74, loss = 0.38984277\n",
      "Iteration 75, loss = 0.38905648\n",
      "Iteration 76, loss = 0.38824413\n",
      "Iteration 77, loss = 0.38744898\n",
      "Iteration 78, loss = 0.38662315\n",
      "Iteration 79, loss = 0.38585006\n",
      "Iteration 80, loss = 0.38501529\n",
      "Iteration 81, loss = 0.38419540\n",
      "Iteration 82, loss = 0.38336858\n",
      "Iteration 83, loss = 0.38255664\n",
      "Iteration 84, loss = 0.38175095\n",
      "Iteration 85, loss = 0.38093637\n",
      "Iteration 86, loss = 0.38015027\n",
      "Iteration 87, loss = 0.37929357\n",
      "Iteration 88, loss = 0.37848593\n",
      "Iteration 89, loss = 0.37765578\n",
      "Iteration 90, loss = 0.37688087\n",
      "Iteration 91, loss = 0.37602597\n",
      "Iteration 92, loss = 0.37519533\n",
      "Iteration 93, loss = 0.37440313\n",
      "Iteration 94, loss = 0.37357195\n",
      "Iteration 95, loss = 0.37272776\n",
      "Iteration 96, loss = 0.37190033\n",
      "Iteration 97, loss = 0.37112128\n",
      "Iteration 98, loss = 0.37031758\n",
      "Iteration 99, loss = 0.36944085\n",
      "Iteration 100, loss = 0.36863726\n",
      "Iteration 101, loss = 0.36780411\n",
      "Iteration 102, loss = 0.36699020\n",
      "Iteration 103, loss = 0.36618004\n",
      "Iteration 104, loss = 0.36535772\n",
      "Iteration 105, loss = 0.36454497\n",
      "Iteration 106, loss = 0.36372105\n",
      "Iteration 107, loss = 0.36291709\n",
      "Iteration 108, loss = 0.36210543\n",
      "Iteration 109, loss = 0.36130152\n",
      "Iteration 110, loss = 0.36053876\n",
      "Iteration 111, loss = 0.35968585\n",
      "Iteration 112, loss = 0.35889865\n",
      "Iteration 113, loss = 0.35809234\n",
      "Iteration 114, loss = 0.35729679\n",
      "Iteration 115, loss = 0.35649335\n",
      "Iteration 116, loss = 0.35569303\n",
      "Iteration 117, loss = 0.35491266\n",
      "Iteration 118, loss = 0.35413416\n",
      "Iteration 119, loss = 0.35334090\n",
      "Iteration 120, loss = 0.35256633\n",
      "Iteration 121, loss = 0.35183231\n",
      "Iteration 122, loss = 0.35097867\n",
      "Iteration 123, loss = 0.35023829\n",
      "Iteration 124, loss = 0.34944912\n",
      "Iteration 125, loss = 0.34870164\n",
      "Iteration 126, loss = 0.34791503\n",
      "Iteration 127, loss = 0.34715270\n",
      "Iteration 128, loss = 0.34641357\n",
      "Iteration 129, loss = 0.34565514\n",
      "Iteration 130, loss = 0.34490447\n",
      "Iteration 131, loss = 0.34417566\n",
      "Iteration 132, loss = 0.34342123\n",
      "Iteration 133, loss = 0.34267718\n",
      "Iteration 134, loss = 0.34195538\n",
      "Iteration 135, loss = 0.34123096\n",
      "Iteration 136, loss = 0.34049577\n",
      "Iteration 137, loss = 0.33979039\n",
      "Iteration 138, loss = 0.33908010\n",
      "Iteration 139, loss = 0.33835777\n",
      "Iteration 140, loss = 0.33764163\n",
      "Iteration 141, loss = 0.33693406\n",
      "Iteration 142, loss = 0.33625235\n",
      "Iteration 143, loss = 0.33555024\n",
      "Iteration 144, loss = 0.33485112\n",
      "Iteration 145, loss = 0.33417640\n",
      "Iteration 146, loss = 0.33351234\n",
      "Iteration 147, loss = 0.33284196\n",
      "Iteration 148, loss = 0.33217217\n",
      "Iteration 149, loss = 0.33149002\n",
      "Iteration 150, loss = 0.33083576\n",
      "Iteration 151, loss = 0.33017067\n",
      "Iteration 152, loss = 0.32953113\n",
      "Iteration 153, loss = 0.32889316\n",
      "Iteration 154, loss = 0.32822166\n",
      "Iteration 155, loss = 0.32759405\n",
      "Iteration 156, loss = 0.32699171\n",
      "Iteration 157, loss = 0.32635037\n",
      "Iteration 158, loss = 0.32575029\n",
      "Iteration 159, loss = 0.32510111\n",
      "Iteration 160, loss = 0.32449404\n",
      "Iteration 161, loss = 0.32391585\n",
      "Iteration 162, loss = 0.32329834\n",
      "Iteration 163, loss = 0.32270951\n",
      "Iteration 164, loss = 0.32211563\n",
      "Iteration 165, loss = 0.32153544\n",
      "Iteration 166, loss = 0.32096725\n",
      "Iteration 167, loss = 0.32040362\n",
      "Iteration 168, loss = 0.31980585\n",
      "Iteration 169, loss = 0.31927100\n",
      "Iteration 170, loss = 0.31869213\n",
      "Iteration 171, loss = 0.31813278\n",
      "Iteration 172, loss = 0.31758386\n",
      "Iteration 173, loss = 0.31704317\n",
      "Iteration 174, loss = 0.31650416\n",
      "Iteration 175, loss = 0.31596986\n",
      "Iteration 176, loss = 0.31544244\n",
      "Iteration 177, loss = 0.31492310\n",
      "Iteration 178, loss = 0.31443222\n",
      "Iteration 179, loss = 0.31387816\n",
      "Iteration 180, loss = 0.31340560\n",
      "Iteration 181, loss = 0.31289814\n",
      "Iteration 182, loss = 0.31246777\n",
      "Iteration 183, loss = 0.31189118\n",
      "Iteration 184, loss = 0.31142746\n",
      "Iteration 185, loss = 0.31090680\n",
      "Iteration 186, loss = 0.31045801\n",
      "Iteration 187, loss = 0.30996544\n",
      "Iteration 188, loss = 0.30951526\n",
      "Iteration 189, loss = 0.30903499\n",
      "Iteration 190, loss = 0.30858317\n",
      "Iteration 191, loss = 0.30810266\n",
      "Iteration 192, loss = 0.30766170\n",
      "Iteration 193, loss = 0.30722292\n",
      "Iteration 194, loss = 0.30676161\n",
      "Iteration 195, loss = 0.30634076\n",
      "Iteration 196, loss = 0.30589683\n",
      "Iteration 197, loss = 0.30547012\n",
      "Iteration 198, loss = 0.30505831\n",
      "Iteration 199, loss = 0.30462259\n",
      "Iteration 200, loss = 0.30421254\n",
      "Iteration 1, loss = 0.70782552\n",
      "Iteration 2, loss = 0.48933998\n",
      "Iteration 3, loss = 0.44485435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.44199818\n",
      "Iteration 5, loss = 0.44124079\n",
      "Iteration 6, loss = 0.44041811\n",
      "Iteration 7, loss = 0.43962721\n",
      "Iteration 8, loss = 0.43887081\n",
      "Iteration 9, loss = 0.43811179\n",
      "Iteration 10, loss = 0.43739480\n",
      "Iteration 11, loss = 0.43665954\n",
      "Iteration 12, loss = 0.43587453\n",
      "Iteration 13, loss = 0.43519886\n",
      "Iteration 14, loss = 0.43441848\n",
      "Iteration 15, loss = 0.43366969\n",
      "Iteration 16, loss = 0.43294152\n",
      "Iteration 17, loss = 0.43224149\n",
      "Iteration 18, loss = 0.43154489\n",
      "Iteration 19, loss = 0.43081115\n",
      "Iteration 20, loss = 0.43003784\n",
      "Iteration 21, loss = 0.42929855\n",
      "Iteration 22, loss = 0.42858726\n",
      "Iteration 23, loss = 0.42786489\n",
      "Iteration 24, loss = 0.42717778\n",
      "Iteration 25, loss = 0.42640512\n",
      "Iteration 26, loss = 0.42569442\n",
      "Iteration 27, loss = 0.42498836\n",
      "Iteration 28, loss = 0.42428294\n",
      "Iteration 29, loss = 0.42351208\n",
      "Iteration 30, loss = 0.42278417\n",
      "Iteration 31, loss = 0.42209988\n",
      "Iteration 32, loss = 0.42132362\n",
      "Iteration 33, loss = 0.42064316\n",
      "Iteration 34, loss = 0.41987128\n",
      "Iteration 35, loss = 0.41920639\n",
      "Iteration 36, loss = 0.41842735\n",
      "Iteration 37, loss = 0.41771201\n",
      "Iteration 38, loss = 0.41695844\n",
      "Iteration 39, loss = 0.41621722\n",
      "Iteration 40, loss = 0.41547933\n",
      "Iteration 41, loss = 0.41476300\n",
      "Iteration 42, loss = 0.41405142\n",
      "Iteration 43, loss = 0.41325973\n",
      "Iteration 44, loss = 0.41253282\n",
      "Iteration 45, loss = 0.41182462\n",
      "Iteration 46, loss = 0.41111687\n",
      "Iteration 47, loss = 0.41031844\n",
      "Iteration 48, loss = 0.40954794\n",
      "Iteration 49, loss = 0.40880717\n",
      "Iteration 50, loss = 0.40803530\n",
      "Iteration 51, loss = 0.40727505\n",
      "Iteration 52, loss = 0.40653608\n",
      "Iteration 53, loss = 0.40579026\n",
      "Iteration 54, loss = 0.40500005\n",
      "Iteration 55, loss = 0.40424884\n",
      "Iteration 56, loss = 0.40347347\n",
      "Iteration 57, loss = 0.40272410\n",
      "Iteration 58, loss = 0.40194307\n",
      "Iteration 59, loss = 0.40117789\n",
      "Iteration 60, loss = 0.40042204\n",
      "Iteration 61, loss = 0.39962498\n",
      "Iteration 62, loss = 0.39885691\n",
      "Iteration 63, loss = 0.39808037\n",
      "Iteration 64, loss = 0.39729948\n",
      "Iteration 65, loss = 0.39650730\n",
      "Iteration 66, loss = 0.39572262\n",
      "Iteration 67, loss = 0.39495060\n",
      "Iteration 68, loss = 0.39416096\n",
      "Iteration 69, loss = 0.39337250\n",
      "Iteration 70, loss = 0.39257350\n",
      "Iteration 71, loss = 0.39176970\n",
      "Iteration 72, loss = 0.39099046\n",
      "Iteration 73, loss = 0.39017604\n",
      "Iteration 74, loss = 0.38939171\n",
      "Iteration 75, loss = 0.38862326\n",
      "Iteration 76, loss = 0.38777766\n",
      "Iteration 77, loss = 0.38699020\n",
      "Iteration 78, loss = 0.38618076\n",
      "Iteration 79, loss = 0.38540498\n",
      "Iteration 80, loss = 0.38459678\n",
      "Iteration 81, loss = 0.38377750\n",
      "Iteration 82, loss = 0.38296756\n",
      "Iteration 83, loss = 0.38215854\n",
      "Iteration 84, loss = 0.38135314\n",
      "Iteration 85, loss = 0.38054581\n",
      "Iteration 86, loss = 0.37971623\n",
      "Iteration 87, loss = 0.37894357\n",
      "Iteration 88, loss = 0.37811903\n",
      "Iteration 89, loss = 0.37730007\n",
      "Iteration 90, loss = 0.37652658\n",
      "Iteration 91, loss = 0.37568542\n",
      "Iteration 92, loss = 0.37484734\n",
      "Iteration 93, loss = 0.37405357\n",
      "Iteration 94, loss = 0.37323990\n",
      "Iteration 95, loss = 0.37242907\n",
      "Iteration 96, loss = 0.37161921\n",
      "Iteration 97, loss = 0.37083932\n",
      "Iteration 98, loss = 0.37008054\n",
      "Iteration 99, loss = 0.36919640\n",
      "Iteration 100, loss = 0.36840497\n",
      "Iteration 101, loss = 0.36757927\n",
      "Iteration 102, loss = 0.36678407\n",
      "Iteration 103, loss = 0.36597017\n",
      "Iteration 104, loss = 0.36518088\n",
      "Iteration 105, loss = 0.36437024\n",
      "Iteration 106, loss = 0.36358522\n",
      "Iteration 107, loss = 0.36280521\n",
      "Iteration 108, loss = 0.36196514\n",
      "Iteration 109, loss = 0.36117847\n",
      "Iteration 110, loss = 0.36041638\n",
      "Iteration 111, loss = 0.35960143\n",
      "Iteration 112, loss = 0.35881564\n",
      "Iteration 113, loss = 0.35802132\n",
      "Iteration 114, loss = 0.35724741\n",
      "Iteration 115, loss = 0.35643764\n",
      "Iteration 116, loss = 0.35570347\n",
      "Iteration 117, loss = 0.35489226\n",
      "Iteration 118, loss = 0.35415168\n",
      "Iteration 119, loss = 0.35336945\n",
      "Iteration 120, loss = 0.35266868\n",
      "Iteration 121, loss = 0.35184208\n",
      "Iteration 122, loss = 0.35107164\n",
      "Iteration 123, loss = 0.35031679\n",
      "Iteration 124, loss = 0.34956036\n",
      "Iteration 125, loss = 0.34881593\n",
      "Iteration 126, loss = 0.34805521\n",
      "Iteration 127, loss = 0.34730814\n",
      "Iteration 128, loss = 0.34659391\n",
      "Iteration 129, loss = 0.34584060\n",
      "Iteration 130, loss = 0.34510917\n",
      "Iteration 131, loss = 0.34441218\n",
      "Iteration 132, loss = 0.34365081\n",
      "Iteration 133, loss = 0.34292857\n",
      "Iteration 134, loss = 0.34221261\n",
      "Iteration 135, loss = 0.34151431\n",
      "Iteration 136, loss = 0.34080162\n",
      "Iteration 137, loss = 0.34011560\n",
      "Iteration 138, loss = 0.33949953\n",
      "Iteration 139, loss = 0.33872418\n",
      "Iteration 140, loss = 0.33804543\n",
      "Iteration 141, loss = 0.33737227\n",
      "Iteration 142, loss = 0.33665331\n",
      "Iteration 143, loss = 0.33597867\n",
      "Iteration 144, loss = 0.33530214\n",
      "Iteration 145, loss = 0.33464925\n",
      "Iteration 146, loss = 0.33399040\n",
      "Iteration 147, loss = 0.33332261\n",
      "Iteration 148, loss = 0.33266972\n",
      "Iteration 149, loss = 0.33203501\n",
      "Iteration 150, loss = 0.33138516\n",
      "Iteration 151, loss = 0.33074460\n",
      "Iteration 152, loss = 0.33011132\n",
      "Iteration 153, loss = 0.32952771\n",
      "Iteration 154, loss = 0.32884330\n",
      "Iteration 155, loss = 0.32825352\n",
      "Iteration 156, loss = 0.32763286\n",
      "Iteration 157, loss = 0.32702776\n",
      "Iteration 158, loss = 0.32643282\n",
      "Iteration 159, loss = 0.32582119\n",
      "Iteration 160, loss = 0.32525442\n",
      "Iteration 161, loss = 0.32464609\n",
      "Iteration 162, loss = 0.32405787\n",
      "Iteration 163, loss = 0.32349133\n",
      "Iteration 164, loss = 0.32289573\n",
      "Iteration 165, loss = 0.32234585\n",
      "Iteration 166, loss = 0.32182735\n",
      "Iteration 167, loss = 0.32123710\n",
      "Iteration 168, loss = 0.32066301\n",
      "Iteration 169, loss = 0.32013862\n",
      "Iteration 170, loss = 0.31958432\n",
      "Iteration 171, loss = 0.31906128\n",
      "Iteration 172, loss = 0.31851581\n",
      "Iteration 173, loss = 0.31800394\n",
      "Iteration 174, loss = 0.31749002\n",
      "Iteration 175, loss = 0.31694321\n",
      "Iteration 176, loss = 0.31642982\n",
      "Iteration 177, loss = 0.31593924\n",
      "Iteration 178, loss = 0.31543146\n",
      "Iteration 179, loss = 0.31492794\n",
      "Iteration 180, loss = 0.31448288\n",
      "Iteration 181, loss = 0.31397576\n",
      "Iteration 182, loss = 0.31348798\n",
      "Iteration 183, loss = 0.31297907\n",
      "Iteration 184, loss = 0.31251605\n",
      "Iteration 185, loss = 0.31204445\n",
      "Iteration 186, loss = 0.31163984\n",
      "Iteration 187, loss = 0.31114673\n",
      "Iteration 188, loss = 0.31071029\n",
      "Iteration 189, loss = 0.31024720\n",
      "Iteration 190, loss = 0.30977304\n",
      "Iteration 191, loss = 0.30933935\n",
      "Iteration 192, loss = 0.30890104\n",
      "Iteration 193, loss = 0.30849537\n",
      "Iteration 194, loss = 0.30803742\n",
      "Iteration 195, loss = 0.30764789\n",
      "Iteration 196, loss = 0.30719246\n",
      "Iteration 197, loss = 0.30679352\n",
      "Iteration 198, loss = 0.30641103\n",
      "Iteration 199, loss = 0.30596090\n",
      "Iteration 200, loss = 0.30555675\n",
      "Iteration 1, loss = 0.70928992\n",
      "Iteration 2, loss = 0.48814853\n",
      "Iteration 3, loss = 0.44587314\n",
      "Iteration 4, loss = 0.44229369\n",
      "Iteration 5, loss = 0.44157319\n",
      "Iteration 6, loss = 0.44094109\n",
      "Iteration 7, loss = 0.44003295\n",
      "Iteration 8, loss = 0.43931151\n",
      "Iteration 9, loss = 0.43871837\n",
      "Iteration 10, loss = 0.43785023\n",
      "Iteration 11, loss = 0.43715376\n",
      "Iteration 12, loss = 0.43643176\n",
      "Iteration 13, loss = 0.43572918\n",
      "Iteration 14, loss = 0.43505466\n",
      "Iteration 15, loss = 0.43428715\n",
      "Iteration 16, loss = 0.43360630\n",
      "Iteration 17, loss = 0.43289121\n",
      "Iteration 18, loss = 0.43220344\n",
      "Iteration 19, loss = 0.43147213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.43075974\n",
      "Iteration 21, loss = 0.43005872\n",
      "Iteration 22, loss = 0.42938437\n",
      "Iteration 23, loss = 0.42866506\n",
      "Iteration 24, loss = 0.42794978\n",
      "Iteration 25, loss = 0.42726743\n",
      "Iteration 26, loss = 0.42655149\n",
      "Iteration 27, loss = 0.42586653\n",
      "Iteration 28, loss = 0.42516909\n",
      "Iteration 29, loss = 0.42445191\n",
      "Iteration 30, loss = 0.42373462\n",
      "Iteration 31, loss = 0.42308256\n",
      "Iteration 32, loss = 0.42238233\n",
      "Iteration 33, loss = 0.42168684\n",
      "Iteration 34, loss = 0.42092221\n",
      "Iteration 35, loss = 0.42023021\n",
      "Iteration 36, loss = 0.41953189\n",
      "Iteration 37, loss = 0.41882119\n",
      "Iteration 38, loss = 0.41815511\n",
      "Iteration 39, loss = 0.41741290\n",
      "Iteration 40, loss = 0.41671177\n",
      "Iteration 41, loss = 0.41597821\n",
      "Iteration 42, loss = 0.41527605\n",
      "Iteration 43, loss = 0.41454958\n",
      "Iteration 44, loss = 0.41382540\n",
      "Iteration 45, loss = 0.41308123\n",
      "Iteration 46, loss = 0.41238927\n",
      "Iteration 47, loss = 0.41179104\n",
      "Iteration 48, loss = 0.41096735\n",
      "Iteration 49, loss = 0.41023744\n",
      "Iteration 50, loss = 0.40945537\n",
      "Iteration 51, loss = 0.40872784\n",
      "Iteration 52, loss = 0.40802173\n",
      "Iteration 53, loss = 0.40724934\n",
      "Iteration 54, loss = 0.40652717\n",
      "Iteration 55, loss = 0.40577750\n",
      "Iteration 56, loss = 0.40506770\n",
      "Iteration 57, loss = 0.40427981\n",
      "Iteration 58, loss = 0.40353823\n",
      "Iteration 59, loss = 0.40280588\n",
      "Iteration 60, loss = 0.40202716\n",
      "Iteration 61, loss = 0.40134781\n",
      "Iteration 62, loss = 0.40053166\n",
      "Iteration 63, loss = 0.39979818\n",
      "Iteration 64, loss = 0.39902060\n",
      "Iteration 65, loss = 0.39833956\n",
      "Iteration 66, loss = 0.39747740\n",
      "Iteration 67, loss = 0.39673755\n",
      "Iteration 68, loss = 0.39597509\n",
      "Iteration 69, loss = 0.39518444\n",
      "Iteration 70, loss = 0.39446358\n",
      "Iteration 71, loss = 0.39362880\n",
      "Iteration 72, loss = 0.39288405\n",
      "Iteration 73, loss = 0.39209561\n",
      "Iteration 74, loss = 0.39133847\n",
      "Iteration 75, loss = 0.39054769\n",
      "Iteration 76, loss = 0.38974803\n",
      "Iteration 77, loss = 0.38896481\n",
      "Iteration 78, loss = 0.38818068\n",
      "Iteration 79, loss = 0.38738614\n",
      "Iteration 80, loss = 0.38659002\n",
      "Iteration 81, loss = 0.38580894\n",
      "Iteration 82, loss = 0.38508664\n",
      "Iteration 83, loss = 0.38424440\n",
      "Iteration 84, loss = 0.38343721\n",
      "Iteration 85, loss = 0.38265301\n",
      "Iteration 86, loss = 0.38186224\n",
      "Iteration 87, loss = 0.38107004\n",
      "Iteration 88, loss = 0.38026071\n",
      "Iteration 89, loss = 0.37945412\n",
      "Iteration 90, loss = 0.37864553\n",
      "Iteration 91, loss = 0.37788154\n",
      "Iteration 92, loss = 0.37705636\n",
      "Iteration 93, loss = 0.37624470\n",
      "Iteration 94, loss = 0.37548632\n",
      "Iteration 95, loss = 0.37464197\n",
      "Iteration 96, loss = 0.37385950\n",
      "Iteration 97, loss = 0.37310072\n",
      "Iteration 98, loss = 0.37225274\n",
      "Iteration 99, loss = 0.37146427\n",
      "Iteration 100, loss = 0.37065804\n",
      "Iteration 101, loss = 0.36987535\n",
      "Iteration 102, loss = 0.36904647\n",
      "Iteration 103, loss = 0.36829878\n",
      "Iteration 104, loss = 0.36745876\n",
      "Iteration 105, loss = 0.36665976\n",
      "Iteration 106, loss = 0.36588915\n",
      "Iteration 107, loss = 0.36506897\n",
      "Iteration 108, loss = 0.36429400\n",
      "Iteration 109, loss = 0.36348008\n",
      "Iteration 110, loss = 0.36270679\n",
      "Iteration 111, loss = 0.36191555\n",
      "Iteration 112, loss = 0.36112559\n",
      "Iteration 113, loss = 0.36033717\n",
      "Iteration 114, loss = 0.35960793\n",
      "Iteration 115, loss = 0.35883451\n",
      "Iteration 116, loss = 0.35799835\n",
      "Iteration 117, loss = 0.35722384\n",
      "Iteration 118, loss = 0.35645329\n",
      "Iteration 119, loss = 0.35567632\n",
      "Iteration 120, loss = 0.35490998\n",
      "Iteration 121, loss = 0.35414917\n",
      "Iteration 122, loss = 0.35338339\n",
      "Iteration 123, loss = 0.35263669\n",
      "Iteration 124, loss = 0.35186918\n",
      "Iteration 125, loss = 0.35110697\n",
      "Iteration 126, loss = 0.35035592\n",
      "Iteration 127, loss = 0.34961989\n",
      "Iteration 128, loss = 0.34886143\n",
      "Iteration 129, loss = 0.34810384\n",
      "Iteration 130, loss = 0.34741603\n",
      "Iteration 131, loss = 0.34666471\n",
      "Iteration 132, loss = 0.34592042\n",
      "Iteration 133, loss = 0.34521582\n",
      "Iteration 134, loss = 0.34448765\n",
      "Iteration 135, loss = 0.34377308\n",
      "Iteration 136, loss = 0.34301542\n",
      "Iteration 137, loss = 0.34233286\n",
      "Iteration 138, loss = 0.34159746\n",
      "Iteration 139, loss = 0.34094575\n",
      "Iteration 140, loss = 0.34022937\n",
      "Iteration 141, loss = 0.33951914\n",
      "Iteration 142, loss = 0.33881841\n",
      "Iteration 143, loss = 0.33816420\n",
      "Iteration 144, loss = 0.33749453\n",
      "Iteration 145, loss = 0.33677017\n",
      "Iteration 146, loss = 0.33611944\n",
      "Iteration 147, loss = 0.33542118\n",
      "Iteration 148, loss = 0.33478596\n",
      "Iteration 149, loss = 0.33410006\n",
      "Iteration 150, loss = 0.33346948\n",
      "Iteration 151, loss = 0.33285188\n",
      "Iteration 152, loss = 0.33214828\n",
      "Iteration 153, loss = 0.33153318\n",
      "Iteration 154, loss = 0.33087699\n",
      "Iteration 155, loss = 0.33025926\n",
      "Iteration 156, loss = 0.32961587\n",
      "Iteration 157, loss = 0.32900226\n",
      "Iteration 158, loss = 0.32838274\n",
      "Iteration 159, loss = 0.32780017\n",
      "Iteration 160, loss = 0.32715573\n",
      "Iteration 161, loss = 0.32655856\n",
      "Iteration 162, loss = 0.32600311\n",
      "Iteration 163, loss = 0.32535380\n",
      "Iteration 164, loss = 0.32476816\n",
      "Iteration 165, loss = 0.32422552\n",
      "Iteration 166, loss = 0.32367121\n",
      "Iteration 167, loss = 0.32305205\n",
      "Iteration 168, loss = 0.32248305\n",
      "Iteration 169, loss = 0.32190623\n",
      "Iteration 170, loss = 0.32136653\n",
      "Iteration 171, loss = 0.32080528\n",
      "Iteration 172, loss = 0.32025687\n",
      "Iteration 173, loss = 0.31974321\n",
      "Iteration 174, loss = 0.31918578\n",
      "Iteration 175, loss = 0.31865074\n",
      "Iteration 176, loss = 0.31812949\n",
      "Iteration 177, loss = 0.31758591\n",
      "Iteration 178, loss = 0.31705481\n",
      "Iteration 179, loss = 0.31654521\n",
      "Iteration 180, loss = 0.31604030\n",
      "Iteration 181, loss = 0.31560388\n",
      "Iteration 182, loss = 0.31504266\n",
      "Iteration 183, loss = 0.31453214\n",
      "Iteration 184, loss = 0.31409379\n",
      "Iteration 185, loss = 0.31356406\n",
      "Iteration 186, loss = 0.31308485\n",
      "Iteration 187, loss = 0.31261580\n",
      "Iteration 188, loss = 0.31217890\n",
      "Iteration 189, loss = 0.31165047\n",
      "Iteration 190, loss = 0.31132365\n",
      "Iteration 191, loss = 0.31081411\n",
      "Iteration 192, loss = 0.31034187\n",
      "Iteration 193, loss = 0.30985256\n",
      "Iteration 194, loss = 0.30940142\n",
      "Iteration 195, loss = 0.30895786\n",
      "Iteration 196, loss = 0.30852132\n",
      "Iteration 197, loss = 0.30809149\n",
      "Iteration 198, loss = 0.30767413\n",
      "Iteration 199, loss = 0.30724158\n",
      "Iteration 200, loss = 0.30687068\n",
      "Iteration 1, loss = 0.70951243\n",
      "Iteration 2, loss = 0.48863949\n",
      "Iteration 3, loss = 0.44541185\n",
      "Iteration 4, loss = 0.44225827\n",
      "Iteration 5, loss = 0.44169612\n",
      "Iteration 6, loss = 0.44106989\n",
      "Iteration 7, loss = 0.44020513\n",
      "Iteration 8, loss = 0.43946797\n",
      "Iteration 9, loss = 0.43880519\n",
      "Iteration 10, loss = 0.43800326\n",
      "Iteration 11, loss = 0.43731916\n",
      "Iteration 12, loss = 0.43656089\n",
      "Iteration 13, loss = 0.43589551\n",
      "Iteration 14, loss = 0.43516092\n",
      "Iteration 15, loss = 0.43441098\n",
      "Iteration 16, loss = 0.43372531\n",
      "Iteration 17, loss = 0.43301471\n",
      "Iteration 18, loss = 0.43230206\n",
      "Iteration 19, loss = 0.43159763\n",
      "Iteration 20, loss = 0.43089582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.43018302\n",
      "Iteration 22, loss = 0.42956293\n",
      "Iteration 23, loss = 0.42881451\n",
      "Iteration 24, loss = 0.42809063\n",
      "Iteration 25, loss = 0.42740329\n",
      "Iteration 26, loss = 0.42668036\n",
      "Iteration 27, loss = 0.42598272\n",
      "Iteration 28, loss = 0.42527863\n",
      "Iteration 29, loss = 0.42459002\n",
      "Iteration 30, loss = 0.42386207\n",
      "Iteration 31, loss = 0.42317716\n",
      "Iteration 32, loss = 0.42248434\n",
      "Iteration 33, loss = 0.42180196\n",
      "Iteration 34, loss = 0.42106009\n",
      "Iteration 35, loss = 0.42035742\n",
      "Iteration 36, loss = 0.41967573\n",
      "Iteration 37, loss = 0.41896222\n",
      "Iteration 38, loss = 0.41827855\n",
      "Iteration 39, loss = 0.41755311\n",
      "Iteration 40, loss = 0.41679486\n",
      "Iteration 41, loss = 0.41609801\n",
      "Iteration 42, loss = 0.41538004\n",
      "Iteration 43, loss = 0.41466958\n",
      "Iteration 44, loss = 0.41395592\n",
      "Iteration 45, loss = 0.41323067\n",
      "Iteration 46, loss = 0.41254700\n",
      "Iteration 47, loss = 0.41195293\n",
      "Iteration 48, loss = 0.41106922\n",
      "Iteration 49, loss = 0.41037030\n",
      "Iteration 50, loss = 0.40958359\n",
      "Iteration 51, loss = 0.40886119\n",
      "Iteration 52, loss = 0.40817217\n",
      "Iteration 53, loss = 0.40740015\n",
      "Iteration 54, loss = 0.40668128\n",
      "Iteration 55, loss = 0.40591561\n",
      "Iteration 56, loss = 0.40521351\n",
      "Iteration 57, loss = 0.40442696\n",
      "Iteration 58, loss = 0.40368960\n",
      "Iteration 59, loss = 0.40295454\n",
      "Iteration 60, loss = 0.40219861\n",
      "Iteration 61, loss = 0.40144074\n",
      "Iteration 62, loss = 0.40068120\n",
      "Iteration 63, loss = 0.39996778\n",
      "Iteration 64, loss = 0.39917998\n",
      "Iteration 65, loss = 0.39847525\n",
      "Iteration 66, loss = 0.39766895\n",
      "Iteration 67, loss = 0.39690513\n",
      "Iteration 68, loss = 0.39612845\n",
      "Iteration 69, loss = 0.39536076\n",
      "Iteration 70, loss = 0.39461186\n",
      "Iteration 71, loss = 0.39384871\n",
      "Iteration 72, loss = 0.39304806\n",
      "Iteration 73, loss = 0.39228875\n",
      "Iteration 74, loss = 0.39150510\n",
      "Iteration 75, loss = 0.39074436\n",
      "Iteration 76, loss = 0.38994071\n",
      "Iteration 77, loss = 0.38915733\n",
      "Iteration 78, loss = 0.38836962\n",
      "Iteration 79, loss = 0.38757144\n",
      "Iteration 80, loss = 0.38678682\n",
      "Iteration 81, loss = 0.38601665\n",
      "Iteration 82, loss = 0.38524311\n",
      "Iteration 83, loss = 0.38445995\n",
      "Iteration 84, loss = 0.38370358\n",
      "Iteration 85, loss = 0.38283567\n",
      "Iteration 86, loss = 0.38209098\n",
      "Iteration 87, loss = 0.38128373\n",
      "Iteration 88, loss = 0.38048433\n",
      "Iteration 89, loss = 0.37967632\n",
      "Iteration 90, loss = 0.37887183\n",
      "Iteration 91, loss = 0.37810806\n",
      "Iteration 92, loss = 0.37728349\n",
      "Iteration 93, loss = 0.37652670\n",
      "Iteration 94, loss = 0.37570834\n",
      "Iteration 95, loss = 0.37489975\n",
      "Iteration 96, loss = 0.37410240\n",
      "Iteration 97, loss = 0.37332496\n",
      "Iteration 98, loss = 0.37252153\n",
      "Iteration 99, loss = 0.37172518\n",
      "Iteration 100, loss = 0.37092913\n",
      "Iteration 101, loss = 0.37014112\n",
      "Iteration 102, loss = 0.36934133\n",
      "Iteration 103, loss = 0.36859670\n",
      "Iteration 104, loss = 0.36773702\n",
      "Iteration 105, loss = 0.36694826\n",
      "Iteration 106, loss = 0.36619331\n",
      "Iteration 107, loss = 0.36539688\n",
      "Iteration 108, loss = 0.36460304\n",
      "Iteration 109, loss = 0.36380354\n",
      "Iteration 110, loss = 0.36302500\n",
      "Iteration 111, loss = 0.36225328\n",
      "Iteration 112, loss = 0.36146750\n",
      "Iteration 113, loss = 0.36069272\n",
      "Iteration 114, loss = 0.35990797\n",
      "Iteration 115, loss = 0.35913066\n",
      "Iteration 116, loss = 0.35836425\n",
      "Iteration 117, loss = 0.35758171\n",
      "Iteration 118, loss = 0.35679751\n",
      "Iteration 119, loss = 0.35605113\n",
      "Iteration 120, loss = 0.35528121\n",
      "Iteration 121, loss = 0.35452849\n",
      "Iteration 122, loss = 0.35377079\n",
      "Iteration 123, loss = 0.35301352\n",
      "Iteration 124, loss = 0.35226430\n",
      "Iteration 125, loss = 0.35154260\n",
      "Iteration 126, loss = 0.35081989\n",
      "Iteration 127, loss = 0.35003185\n",
      "Iteration 128, loss = 0.34927163\n",
      "Iteration 129, loss = 0.34853138\n",
      "Iteration 130, loss = 0.34784097\n",
      "Iteration 131, loss = 0.34707308\n",
      "Iteration 132, loss = 0.34638501\n",
      "Iteration 133, loss = 0.34563040\n",
      "Iteration 134, loss = 0.34492427\n",
      "Iteration 135, loss = 0.34420300\n",
      "Iteration 136, loss = 0.34350023\n",
      "Iteration 137, loss = 0.34278915\n",
      "Iteration 138, loss = 0.34207926\n",
      "Iteration 139, loss = 0.34139406\n",
      "Iteration 140, loss = 0.34070011\n",
      "Iteration 141, loss = 0.34000271\n",
      "Iteration 142, loss = 0.33934755\n",
      "Iteration 143, loss = 0.33865476\n",
      "Iteration 144, loss = 0.33797786\n",
      "Iteration 145, loss = 0.33728804\n",
      "Iteration 146, loss = 0.33661727\n",
      "Iteration 147, loss = 0.33594484\n",
      "Iteration 148, loss = 0.33531879\n",
      "Iteration 149, loss = 0.33463917\n",
      "Iteration 150, loss = 0.33401452\n",
      "Iteration 151, loss = 0.33345953\n",
      "Iteration 152, loss = 0.33269564\n",
      "Iteration 153, loss = 0.33209345\n",
      "Iteration 154, loss = 0.33146507\n",
      "Iteration 155, loss = 0.33081551\n",
      "Iteration 156, loss = 0.33019516\n",
      "Iteration 157, loss = 0.32958229\n",
      "Iteration 158, loss = 0.32898209\n",
      "Iteration 159, loss = 0.32837399\n",
      "Iteration 160, loss = 0.32776066\n",
      "Iteration 161, loss = 0.32719575\n",
      "Iteration 162, loss = 0.32661394\n",
      "Iteration 163, loss = 0.32597095\n",
      "Iteration 164, loss = 0.32540609\n",
      "Iteration 165, loss = 0.32485289\n",
      "Iteration 166, loss = 0.32425601\n",
      "Iteration 167, loss = 0.32369598\n",
      "Iteration 168, loss = 0.32314370\n",
      "Iteration 169, loss = 0.32257200\n",
      "Iteration 170, loss = 0.32203303\n",
      "Iteration 171, loss = 0.32151798\n",
      "Iteration 172, loss = 0.32093871\n",
      "Iteration 173, loss = 0.32044892\n",
      "Iteration 174, loss = 0.31994051\n",
      "Iteration 175, loss = 0.31936974\n",
      "Iteration 176, loss = 0.31881147\n",
      "Iteration 177, loss = 0.31834573\n",
      "Iteration 178, loss = 0.31778515\n",
      "Iteration 179, loss = 0.31727892\n",
      "Iteration 180, loss = 0.31678900\n",
      "Iteration 181, loss = 0.31630501\n",
      "Iteration 182, loss = 0.31579220\n",
      "Iteration 183, loss = 0.31529401\n",
      "Iteration 184, loss = 0.31487520\n",
      "Iteration 185, loss = 0.31435005\n",
      "Iteration 186, loss = 0.31386615\n",
      "Iteration 187, loss = 0.31340391\n",
      "Iteration 188, loss = 0.31292846\n",
      "Iteration 189, loss = 0.31246016\n",
      "Iteration 190, loss = 0.31209946\n",
      "Iteration 191, loss = 0.31162657\n",
      "Iteration 192, loss = 0.31114665\n",
      "Iteration 193, loss = 0.31068869\n",
      "Iteration 194, loss = 0.31025065\n",
      "Iteration 195, loss = 0.30979352\n",
      "Iteration 196, loss = 0.30936664\n",
      "Iteration 197, loss = 0.30894942\n",
      "Iteration 198, loss = 0.30852844\n",
      "Iteration 199, loss = 0.30811539\n",
      "Iteration 200, loss = 0.30772157\n",
      "Iteration 1, loss = 0.68975465\n",
      "Iteration 2, loss = 0.48836598\n",
      "Iteration 3, loss = 0.41557887\n",
      "Iteration 4, loss = 0.39063160\n",
      "Iteration 5, loss = 0.37608027\n",
      "Iteration 6, loss = 0.36390155\n",
      "Iteration 7, loss = 0.35329455\n",
      "Iteration 8, loss = 0.34394750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.33551455\n",
      "Iteration 10, loss = 0.32807508\n",
      "Iteration 11, loss = 0.32152240\n",
      "Iteration 12, loss = 0.31564259\n",
      "Iteration 13, loss = 0.31053298\n",
      "Iteration 14, loss = 0.30593462\n",
      "Iteration 15, loss = 0.30162462\n",
      "Iteration 16, loss = 0.29807238\n",
      "Iteration 17, loss = 0.29466139\n",
      "Iteration 18, loss = 0.29174924\n",
      "Iteration 19, loss = 0.28895254\n",
      "Iteration 20, loss = 0.28656546\n",
      "Iteration 21, loss = 0.28425093\n",
      "Iteration 22, loss = 0.28227695\n",
      "Iteration 23, loss = 0.28021849\n",
      "Iteration 24, loss = 0.27832114\n",
      "Iteration 25, loss = 0.27668710\n",
      "Iteration 26, loss = 0.27510271\n",
      "Iteration 27, loss = 0.27356097\n",
      "Iteration 28, loss = 0.27209061\n",
      "Iteration 29, loss = 0.27064618\n",
      "Iteration 30, loss = 0.26929982\n",
      "Iteration 31, loss = 0.26797408\n",
      "Iteration 32, loss = 0.26675318\n",
      "Iteration 33, loss = 0.26557292\n",
      "Iteration 34, loss = 0.26442489\n",
      "Iteration 35, loss = 0.26339552\n",
      "Iteration 36, loss = 0.26237654\n",
      "Iteration 37, loss = 0.26090898\n",
      "Iteration 38, loss = 0.25991750\n",
      "Iteration 39, loss = 0.25882663\n",
      "Iteration 40, loss = 0.25791242\n",
      "Iteration 41, loss = 0.25681358\n",
      "Iteration 42, loss = 0.25606194\n",
      "Iteration 43, loss = 0.25480365\n",
      "Iteration 44, loss = 0.25394671\n",
      "Iteration 45, loss = 0.25311682\n",
      "Iteration 46, loss = 0.25209171\n",
      "Iteration 47, loss = 0.25150928\n",
      "Iteration 48, loss = 0.25040373\n",
      "Iteration 49, loss = 0.24957675\n",
      "Iteration 50, loss = 0.24862666\n",
      "Iteration 51, loss = 0.24796307\n",
      "Iteration 52, loss = 0.24736855\n",
      "Iteration 53, loss = 0.24641172\n",
      "Iteration 54, loss = 0.24568414\n",
      "Iteration 55, loss = 0.24505062\n",
      "Iteration 56, loss = 0.24433218\n",
      "Iteration 57, loss = 0.24390348\n",
      "Iteration 58, loss = 0.24319989\n",
      "Iteration 59, loss = 0.24251306\n",
      "Iteration 60, loss = 0.24210421\n",
      "Iteration 61, loss = 0.24146451\n",
      "Iteration 62, loss = 0.24100609\n",
      "Iteration 63, loss = 0.24066626\n",
      "Iteration 64, loss = 0.24005230\n",
      "Iteration 65, loss = 0.23955356\n",
      "Iteration 66, loss = 0.23934324\n",
      "Iteration 67, loss = 0.23866275\n",
      "Iteration 68, loss = 0.23824765\n",
      "Iteration 69, loss = 0.23815472\n",
      "Iteration 70, loss = 0.23753175\n",
      "Iteration 71, loss = 0.23748687\n",
      "Iteration 72, loss = 0.23695954\n",
      "Iteration 73, loss = 0.23658943\n",
      "Iteration 74, loss = 0.23619916\n",
      "Iteration 75, loss = 0.23578577\n",
      "Iteration 76, loss = 0.23568875\n",
      "Iteration 77, loss = 0.23556755\n",
      "Iteration 78, loss = 0.23512379\n",
      "Iteration 79, loss = 0.23482117\n",
      "Iteration 80, loss = 0.23450324\n",
      "Iteration 81, loss = 0.23423730\n",
      "Iteration 82, loss = 0.23431093\n",
      "Iteration 83, loss = 0.23346529\n",
      "Iteration 84, loss = 0.23341917\n",
      "Iteration 85, loss = 0.23319165\n",
      "Iteration 86, loss = 0.23318066\n",
      "Iteration 87, loss = 0.23259621\n",
      "Iteration 88, loss = 0.23245502\n",
      "Iteration 89, loss = 0.23197886\n",
      "Iteration 90, loss = 0.23196561\n",
      "Iteration 91, loss = 0.23149631\n",
      "Iteration 92, loss = 0.23146108\n",
      "Iteration 93, loss = 0.23098058\n",
      "Iteration 94, loss = 0.23071351\n",
      "Iteration 95, loss = 0.23033918\n",
      "Iteration 96, loss = 0.23012863\n",
      "Iteration 97, loss = 0.22983073\n",
      "Iteration 98, loss = 0.22932986\n",
      "Iteration 99, loss = 0.22904558\n",
      "Iteration 100, loss = 0.22899693\n",
      "Iteration 101, loss = 0.22863858\n",
      "Iteration 102, loss = 0.22805604\n",
      "Iteration 103, loss = 0.22781439\n",
      "Iteration 104, loss = 0.22761901\n",
      "Iteration 105, loss = 0.22711450\n",
      "Iteration 106, loss = 0.22680055\n",
      "Iteration 107, loss = 0.22664140\n",
      "Iteration 108, loss = 0.22609673\n",
      "Iteration 109, loss = 0.22563714\n",
      "Iteration 110, loss = 0.22560886\n",
      "Iteration 111, loss = 0.22494351\n",
      "Iteration 112, loss = 0.22455258\n",
      "Iteration 113, loss = 0.22422029\n",
      "Iteration 114, loss = 0.22366702\n",
      "Iteration 115, loss = 0.22350547\n",
      "Iteration 116, loss = 0.22304208\n",
      "Iteration 117, loss = 0.22266332\n",
      "Iteration 118, loss = 0.22198823\n",
      "Iteration 119, loss = 0.22176599\n",
      "Iteration 120, loss = 0.22126434\n",
      "Iteration 121, loss = 0.22082565\n",
      "Iteration 122, loss = 0.22041719\n",
      "Iteration 123, loss = 0.21979251\n",
      "Iteration 124, loss = 0.21943033\n",
      "Iteration 125, loss = 0.21908481\n",
      "Iteration 126, loss = 0.21858113\n",
      "Iteration 127, loss = 0.21800698\n",
      "Iteration 128, loss = 0.21750389\n",
      "Iteration 129, loss = 0.21717246\n",
      "Iteration 130, loss = 0.21657470\n",
      "Iteration 131, loss = 0.21624621\n",
      "Iteration 132, loss = 0.21581850\n",
      "Iteration 133, loss = 0.21511735\n",
      "Iteration 134, loss = 0.21473162\n",
      "Iteration 135, loss = 0.21413633\n",
      "Iteration 136, loss = 0.21428339\n",
      "Iteration 137, loss = 0.21310858\n",
      "Iteration 138, loss = 0.21255978\n",
      "Iteration 139, loss = 0.21247246\n",
      "Iteration 140, loss = 0.21152839\n",
      "Iteration 141, loss = 0.21099415\n",
      "Iteration 142, loss = 0.21066143\n",
      "Iteration 143, loss = 0.21006147\n",
      "Iteration 144, loss = 0.20938199\n",
      "Iteration 145, loss = 0.20889006\n",
      "Iteration 146, loss = 0.20838016\n",
      "Iteration 147, loss = 0.20792375\n",
      "Iteration 148, loss = 0.20739714\n",
      "Iteration 149, loss = 0.20667757\n",
      "Iteration 150, loss = 0.20618974\n",
      "Iteration 151, loss = 0.20569071\n",
      "Iteration 152, loss = 0.20509354\n",
      "Iteration 153, loss = 0.20462683\n",
      "Iteration 154, loss = 0.20384395\n",
      "Iteration 155, loss = 0.20347589\n",
      "Iteration 156, loss = 0.20278303\n",
      "Iteration 157, loss = 0.20226716\n",
      "Iteration 158, loss = 0.20178061\n",
      "Iteration 159, loss = 0.20131401\n",
      "Iteration 160, loss = 0.20081696\n",
      "Iteration 161, loss = 0.19997733\n",
      "Iteration 162, loss = 0.19945655\n",
      "Iteration 163, loss = 0.19869112\n",
      "Iteration 164, loss = 0.19818151\n",
      "Iteration 165, loss = 0.19771607\n",
      "Iteration 166, loss = 0.19716424\n",
      "Iteration 167, loss = 0.19684431\n",
      "Iteration 168, loss = 0.19585485\n",
      "Iteration 169, loss = 0.19553956\n",
      "Iteration 170, loss = 0.19481645\n",
      "Iteration 171, loss = 0.19418210\n",
      "Iteration 172, loss = 0.19350556\n",
      "Iteration 173, loss = 0.19292553\n",
      "Iteration 174, loss = 0.19249483\n",
      "Iteration 175, loss = 0.19168049\n",
      "Iteration 176, loss = 0.19129733\n",
      "Iteration 177, loss = 0.19079825\n",
      "Iteration 178, loss = 0.19016816\n",
      "Iteration 179, loss = 0.18974191\n",
      "Iteration 180, loss = 0.18894085\n",
      "Iteration 181, loss = 0.18818000\n",
      "Iteration 182, loss = 0.18791547\n",
      "Iteration 183, loss = 0.18712817\n",
      "Iteration 184, loss = 0.18666261\n",
      "Iteration 185, loss = 0.18611759\n",
      "Iteration 186, loss = 0.18547436\n",
      "Iteration 187, loss = 0.18481853\n",
      "Iteration 188, loss = 0.18423536\n",
      "Iteration 189, loss = 0.18375272\n",
      "Iteration 190, loss = 0.18319573\n",
      "Iteration 191, loss = 0.18228787\n",
      "Iteration 192, loss = 0.18197195\n",
      "Iteration 193, loss = 0.18126951\n",
      "Iteration 194, loss = 0.18090411\n",
      "Iteration 195, loss = 0.18022092\n",
      "Iteration 196, loss = 0.17968414\n",
      "Iteration 197, loss = 0.17918094\n",
      "Iteration 198, loss = 0.17867037\n",
      "Iteration 199, loss = 0.17815868\n",
      "Iteration 200, loss = 0.17739607\n",
      "Iteration 1, loss = 0.68947204\n",
      "Iteration 2, loss = 0.48862988\n",
      "Iteration 3, loss = 0.41525731\n",
      "Iteration 4, loss = 0.39028092\n",
      "Iteration 5, loss = 0.37534252\n",
      "Iteration 6, loss = 0.36293483\n",
      "Iteration 7, loss = 0.35237258\n",
      "Iteration 8, loss = 0.34296606\n",
      "Iteration 9, loss = 0.33451955\n",
      "Iteration 10, loss = 0.32725328\n",
      "Iteration 11, loss = 0.32059690\n",
      "Iteration 12, loss = 0.31482047\n",
      "Iteration 13, loss = 0.30967880\n",
      "Iteration 14, loss = 0.30502520\n",
      "Iteration 15, loss = 0.30079610\n",
      "Iteration 16, loss = 0.29715346\n",
      "Iteration 17, loss = 0.29390329\n",
      "Iteration 18, loss = 0.29081878\n",
      "Iteration 19, loss = 0.28812573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.28550483\n",
      "Iteration 21, loss = 0.28328897\n",
      "Iteration 22, loss = 0.28118309\n",
      "Iteration 23, loss = 0.27919679\n",
      "Iteration 24, loss = 0.27734224\n",
      "Iteration 25, loss = 0.27561236\n",
      "Iteration 26, loss = 0.27408949\n",
      "Iteration 27, loss = 0.27251475\n",
      "Iteration 28, loss = 0.27110130\n",
      "Iteration 29, loss = 0.26965506\n",
      "Iteration 30, loss = 0.26832066\n",
      "Iteration 31, loss = 0.26698603\n",
      "Iteration 32, loss = 0.26582727\n",
      "Iteration 33, loss = 0.26472430\n",
      "Iteration 34, loss = 0.26356610\n",
      "Iteration 35, loss = 0.26249198\n",
      "Iteration 36, loss = 0.26124123\n",
      "Iteration 37, loss = 0.26013638\n",
      "Iteration 38, loss = 0.25905868\n",
      "Iteration 39, loss = 0.25805276\n",
      "Iteration 40, loss = 0.25713282\n",
      "Iteration 41, loss = 0.25610381\n",
      "Iteration 42, loss = 0.25514737\n",
      "Iteration 43, loss = 0.25414268\n",
      "Iteration 44, loss = 0.25324654\n",
      "Iteration 45, loss = 0.25238451\n",
      "Iteration 46, loss = 0.25167560\n",
      "Iteration 47, loss = 0.25080734\n",
      "Iteration 48, loss = 0.24985737\n",
      "Iteration 49, loss = 0.24901675\n",
      "Iteration 50, loss = 0.24804049\n",
      "Iteration 51, loss = 0.24732584\n",
      "Iteration 52, loss = 0.24673521\n",
      "Iteration 53, loss = 0.24576801\n",
      "Iteration 54, loss = 0.24517434\n",
      "Iteration 55, loss = 0.24450812\n",
      "Iteration 56, loss = 0.24373022\n",
      "Iteration 57, loss = 0.24325047\n",
      "Iteration 58, loss = 0.24278900\n",
      "Iteration 59, loss = 0.24199068\n",
      "Iteration 60, loss = 0.24152680\n",
      "Iteration 61, loss = 0.24093628\n",
      "Iteration 62, loss = 0.24048933\n",
      "Iteration 63, loss = 0.23991427\n",
      "Iteration 64, loss = 0.23943239\n",
      "Iteration 65, loss = 0.23886300\n",
      "Iteration 66, loss = 0.23867928\n",
      "Iteration 67, loss = 0.23803284\n",
      "Iteration 68, loss = 0.23773727\n",
      "Iteration 69, loss = 0.23734482\n",
      "Iteration 70, loss = 0.23695230\n",
      "Iteration 71, loss = 0.23672622\n",
      "Iteration 72, loss = 0.23647003\n",
      "Iteration 73, loss = 0.23595173\n",
      "Iteration 74, loss = 0.23565680\n",
      "Iteration 75, loss = 0.23547128\n",
      "Iteration 76, loss = 0.23517205\n",
      "Iteration 77, loss = 0.23500315\n",
      "Iteration 78, loss = 0.23476598\n",
      "Iteration 79, loss = 0.23469427\n",
      "Iteration 80, loss = 0.23426250\n",
      "Iteration 81, loss = 0.23384555\n",
      "Iteration 82, loss = 0.23392544\n",
      "Iteration 83, loss = 0.23345288\n",
      "Iteration 84, loss = 0.23326055\n",
      "Iteration 85, loss = 0.23312968\n",
      "Iteration 86, loss = 0.23299448\n",
      "Iteration 87, loss = 0.23272646\n",
      "Iteration 88, loss = 0.23258382\n",
      "Iteration 89, loss = 0.23226061\n",
      "Iteration 90, loss = 0.23238758\n",
      "Iteration 91, loss = 0.23190887\n",
      "Iteration 92, loss = 0.23159959\n",
      "Iteration 93, loss = 0.23139012\n",
      "Iteration 94, loss = 0.23120856\n",
      "Iteration 95, loss = 0.23098586\n",
      "Iteration 96, loss = 0.23066610\n",
      "Iteration 97, loss = 0.23077314\n",
      "Iteration 98, loss = 0.23047699\n",
      "Iteration 99, loss = 0.23005495\n",
      "Iteration 100, loss = 0.23004733\n",
      "Iteration 101, loss = 0.22962817\n",
      "Iteration 102, loss = 0.22930341\n",
      "Iteration 103, loss = 0.22918897\n",
      "Iteration 104, loss = 0.22893671\n",
      "Iteration 105, loss = 0.22878946\n",
      "Iteration 106, loss = 0.22851671\n",
      "Iteration 107, loss = 0.22841024\n",
      "Iteration 108, loss = 0.22798020\n",
      "Iteration 109, loss = 0.22766249\n",
      "Iteration 110, loss = 0.22748241\n",
      "Iteration 111, loss = 0.22708495\n",
      "Iteration 112, loss = 0.22675171\n",
      "Iteration 113, loss = 0.22661665\n",
      "Iteration 114, loss = 0.22626156\n",
      "Iteration 115, loss = 0.22606437\n",
      "Iteration 116, loss = 0.22562192\n",
      "Iteration 117, loss = 0.22529888\n",
      "Iteration 118, loss = 0.22486574\n",
      "Iteration 119, loss = 0.22464423\n",
      "Iteration 120, loss = 0.22459233\n",
      "Iteration 121, loss = 0.22383203\n",
      "Iteration 122, loss = 0.22372508\n",
      "Iteration 123, loss = 0.22320280\n",
      "Iteration 124, loss = 0.22300120\n",
      "Iteration 125, loss = 0.22286004\n",
      "Iteration 126, loss = 0.22214318\n",
      "Iteration 127, loss = 0.22186218\n",
      "Iteration 128, loss = 0.22132724\n",
      "Iteration 129, loss = 0.22112566\n",
      "Iteration 130, loss = 0.22065871\n",
      "Iteration 131, loss = 0.22055277\n",
      "Iteration 132, loss = 0.21996067\n",
      "Iteration 133, loss = 0.21964932\n",
      "Iteration 134, loss = 0.21922160\n",
      "Iteration 135, loss = 0.21874121\n",
      "Iteration 136, loss = 0.21824277\n",
      "Iteration 137, loss = 0.21816661\n",
      "Iteration 138, loss = 0.21748049\n",
      "Iteration 139, loss = 0.21719788\n",
      "Iteration 140, loss = 0.21637497\n",
      "Iteration 141, loss = 0.21625618\n",
      "Iteration 142, loss = 0.21588576\n",
      "Iteration 143, loss = 0.21552528\n",
      "Iteration 144, loss = 0.21488559\n",
      "Iteration 145, loss = 0.21451015\n",
      "Iteration 146, loss = 0.21401680\n",
      "Iteration 147, loss = 0.21356081\n",
      "Iteration 148, loss = 0.21331372\n",
      "Iteration 149, loss = 0.21264869\n",
      "Iteration 150, loss = 0.21229449\n",
      "Iteration 151, loss = 0.21161365\n",
      "Iteration 152, loss = 0.21123809\n",
      "Iteration 153, loss = 0.21072557\n",
      "Iteration 154, loss = 0.21007134\n",
      "Iteration 155, loss = 0.20968805\n",
      "Iteration 156, loss = 0.20913456\n",
      "Iteration 157, loss = 0.20854963\n",
      "Iteration 158, loss = 0.20836538\n",
      "Iteration 159, loss = 0.20778990\n",
      "Iteration 160, loss = 0.20735527\n",
      "Iteration 161, loss = 0.20666159\n",
      "Iteration 162, loss = 0.20619621\n",
      "Iteration 163, loss = 0.20575365\n",
      "Iteration 164, loss = 0.20517396\n",
      "Iteration 165, loss = 0.20447923\n",
      "Iteration 166, loss = 0.20403583\n",
      "Iteration 167, loss = 0.20364205\n",
      "Iteration 168, loss = 0.20303492\n",
      "Iteration 169, loss = 0.20275778\n",
      "Iteration 170, loss = 0.20209348\n",
      "Iteration 171, loss = 0.20150869\n",
      "Iteration 172, loss = 0.20094215\n",
      "Iteration 173, loss = 0.20036873\n",
      "Iteration 174, loss = 0.19986879\n",
      "Iteration 175, loss = 0.19924363\n",
      "Iteration 176, loss = 0.19873878\n",
      "Iteration 177, loss = 0.19824938\n",
      "Iteration 178, loss = 0.19768177\n",
      "Iteration 179, loss = 0.19732446\n",
      "Iteration 180, loss = 0.19693849\n",
      "Iteration 181, loss = 0.19612455\n",
      "Iteration 182, loss = 0.19585628\n",
      "Iteration 183, loss = 0.19513806\n",
      "Iteration 184, loss = 0.19454585\n",
      "Iteration 185, loss = 0.19399807\n",
      "Iteration 186, loss = 0.19346659\n",
      "Iteration 187, loss = 0.19285513\n",
      "Iteration 188, loss = 0.19223795\n",
      "Iteration 189, loss = 0.19166651\n",
      "Iteration 190, loss = 0.19133009\n",
      "Iteration 191, loss = 0.19050495\n",
      "Iteration 192, loss = 0.18995706\n",
      "Iteration 193, loss = 0.18949654\n",
      "Iteration 194, loss = 0.18887898\n",
      "Iteration 195, loss = 0.18840418\n",
      "Iteration 196, loss = 0.18781685\n",
      "Iteration 197, loss = 0.18736913\n",
      "Iteration 198, loss = 0.18677583\n",
      "Iteration 199, loss = 0.18621008\n",
      "Iteration 200, loss = 0.18549447\n",
      "Iteration 1, loss = 0.68840086\n",
      "Iteration 2, loss = 0.48756835\n",
      "Iteration 3, loss = 0.41426843\n",
      "Iteration 4, loss = 0.39000071\n",
      "Iteration 5, loss = 0.37509824\n",
      "Iteration 6, loss = 0.36278993\n",
      "Iteration 7, loss = 0.35220539\n",
      "Iteration 8, loss = 0.34285013\n",
      "Iteration 9, loss = 0.33435979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.32717173\n",
      "Iteration 11, loss = 0.32066066\n",
      "Iteration 12, loss = 0.31476750\n",
      "Iteration 13, loss = 0.30997194\n",
      "Iteration 14, loss = 0.30532555\n",
      "Iteration 15, loss = 0.30116122\n",
      "Iteration 16, loss = 0.29763672\n",
      "Iteration 17, loss = 0.29458842\n",
      "Iteration 18, loss = 0.29166445\n",
      "Iteration 19, loss = 0.28909337\n",
      "Iteration 20, loss = 0.28658948\n",
      "Iteration 21, loss = 0.28453136\n",
      "Iteration 22, loss = 0.28270433\n",
      "Iteration 23, loss = 0.28072895\n",
      "Iteration 24, loss = 0.27904593\n",
      "Iteration 25, loss = 0.27741068\n",
      "Iteration 26, loss = 0.27598263\n",
      "Iteration 27, loss = 0.27457745\n",
      "Iteration 28, loss = 0.27328161\n",
      "Iteration 29, loss = 0.27201141\n",
      "Iteration 30, loss = 0.27074682\n",
      "Iteration 31, loss = 0.26962774\n",
      "Iteration 32, loss = 0.26859217\n",
      "Iteration 33, loss = 0.26750748\n",
      "Iteration 34, loss = 0.26642492\n",
      "Iteration 35, loss = 0.26542752\n",
      "Iteration 36, loss = 0.26438269\n",
      "Iteration 37, loss = 0.26341793\n",
      "Iteration 38, loss = 0.26244930\n",
      "Iteration 39, loss = 0.26145011\n",
      "Iteration 40, loss = 0.26055875\n",
      "Iteration 41, loss = 0.25977581\n",
      "Iteration 42, loss = 0.25890945\n",
      "Iteration 43, loss = 0.25785737\n",
      "Iteration 44, loss = 0.25713349\n",
      "Iteration 45, loss = 0.25629149\n",
      "Iteration 46, loss = 0.25566991\n",
      "Iteration 47, loss = 0.25474050\n",
      "Iteration 48, loss = 0.25401122\n",
      "Iteration 49, loss = 0.25318785\n",
      "Iteration 50, loss = 0.25249690\n",
      "Iteration 51, loss = 0.25174424\n",
      "Iteration 52, loss = 0.25104028\n",
      "Iteration 53, loss = 0.25038311\n",
      "Iteration 54, loss = 0.24985755\n",
      "Iteration 55, loss = 0.24905195\n",
      "Iteration 56, loss = 0.24835431\n",
      "Iteration 57, loss = 0.24803845\n",
      "Iteration 58, loss = 0.24743149\n",
      "Iteration 59, loss = 0.24681835\n",
      "Iteration 60, loss = 0.24639348\n",
      "Iteration 61, loss = 0.24580877\n",
      "Iteration 62, loss = 0.24535530\n",
      "Iteration 63, loss = 0.24481487\n",
      "Iteration 64, loss = 0.24431575\n",
      "Iteration 65, loss = 0.24392088\n",
      "Iteration 66, loss = 0.24375312\n",
      "Iteration 67, loss = 0.24313248\n",
      "Iteration 68, loss = 0.24280780\n",
      "Iteration 69, loss = 0.24235207\n",
      "Iteration 70, loss = 0.24195108\n",
      "Iteration 71, loss = 0.24179309\n",
      "Iteration 72, loss = 0.24141354\n",
      "Iteration 73, loss = 0.24100362\n",
      "Iteration 74, loss = 0.24081936\n",
      "Iteration 75, loss = 0.24061360\n",
      "Iteration 76, loss = 0.24008699\n",
      "Iteration 77, loss = 0.23992080\n",
      "Iteration 78, loss = 0.23958720\n",
      "Iteration 79, loss = 0.23936734\n",
      "Iteration 80, loss = 0.23916517\n",
      "Iteration 81, loss = 0.23876016\n",
      "Iteration 82, loss = 0.23872918\n",
      "Iteration 83, loss = 0.23834621\n",
      "Iteration 84, loss = 0.23795327\n",
      "Iteration 85, loss = 0.23793210\n",
      "Iteration 86, loss = 0.23760893\n",
      "Iteration 87, loss = 0.23753218\n",
      "Iteration 88, loss = 0.23739990\n",
      "Iteration 89, loss = 0.23706403\n",
      "Iteration 90, loss = 0.23702183\n",
      "Iteration 91, loss = 0.23674587\n",
      "Iteration 92, loss = 0.23618940\n",
      "Iteration 93, loss = 0.23594436\n",
      "Iteration 94, loss = 0.23564693\n",
      "Iteration 95, loss = 0.23549038\n",
      "Iteration 96, loss = 0.23526633\n",
      "Iteration 97, loss = 0.23515472\n",
      "Iteration 98, loss = 0.23481572\n",
      "Iteration 99, loss = 0.23444366\n",
      "Iteration 100, loss = 0.23444093\n",
      "Iteration 101, loss = 0.23410264\n",
      "Iteration 102, loss = 0.23368398\n",
      "Iteration 103, loss = 0.23344801\n",
      "Iteration 104, loss = 0.23317273\n",
      "Iteration 105, loss = 0.23287103\n",
      "Iteration 106, loss = 0.23285563\n",
      "Iteration 107, loss = 0.23328667\n",
      "Iteration 108, loss = 0.23211262\n",
      "Iteration 109, loss = 0.23195712\n",
      "Iteration 110, loss = 0.23167447\n",
      "Iteration 111, loss = 0.23115180\n",
      "Iteration 112, loss = 0.23109873\n",
      "Iteration 113, loss = 0.23072345\n",
      "Iteration 114, loss = 0.23032170\n",
      "Iteration 115, loss = 0.22989798\n",
      "Iteration 116, loss = 0.22962733\n",
      "Iteration 117, loss = 0.22924463\n",
      "Iteration 118, loss = 0.22895746\n",
      "Iteration 119, loss = 0.22869966\n",
      "Iteration 120, loss = 0.22856656\n",
      "Iteration 121, loss = 0.22793841\n",
      "Iteration 122, loss = 0.22779949\n",
      "Iteration 123, loss = 0.22721876\n",
      "Iteration 124, loss = 0.22682176\n",
      "Iteration 125, loss = 0.22682659\n",
      "Iteration 126, loss = 0.22615396\n",
      "Iteration 127, loss = 0.22589848\n",
      "Iteration 128, loss = 0.22543046\n",
      "Iteration 129, loss = 0.22509034\n",
      "Iteration 130, loss = 0.22452113\n",
      "Iteration 131, loss = 0.22450935\n",
      "Iteration 132, loss = 0.22393295\n",
      "Iteration 133, loss = 0.22347176\n",
      "Iteration 134, loss = 0.22307656\n",
      "Iteration 135, loss = 0.22264842\n",
      "Iteration 136, loss = 0.22229591\n",
      "Iteration 137, loss = 0.22191859\n",
      "Iteration 138, loss = 0.22168494\n",
      "Iteration 139, loss = 0.22124045\n",
      "Iteration 140, loss = 0.22063889\n",
      "Iteration 141, loss = 0.22057858\n",
      "Iteration 142, loss = 0.22019824\n",
      "Iteration 143, loss = 0.21950936\n",
      "Iteration 144, loss = 0.21907926\n",
      "Iteration 145, loss = 0.21870551\n",
      "Iteration 146, loss = 0.21797308\n",
      "Iteration 147, loss = 0.21776585\n",
      "Iteration 148, loss = 0.21733648\n",
      "Iteration 149, loss = 0.21691906\n",
      "Iteration 150, loss = 0.21651525\n",
      "Iteration 151, loss = 0.21589572\n",
      "Iteration 152, loss = 0.21549847\n",
      "Iteration 153, loss = 0.21506014\n",
      "Iteration 154, loss = 0.21442221\n",
      "Iteration 155, loss = 0.21423348\n",
      "Iteration 156, loss = 0.21353982\n",
      "Iteration 157, loss = 0.21320660\n",
      "Iteration 158, loss = 0.21290261\n",
      "Iteration 159, loss = 0.21239943\n",
      "Iteration 160, loss = 0.21205423\n",
      "Iteration 161, loss = 0.21121086\n",
      "Iteration 162, loss = 0.21064909\n",
      "Iteration 163, loss = 0.21052346\n",
      "Iteration 164, loss = 0.20988130\n",
      "Iteration 165, loss = 0.20930197\n",
      "Iteration 166, loss = 0.20886673\n",
      "Iteration 167, loss = 0.20831942\n",
      "Iteration 168, loss = 0.20774541\n",
      "Iteration 169, loss = 0.20744714\n",
      "Iteration 170, loss = 0.20677528\n",
      "Iteration 171, loss = 0.20629285\n",
      "Iteration 172, loss = 0.20577189\n",
      "Iteration 173, loss = 0.20550260\n",
      "Iteration 174, loss = 0.20490067\n",
      "Iteration 175, loss = 0.20414481\n",
      "Iteration 176, loss = 0.20363259\n",
      "Iteration 177, loss = 0.20327724\n",
      "Iteration 178, loss = 0.20295125\n",
      "Iteration 179, loss = 0.20229458\n",
      "Iteration 180, loss = 0.20203967\n",
      "Iteration 181, loss = 0.20126614\n",
      "Iteration 182, loss = 0.20070700\n",
      "Iteration 183, loss = 0.20007513\n",
      "Iteration 184, loss = 0.19968606\n",
      "Iteration 185, loss = 0.19908624\n",
      "Iteration 186, loss = 0.19878665\n",
      "Iteration 187, loss = 0.19813528\n",
      "Iteration 188, loss = 0.19749931\n",
      "Iteration 189, loss = 0.19710624\n",
      "Iteration 190, loss = 0.19641384\n",
      "Iteration 191, loss = 0.19575833\n",
      "Iteration 192, loss = 0.19528671\n",
      "Iteration 193, loss = 0.19467515\n",
      "Iteration 194, loss = 0.19431746\n",
      "Iteration 195, loss = 0.19367540\n",
      "Iteration 196, loss = 0.19300635\n",
      "Iteration 197, loss = 0.19257870\n",
      "Iteration 198, loss = 0.19212054\n",
      "Iteration 199, loss = 0.19169583\n",
      "Iteration 200, loss = 0.19097293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69036692\n",
      "Iteration 2, loss = 0.48773065\n",
      "Iteration 3, loss = 0.41593754\n",
      "Iteration 4, loss = 0.39088116\n",
      "Iteration 5, loss = 0.37652241\n",
      "Iteration 6, loss = 0.36489177\n",
      "Iteration 7, loss = 0.35408609\n",
      "Iteration 8, loss = 0.34483602\n",
      "Iteration 9, loss = 0.33685464\n",
      "Iteration 10, loss = 0.32933333\n",
      "Iteration 11, loss = 0.32295098\n",
      "Iteration 12, loss = 0.31694628\n",
      "Iteration 13, loss = 0.31193467\n",
      "Iteration 14, loss = 0.30723734\n",
      "Iteration 15, loss = 0.30310490\n",
      "Iteration 16, loss = 0.29936971\n",
      "Iteration 17, loss = 0.29608273\n",
      "Iteration 18, loss = 0.29302381\n",
      "Iteration 19, loss = 0.29025765\n",
      "Iteration 20, loss = 0.28772037\n",
      "Iteration 21, loss = 0.28539482\n",
      "Iteration 22, loss = 0.28325657\n",
      "Iteration 23, loss = 0.28122942\n",
      "Iteration 24, loss = 0.27927274\n",
      "Iteration 25, loss = 0.27755793\n",
      "Iteration 26, loss = 0.27575502\n",
      "Iteration 27, loss = 0.27430863\n",
      "Iteration 28, loss = 0.27274782\n",
      "Iteration 29, loss = 0.27127362\n",
      "Iteration 30, loss = 0.26979665\n",
      "Iteration 31, loss = 0.26844623\n",
      "Iteration 32, loss = 0.26713921\n",
      "Iteration 33, loss = 0.26588924\n",
      "Iteration 34, loss = 0.26457885\n",
      "Iteration 35, loss = 0.26336211\n",
      "Iteration 36, loss = 0.26213088\n",
      "Iteration 37, loss = 0.26091898\n",
      "Iteration 38, loss = 0.26000127\n",
      "Iteration 39, loss = 0.25881327\n",
      "Iteration 40, loss = 0.25777830\n",
      "Iteration 41, loss = 0.25655918\n",
      "Iteration 42, loss = 0.25559715\n",
      "Iteration 43, loss = 0.25469695\n",
      "Iteration 44, loss = 0.25369492\n",
      "Iteration 45, loss = 0.25259238\n",
      "Iteration 46, loss = 0.25193732\n",
      "Iteration 47, loss = 0.25092619\n",
      "Iteration 48, loss = 0.24985294\n",
      "Iteration 49, loss = 0.24912240\n",
      "Iteration 50, loss = 0.24822362\n",
      "Iteration 51, loss = 0.24733979\n",
      "Iteration 52, loss = 0.24670317\n",
      "Iteration 53, loss = 0.24576692\n",
      "Iteration 54, loss = 0.24513297\n",
      "Iteration 55, loss = 0.24422161\n",
      "Iteration 56, loss = 0.24364742\n",
      "Iteration 57, loss = 0.24301246\n",
      "Iteration 58, loss = 0.24240252\n",
      "Iteration 59, loss = 0.24181317\n",
      "Iteration 60, loss = 0.24124646\n",
      "Iteration 61, loss = 0.24080736\n",
      "Iteration 62, loss = 0.24014446\n",
      "Iteration 63, loss = 0.23970920\n",
      "Iteration 64, loss = 0.23922764\n",
      "Iteration 65, loss = 0.23898510\n",
      "Iteration 66, loss = 0.23807043\n",
      "Iteration 67, loss = 0.23785374\n",
      "Iteration 68, loss = 0.23739197\n",
      "Iteration 69, loss = 0.23698555\n",
      "Iteration 70, loss = 0.23669093\n",
      "Iteration 71, loss = 0.23633144\n",
      "Iteration 72, loss = 0.23608490\n",
      "Iteration 73, loss = 0.23563523\n",
      "Iteration 74, loss = 0.23532711\n",
      "Iteration 75, loss = 0.23532467\n",
      "Iteration 76, loss = 0.23478879\n",
      "Iteration 77, loss = 0.23430993\n",
      "Iteration 78, loss = 0.23419004\n",
      "Iteration 79, loss = 0.23387755\n",
      "Iteration 80, loss = 0.23352152\n",
      "Iteration 81, loss = 0.23338248\n",
      "Iteration 82, loss = 0.23396522\n",
      "Iteration 83, loss = 0.23305617\n",
      "Iteration 84, loss = 0.23262492\n",
      "Iteration 85, loss = 0.23219116\n",
      "Iteration 86, loss = 0.23236319\n",
      "Iteration 87, loss = 0.23177149\n",
      "Iteration 88, loss = 0.23167546\n",
      "Iteration 89, loss = 0.23131273\n",
      "Iteration 90, loss = 0.23102429\n",
      "Iteration 91, loss = 0.23060896\n",
      "Iteration 92, loss = 0.23059579\n",
      "Iteration 93, loss = 0.23003191\n",
      "Iteration 94, loss = 0.22990910\n",
      "Iteration 95, loss = 0.22954130\n",
      "Iteration 96, loss = 0.22961552\n",
      "Iteration 97, loss = 0.22933400\n",
      "Iteration 98, loss = 0.22855921\n",
      "Iteration 99, loss = 0.22849163\n",
      "Iteration 100, loss = 0.22846738\n",
      "Iteration 101, loss = 0.22805303\n",
      "Iteration 102, loss = 0.22769039\n",
      "Iteration 103, loss = 0.22737790\n",
      "Iteration 104, loss = 0.22673726\n",
      "Iteration 105, loss = 0.22650404\n",
      "Iteration 106, loss = 0.22617252\n",
      "Iteration 107, loss = 0.22590242\n",
      "Iteration 108, loss = 0.22546211\n",
      "Iteration 109, loss = 0.22506188\n",
      "Iteration 110, loss = 0.22478477\n",
      "Iteration 111, loss = 0.22466373\n",
      "Iteration 112, loss = 0.22410652\n",
      "Iteration 113, loss = 0.22362263\n",
      "Iteration 114, loss = 0.22393938\n",
      "Iteration 115, loss = 0.22309422\n",
      "Iteration 116, loss = 0.22255856\n",
      "Iteration 117, loss = 0.22202146\n",
      "Iteration 118, loss = 0.22148725\n",
      "Iteration 119, loss = 0.22129987\n",
      "Iteration 120, loss = 0.22082882\n",
      "Iteration 121, loss = 0.22040302\n",
      "Iteration 122, loss = 0.21994249\n",
      "Iteration 123, loss = 0.21953968\n",
      "Iteration 124, loss = 0.21911559\n",
      "Iteration 125, loss = 0.21863801\n",
      "Iteration 126, loss = 0.21804316\n",
      "Iteration 127, loss = 0.21765150\n",
      "Iteration 128, loss = 0.21723687\n",
      "Iteration 129, loss = 0.21666887\n",
      "Iteration 130, loss = 0.21665193\n",
      "Iteration 131, loss = 0.21577868\n",
      "Iteration 132, loss = 0.21520236\n",
      "Iteration 133, loss = 0.21473979\n",
      "Iteration 134, loss = 0.21458947\n",
      "Iteration 135, loss = 0.21345146\n",
      "Iteration 136, loss = 0.21315034\n",
      "Iteration 137, loss = 0.21279856\n",
      "Iteration 138, loss = 0.21207525\n",
      "Iteration 139, loss = 0.21156348\n",
      "Iteration 140, loss = 0.21136793\n",
      "Iteration 141, loss = 0.21040201\n",
      "Iteration 142, loss = 0.20987614\n",
      "Iteration 143, loss = 0.20960129\n",
      "Iteration 144, loss = 0.20888558\n",
      "Iteration 145, loss = 0.20838839\n",
      "Iteration 146, loss = 0.20770593\n",
      "Iteration 147, loss = 0.20717106\n",
      "Iteration 148, loss = 0.20663337\n",
      "Iteration 149, loss = 0.20607629\n",
      "Iteration 150, loss = 0.20546337\n",
      "Iteration 151, loss = 0.20499639\n",
      "Iteration 152, loss = 0.20487629\n",
      "Iteration 153, loss = 0.20380010\n",
      "Iteration 154, loss = 0.20315565\n",
      "Iteration 155, loss = 0.20261599\n",
      "Iteration 156, loss = 0.20198136\n",
      "Iteration 157, loss = 0.20153354\n",
      "Iteration 158, loss = 0.20065378\n",
      "Iteration 159, loss = 0.20034724\n",
      "Iteration 160, loss = 0.19956331\n",
      "Iteration 161, loss = 0.19909733\n",
      "Iteration 162, loss = 0.19892916\n",
      "Iteration 163, loss = 0.19797957\n",
      "Iteration 164, loss = 0.19744146\n",
      "Iteration 165, loss = 0.19691893\n",
      "Iteration 166, loss = 0.19654670\n",
      "Iteration 167, loss = 0.19565680\n",
      "Iteration 168, loss = 0.19522987\n",
      "Iteration 169, loss = 0.19455050\n",
      "Iteration 170, loss = 0.19386564\n",
      "Iteration 171, loss = 0.19329091\n",
      "Iteration 172, loss = 0.19283497\n",
      "Iteration 173, loss = 0.19216302\n",
      "Iteration 174, loss = 0.19145415\n",
      "Iteration 175, loss = 0.19132439\n",
      "Iteration 176, loss = 0.19056424\n",
      "Iteration 177, loss = 0.18975634\n",
      "Iteration 178, loss = 0.18950897\n",
      "Iteration 179, loss = 0.18866537\n",
      "Iteration 180, loss = 0.18812096\n",
      "Iteration 181, loss = 0.18832506\n",
      "Iteration 182, loss = 0.18684856\n",
      "Iteration 183, loss = 0.18669180\n",
      "Iteration 184, loss = 0.18598174\n",
      "Iteration 185, loss = 0.18536317\n",
      "Iteration 186, loss = 0.18483878\n",
      "Iteration 187, loss = 0.18430094\n",
      "Iteration 188, loss = 0.18399016\n",
      "Iteration 189, loss = 0.18303945\n",
      "Iteration 190, loss = 0.18306297\n",
      "Iteration 191, loss = 0.18314700\n",
      "Iteration 192, loss = 0.18213513\n",
      "Iteration 193, loss = 0.18094324\n",
      "Iteration 194, loss = 0.18069813\n",
      "Iteration 195, loss = 0.18031055\n",
      "Iteration 196, loss = 0.17949669\n",
      "Iteration 197, loss = 0.17895803\n",
      "Iteration 198, loss = 0.17832177\n",
      "Iteration 199, loss = 0.17778203\n",
      "Iteration 200, loss = 0.17778029\n",
      "Iteration 1, loss = 0.69118921\n",
      "Iteration 2, loss = 0.48799379\n",
      "Iteration 3, loss = 0.41504811\n",
      "Iteration 4, loss = 0.39030737\n",
      "Iteration 5, loss = 0.37620307\n",
      "Iteration 6, loss = 0.36439277\n",
      "Iteration 7, loss = 0.35386807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.34461792\n",
      "Iteration 9, loss = 0.33647702\n",
      "Iteration 10, loss = 0.32930211\n",
      "Iteration 11, loss = 0.32286058\n",
      "Iteration 12, loss = 0.31691345\n",
      "Iteration 13, loss = 0.31207950\n",
      "Iteration 14, loss = 0.30741836\n",
      "Iteration 15, loss = 0.30338309\n",
      "Iteration 16, loss = 0.29975564\n",
      "Iteration 17, loss = 0.29658503\n",
      "Iteration 18, loss = 0.29368370\n",
      "Iteration 19, loss = 0.29101928\n",
      "Iteration 20, loss = 0.28868472\n",
      "Iteration 21, loss = 0.28660584\n",
      "Iteration 22, loss = 0.28474807\n",
      "Iteration 23, loss = 0.28274006\n",
      "Iteration 24, loss = 0.28104690\n",
      "Iteration 25, loss = 0.27941747\n",
      "Iteration 26, loss = 0.27793230\n",
      "Iteration 27, loss = 0.27661438\n",
      "Iteration 28, loss = 0.27513331\n",
      "Iteration 29, loss = 0.27400748\n",
      "Iteration 30, loss = 0.27263071\n",
      "Iteration 31, loss = 0.27135183\n",
      "Iteration 32, loss = 0.27021436\n",
      "Iteration 33, loss = 0.26912907\n",
      "Iteration 34, loss = 0.26809986\n",
      "Iteration 35, loss = 0.26695258\n",
      "Iteration 36, loss = 0.26597713\n",
      "Iteration 37, loss = 0.26499840\n",
      "Iteration 38, loss = 0.26392152\n",
      "Iteration 39, loss = 0.26307799\n",
      "Iteration 40, loss = 0.26191459\n",
      "Iteration 41, loss = 0.26087057\n",
      "Iteration 42, loss = 0.26005342\n",
      "Iteration 43, loss = 0.25920633\n",
      "Iteration 44, loss = 0.25837496\n",
      "Iteration 45, loss = 0.25751481\n",
      "Iteration 46, loss = 0.25683645\n",
      "Iteration 47, loss = 0.25597529\n",
      "Iteration 48, loss = 0.25486454\n",
      "Iteration 49, loss = 0.25440167\n",
      "Iteration 50, loss = 0.25347003\n",
      "Iteration 51, loss = 0.25296749\n",
      "Iteration 52, loss = 0.25226367\n",
      "Iteration 53, loss = 0.25139348\n",
      "Iteration 54, loss = 0.25067558\n",
      "Iteration 55, loss = 0.25001642\n",
      "Iteration 56, loss = 0.24939132\n",
      "Iteration 57, loss = 0.24889178\n",
      "Iteration 58, loss = 0.24837785\n",
      "Iteration 59, loss = 0.24780688\n",
      "Iteration 60, loss = 0.24729588\n",
      "Iteration 61, loss = 0.24683617\n",
      "Iteration 62, loss = 0.24621830\n",
      "Iteration 63, loss = 0.24586855\n",
      "Iteration 64, loss = 0.24540774\n",
      "Iteration 65, loss = 0.24531083\n",
      "Iteration 66, loss = 0.24459887\n",
      "Iteration 67, loss = 0.24433754\n",
      "Iteration 68, loss = 0.24388537\n",
      "Iteration 69, loss = 0.24358579\n",
      "Iteration 70, loss = 0.24330427\n",
      "Iteration 71, loss = 0.24302854\n",
      "Iteration 72, loss = 0.24249375\n",
      "Iteration 73, loss = 0.24228840\n",
      "Iteration 74, loss = 0.24201100\n",
      "Iteration 75, loss = 0.24176155\n",
      "Iteration 76, loss = 0.24144204\n",
      "Iteration 77, loss = 0.24125054\n",
      "Iteration 78, loss = 0.24106302\n",
      "Iteration 79, loss = 0.24065746\n",
      "Iteration 80, loss = 0.24057729\n",
      "Iteration 81, loss = 0.24029515\n",
      "Iteration 82, loss = 0.24036901\n",
      "Iteration 83, loss = 0.24007637\n",
      "Iteration 84, loss = 0.23960930\n",
      "Iteration 85, loss = 0.23906347\n",
      "Iteration 86, loss = 0.23922565\n",
      "Iteration 87, loss = 0.23877600\n",
      "Iteration 88, loss = 0.23863221\n",
      "Iteration 89, loss = 0.23830953\n",
      "Iteration 90, loss = 0.23805885\n",
      "Iteration 91, loss = 0.23790855\n",
      "Iteration 92, loss = 0.23763368\n",
      "Iteration 93, loss = 0.23734068\n",
      "Iteration 94, loss = 0.23722048\n",
      "Iteration 95, loss = 0.23703374\n",
      "Iteration 96, loss = 0.23707591\n",
      "Iteration 97, loss = 0.23648602\n",
      "Iteration 98, loss = 0.23614754\n",
      "Iteration 99, loss = 0.23592373\n",
      "Iteration 100, loss = 0.23578892\n",
      "Iteration 101, loss = 0.23585021\n",
      "Iteration 102, loss = 0.23549096\n",
      "Iteration 103, loss = 0.23486710\n",
      "Iteration 104, loss = 0.23441954\n",
      "Iteration 105, loss = 0.23428927\n",
      "Iteration 106, loss = 0.23396805\n",
      "Iteration 107, loss = 0.23372888\n",
      "Iteration 108, loss = 0.23338285\n",
      "Iteration 109, loss = 0.23299476\n",
      "Iteration 110, loss = 0.23270212\n",
      "Iteration 111, loss = 0.23255749\n",
      "Iteration 112, loss = 0.23209645\n",
      "Iteration 113, loss = 0.23178684\n",
      "Iteration 114, loss = 0.23157421\n",
      "Iteration 115, loss = 0.23098249\n",
      "Iteration 116, loss = 0.23080481\n",
      "Iteration 117, loss = 0.23026174\n",
      "Iteration 118, loss = 0.22978880\n",
      "Iteration 119, loss = 0.22958599\n",
      "Iteration 120, loss = 0.22909280\n",
      "Iteration 121, loss = 0.22881809\n",
      "Iteration 122, loss = 0.22845896\n",
      "Iteration 123, loss = 0.22811396\n",
      "Iteration 124, loss = 0.22763961\n",
      "Iteration 125, loss = 0.22724104\n",
      "Iteration 126, loss = 0.22708805\n",
      "Iteration 127, loss = 0.22647031\n",
      "Iteration 128, loss = 0.22599340\n",
      "Iteration 129, loss = 0.22550158\n",
      "Iteration 130, loss = 0.22529329\n",
      "Iteration 131, loss = 0.22450357\n",
      "Iteration 132, loss = 0.22432596\n",
      "Iteration 133, loss = 0.22361257\n",
      "Iteration 134, loss = 0.22344507\n",
      "Iteration 135, loss = 0.22265498\n",
      "Iteration 136, loss = 0.22240845\n",
      "Iteration 137, loss = 0.22170257\n",
      "Iteration 138, loss = 0.22132604\n",
      "Iteration 139, loss = 0.22081828\n",
      "Iteration 140, loss = 0.22057292\n",
      "Iteration 141, loss = 0.21974391\n",
      "Iteration 142, loss = 0.21946698\n",
      "Iteration 143, loss = 0.21909439\n",
      "Iteration 144, loss = 0.21836497\n",
      "Iteration 145, loss = 0.21793510\n",
      "Iteration 146, loss = 0.21725574\n",
      "Iteration 147, loss = 0.21681361\n",
      "Iteration 148, loss = 0.21638189\n",
      "Iteration 149, loss = 0.21576025\n",
      "Iteration 150, loss = 0.21543255\n",
      "Iteration 151, loss = 0.21526873\n",
      "Iteration 152, loss = 0.21484103\n",
      "Iteration 153, loss = 0.21348700\n",
      "Iteration 154, loss = 0.21342252\n",
      "Iteration 155, loss = 0.21243403\n",
      "Iteration 156, loss = 0.21187814\n",
      "Iteration 157, loss = 0.21144864\n",
      "Iteration 158, loss = 0.21083789\n",
      "Iteration 159, loss = 0.21031512\n",
      "Iteration 160, loss = 0.20960599\n",
      "Iteration 161, loss = 0.20928902\n",
      "Iteration 162, loss = 0.20908542\n",
      "Iteration 163, loss = 0.20810159\n",
      "Iteration 164, loss = 0.20747404\n",
      "Iteration 165, loss = 0.20703775\n",
      "Iteration 166, loss = 0.20610855\n",
      "Iteration 167, loss = 0.20562506\n",
      "Iteration 168, loss = 0.20518848\n",
      "Iteration 169, loss = 0.20456416\n",
      "Iteration 170, loss = 0.20386925\n",
      "Iteration 171, loss = 0.20350637\n",
      "Iteration 172, loss = 0.20276251\n",
      "Iteration 173, loss = 0.20232039\n",
      "Iteration 174, loss = 0.20154939\n",
      "Iteration 175, loss = 0.20150482\n",
      "Iteration 176, loss = 0.20045770\n",
      "Iteration 177, loss = 0.19984471\n",
      "Iteration 178, loss = 0.19934223\n",
      "Iteration 179, loss = 0.19851297\n",
      "Iteration 180, loss = 0.19803359\n",
      "Iteration 181, loss = 0.19765901\n",
      "Iteration 182, loss = 0.19669598\n",
      "Iteration 183, loss = 0.19640214\n",
      "Iteration 184, loss = 0.19560484\n",
      "Iteration 185, loss = 0.19492170\n",
      "Iteration 186, loss = 0.19448547\n",
      "Iteration 187, loss = 0.19395393\n",
      "Iteration 188, loss = 0.19309344\n",
      "Iteration 189, loss = 0.19248910\n",
      "Iteration 190, loss = 0.19233110\n",
      "Iteration 191, loss = 0.19223414\n",
      "Iteration 192, loss = 0.19152930\n",
      "Iteration 193, loss = 0.19008065\n",
      "Iteration 194, loss = 0.18985411\n",
      "Iteration 195, loss = 0.18935751\n",
      "Iteration 196, loss = 0.18843006\n",
      "Iteration 197, loss = 0.18785551\n",
      "Iteration 198, loss = 0.18724516\n",
      "Iteration 199, loss = 0.18652389\n",
      "Iteration 200, loss = 0.18621455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70810519\n",
      "Iteration 2, loss = 0.48936615\n",
      "Iteration 3, loss = 0.44522553\n",
      "Iteration 4, loss = 0.44164346\n",
      "Iteration 5, loss = 0.44109047\n",
      "Iteration 6, loss = 0.44030706\n",
      "Iteration 7, loss = 0.43952365\n",
      "Iteration 8, loss = 0.43880571\n",
      "Iteration 9, loss = 0.43808335\n",
      "Iteration 10, loss = 0.43738486\n",
      "Iteration 11, loss = 0.43664688\n",
      "Iteration 12, loss = 0.43594581\n",
      "Iteration 13, loss = 0.43522431\n",
      "Iteration 14, loss = 0.43450689\n",
      "Iteration 15, loss = 0.43379917\n",
      "Iteration 16, loss = 0.43309511\n",
      "Iteration 17, loss = 0.43240442\n",
      "Iteration 18, loss = 0.43170659\n",
      "Iteration 19, loss = 0.43096392\n",
      "Iteration 20, loss = 0.43033072\n",
      "Iteration 21, loss = 0.42957575\n",
      "Iteration 22, loss = 0.42887539\n",
      "Iteration 23, loss = 0.42818533\n",
      "Iteration 24, loss = 0.42750098\n",
      "Iteration 25, loss = 0.42678601\n",
      "Iteration 26, loss = 0.42611365\n",
      "Iteration 27, loss = 0.42539940\n",
      "Iteration 28, loss = 0.42469418\n",
      "Iteration 29, loss = 0.42398237\n",
      "Iteration 30, loss = 0.42327339\n",
      "Iteration 31, loss = 0.42258642\n",
      "Iteration 32, loss = 0.42189389\n",
      "Iteration 33, loss = 0.42119977\n",
      "Iteration 34, loss = 0.42048652\n",
      "Iteration 35, loss = 0.41978878\n",
      "Iteration 36, loss = 0.41913269\n",
      "Iteration 37, loss = 0.41836062\n",
      "Iteration 38, loss = 0.41764791\n",
      "Iteration 39, loss = 0.41695020\n",
      "Iteration 40, loss = 0.41624017\n",
      "Iteration 41, loss = 0.41551520\n",
      "Iteration 42, loss = 0.41484958\n",
      "Iteration 43, loss = 0.41407944\n",
      "Iteration 44, loss = 0.41339468\n",
      "Iteration 45, loss = 0.41268239\n",
      "Iteration 46, loss = 0.41197300\n",
      "Iteration 47, loss = 0.41124169\n",
      "Iteration 48, loss = 0.41050382\n",
      "Iteration 49, loss = 0.40980239\n",
      "Iteration 50, loss = 0.40905809\n",
      "Iteration 51, loss = 0.40835375\n",
      "Iteration 52, loss = 0.40765366\n",
      "Iteration 53, loss = 0.40684651\n",
      "Iteration 54, loss = 0.40609465\n",
      "Iteration 55, loss = 0.40536162\n",
      "Iteration 56, loss = 0.40464385\n",
      "Iteration 57, loss = 0.40389596\n",
      "Iteration 58, loss = 0.40314803\n",
      "Iteration 59, loss = 0.40239326\n",
      "Iteration 60, loss = 0.40168597\n",
      "Iteration 61, loss = 0.40090798\n",
      "Iteration 62, loss = 0.40015546\n",
      "Iteration 63, loss = 0.39940327\n",
      "Iteration 64, loss = 0.39862110\n",
      "Iteration 65, loss = 0.39788664\n",
      "Iteration 66, loss = 0.39711145\n",
      "Iteration 67, loss = 0.39633584\n",
      "Iteration 68, loss = 0.39558879\n",
      "Iteration 69, loss = 0.39482400\n",
      "Iteration 70, loss = 0.39409820\n",
      "Iteration 71, loss = 0.39328784\n",
      "Iteration 72, loss = 0.39251689\n",
      "Iteration 73, loss = 0.39173376\n",
      "Iteration 74, loss = 0.39094587\n",
      "Iteration 75, loss = 0.39017900\n",
      "Iteration 76, loss = 0.38938261\n",
      "Iteration 77, loss = 0.38862957\n",
      "Iteration 78, loss = 0.38782375\n",
      "Iteration 79, loss = 0.38704566\n",
      "Iteration 80, loss = 0.38626155\n",
      "Iteration 81, loss = 0.38546913\n",
      "Iteration 82, loss = 0.38470265\n",
      "Iteration 83, loss = 0.38387959\n",
      "Iteration 84, loss = 0.38310150\n",
      "Iteration 85, loss = 0.38232541\n",
      "Iteration 86, loss = 0.38158338\n",
      "Iteration 87, loss = 0.38072124\n",
      "Iteration 88, loss = 0.37995243\n",
      "Iteration 89, loss = 0.37915025\n",
      "Iteration 90, loss = 0.37834563\n",
      "Iteration 91, loss = 0.37756389\n",
      "Iteration 92, loss = 0.37676679\n",
      "Iteration 93, loss = 0.37601537\n",
      "Iteration 94, loss = 0.37516550\n",
      "Iteration 95, loss = 0.37435270\n",
      "Iteration 96, loss = 0.37357040\n",
      "Iteration 97, loss = 0.37275512\n",
      "Iteration 98, loss = 0.37194935\n",
      "Iteration 99, loss = 0.37116149\n",
      "Iteration 100, loss = 0.37037429\n",
      "Iteration 101, loss = 0.36956083\n",
      "Iteration 102, loss = 0.36878565\n",
      "Iteration 103, loss = 0.36797331\n",
      "Iteration 104, loss = 0.36720702\n",
      "Iteration 105, loss = 0.36639733\n",
      "Iteration 106, loss = 0.36560218\n",
      "Iteration 107, loss = 0.36482950\n",
      "Iteration 108, loss = 0.36400745\n",
      "Iteration 109, loss = 0.36324302\n",
      "Iteration 110, loss = 0.36250227\n",
      "Iteration 111, loss = 0.36165384\n",
      "Iteration 112, loss = 0.36088807\n",
      "Iteration 113, loss = 0.36009370\n",
      "Iteration 114, loss = 0.35931435\n",
      "Iteration 115, loss = 0.35856737\n",
      "Iteration 116, loss = 0.35775177\n",
      "Iteration 117, loss = 0.35698719\n",
      "Iteration 118, loss = 0.35622620\n",
      "Iteration 119, loss = 0.35545808\n",
      "Iteration 120, loss = 0.35467172\n",
      "Iteration 121, loss = 0.35393089\n",
      "Iteration 122, loss = 0.35318918\n",
      "Iteration 123, loss = 0.35237915\n",
      "Iteration 124, loss = 0.35162900\n",
      "Iteration 125, loss = 0.35087555\n",
      "Iteration 126, loss = 0.35014181\n",
      "Iteration 127, loss = 0.34938203\n",
      "Iteration 128, loss = 0.34863830\n",
      "Iteration 129, loss = 0.34790237\n",
      "Iteration 130, loss = 0.34717266\n",
      "Iteration 131, loss = 0.34646616\n",
      "Iteration 132, loss = 0.34570985\n",
      "Iteration 133, loss = 0.34497348\n",
      "Iteration 134, loss = 0.34426095\n",
      "Iteration 135, loss = 0.34353315\n",
      "Iteration 136, loss = 0.34285028\n",
      "Iteration 137, loss = 0.34211999\n",
      "Iteration 138, loss = 0.34141339\n",
      "Iteration 139, loss = 0.34072892\n",
      "Iteration 140, loss = 0.34000282\n",
      "Iteration 141, loss = 0.33930710\n",
      "Iteration 142, loss = 0.33864138\n",
      "Iteration 143, loss = 0.33792869\n",
      "Iteration 144, loss = 0.33725969\n",
      "Iteration 145, loss = 0.33657735\n",
      "Iteration 146, loss = 0.33591784\n",
      "Iteration 147, loss = 0.33525781\n",
      "Iteration 148, loss = 0.33458013\n",
      "Iteration 149, loss = 0.33392690\n",
      "Iteration 150, loss = 0.33325414\n",
      "Iteration 151, loss = 0.33263717\n",
      "Iteration 152, loss = 0.33193473\n",
      "Iteration 153, loss = 0.33133214\n",
      "Iteration 154, loss = 0.33066795\n",
      "Iteration 155, loss = 0.33005553\n",
      "Iteration 156, loss = 0.32942189\n",
      "Iteration 157, loss = 0.32880744\n",
      "Iteration 158, loss = 0.32817784\n",
      "Iteration 159, loss = 0.32759122\n",
      "Iteration 160, loss = 0.32697436\n",
      "Iteration 161, loss = 0.32636636\n",
      "Iteration 162, loss = 0.32576781\n",
      "Iteration 163, loss = 0.32515442\n",
      "Iteration 164, loss = 0.32457154\n",
      "Iteration 165, loss = 0.32400888\n",
      "Iteration 166, loss = 0.32341481\n",
      "Iteration 167, loss = 0.32288640\n",
      "Iteration 168, loss = 0.32226949\n",
      "Iteration 169, loss = 0.32172678\n",
      "Iteration 170, loss = 0.32114548\n",
      "Iteration 171, loss = 0.32058613\n",
      "Iteration 172, loss = 0.32003365\n",
      "Iteration 173, loss = 0.31950716\n",
      "Iteration 174, loss = 0.31895309\n",
      "Iteration 175, loss = 0.31841902\n",
      "Iteration 176, loss = 0.31788300\n",
      "Iteration 177, loss = 0.31736919\n",
      "Iteration 178, loss = 0.31687585\n",
      "Iteration 179, loss = 0.31634695\n",
      "Iteration 180, loss = 0.31582379\n",
      "Iteration 181, loss = 0.31531451\n",
      "Iteration 182, loss = 0.31487207\n",
      "Iteration 183, loss = 0.31431140\n",
      "Iteration 184, loss = 0.31383799\n",
      "Iteration 185, loss = 0.31331557\n",
      "Iteration 186, loss = 0.31283345\n",
      "Iteration 187, loss = 0.31236429\n",
      "Iteration 188, loss = 0.31189682\n",
      "Iteration 189, loss = 0.31142550\n",
      "Iteration 190, loss = 0.31094789\n",
      "Iteration 191, loss = 0.31049065\n",
      "Iteration 192, loss = 0.31004848\n",
      "Iteration 193, loss = 0.30956992\n",
      "Iteration 194, loss = 0.30914677\n",
      "Iteration 195, loss = 0.30869609\n",
      "Iteration 196, loss = 0.30825090\n",
      "Iteration 197, loss = 0.30781508\n",
      "Iteration 198, loss = 0.30739044\n",
      "Iteration 199, loss = 0.30696428\n",
      "Iteration 200, loss = 0.30653407\n",
      "Iteration 1, loss = 0.70792320\n",
      "Iteration 2, loss = 0.49004319\n",
      "Iteration 3, loss = 0.44566063\n",
      "Iteration 4, loss = 0.44227696\n",
      "Iteration 5, loss = 0.44157085\n",
      "Iteration 6, loss = 0.44078474\n",
      "Iteration 7, loss = 0.43998309\n",
      "Iteration 8, loss = 0.43925104\n",
      "Iteration 9, loss = 0.43848377\n",
      "Iteration 10, loss = 0.43777981\n",
      "Iteration 11, loss = 0.43701496\n",
      "Iteration 12, loss = 0.43629936\n",
      "Iteration 13, loss = 0.43556204\n",
      "Iteration 14, loss = 0.43480714\n",
      "Iteration 15, loss = 0.43410090\n",
      "Iteration 16, loss = 0.43336660\n",
      "Iteration 17, loss = 0.43268116\n",
      "Iteration 18, loss = 0.43196438\n",
      "Iteration 19, loss = 0.43121835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.43049402\n",
      "Iteration 21, loss = 0.42976202\n",
      "Iteration 22, loss = 0.42902296\n",
      "Iteration 23, loss = 0.42830130\n",
      "Iteration 24, loss = 0.42761266\n",
      "Iteration 25, loss = 0.42686611\n",
      "Iteration 26, loss = 0.42615081\n",
      "Iteration 27, loss = 0.42545326\n",
      "Iteration 28, loss = 0.42473653\n",
      "Iteration 29, loss = 0.42396825\n",
      "Iteration 30, loss = 0.42327485\n",
      "Iteration 31, loss = 0.42253485\n",
      "Iteration 32, loss = 0.42180093\n",
      "Iteration 33, loss = 0.42111937\n",
      "Iteration 34, loss = 0.42035761\n",
      "Iteration 35, loss = 0.41966900\n",
      "Iteration 36, loss = 0.41891197\n",
      "Iteration 37, loss = 0.41819118\n",
      "Iteration 38, loss = 0.41744328\n",
      "Iteration 39, loss = 0.41671517\n",
      "Iteration 40, loss = 0.41598267\n",
      "Iteration 41, loss = 0.41527218\n",
      "Iteration 42, loss = 0.41452525\n",
      "Iteration 43, loss = 0.41377175\n",
      "Iteration 44, loss = 0.41307095\n",
      "Iteration 45, loss = 0.41230452\n",
      "Iteration 46, loss = 0.41161836\n",
      "Iteration 47, loss = 0.41083556\n",
      "Iteration 48, loss = 0.41006499\n",
      "Iteration 49, loss = 0.40937395\n",
      "Iteration 50, loss = 0.40858545\n",
      "Iteration 51, loss = 0.40780266\n",
      "Iteration 52, loss = 0.40710813\n",
      "Iteration 53, loss = 0.40628725\n",
      "Iteration 54, loss = 0.40551154\n",
      "Iteration 55, loss = 0.40475509\n",
      "Iteration 56, loss = 0.40400932\n",
      "Iteration 57, loss = 0.40323224\n",
      "Iteration 58, loss = 0.40245879\n",
      "Iteration 59, loss = 0.40169593\n",
      "Iteration 60, loss = 0.40093096\n",
      "Iteration 61, loss = 0.40014393\n",
      "Iteration 62, loss = 0.39938649\n",
      "Iteration 63, loss = 0.39857681\n",
      "Iteration 64, loss = 0.39781189\n",
      "Iteration 65, loss = 0.39701529\n",
      "Iteration 66, loss = 0.39622464\n",
      "Iteration 67, loss = 0.39543513\n",
      "Iteration 68, loss = 0.39464606\n",
      "Iteration 69, loss = 0.39385452\n",
      "Iteration 70, loss = 0.39308236\n",
      "Iteration 71, loss = 0.39225567\n",
      "Iteration 72, loss = 0.39148161\n",
      "Iteration 73, loss = 0.39066671\n",
      "Iteration 74, loss = 0.38984277\n",
      "Iteration 75, loss = 0.38905648\n",
      "Iteration 76, loss = 0.38824413\n",
      "Iteration 77, loss = 0.38744898\n",
      "Iteration 78, loss = 0.38662315\n",
      "Iteration 79, loss = 0.38585006\n",
      "Iteration 80, loss = 0.38501529\n",
      "Iteration 81, loss = 0.38419540\n",
      "Iteration 82, loss = 0.38336858\n",
      "Iteration 83, loss = 0.38255664\n",
      "Iteration 84, loss = 0.38175095\n",
      "Iteration 85, loss = 0.38093637\n",
      "Iteration 86, loss = 0.38015027\n",
      "Iteration 87, loss = 0.37929357\n",
      "Iteration 88, loss = 0.37848593\n",
      "Iteration 89, loss = 0.37765578\n",
      "Iteration 90, loss = 0.37688087\n",
      "Iteration 91, loss = 0.37602597\n",
      "Iteration 92, loss = 0.37519533\n",
      "Iteration 93, loss = 0.37440313\n",
      "Iteration 94, loss = 0.37357195\n",
      "Iteration 95, loss = 0.37272776\n",
      "Iteration 96, loss = 0.37190033\n",
      "Iteration 97, loss = 0.37112128\n",
      "Iteration 98, loss = 0.37031758\n",
      "Iteration 99, loss = 0.36944085\n",
      "Iteration 100, loss = 0.36863726\n",
      "Iteration 101, loss = 0.36780411\n",
      "Iteration 102, loss = 0.36699020\n",
      "Iteration 103, loss = 0.36618004\n",
      "Iteration 104, loss = 0.36535772\n",
      "Iteration 105, loss = 0.36454497\n",
      "Iteration 106, loss = 0.36372105\n",
      "Iteration 107, loss = 0.36291709\n",
      "Iteration 108, loss = 0.36210543\n",
      "Iteration 109, loss = 0.36130152\n",
      "Iteration 110, loss = 0.36053876\n",
      "Iteration 111, loss = 0.35968585\n",
      "Iteration 112, loss = 0.35889865\n",
      "Iteration 113, loss = 0.35809234\n",
      "Iteration 114, loss = 0.35729679\n",
      "Iteration 115, loss = 0.35649335\n",
      "Iteration 116, loss = 0.35569303\n",
      "Iteration 117, loss = 0.35491266\n",
      "Iteration 118, loss = 0.35413416\n",
      "Iteration 119, loss = 0.35334090\n",
      "Iteration 120, loss = 0.35256633\n",
      "Iteration 121, loss = 0.35183231\n",
      "Iteration 122, loss = 0.35097867\n",
      "Iteration 123, loss = 0.35023829\n",
      "Iteration 124, loss = 0.34944912\n",
      "Iteration 125, loss = 0.34870164\n",
      "Iteration 126, loss = 0.34791503\n",
      "Iteration 127, loss = 0.34715270\n",
      "Iteration 128, loss = 0.34641357\n",
      "Iteration 129, loss = 0.34565514\n",
      "Iteration 130, loss = 0.34490447\n",
      "Iteration 131, loss = 0.34417566\n",
      "Iteration 132, loss = 0.34342123\n",
      "Iteration 133, loss = 0.34267718\n",
      "Iteration 134, loss = 0.34195538\n",
      "Iteration 135, loss = 0.34123096\n",
      "Iteration 136, loss = 0.34049577\n",
      "Iteration 137, loss = 0.33979039\n",
      "Iteration 138, loss = 0.33908010\n",
      "Iteration 139, loss = 0.33835777\n",
      "Iteration 140, loss = 0.33764163\n",
      "Iteration 141, loss = 0.33693406\n",
      "Iteration 142, loss = 0.33625235\n",
      "Iteration 143, loss = 0.33555024\n",
      "Iteration 144, loss = 0.33485112\n",
      "Iteration 145, loss = 0.33417640\n",
      "Iteration 146, loss = 0.33351234\n",
      "Iteration 147, loss = 0.33284196\n",
      "Iteration 148, loss = 0.33217217\n",
      "Iteration 149, loss = 0.33149002\n",
      "Iteration 150, loss = 0.33083576\n",
      "Iteration 151, loss = 0.33017067\n",
      "Iteration 152, loss = 0.32953113\n",
      "Iteration 153, loss = 0.32889316\n",
      "Iteration 154, loss = 0.32822166\n",
      "Iteration 155, loss = 0.32759405\n",
      "Iteration 156, loss = 0.32699171\n",
      "Iteration 157, loss = 0.32635037\n",
      "Iteration 158, loss = 0.32575029\n",
      "Iteration 159, loss = 0.32510111\n",
      "Iteration 160, loss = 0.32449404\n",
      "Iteration 161, loss = 0.32391585\n",
      "Iteration 162, loss = 0.32329834\n",
      "Iteration 163, loss = 0.32270951\n",
      "Iteration 164, loss = 0.32211563\n",
      "Iteration 165, loss = 0.32153544\n",
      "Iteration 166, loss = 0.32096725\n",
      "Iteration 167, loss = 0.32040362\n",
      "Iteration 168, loss = 0.31980585\n",
      "Iteration 169, loss = 0.31927100\n",
      "Iteration 170, loss = 0.31869213\n",
      "Iteration 171, loss = 0.31813278\n",
      "Iteration 172, loss = 0.31758386\n",
      "Iteration 173, loss = 0.31704317\n",
      "Iteration 174, loss = 0.31650416\n",
      "Iteration 175, loss = 0.31596986\n",
      "Iteration 176, loss = 0.31544244\n",
      "Iteration 177, loss = 0.31492310\n",
      "Iteration 178, loss = 0.31443222\n",
      "Iteration 179, loss = 0.31387816\n",
      "Iteration 180, loss = 0.31340560\n",
      "Iteration 181, loss = 0.31289814\n",
      "Iteration 182, loss = 0.31246777\n",
      "Iteration 183, loss = 0.31189118\n",
      "Iteration 184, loss = 0.31142746\n",
      "Iteration 185, loss = 0.31090680\n",
      "Iteration 186, loss = 0.31045801\n",
      "Iteration 187, loss = 0.30996544\n",
      "Iteration 188, loss = 0.30951526\n",
      "Iteration 189, loss = 0.30903499\n",
      "Iteration 190, loss = 0.30858317\n",
      "Iteration 191, loss = 0.30810266\n",
      "Iteration 192, loss = 0.30766170\n",
      "Iteration 193, loss = 0.30722292\n",
      "Iteration 194, loss = 0.30676161\n",
      "Iteration 195, loss = 0.30634076\n",
      "Iteration 196, loss = 0.30589683\n",
      "Iteration 197, loss = 0.30547012\n",
      "Iteration 198, loss = 0.30505831\n",
      "Iteration 199, loss = 0.30462259\n",
      "Iteration 200, loss = 0.30421254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70782552\n",
      "Iteration 2, loss = 0.48933998\n",
      "Iteration 3, loss = 0.44485435\n",
      "Iteration 4, loss = 0.44199818\n",
      "Iteration 5, loss = 0.44124079\n",
      "Iteration 6, loss = 0.44041811\n",
      "Iteration 7, loss = 0.43962721\n",
      "Iteration 8, loss = 0.43887081\n",
      "Iteration 9, loss = 0.43811179\n",
      "Iteration 10, loss = 0.43739480\n",
      "Iteration 11, loss = 0.43665954\n",
      "Iteration 12, loss = 0.43587453\n",
      "Iteration 13, loss = 0.43519886\n",
      "Iteration 14, loss = 0.43441848\n",
      "Iteration 15, loss = 0.43366969\n",
      "Iteration 16, loss = 0.43294152\n",
      "Iteration 17, loss = 0.43224149\n",
      "Iteration 18, loss = 0.43154489\n",
      "Iteration 19, loss = 0.43081115\n",
      "Iteration 20, loss = 0.43003784\n",
      "Iteration 21, loss = 0.42929855\n",
      "Iteration 22, loss = 0.42858726\n",
      "Iteration 23, loss = 0.42786489\n",
      "Iteration 24, loss = 0.42717778\n",
      "Iteration 25, loss = 0.42640512\n",
      "Iteration 26, loss = 0.42569442\n",
      "Iteration 27, loss = 0.42498836\n",
      "Iteration 28, loss = 0.42428294\n",
      "Iteration 29, loss = 0.42351208\n",
      "Iteration 30, loss = 0.42278417\n",
      "Iteration 31, loss = 0.42209988\n",
      "Iteration 32, loss = 0.42132362\n",
      "Iteration 33, loss = 0.42064316\n",
      "Iteration 34, loss = 0.41987128\n",
      "Iteration 35, loss = 0.41920639\n",
      "Iteration 36, loss = 0.41842735\n",
      "Iteration 37, loss = 0.41771201\n",
      "Iteration 38, loss = 0.41695844\n",
      "Iteration 39, loss = 0.41621722\n",
      "Iteration 40, loss = 0.41547933\n",
      "Iteration 41, loss = 0.41476300\n",
      "Iteration 42, loss = 0.41405142\n",
      "Iteration 43, loss = 0.41325973\n",
      "Iteration 44, loss = 0.41253282\n",
      "Iteration 45, loss = 0.41182462\n",
      "Iteration 46, loss = 0.41111687\n",
      "Iteration 47, loss = 0.41031844\n",
      "Iteration 48, loss = 0.40954794\n",
      "Iteration 49, loss = 0.40880717\n",
      "Iteration 50, loss = 0.40803530\n",
      "Iteration 51, loss = 0.40727505\n",
      "Iteration 52, loss = 0.40653608\n",
      "Iteration 53, loss = 0.40579026\n",
      "Iteration 54, loss = 0.40500005\n",
      "Iteration 55, loss = 0.40424884\n",
      "Iteration 56, loss = 0.40347347\n",
      "Iteration 57, loss = 0.40272410\n",
      "Iteration 58, loss = 0.40194307\n",
      "Iteration 59, loss = 0.40117789\n",
      "Iteration 60, loss = 0.40042204\n",
      "Iteration 61, loss = 0.39962498\n",
      "Iteration 62, loss = 0.39885691\n",
      "Iteration 63, loss = 0.39808037\n",
      "Iteration 64, loss = 0.39729948\n",
      "Iteration 65, loss = 0.39650730\n",
      "Iteration 66, loss = 0.39572262\n",
      "Iteration 67, loss = 0.39495060\n",
      "Iteration 68, loss = 0.39416096\n",
      "Iteration 69, loss = 0.39337250\n",
      "Iteration 70, loss = 0.39257350\n",
      "Iteration 71, loss = 0.39176970\n",
      "Iteration 72, loss = 0.39099046\n",
      "Iteration 73, loss = 0.39017604\n",
      "Iteration 74, loss = 0.38939171\n",
      "Iteration 75, loss = 0.38862326\n",
      "Iteration 76, loss = 0.38777766\n",
      "Iteration 77, loss = 0.38699020\n",
      "Iteration 78, loss = 0.38618076\n",
      "Iteration 79, loss = 0.38540498\n",
      "Iteration 80, loss = 0.38459678\n",
      "Iteration 81, loss = 0.38377750\n",
      "Iteration 82, loss = 0.38296756\n",
      "Iteration 83, loss = 0.38215854\n",
      "Iteration 84, loss = 0.38135314\n",
      "Iteration 85, loss = 0.38054581\n",
      "Iteration 86, loss = 0.37971623\n",
      "Iteration 87, loss = 0.37894357\n",
      "Iteration 88, loss = 0.37811903\n",
      "Iteration 89, loss = 0.37730007\n",
      "Iteration 90, loss = 0.37652658\n",
      "Iteration 91, loss = 0.37568542\n",
      "Iteration 92, loss = 0.37484734\n",
      "Iteration 93, loss = 0.37405357\n",
      "Iteration 94, loss = 0.37323990\n",
      "Iteration 95, loss = 0.37242907\n",
      "Iteration 96, loss = 0.37161921\n",
      "Iteration 97, loss = 0.37083932\n",
      "Iteration 98, loss = 0.37008054\n",
      "Iteration 99, loss = 0.36919640\n",
      "Iteration 100, loss = 0.36840497\n",
      "Iteration 101, loss = 0.36757927\n",
      "Iteration 102, loss = 0.36678407\n",
      "Iteration 103, loss = 0.36597017\n",
      "Iteration 104, loss = 0.36518088\n",
      "Iteration 105, loss = 0.36437024\n",
      "Iteration 106, loss = 0.36358522\n",
      "Iteration 107, loss = 0.36280521\n",
      "Iteration 108, loss = 0.36196514\n",
      "Iteration 109, loss = 0.36117847\n",
      "Iteration 110, loss = 0.36041638\n",
      "Iteration 111, loss = 0.35960143\n",
      "Iteration 112, loss = 0.35881564\n",
      "Iteration 113, loss = 0.35802132\n",
      "Iteration 114, loss = 0.35724741\n",
      "Iteration 115, loss = 0.35643764\n",
      "Iteration 116, loss = 0.35570347\n",
      "Iteration 117, loss = 0.35489226\n",
      "Iteration 118, loss = 0.35415168\n",
      "Iteration 119, loss = 0.35336945\n",
      "Iteration 120, loss = 0.35266868\n",
      "Iteration 121, loss = 0.35184208\n",
      "Iteration 122, loss = 0.35107164\n",
      "Iteration 123, loss = 0.35031679\n",
      "Iteration 124, loss = 0.34956036\n",
      "Iteration 125, loss = 0.34881593\n",
      "Iteration 126, loss = 0.34805521\n",
      "Iteration 127, loss = 0.34730814\n",
      "Iteration 128, loss = 0.34659391\n",
      "Iteration 129, loss = 0.34584060\n",
      "Iteration 130, loss = 0.34510917\n",
      "Iteration 131, loss = 0.34441218\n",
      "Iteration 132, loss = 0.34365081\n",
      "Iteration 133, loss = 0.34292857\n",
      "Iteration 134, loss = 0.34221261\n",
      "Iteration 135, loss = 0.34151431\n",
      "Iteration 136, loss = 0.34080162\n",
      "Iteration 137, loss = 0.34011560\n",
      "Iteration 138, loss = 0.33949953\n",
      "Iteration 139, loss = 0.33872418\n",
      "Iteration 140, loss = 0.33804543\n",
      "Iteration 141, loss = 0.33737227\n",
      "Iteration 142, loss = 0.33665331\n",
      "Iteration 143, loss = 0.33597867\n",
      "Iteration 144, loss = 0.33530214\n",
      "Iteration 145, loss = 0.33464925\n",
      "Iteration 146, loss = 0.33399040\n",
      "Iteration 147, loss = 0.33332261\n",
      "Iteration 148, loss = 0.33266972\n",
      "Iteration 149, loss = 0.33203501\n",
      "Iteration 150, loss = 0.33138516\n",
      "Iteration 151, loss = 0.33074460\n",
      "Iteration 152, loss = 0.33011132\n",
      "Iteration 153, loss = 0.32952771\n",
      "Iteration 154, loss = 0.32884330\n",
      "Iteration 155, loss = 0.32825352\n",
      "Iteration 156, loss = 0.32763286\n",
      "Iteration 157, loss = 0.32702776\n",
      "Iteration 158, loss = 0.32643282\n",
      "Iteration 159, loss = 0.32582119\n",
      "Iteration 160, loss = 0.32525442\n",
      "Iteration 161, loss = 0.32464609\n",
      "Iteration 162, loss = 0.32405787\n",
      "Iteration 163, loss = 0.32349133\n",
      "Iteration 164, loss = 0.32289573\n",
      "Iteration 165, loss = 0.32234585\n",
      "Iteration 166, loss = 0.32182735\n",
      "Iteration 167, loss = 0.32123710\n",
      "Iteration 168, loss = 0.32066301\n",
      "Iteration 169, loss = 0.32013862\n",
      "Iteration 170, loss = 0.31958432\n",
      "Iteration 171, loss = 0.31906128\n",
      "Iteration 172, loss = 0.31851581\n",
      "Iteration 173, loss = 0.31800394\n",
      "Iteration 174, loss = 0.31749002\n",
      "Iteration 175, loss = 0.31694321\n",
      "Iteration 176, loss = 0.31642982\n",
      "Iteration 177, loss = 0.31593924\n",
      "Iteration 178, loss = 0.31543146\n",
      "Iteration 179, loss = 0.31492794\n",
      "Iteration 180, loss = 0.31448288\n",
      "Iteration 181, loss = 0.31397576\n",
      "Iteration 182, loss = 0.31348798\n",
      "Iteration 183, loss = 0.31297907\n",
      "Iteration 184, loss = 0.31251605\n",
      "Iteration 185, loss = 0.31204445\n",
      "Iteration 186, loss = 0.31163984\n",
      "Iteration 187, loss = 0.31114673\n",
      "Iteration 188, loss = 0.31071029\n",
      "Iteration 189, loss = 0.31024720\n",
      "Iteration 190, loss = 0.30977304\n",
      "Iteration 191, loss = 0.30933935\n",
      "Iteration 192, loss = 0.30890104\n",
      "Iteration 193, loss = 0.30849537\n",
      "Iteration 194, loss = 0.30803742\n",
      "Iteration 195, loss = 0.30764789\n",
      "Iteration 196, loss = 0.30719246\n",
      "Iteration 197, loss = 0.30679352\n",
      "Iteration 198, loss = 0.30641103\n",
      "Iteration 199, loss = 0.30596090\n",
      "Iteration 200, loss = 0.30555675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70928992\n",
      "Iteration 2, loss = 0.48814853\n",
      "Iteration 3, loss = 0.44587314\n",
      "Iteration 4, loss = 0.44229369\n",
      "Iteration 5, loss = 0.44157319\n",
      "Iteration 6, loss = 0.44094109\n",
      "Iteration 7, loss = 0.44003295\n",
      "Iteration 8, loss = 0.43931151\n",
      "Iteration 9, loss = 0.43871837\n",
      "Iteration 10, loss = 0.43785023\n",
      "Iteration 11, loss = 0.43715376\n",
      "Iteration 12, loss = 0.43643176\n",
      "Iteration 13, loss = 0.43572918\n",
      "Iteration 14, loss = 0.43505466\n",
      "Iteration 15, loss = 0.43428715\n",
      "Iteration 16, loss = 0.43360630\n",
      "Iteration 17, loss = 0.43289121\n",
      "Iteration 18, loss = 0.43220344\n",
      "Iteration 19, loss = 0.43147213\n",
      "Iteration 20, loss = 0.43075974\n",
      "Iteration 21, loss = 0.43005872\n",
      "Iteration 22, loss = 0.42938437\n",
      "Iteration 23, loss = 0.42866506\n",
      "Iteration 24, loss = 0.42794978\n",
      "Iteration 25, loss = 0.42726743\n",
      "Iteration 26, loss = 0.42655149\n",
      "Iteration 27, loss = 0.42586653\n",
      "Iteration 28, loss = 0.42516909\n",
      "Iteration 29, loss = 0.42445191\n",
      "Iteration 30, loss = 0.42373462\n",
      "Iteration 31, loss = 0.42308256\n",
      "Iteration 32, loss = 0.42238233\n",
      "Iteration 33, loss = 0.42168684\n",
      "Iteration 34, loss = 0.42092221\n",
      "Iteration 35, loss = 0.42023021\n",
      "Iteration 36, loss = 0.41953189\n",
      "Iteration 37, loss = 0.41882119\n",
      "Iteration 38, loss = 0.41815511\n",
      "Iteration 39, loss = 0.41741290\n",
      "Iteration 40, loss = 0.41671177\n",
      "Iteration 41, loss = 0.41597821\n",
      "Iteration 42, loss = 0.41527605\n",
      "Iteration 43, loss = 0.41454958\n",
      "Iteration 44, loss = 0.41382540\n",
      "Iteration 45, loss = 0.41308123\n",
      "Iteration 46, loss = 0.41238927\n",
      "Iteration 47, loss = 0.41179104\n",
      "Iteration 48, loss = 0.41096735\n",
      "Iteration 49, loss = 0.41023744\n",
      "Iteration 50, loss = 0.40945537\n",
      "Iteration 51, loss = 0.40872784\n",
      "Iteration 52, loss = 0.40802173\n",
      "Iteration 53, loss = 0.40724934\n",
      "Iteration 54, loss = 0.40652717\n",
      "Iteration 55, loss = 0.40577750\n",
      "Iteration 56, loss = 0.40506770\n",
      "Iteration 57, loss = 0.40427981\n",
      "Iteration 58, loss = 0.40353823\n",
      "Iteration 59, loss = 0.40280588\n",
      "Iteration 60, loss = 0.40202716\n",
      "Iteration 61, loss = 0.40134781\n",
      "Iteration 62, loss = 0.40053166\n",
      "Iteration 63, loss = 0.39979818\n",
      "Iteration 64, loss = 0.39902060\n",
      "Iteration 65, loss = 0.39833956\n",
      "Iteration 66, loss = 0.39747740\n",
      "Iteration 67, loss = 0.39673755\n",
      "Iteration 68, loss = 0.39597509\n",
      "Iteration 69, loss = 0.39518444\n",
      "Iteration 70, loss = 0.39446358\n",
      "Iteration 71, loss = 0.39362880\n",
      "Iteration 72, loss = 0.39288405\n",
      "Iteration 73, loss = 0.39209561\n",
      "Iteration 74, loss = 0.39133847\n",
      "Iteration 75, loss = 0.39054769\n",
      "Iteration 76, loss = 0.38974803\n",
      "Iteration 77, loss = 0.38896481\n",
      "Iteration 78, loss = 0.38818068\n",
      "Iteration 79, loss = 0.38738614\n",
      "Iteration 80, loss = 0.38659002\n",
      "Iteration 81, loss = 0.38580894\n",
      "Iteration 82, loss = 0.38508664\n",
      "Iteration 83, loss = 0.38424440\n",
      "Iteration 84, loss = 0.38343721\n",
      "Iteration 85, loss = 0.38265301\n",
      "Iteration 86, loss = 0.38186224\n",
      "Iteration 87, loss = 0.38107004\n",
      "Iteration 88, loss = 0.38026071\n",
      "Iteration 89, loss = 0.37945412\n",
      "Iteration 90, loss = 0.37864553\n",
      "Iteration 91, loss = 0.37788154\n",
      "Iteration 92, loss = 0.37705636\n",
      "Iteration 93, loss = 0.37624470\n",
      "Iteration 94, loss = 0.37548632\n",
      "Iteration 95, loss = 0.37464197\n",
      "Iteration 96, loss = 0.37385950\n",
      "Iteration 97, loss = 0.37310072\n",
      "Iteration 98, loss = 0.37225274\n",
      "Iteration 99, loss = 0.37146427\n",
      "Iteration 100, loss = 0.37065804\n",
      "Iteration 101, loss = 0.36987535\n",
      "Iteration 102, loss = 0.36904647\n",
      "Iteration 103, loss = 0.36829878\n",
      "Iteration 104, loss = 0.36745876\n",
      "Iteration 105, loss = 0.36665976\n",
      "Iteration 106, loss = 0.36588915\n",
      "Iteration 107, loss = 0.36506897\n",
      "Iteration 108, loss = 0.36429400\n",
      "Iteration 109, loss = 0.36348008\n",
      "Iteration 110, loss = 0.36270679\n",
      "Iteration 111, loss = 0.36191555\n",
      "Iteration 112, loss = 0.36112559\n",
      "Iteration 113, loss = 0.36033717\n",
      "Iteration 114, loss = 0.35960793\n",
      "Iteration 115, loss = 0.35883451\n",
      "Iteration 116, loss = 0.35799835\n",
      "Iteration 117, loss = 0.35722384\n",
      "Iteration 118, loss = 0.35645329\n",
      "Iteration 119, loss = 0.35567632\n",
      "Iteration 120, loss = 0.35490998\n",
      "Iteration 121, loss = 0.35414917\n",
      "Iteration 122, loss = 0.35338339\n",
      "Iteration 123, loss = 0.35263669\n",
      "Iteration 124, loss = 0.35186918\n",
      "Iteration 125, loss = 0.35110697\n",
      "Iteration 126, loss = 0.35035592\n",
      "Iteration 127, loss = 0.34961989\n",
      "Iteration 128, loss = 0.34886143\n",
      "Iteration 129, loss = 0.34810384\n",
      "Iteration 130, loss = 0.34741603\n",
      "Iteration 131, loss = 0.34666471\n",
      "Iteration 132, loss = 0.34592042\n",
      "Iteration 133, loss = 0.34521582\n",
      "Iteration 134, loss = 0.34448765\n",
      "Iteration 135, loss = 0.34377308\n",
      "Iteration 136, loss = 0.34301542\n",
      "Iteration 137, loss = 0.34233286\n",
      "Iteration 138, loss = 0.34159746\n",
      "Iteration 139, loss = 0.34094575\n",
      "Iteration 140, loss = 0.34022937\n",
      "Iteration 141, loss = 0.33951914\n",
      "Iteration 142, loss = 0.33881841\n",
      "Iteration 143, loss = 0.33816420\n",
      "Iteration 144, loss = 0.33749453\n",
      "Iteration 145, loss = 0.33677017\n",
      "Iteration 146, loss = 0.33611944\n",
      "Iteration 147, loss = 0.33542118\n",
      "Iteration 148, loss = 0.33478596\n",
      "Iteration 149, loss = 0.33410006\n",
      "Iteration 150, loss = 0.33346948\n",
      "Iteration 151, loss = 0.33285188\n",
      "Iteration 152, loss = 0.33214828\n",
      "Iteration 153, loss = 0.33153318\n",
      "Iteration 154, loss = 0.33087699\n",
      "Iteration 155, loss = 0.33025926\n",
      "Iteration 156, loss = 0.32961587\n",
      "Iteration 157, loss = 0.32900226\n",
      "Iteration 158, loss = 0.32838274\n",
      "Iteration 159, loss = 0.32780017\n",
      "Iteration 160, loss = 0.32715573\n",
      "Iteration 161, loss = 0.32655856\n",
      "Iteration 162, loss = 0.32600311\n",
      "Iteration 163, loss = 0.32535380\n",
      "Iteration 164, loss = 0.32476816\n",
      "Iteration 165, loss = 0.32422552\n",
      "Iteration 166, loss = 0.32367121\n",
      "Iteration 167, loss = 0.32305205\n",
      "Iteration 168, loss = 0.32248305\n",
      "Iteration 169, loss = 0.32190623\n",
      "Iteration 170, loss = 0.32136653\n",
      "Iteration 171, loss = 0.32080528\n",
      "Iteration 172, loss = 0.32025687\n",
      "Iteration 173, loss = 0.31974321\n",
      "Iteration 174, loss = 0.31918578\n",
      "Iteration 175, loss = 0.31865074\n",
      "Iteration 176, loss = 0.31812949\n",
      "Iteration 177, loss = 0.31758591\n",
      "Iteration 178, loss = 0.31705481\n",
      "Iteration 179, loss = 0.31654521\n",
      "Iteration 180, loss = 0.31604030\n",
      "Iteration 181, loss = 0.31560388\n",
      "Iteration 182, loss = 0.31504266\n",
      "Iteration 183, loss = 0.31453214\n",
      "Iteration 184, loss = 0.31409379\n",
      "Iteration 185, loss = 0.31356406\n",
      "Iteration 186, loss = 0.31308485\n",
      "Iteration 187, loss = 0.31261580\n",
      "Iteration 188, loss = 0.31217890\n",
      "Iteration 189, loss = 0.31165047\n",
      "Iteration 190, loss = 0.31132365\n",
      "Iteration 191, loss = 0.31081411\n",
      "Iteration 192, loss = 0.31034187\n",
      "Iteration 193, loss = 0.30985256\n",
      "Iteration 194, loss = 0.30940142\n",
      "Iteration 195, loss = 0.30895786\n",
      "Iteration 196, loss = 0.30852132\n",
      "Iteration 197, loss = 0.30809149\n",
      "Iteration 198, loss = 0.30767413\n",
      "Iteration 199, loss = 0.30724158\n",
      "Iteration 200, loss = 0.30687068\n",
      "Iteration 1, loss = 0.70951243\n",
      "Iteration 2, loss = 0.48863949\n",
      "Iteration 3, loss = 0.44541185\n",
      "Iteration 4, loss = 0.44225827\n",
      "Iteration 5, loss = 0.44169612\n",
      "Iteration 6, loss = 0.44106989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.44020513\n",
      "Iteration 8, loss = 0.43946797\n",
      "Iteration 9, loss = 0.43880519\n",
      "Iteration 10, loss = 0.43800326\n",
      "Iteration 11, loss = 0.43731916\n",
      "Iteration 12, loss = 0.43656089\n",
      "Iteration 13, loss = 0.43589551\n",
      "Iteration 14, loss = 0.43516092\n",
      "Iteration 15, loss = 0.43441098\n",
      "Iteration 16, loss = 0.43372531\n",
      "Iteration 17, loss = 0.43301471\n",
      "Iteration 18, loss = 0.43230206\n",
      "Iteration 19, loss = 0.43159763\n",
      "Iteration 20, loss = 0.43089582\n",
      "Iteration 21, loss = 0.43018302\n",
      "Iteration 22, loss = 0.42956293\n",
      "Iteration 23, loss = 0.42881451\n",
      "Iteration 24, loss = 0.42809063\n",
      "Iteration 25, loss = 0.42740329\n",
      "Iteration 26, loss = 0.42668036\n",
      "Iteration 27, loss = 0.42598272\n",
      "Iteration 28, loss = 0.42527863\n",
      "Iteration 29, loss = 0.42459002\n",
      "Iteration 30, loss = 0.42386207\n",
      "Iteration 31, loss = 0.42317716\n",
      "Iteration 32, loss = 0.42248434\n",
      "Iteration 33, loss = 0.42180196\n",
      "Iteration 34, loss = 0.42106009\n",
      "Iteration 35, loss = 0.42035742\n",
      "Iteration 36, loss = 0.41967573\n",
      "Iteration 37, loss = 0.41896222\n",
      "Iteration 38, loss = 0.41827855\n",
      "Iteration 39, loss = 0.41755311\n",
      "Iteration 40, loss = 0.41679486\n",
      "Iteration 41, loss = 0.41609801\n",
      "Iteration 42, loss = 0.41538004\n",
      "Iteration 43, loss = 0.41466958\n",
      "Iteration 44, loss = 0.41395592\n",
      "Iteration 45, loss = 0.41323067\n",
      "Iteration 46, loss = 0.41254700\n",
      "Iteration 47, loss = 0.41195293\n",
      "Iteration 48, loss = 0.41106922\n",
      "Iteration 49, loss = 0.41037030\n",
      "Iteration 50, loss = 0.40958359\n",
      "Iteration 51, loss = 0.40886119\n",
      "Iteration 52, loss = 0.40817217\n",
      "Iteration 53, loss = 0.40740015\n",
      "Iteration 54, loss = 0.40668128\n",
      "Iteration 55, loss = 0.40591561\n",
      "Iteration 56, loss = 0.40521351\n",
      "Iteration 57, loss = 0.40442696\n",
      "Iteration 58, loss = 0.40368960\n",
      "Iteration 59, loss = 0.40295454\n",
      "Iteration 60, loss = 0.40219861\n",
      "Iteration 61, loss = 0.40144074\n",
      "Iteration 62, loss = 0.40068120\n",
      "Iteration 63, loss = 0.39996778\n",
      "Iteration 64, loss = 0.39917998\n",
      "Iteration 65, loss = 0.39847525\n",
      "Iteration 66, loss = 0.39766895\n",
      "Iteration 67, loss = 0.39690513\n",
      "Iteration 68, loss = 0.39612845\n",
      "Iteration 69, loss = 0.39536076\n",
      "Iteration 70, loss = 0.39461186\n",
      "Iteration 71, loss = 0.39384871\n",
      "Iteration 72, loss = 0.39304806\n",
      "Iteration 73, loss = 0.39228875\n",
      "Iteration 74, loss = 0.39150510\n",
      "Iteration 75, loss = 0.39074436\n",
      "Iteration 76, loss = 0.38994071\n",
      "Iteration 77, loss = 0.38915733\n",
      "Iteration 78, loss = 0.38836962\n",
      "Iteration 79, loss = 0.38757144\n",
      "Iteration 80, loss = 0.38678682\n",
      "Iteration 81, loss = 0.38601665\n",
      "Iteration 82, loss = 0.38524311\n",
      "Iteration 83, loss = 0.38445995\n",
      "Iteration 84, loss = 0.38370358\n",
      "Iteration 85, loss = 0.38283567\n",
      "Iteration 86, loss = 0.38209098\n",
      "Iteration 87, loss = 0.38128373\n",
      "Iteration 88, loss = 0.38048433\n",
      "Iteration 89, loss = 0.37967632\n",
      "Iteration 90, loss = 0.37887183\n",
      "Iteration 91, loss = 0.37810806\n",
      "Iteration 92, loss = 0.37728349\n",
      "Iteration 93, loss = 0.37652670\n",
      "Iteration 94, loss = 0.37570834\n",
      "Iteration 95, loss = 0.37489975\n",
      "Iteration 96, loss = 0.37410240\n",
      "Iteration 97, loss = 0.37332496\n",
      "Iteration 98, loss = 0.37252153\n",
      "Iteration 99, loss = 0.37172518\n",
      "Iteration 100, loss = 0.37092913\n",
      "Iteration 101, loss = 0.37014112\n",
      "Iteration 102, loss = 0.36934133\n",
      "Iteration 103, loss = 0.36859670\n",
      "Iteration 104, loss = 0.36773702\n",
      "Iteration 105, loss = 0.36694826\n",
      "Iteration 106, loss = 0.36619331\n",
      "Iteration 107, loss = 0.36539688\n",
      "Iteration 108, loss = 0.36460304\n",
      "Iteration 109, loss = 0.36380354\n",
      "Iteration 110, loss = 0.36302500\n",
      "Iteration 111, loss = 0.36225328\n",
      "Iteration 112, loss = 0.36146750\n",
      "Iteration 113, loss = 0.36069272\n",
      "Iteration 114, loss = 0.35990797\n",
      "Iteration 115, loss = 0.35913066\n",
      "Iteration 116, loss = 0.35836425\n",
      "Iteration 117, loss = 0.35758171\n",
      "Iteration 118, loss = 0.35679751\n",
      "Iteration 119, loss = 0.35605113\n",
      "Iteration 120, loss = 0.35528121\n",
      "Iteration 121, loss = 0.35452849\n",
      "Iteration 122, loss = 0.35377079\n",
      "Iteration 123, loss = 0.35301352\n",
      "Iteration 124, loss = 0.35226430\n",
      "Iteration 125, loss = 0.35154260\n",
      "Iteration 126, loss = 0.35081989\n",
      "Iteration 127, loss = 0.35003185\n",
      "Iteration 128, loss = 0.34927163\n",
      "Iteration 129, loss = 0.34853138\n",
      "Iteration 130, loss = 0.34784097\n",
      "Iteration 131, loss = 0.34707308\n",
      "Iteration 132, loss = 0.34638501\n",
      "Iteration 133, loss = 0.34563040\n",
      "Iteration 134, loss = 0.34492427\n",
      "Iteration 135, loss = 0.34420300\n",
      "Iteration 136, loss = 0.34350023\n",
      "Iteration 137, loss = 0.34278915\n",
      "Iteration 138, loss = 0.34207926\n",
      "Iteration 139, loss = 0.34139406\n",
      "Iteration 140, loss = 0.34070011\n",
      "Iteration 141, loss = 0.34000271\n",
      "Iteration 142, loss = 0.33934755\n",
      "Iteration 143, loss = 0.33865476\n",
      "Iteration 144, loss = 0.33797786\n",
      "Iteration 145, loss = 0.33728804\n",
      "Iteration 146, loss = 0.33661727\n",
      "Iteration 147, loss = 0.33594484\n",
      "Iteration 148, loss = 0.33531879\n",
      "Iteration 149, loss = 0.33463917\n",
      "Iteration 150, loss = 0.33401452\n",
      "Iteration 151, loss = 0.33345953\n",
      "Iteration 152, loss = 0.33269564\n",
      "Iteration 153, loss = 0.33209345\n",
      "Iteration 154, loss = 0.33146507\n",
      "Iteration 155, loss = 0.33081551\n",
      "Iteration 156, loss = 0.33019516\n",
      "Iteration 157, loss = 0.32958229\n",
      "Iteration 158, loss = 0.32898209\n",
      "Iteration 159, loss = 0.32837399\n",
      "Iteration 160, loss = 0.32776066\n",
      "Iteration 161, loss = 0.32719575\n",
      "Iteration 162, loss = 0.32661394\n",
      "Iteration 163, loss = 0.32597095\n",
      "Iteration 164, loss = 0.32540609\n",
      "Iteration 165, loss = 0.32485289\n",
      "Iteration 166, loss = 0.32425601\n",
      "Iteration 167, loss = 0.32369598\n",
      "Iteration 168, loss = 0.32314370\n",
      "Iteration 169, loss = 0.32257200\n",
      "Iteration 170, loss = 0.32203303\n",
      "Iteration 171, loss = 0.32151798\n",
      "Iteration 172, loss = 0.32093871\n",
      "Iteration 173, loss = 0.32044892\n",
      "Iteration 174, loss = 0.31994051\n",
      "Iteration 175, loss = 0.31936974\n",
      "Iteration 176, loss = 0.31881147\n",
      "Iteration 177, loss = 0.31834573\n",
      "Iteration 178, loss = 0.31778515\n",
      "Iteration 179, loss = 0.31727892\n",
      "Iteration 180, loss = 0.31678900\n",
      "Iteration 181, loss = 0.31630501\n",
      "Iteration 182, loss = 0.31579220\n",
      "Iteration 183, loss = 0.31529401\n",
      "Iteration 184, loss = 0.31487520\n",
      "Iteration 185, loss = 0.31435005\n",
      "Iteration 186, loss = 0.31386615\n",
      "Iteration 187, loss = 0.31340391\n",
      "Iteration 188, loss = 0.31292846\n",
      "Iteration 189, loss = 0.31246016\n",
      "Iteration 190, loss = 0.31209946\n",
      "Iteration 191, loss = 0.31162657\n",
      "Iteration 192, loss = 0.31114665\n",
      "Iteration 193, loss = 0.31068869\n",
      "Iteration 194, loss = 0.31025065\n",
      "Iteration 195, loss = 0.30979352\n",
      "Iteration 196, loss = 0.30936664\n",
      "Iteration 197, loss = 0.30894942\n",
      "Iteration 198, loss = 0.30852844\n",
      "Iteration 199, loss = 0.30811539\n",
      "Iteration 200, loss = 0.30772157\n",
      "Iteration 1, loss = 0.68975465\n",
      "Iteration 2, loss = 0.48836598\n",
      "Iteration 3, loss = 0.41557887\n",
      "Iteration 4, loss = 0.39063160\n",
      "Iteration 5, loss = 0.37608027\n",
      "Iteration 6, loss = 0.36390155\n",
      "Iteration 7, loss = 0.35329455\n",
      "Iteration 8, loss = 0.34394750\n",
      "Iteration 9, loss = 0.33551455\n",
      "Iteration 10, loss = 0.32807508\n",
      "Iteration 11, loss = 0.32152240\n",
      "Iteration 12, loss = 0.31564259\n",
      "Iteration 13, loss = 0.31053298\n",
      "Iteration 14, loss = 0.30593462\n",
      "Iteration 15, loss = 0.30162462\n",
      "Iteration 16, loss = 0.29807238\n",
      "Iteration 17, loss = 0.29466139\n",
      "Iteration 18, loss = 0.29174924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.28895254\n",
      "Iteration 20, loss = 0.28656546\n",
      "Iteration 21, loss = 0.28425093\n",
      "Iteration 22, loss = 0.28227695\n",
      "Iteration 23, loss = 0.28021849\n",
      "Iteration 24, loss = 0.27832114\n",
      "Iteration 25, loss = 0.27668710\n",
      "Iteration 26, loss = 0.27510271\n",
      "Iteration 27, loss = 0.27356097\n",
      "Iteration 28, loss = 0.27209061\n",
      "Iteration 29, loss = 0.27064618\n",
      "Iteration 30, loss = 0.26929982\n",
      "Iteration 31, loss = 0.26797408\n",
      "Iteration 32, loss = 0.26675318\n",
      "Iteration 33, loss = 0.26557292\n",
      "Iteration 34, loss = 0.26442489\n",
      "Iteration 35, loss = 0.26339552\n",
      "Iteration 36, loss = 0.26237654\n",
      "Iteration 37, loss = 0.26090898\n",
      "Iteration 38, loss = 0.25991750\n",
      "Iteration 39, loss = 0.25882663\n",
      "Iteration 40, loss = 0.25791242\n",
      "Iteration 41, loss = 0.25681358\n",
      "Iteration 42, loss = 0.25606194\n",
      "Iteration 43, loss = 0.25480365\n",
      "Iteration 44, loss = 0.25394671\n",
      "Iteration 45, loss = 0.25311682\n",
      "Iteration 46, loss = 0.25209171\n",
      "Iteration 47, loss = 0.25150928\n",
      "Iteration 48, loss = 0.25040373\n",
      "Iteration 49, loss = 0.24957675\n",
      "Iteration 50, loss = 0.24862666\n",
      "Iteration 51, loss = 0.24796307\n",
      "Iteration 52, loss = 0.24736855\n",
      "Iteration 53, loss = 0.24641172\n",
      "Iteration 54, loss = 0.24568414\n",
      "Iteration 55, loss = 0.24505062\n",
      "Iteration 56, loss = 0.24433218\n",
      "Iteration 57, loss = 0.24390348\n",
      "Iteration 58, loss = 0.24319989\n",
      "Iteration 59, loss = 0.24251306\n",
      "Iteration 60, loss = 0.24210421\n",
      "Iteration 61, loss = 0.24146451\n",
      "Iteration 62, loss = 0.24100609\n",
      "Iteration 63, loss = 0.24066626\n",
      "Iteration 64, loss = 0.24005230\n",
      "Iteration 65, loss = 0.23955356\n",
      "Iteration 66, loss = 0.23934324\n",
      "Iteration 67, loss = 0.23866275\n",
      "Iteration 68, loss = 0.23824765\n",
      "Iteration 69, loss = 0.23815472\n",
      "Iteration 70, loss = 0.23753175\n",
      "Iteration 71, loss = 0.23748687\n",
      "Iteration 72, loss = 0.23695954\n",
      "Iteration 73, loss = 0.23658943\n",
      "Iteration 74, loss = 0.23619916\n",
      "Iteration 75, loss = 0.23578577\n",
      "Iteration 76, loss = 0.23568875\n",
      "Iteration 77, loss = 0.23556755\n",
      "Iteration 78, loss = 0.23512379\n",
      "Iteration 79, loss = 0.23482117\n",
      "Iteration 80, loss = 0.23450324\n",
      "Iteration 81, loss = 0.23423730\n",
      "Iteration 82, loss = 0.23431093\n",
      "Iteration 83, loss = 0.23346529\n",
      "Iteration 84, loss = 0.23341917\n",
      "Iteration 85, loss = 0.23319165\n",
      "Iteration 86, loss = 0.23318066\n",
      "Iteration 87, loss = 0.23259621\n",
      "Iteration 88, loss = 0.23245502\n",
      "Iteration 89, loss = 0.23197886\n",
      "Iteration 90, loss = 0.23196561\n",
      "Iteration 91, loss = 0.23149631\n",
      "Iteration 92, loss = 0.23146108\n",
      "Iteration 93, loss = 0.23098058\n",
      "Iteration 94, loss = 0.23071351\n",
      "Iteration 95, loss = 0.23033918\n",
      "Iteration 96, loss = 0.23012863\n",
      "Iteration 97, loss = 0.22983073\n",
      "Iteration 98, loss = 0.22932986\n",
      "Iteration 99, loss = 0.22904558\n",
      "Iteration 100, loss = 0.22899693\n",
      "Iteration 101, loss = 0.22863858\n",
      "Iteration 102, loss = 0.22805604\n",
      "Iteration 103, loss = 0.22781439\n",
      "Iteration 104, loss = 0.22761901\n",
      "Iteration 105, loss = 0.22711450\n",
      "Iteration 106, loss = 0.22680055\n",
      "Iteration 107, loss = 0.22664140\n",
      "Iteration 108, loss = 0.22609673\n",
      "Iteration 109, loss = 0.22563714\n",
      "Iteration 110, loss = 0.22560886\n",
      "Iteration 111, loss = 0.22494351\n",
      "Iteration 112, loss = 0.22455258\n",
      "Iteration 113, loss = 0.22422029\n",
      "Iteration 114, loss = 0.22366702\n",
      "Iteration 115, loss = 0.22350547\n",
      "Iteration 116, loss = 0.22304208\n",
      "Iteration 117, loss = 0.22266332\n",
      "Iteration 118, loss = 0.22198823\n",
      "Iteration 119, loss = 0.22176599\n",
      "Iteration 120, loss = 0.22126434\n",
      "Iteration 121, loss = 0.22082565\n",
      "Iteration 122, loss = 0.22041719\n",
      "Iteration 123, loss = 0.21979251\n",
      "Iteration 124, loss = 0.21943033\n",
      "Iteration 125, loss = 0.21908481\n",
      "Iteration 126, loss = 0.21858113\n",
      "Iteration 127, loss = 0.21800698\n",
      "Iteration 128, loss = 0.21750389\n",
      "Iteration 129, loss = 0.21717246\n",
      "Iteration 130, loss = 0.21657470\n",
      "Iteration 131, loss = 0.21624621\n",
      "Iteration 132, loss = 0.21581850\n",
      "Iteration 133, loss = 0.21511735\n",
      "Iteration 134, loss = 0.21473162\n",
      "Iteration 135, loss = 0.21413633\n",
      "Iteration 136, loss = 0.21428339\n",
      "Iteration 137, loss = 0.21310858\n",
      "Iteration 138, loss = 0.21255978\n",
      "Iteration 139, loss = 0.21247246\n",
      "Iteration 140, loss = 0.21152839\n",
      "Iteration 141, loss = 0.21099415\n",
      "Iteration 142, loss = 0.21066143\n",
      "Iteration 143, loss = 0.21006147\n",
      "Iteration 144, loss = 0.20938199\n",
      "Iteration 145, loss = 0.20889006\n",
      "Iteration 146, loss = 0.20838016\n",
      "Iteration 147, loss = 0.20792375\n",
      "Iteration 148, loss = 0.20739714\n",
      "Iteration 149, loss = 0.20667757\n",
      "Iteration 150, loss = 0.20618974\n",
      "Iteration 151, loss = 0.20569071\n",
      "Iteration 152, loss = 0.20509354\n",
      "Iteration 153, loss = 0.20462683\n",
      "Iteration 154, loss = 0.20384395\n",
      "Iteration 155, loss = 0.20347589\n",
      "Iteration 156, loss = 0.20278303\n",
      "Iteration 157, loss = 0.20226716\n",
      "Iteration 158, loss = 0.20178061\n",
      "Iteration 159, loss = 0.20131401\n",
      "Iteration 160, loss = 0.20081696\n",
      "Iteration 161, loss = 0.19997733\n",
      "Iteration 162, loss = 0.19945655\n",
      "Iteration 163, loss = 0.19869112\n",
      "Iteration 164, loss = 0.19818151\n",
      "Iteration 165, loss = 0.19771607\n",
      "Iteration 166, loss = 0.19716424\n",
      "Iteration 167, loss = 0.19684431\n",
      "Iteration 168, loss = 0.19585485\n",
      "Iteration 169, loss = 0.19553956\n",
      "Iteration 170, loss = 0.19481645\n",
      "Iteration 171, loss = 0.19418210\n",
      "Iteration 172, loss = 0.19350556\n",
      "Iteration 173, loss = 0.19292553\n",
      "Iteration 174, loss = 0.19249483\n",
      "Iteration 175, loss = 0.19168049\n",
      "Iteration 176, loss = 0.19129733\n",
      "Iteration 177, loss = 0.19079825\n",
      "Iteration 178, loss = 0.19016816\n",
      "Iteration 179, loss = 0.18974191\n",
      "Iteration 180, loss = 0.18894085\n",
      "Iteration 181, loss = 0.18818000\n",
      "Iteration 182, loss = 0.18791547\n",
      "Iteration 183, loss = 0.18712817\n",
      "Iteration 184, loss = 0.18666261\n",
      "Iteration 185, loss = 0.18611759\n",
      "Iteration 186, loss = 0.18547436\n",
      "Iteration 187, loss = 0.18481853\n",
      "Iteration 188, loss = 0.18423536\n",
      "Iteration 189, loss = 0.18375272\n",
      "Iteration 190, loss = 0.18319573\n",
      "Iteration 191, loss = 0.18228787\n",
      "Iteration 192, loss = 0.18197195\n",
      "Iteration 193, loss = 0.18126951\n",
      "Iteration 194, loss = 0.18090411\n",
      "Iteration 195, loss = 0.18022092\n",
      "Iteration 196, loss = 0.17968414\n",
      "Iteration 197, loss = 0.17918094\n",
      "Iteration 198, loss = 0.17867037\n",
      "Iteration 199, loss = 0.17815868\n",
      "Iteration 200, loss = 0.17739607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68947204\n",
      "Iteration 2, loss = 0.48862988\n",
      "Iteration 3, loss = 0.41525731\n",
      "Iteration 4, loss = 0.39028092\n",
      "Iteration 5, loss = 0.37534252\n",
      "Iteration 6, loss = 0.36293483\n",
      "Iteration 7, loss = 0.35237258\n",
      "Iteration 8, loss = 0.34296606\n",
      "Iteration 9, loss = 0.33451955\n",
      "Iteration 10, loss = 0.32725328\n",
      "Iteration 11, loss = 0.32059690\n",
      "Iteration 12, loss = 0.31482047\n",
      "Iteration 13, loss = 0.30967880\n",
      "Iteration 14, loss = 0.30502520\n",
      "Iteration 15, loss = 0.30079610\n",
      "Iteration 16, loss = 0.29715346\n",
      "Iteration 17, loss = 0.29390329\n",
      "Iteration 18, loss = 0.29081878\n",
      "Iteration 19, loss = 0.28812573\n",
      "Iteration 20, loss = 0.28550483\n",
      "Iteration 21, loss = 0.28328897\n",
      "Iteration 22, loss = 0.28118309\n",
      "Iteration 23, loss = 0.27919679\n",
      "Iteration 24, loss = 0.27734224\n",
      "Iteration 25, loss = 0.27561236\n",
      "Iteration 26, loss = 0.27408949\n",
      "Iteration 27, loss = 0.27251475\n",
      "Iteration 28, loss = 0.27110130\n",
      "Iteration 29, loss = 0.26965506\n",
      "Iteration 30, loss = 0.26832066\n",
      "Iteration 31, loss = 0.26698603\n",
      "Iteration 32, loss = 0.26582727\n",
      "Iteration 33, loss = 0.26472430\n",
      "Iteration 34, loss = 0.26356610\n",
      "Iteration 35, loss = 0.26249198\n",
      "Iteration 36, loss = 0.26124123\n",
      "Iteration 37, loss = 0.26013638\n",
      "Iteration 38, loss = 0.25905868\n",
      "Iteration 39, loss = 0.25805276\n",
      "Iteration 40, loss = 0.25713282\n",
      "Iteration 41, loss = 0.25610381\n",
      "Iteration 42, loss = 0.25514737\n",
      "Iteration 43, loss = 0.25414268\n",
      "Iteration 44, loss = 0.25324654\n",
      "Iteration 45, loss = 0.25238451\n",
      "Iteration 46, loss = 0.25167560\n",
      "Iteration 47, loss = 0.25080734\n",
      "Iteration 48, loss = 0.24985737\n",
      "Iteration 49, loss = 0.24901675\n",
      "Iteration 50, loss = 0.24804049\n",
      "Iteration 51, loss = 0.24732584\n",
      "Iteration 52, loss = 0.24673521\n",
      "Iteration 53, loss = 0.24576801\n",
      "Iteration 54, loss = 0.24517434\n",
      "Iteration 55, loss = 0.24450812\n",
      "Iteration 56, loss = 0.24373022\n",
      "Iteration 57, loss = 0.24325047\n",
      "Iteration 58, loss = 0.24278900\n",
      "Iteration 59, loss = 0.24199068\n",
      "Iteration 60, loss = 0.24152680\n",
      "Iteration 61, loss = 0.24093628\n",
      "Iteration 62, loss = 0.24048933\n",
      "Iteration 63, loss = 0.23991427\n",
      "Iteration 64, loss = 0.23943239\n",
      "Iteration 65, loss = 0.23886300\n",
      "Iteration 66, loss = 0.23867928\n",
      "Iteration 67, loss = 0.23803284\n",
      "Iteration 68, loss = 0.23773727\n",
      "Iteration 69, loss = 0.23734482\n",
      "Iteration 70, loss = 0.23695230\n",
      "Iteration 71, loss = 0.23672622\n",
      "Iteration 72, loss = 0.23647003\n",
      "Iteration 73, loss = 0.23595173\n",
      "Iteration 74, loss = 0.23565680\n",
      "Iteration 75, loss = 0.23547128\n",
      "Iteration 76, loss = 0.23517205\n",
      "Iteration 77, loss = 0.23500315\n",
      "Iteration 78, loss = 0.23476598\n",
      "Iteration 79, loss = 0.23469427\n",
      "Iteration 80, loss = 0.23426250\n",
      "Iteration 81, loss = 0.23384555\n",
      "Iteration 82, loss = 0.23392544\n",
      "Iteration 83, loss = 0.23345288\n",
      "Iteration 84, loss = 0.23326055\n",
      "Iteration 85, loss = 0.23312968\n",
      "Iteration 86, loss = 0.23299448\n",
      "Iteration 87, loss = 0.23272646\n",
      "Iteration 88, loss = 0.23258382\n",
      "Iteration 89, loss = 0.23226061\n",
      "Iteration 90, loss = 0.23238758\n",
      "Iteration 91, loss = 0.23190887\n",
      "Iteration 92, loss = 0.23159959\n",
      "Iteration 93, loss = 0.23139012\n",
      "Iteration 94, loss = 0.23120856\n",
      "Iteration 95, loss = 0.23098586\n",
      "Iteration 96, loss = 0.23066610\n",
      "Iteration 97, loss = 0.23077314\n",
      "Iteration 98, loss = 0.23047699\n",
      "Iteration 99, loss = 0.23005495\n",
      "Iteration 100, loss = 0.23004733\n",
      "Iteration 101, loss = 0.22962817\n",
      "Iteration 102, loss = 0.22930341\n",
      "Iteration 103, loss = 0.22918897\n",
      "Iteration 104, loss = 0.22893671\n",
      "Iteration 105, loss = 0.22878946\n",
      "Iteration 106, loss = 0.22851671\n",
      "Iteration 107, loss = 0.22841024\n",
      "Iteration 108, loss = 0.22798020\n",
      "Iteration 109, loss = 0.22766249\n",
      "Iteration 110, loss = 0.22748241\n",
      "Iteration 111, loss = 0.22708495\n",
      "Iteration 112, loss = 0.22675171\n",
      "Iteration 113, loss = 0.22661665\n",
      "Iteration 114, loss = 0.22626156\n",
      "Iteration 115, loss = 0.22606437\n",
      "Iteration 116, loss = 0.22562192\n",
      "Iteration 117, loss = 0.22529888\n",
      "Iteration 118, loss = 0.22486574\n",
      "Iteration 119, loss = 0.22464423\n",
      "Iteration 120, loss = 0.22459233\n",
      "Iteration 121, loss = 0.22383203\n",
      "Iteration 122, loss = 0.22372508\n",
      "Iteration 123, loss = 0.22320280\n",
      "Iteration 124, loss = 0.22300120\n",
      "Iteration 125, loss = 0.22286004\n",
      "Iteration 126, loss = 0.22214318\n",
      "Iteration 127, loss = 0.22186218\n",
      "Iteration 128, loss = 0.22132724\n",
      "Iteration 129, loss = 0.22112566\n",
      "Iteration 130, loss = 0.22065871\n",
      "Iteration 131, loss = 0.22055277\n",
      "Iteration 132, loss = 0.21996067\n",
      "Iteration 133, loss = 0.21964932\n",
      "Iteration 134, loss = 0.21922160\n",
      "Iteration 135, loss = 0.21874121\n",
      "Iteration 136, loss = 0.21824277\n",
      "Iteration 137, loss = 0.21816661\n",
      "Iteration 138, loss = 0.21748049\n",
      "Iteration 139, loss = 0.21719788\n",
      "Iteration 140, loss = 0.21637497\n",
      "Iteration 141, loss = 0.21625618\n",
      "Iteration 142, loss = 0.21588576\n",
      "Iteration 143, loss = 0.21552528\n",
      "Iteration 144, loss = 0.21488559\n",
      "Iteration 145, loss = 0.21451015\n",
      "Iteration 146, loss = 0.21401680\n",
      "Iteration 147, loss = 0.21356081\n",
      "Iteration 148, loss = 0.21331372\n",
      "Iteration 149, loss = 0.21264869\n",
      "Iteration 150, loss = 0.21229449\n",
      "Iteration 151, loss = 0.21161365\n",
      "Iteration 152, loss = 0.21123809\n",
      "Iteration 153, loss = 0.21072557\n",
      "Iteration 154, loss = 0.21007134\n",
      "Iteration 155, loss = 0.20968805\n",
      "Iteration 156, loss = 0.20913456\n",
      "Iteration 157, loss = 0.20854963\n",
      "Iteration 158, loss = 0.20836538\n",
      "Iteration 159, loss = 0.20778990\n",
      "Iteration 160, loss = 0.20735527\n",
      "Iteration 161, loss = 0.20666159\n",
      "Iteration 162, loss = 0.20619621\n",
      "Iteration 163, loss = 0.20575365\n",
      "Iteration 164, loss = 0.20517396\n",
      "Iteration 165, loss = 0.20447923\n",
      "Iteration 166, loss = 0.20403583\n",
      "Iteration 167, loss = 0.20364205\n",
      "Iteration 168, loss = 0.20303492\n",
      "Iteration 169, loss = 0.20275778\n",
      "Iteration 170, loss = 0.20209348\n",
      "Iteration 171, loss = 0.20150869\n",
      "Iteration 172, loss = 0.20094215\n",
      "Iteration 173, loss = 0.20036873\n",
      "Iteration 174, loss = 0.19986879\n",
      "Iteration 175, loss = 0.19924363\n",
      "Iteration 176, loss = 0.19873878\n",
      "Iteration 177, loss = 0.19824938\n",
      "Iteration 178, loss = 0.19768177\n",
      "Iteration 179, loss = 0.19732446\n",
      "Iteration 180, loss = 0.19693849\n",
      "Iteration 181, loss = 0.19612455\n",
      "Iteration 182, loss = 0.19585628\n",
      "Iteration 183, loss = 0.19513806\n",
      "Iteration 184, loss = 0.19454585\n",
      "Iteration 185, loss = 0.19399807\n",
      "Iteration 186, loss = 0.19346659\n",
      "Iteration 187, loss = 0.19285513\n",
      "Iteration 188, loss = 0.19223795\n",
      "Iteration 189, loss = 0.19166651\n",
      "Iteration 190, loss = 0.19133009\n",
      "Iteration 191, loss = 0.19050495\n",
      "Iteration 192, loss = 0.18995706\n",
      "Iteration 193, loss = 0.18949654\n",
      "Iteration 194, loss = 0.18887898\n",
      "Iteration 195, loss = 0.18840418\n",
      "Iteration 196, loss = 0.18781685\n",
      "Iteration 197, loss = 0.18736913\n",
      "Iteration 198, loss = 0.18677583\n",
      "Iteration 199, loss = 0.18621008\n",
      "Iteration 200, loss = 0.18549447\n",
      "Iteration 1, loss = 0.68840086\n",
      "Iteration 2, loss = 0.48756835\n",
      "Iteration 3, loss = 0.41426843\n",
      "Iteration 4, loss = 0.39000071\n",
      "Iteration 5, loss = 0.37509824\n",
      "Iteration 6, loss = 0.36278993\n",
      "Iteration 7, loss = 0.35220539\n",
      "Iteration 8, loss = 0.34285013\n",
      "Iteration 9, loss = 0.33435979\n",
      "Iteration 10, loss = 0.32717173\n",
      "Iteration 11, loss = 0.32066066\n",
      "Iteration 12, loss = 0.31476750\n",
      "Iteration 13, loss = 0.30997194\n",
      "Iteration 14, loss = 0.30532555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.30116122\n",
      "Iteration 16, loss = 0.29763672\n",
      "Iteration 17, loss = 0.29458842\n",
      "Iteration 18, loss = 0.29166445\n",
      "Iteration 19, loss = 0.28909337\n",
      "Iteration 20, loss = 0.28658948\n",
      "Iteration 21, loss = 0.28453136\n",
      "Iteration 22, loss = 0.28270433\n",
      "Iteration 23, loss = 0.28072895\n",
      "Iteration 24, loss = 0.27904593\n",
      "Iteration 25, loss = 0.27741068\n",
      "Iteration 26, loss = 0.27598263\n",
      "Iteration 27, loss = 0.27457745\n",
      "Iteration 28, loss = 0.27328161\n",
      "Iteration 29, loss = 0.27201141\n",
      "Iteration 30, loss = 0.27074682\n",
      "Iteration 31, loss = 0.26962774\n",
      "Iteration 32, loss = 0.26859217\n",
      "Iteration 33, loss = 0.26750748\n",
      "Iteration 34, loss = 0.26642492\n",
      "Iteration 35, loss = 0.26542752\n",
      "Iteration 36, loss = 0.26438269\n",
      "Iteration 37, loss = 0.26341793\n",
      "Iteration 38, loss = 0.26244930\n",
      "Iteration 39, loss = 0.26145011\n",
      "Iteration 40, loss = 0.26055875\n",
      "Iteration 41, loss = 0.25977581\n",
      "Iteration 42, loss = 0.25890945\n",
      "Iteration 43, loss = 0.25785737\n",
      "Iteration 44, loss = 0.25713349\n",
      "Iteration 45, loss = 0.25629149\n",
      "Iteration 46, loss = 0.25566991\n",
      "Iteration 47, loss = 0.25474050\n",
      "Iteration 48, loss = 0.25401122\n",
      "Iteration 49, loss = 0.25318785\n",
      "Iteration 50, loss = 0.25249690\n",
      "Iteration 51, loss = 0.25174424\n",
      "Iteration 52, loss = 0.25104028\n",
      "Iteration 53, loss = 0.25038311\n",
      "Iteration 54, loss = 0.24985755\n",
      "Iteration 55, loss = 0.24905195\n",
      "Iteration 56, loss = 0.24835431\n",
      "Iteration 57, loss = 0.24803845\n",
      "Iteration 58, loss = 0.24743149\n",
      "Iteration 59, loss = 0.24681835\n",
      "Iteration 60, loss = 0.24639348\n",
      "Iteration 61, loss = 0.24580877\n",
      "Iteration 62, loss = 0.24535530\n",
      "Iteration 63, loss = 0.24481487\n",
      "Iteration 64, loss = 0.24431575\n",
      "Iteration 65, loss = 0.24392088\n",
      "Iteration 66, loss = 0.24375312\n",
      "Iteration 67, loss = 0.24313248\n",
      "Iteration 68, loss = 0.24280780\n",
      "Iteration 69, loss = 0.24235207\n",
      "Iteration 70, loss = 0.24195108\n",
      "Iteration 71, loss = 0.24179309\n",
      "Iteration 72, loss = 0.24141354\n",
      "Iteration 73, loss = 0.24100362\n",
      "Iteration 74, loss = 0.24081936\n",
      "Iteration 75, loss = 0.24061360\n",
      "Iteration 76, loss = 0.24008699\n",
      "Iteration 77, loss = 0.23992080\n",
      "Iteration 78, loss = 0.23958720\n",
      "Iteration 79, loss = 0.23936734\n",
      "Iteration 80, loss = 0.23916517\n",
      "Iteration 81, loss = 0.23876016\n",
      "Iteration 82, loss = 0.23872918\n",
      "Iteration 83, loss = 0.23834621\n",
      "Iteration 84, loss = 0.23795327\n",
      "Iteration 85, loss = 0.23793210\n",
      "Iteration 86, loss = 0.23760893\n",
      "Iteration 87, loss = 0.23753218\n",
      "Iteration 88, loss = 0.23739990\n",
      "Iteration 89, loss = 0.23706403\n",
      "Iteration 90, loss = 0.23702183\n",
      "Iteration 91, loss = 0.23674587\n",
      "Iteration 92, loss = 0.23618940\n",
      "Iteration 93, loss = 0.23594436\n",
      "Iteration 94, loss = 0.23564693\n",
      "Iteration 95, loss = 0.23549038\n",
      "Iteration 96, loss = 0.23526633\n",
      "Iteration 97, loss = 0.23515472\n",
      "Iteration 98, loss = 0.23481572\n",
      "Iteration 99, loss = 0.23444366\n",
      "Iteration 100, loss = 0.23444093\n",
      "Iteration 101, loss = 0.23410264\n",
      "Iteration 102, loss = 0.23368398\n",
      "Iteration 103, loss = 0.23344801\n",
      "Iteration 104, loss = 0.23317273\n",
      "Iteration 105, loss = 0.23287103\n",
      "Iteration 106, loss = 0.23285563\n",
      "Iteration 107, loss = 0.23328667\n",
      "Iteration 108, loss = 0.23211262\n",
      "Iteration 109, loss = 0.23195712\n",
      "Iteration 110, loss = 0.23167447\n",
      "Iteration 111, loss = 0.23115180\n",
      "Iteration 112, loss = 0.23109873\n",
      "Iteration 113, loss = 0.23072345\n",
      "Iteration 114, loss = 0.23032170\n",
      "Iteration 115, loss = 0.22989798\n",
      "Iteration 116, loss = 0.22962733\n",
      "Iteration 117, loss = 0.22924463\n",
      "Iteration 118, loss = 0.22895746\n",
      "Iteration 119, loss = 0.22869966\n",
      "Iteration 120, loss = 0.22856656\n",
      "Iteration 121, loss = 0.22793841\n",
      "Iteration 122, loss = 0.22779949\n",
      "Iteration 123, loss = 0.22721876\n",
      "Iteration 124, loss = 0.22682176\n",
      "Iteration 125, loss = 0.22682659\n",
      "Iteration 126, loss = 0.22615396\n",
      "Iteration 127, loss = 0.22589848\n",
      "Iteration 128, loss = 0.22543046\n",
      "Iteration 129, loss = 0.22509034\n",
      "Iteration 130, loss = 0.22452113\n",
      "Iteration 131, loss = 0.22450935\n",
      "Iteration 132, loss = 0.22393295\n",
      "Iteration 133, loss = 0.22347176\n",
      "Iteration 134, loss = 0.22307656\n",
      "Iteration 135, loss = 0.22264842\n",
      "Iteration 136, loss = 0.22229591\n",
      "Iteration 137, loss = 0.22191859\n",
      "Iteration 138, loss = 0.22168494\n",
      "Iteration 139, loss = 0.22124045\n",
      "Iteration 140, loss = 0.22063889\n",
      "Iteration 141, loss = 0.22057858\n",
      "Iteration 142, loss = 0.22019824\n",
      "Iteration 143, loss = 0.21950936\n",
      "Iteration 144, loss = 0.21907926\n",
      "Iteration 145, loss = 0.21870551\n",
      "Iteration 146, loss = 0.21797308\n",
      "Iteration 147, loss = 0.21776585\n",
      "Iteration 148, loss = 0.21733648\n",
      "Iteration 149, loss = 0.21691906\n",
      "Iteration 150, loss = 0.21651525\n",
      "Iteration 151, loss = 0.21589572\n",
      "Iteration 152, loss = 0.21549847\n",
      "Iteration 153, loss = 0.21506014\n",
      "Iteration 154, loss = 0.21442221\n",
      "Iteration 155, loss = 0.21423348\n",
      "Iteration 156, loss = 0.21353982\n",
      "Iteration 157, loss = 0.21320660\n",
      "Iteration 158, loss = 0.21290261\n",
      "Iteration 159, loss = 0.21239943\n",
      "Iteration 160, loss = 0.21205423\n",
      "Iteration 161, loss = 0.21121086\n",
      "Iteration 162, loss = 0.21064909\n",
      "Iteration 163, loss = 0.21052346\n",
      "Iteration 164, loss = 0.20988130\n",
      "Iteration 165, loss = 0.20930197\n",
      "Iteration 166, loss = 0.20886673\n",
      "Iteration 167, loss = 0.20831942\n",
      "Iteration 168, loss = 0.20774541\n",
      "Iteration 169, loss = 0.20744714\n",
      "Iteration 170, loss = 0.20677528\n",
      "Iteration 171, loss = 0.20629285\n",
      "Iteration 172, loss = 0.20577189\n",
      "Iteration 173, loss = 0.20550260\n",
      "Iteration 174, loss = 0.20490067\n",
      "Iteration 175, loss = 0.20414481\n",
      "Iteration 176, loss = 0.20363259\n",
      "Iteration 177, loss = 0.20327724\n",
      "Iteration 178, loss = 0.20295125\n",
      "Iteration 179, loss = 0.20229458\n",
      "Iteration 180, loss = 0.20203967\n",
      "Iteration 181, loss = 0.20126614\n",
      "Iteration 182, loss = 0.20070700\n",
      "Iteration 183, loss = 0.20007513\n",
      "Iteration 184, loss = 0.19968606\n",
      "Iteration 185, loss = 0.19908624\n",
      "Iteration 186, loss = 0.19878665\n",
      "Iteration 187, loss = 0.19813528\n",
      "Iteration 188, loss = 0.19749931\n",
      "Iteration 189, loss = 0.19710624\n",
      "Iteration 190, loss = 0.19641384\n",
      "Iteration 191, loss = 0.19575833\n",
      "Iteration 192, loss = 0.19528671\n",
      "Iteration 193, loss = 0.19467515\n",
      "Iteration 194, loss = 0.19431746\n",
      "Iteration 195, loss = 0.19367540\n",
      "Iteration 196, loss = 0.19300635\n",
      "Iteration 197, loss = 0.19257870\n",
      "Iteration 198, loss = 0.19212054\n",
      "Iteration 199, loss = 0.19169583\n",
      "Iteration 200, loss = 0.19097293\n",
      "Iteration 1, loss = 0.69036692\n",
      "Iteration 2, loss = 0.48773065\n",
      "Iteration 3, loss = 0.41593754\n",
      "Iteration 4, loss = 0.39088116\n",
      "Iteration 5, loss = 0.37652241\n",
      "Iteration 6, loss = 0.36489177\n",
      "Iteration 7, loss = 0.35408609\n",
      "Iteration 8, loss = 0.34483602\n",
      "Iteration 9, loss = 0.33685464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.32933333\n",
      "Iteration 11, loss = 0.32295098\n",
      "Iteration 12, loss = 0.31694628\n",
      "Iteration 13, loss = 0.31193467\n",
      "Iteration 14, loss = 0.30723734\n",
      "Iteration 15, loss = 0.30310490\n",
      "Iteration 16, loss = 0.29936971\n",
      "Iteration 17, loss = 0.29608273\n",
      "Iteration 18, loss = 0.29302381\n",
      "Iteration 19, loss = 0.29025765\n",
      "Iteration 20, loss = 0.28772037\n",
      "Iteration 21, loss = 0.28539482\n",
      "Iteration 22, loss = 0.28325657\n",
      "Iteration 23, loss = 0.28122942\n",
      "Iteration 24, loss = 0.27927274\n",
      "Iteration 25, loss = 0.27755793\n",
      "Iteration 26, loss = 0.27575502\n",
      "Iteration 27, loss = 0.27430863\n",
      "Iteration 28, loss = 0.27274782\n",
      "Iteration 29, loss = 0.27127362\n",
      "Iteration 30, loss = 0.26979665\n",
      "Iteration 31, loss = 0.26844623\n",
      "Iteration 32, loss = 0.26713921\n",
      "Iteration 33, loss = 0.26588924\n",
      "Iteration 34, loss = 0.26457885\n",
      "Iteration 35, loss = 0.26336211\n",
      "Iteration 36, loss = 0.26213088\n",
      "Iteration 37, loss = 0.26091898\n",
      "Iteration 38, loss = 0.26000127\n",
      "Iteration 39, loss = 0.25881327\n",
      "Iteration 40, loss = 0.25777830\n",
      "Iteration 41, loss = 0.25655918\n",
      "Iteration 42, loss = 0.25559715\n",
      "Iteration 43, loss = 0.25469695\n",
      "Iteration 44, loss = 0.25369492\n",
      "Iteration 45, loss = 0.25259238\n",
      "Iteration 46, loss = 0.25193732\n",
      "Iteration 47, loss = 0.25092619\n",
      "Iteration 48, loss = 0.24985294\n",
      "Iteration 49, loss = 0.24912240\n",
      "Iteration 50, loss = 0.24822362\n",
      "Iteration 51, loss = 0.24733979\n",
      "Iteration 52, loss = 0.24670317\n",
      "Iteration 53, loss = 0.24576692\n",
      "Iteration 54, loss = 0.24513297\n",
      "Iteration 55, loss = 0.24422161\n",
      "Iteration 56, loss = 0.24364742\n",
      "Iteration 57, loss = 0.24301246\n",
      "Iteration 58, loss = 0.24240252\n",
      "Iteration 59, loss = 0.24181317\n",
      "Iteration 60, loss = 0.24124646\n",
      "Iteration 61, loss = 0.24080736\n",
      "Iteration 62, loss = 0.24014446\n",
      "Iteration 63, loss = 0.23970920\n",
      "Iteration 64, loss = 0.23922764\n",
      "Iteration 65, loss = 0.23898510\n",
      "Iteration 66, loss = 0.23807043\n",
      "Iteration 67, loss = 0.23785374\n",
      "Iteration 68, loss = 0.23739197\n",
      "Iteration 69, loss = 0.23698555\n",
      "Iteration 70, loss = 0.23669093\n",
      "Iteration 71, loss = 0.23633144\n",
      "Iteration 72, loss = 0.23608490\n",
      "Iteration 73, loss = 0.23563523\n",
      "Iteration 74, loss = 0.23532711\n",
      "Iteration 75, loss = 0.23532467\n",
      "Iteration 76, loss = 0.23478879\n",
      "Iteration 77, loss = 0.23430993\n",
      "Iteration 78, loss = 0.23419004\n",
      "Iteration 79, loss = 0.23387755\n",
      "Iteration 80, loss = 0.23352152\n",
      "Iteration 81, loss = 0.23338248\n",
      "Iteration 82, loss = 0.23396522\n",
      "Iteration 83, loss = 0.23305617\n",
      "Iteration 84, loss = 0.23262492\n",
      "Iteration 85, loss = 0.23219116\n",
      "Iteration 86, loss = 0.23236319\n",
      "Iteration 87, loss = 0.23177149\n",
      "Iteration 88, loss = 0.23167546\n",
      "Iteration 89, loss = 0.23131273\n",
      "Iteration 90, loss = 0.23102429\n",
      "Iteration 91, loss = 0.23060896\n",
      "Iteration 92, loss = 0.23059579\n",
      "Iteration 93, loss = 0.23003191\n",
      "Iteration 94, loss = 0.22990910\n",
      "Iteration 95, loss = 0.22954130\n",
      "Iteration 96, loss = 0.22961552\n",
      "Iteration 97, loss = 0.22933400\n",
      "Iteration 98, loss = 0.22855921\n",
      "Iteration 99, loss = 0.22849163\n",
      "Iteration 100, loss = 0.22846738\n",
      "Iteration 101, loss = 0.22805303\n",
      "Iteration 102, loss = 0.22769039\n",
      "Iteration 103, loss = 0.22737790\n",
      "Iteration 104, loss = 0.22673726\n",
      "Iteration 105, loss = 0.22650404\n",
      "Iteration 106, loss = 0.22617252\n",
      "Iteration 107, loss = 0.22590242\n",
      "Iteration 108, loss = 0.22546211\n",
      "Iteration 109, loss = 0.22506188\n",
      "Iteration 110, loss = 0.22478477\n",
      "Iteration 111, loss = 0.22466373\n",
      "Iteration 112, loss = 0.22410652\n",
      "Iteration 113, loss = 0.22362263\n",
      "Iteration 114, loss = 0.22393938\n",
      "Iteration 115, loss = 0.22309422\n",
      "Iteration 116, loss = 0.22255856\n",
      "Iteration 117, loss = 0.22202146\n",
      "Iteration 118, loss = 0.22148725\n",
      "Iteration 119, loss = 0.22129987\n",
      "Iteration 120, loss = 0.22082882\n",
      "Iteration 121, loss = 0.22040302\n",
      "Iteration 122, loss = 0.21994249\n",
      "Iteration 123, loss = 0.21953968\n",
      "Iteration 124, loss = 0.21911559\n",
      "Iteration 125, loss = 0.21863801\n",
      "Iteration 126, loss = 0.21804316\n",
      "Iteration 127, loss = 0.21765150\n",
      "Iteration 128, loss = 0.21723687\n",
      "Iteration 129, loss = 0.21666887\n",
      "Iteration 130, loss = 0.21665193\n",
      "Iteration 131, loss = 0.21577868\n",
      "Iteration 132, loss = 0.21520236\n",
      "Iteration 133, loss = 0.21473979\n",
      "Iteration 134, loss = 0.21458947\n",
      "Iteration 135, loss = 0.21345146\n",
      "Iteration 136, loss = 0.21315034\n",
      "Iteration 137, loss = 0.21279856\n",
      "Iteration 138, loss = 0.21207525\n",
      "Iteration 139, loss = 0.21156348\n",
      "Iteration 140, loss = 0.21136793\n",
      "Iteration 141, loss = 0.21040201\n",
      "Iteration 142, loss = 0.20987614\n",
      "Iteration 143, loss = 0.20960129\n",
      "Iteration 144, loss = 0.20888558\n",
      "Iteration 145, loss = 0.20838839\n",
      "Iteration 146, loss = 0.20770593\n",
      "Iteration 147, loss = 0.20717106\n",
      "Iteration 148, loss = 0.20663337\n",
      "Iteration 149, loss = 0.20607629\n",
      "Iteration 150, loss = 0.20546337\n",
      "Iteration 151, loss = 0.20499639\n",
      "Iteration 152, loss = 0.20487629\n",
      "Iteration 153, loss = 0.20380010\n",
      "Iteration 154, loss = 0.20315565\n",
      "Iteration 155, loss = 0.20261599\n",
      "Iteration 156, loss = 0.20198136\n",
      "Iteration 157, loss = 0.20153354\n",
      "Iteration 158, loss = 0.20065378\n",
      "Iteration 159, loss = 0.20034724\n",
      "Iteration 160, loss = 0.19956331\n",
      "Iteration 161, loss = 0.19909733\n",
      "Iteration 162, loss = 0.19892916\n",
      "Iteration 163, loss = 0.19797957\n",
      "Iteration 164, loss = 0.19744146\n",
      "Iteration 165, loss = 0.19691893\n",
      "Iteration 166, loss = 0.19654670\n",
      "Iteration 167, loss = 0.19565680\n",
      "Iteration 168, loss = 0.19522987\n",
      "Iteration 169, loss = 0.19455050\n",
      "Iteration 170, loss = 0.19386564\n",
      "Iteration 171, loss = 0.19329091\n",
      "Iteration 172, loss = 0.19283497\n",
      "Iteration 173, loss = 0.19216302\n",
      "Iteration 174, loss = 0.19145415\n",
      "Iteration 175, loss = 0.19132439\n",
      "Iteration 176, loss = 0.19056424\n",
      "Iteration 177, loss = 0.18975634\n",
      "Iteration 178, loss = 0.18950897\n",
      "Iteration 179, loss = 0.18866537\n",
      "Iteration 180, loss = 0.18812096\n",
      "Iteration 181, loss = 0.18832506\n",
      "Iteration 182, loss = 0.18684856\n",
      "Iteration 183, loss = 0.18669180\n",
      "Iteration 184, loss = 0.18598174\n",
      "Iteration 185, loss = 0.18536317\n",
      "Iteration 186, loss = 0.18483878\n",
      "Iteration 187, loss = 0.18430094\n",
      "Iteration 188, loss = 0.18399016\n",
      "Iteration 189, loss = 0.18303945\n",
      "Iteration 190, loss = 0.18306297\n",
      "Iteration 191, loss = 0.18314700\n",
      "Iteration 192, loss = 0.18213513\n",
      "Iteration 193, loss = 0.18094324\n",
      "Iteration 194, loss = 0.18069813\n",
      "Iteration 195, loss = 0.18031055\n",
      "Iteration 196, loss = 0.17949669\n",
      "Iteration 197, loss = 0.17895803\n",
      "Iteration 198, loss = 0.17832177\n",
      "Iteration 199, loss = 0.17778203\n",
      "Iteration 200, loss = 0.17778029\n",
      "Iteration 1, loss = 0.69118921\n",
      "Iteration 2, loss = 0.48799379\n",
      "Iteration 3, loss = 0.41504811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.39030737\n",
      "Iteration 5, loss = 0.37620307\n",
      "Iteration 6, loss = 0.36439277\n",
      "Iteration 7, loss = 0.35386807\n",
      "Iteration 8, loss = 0.34461792\n",
      "Iteration 9, loss = 0.33647702\n",
      "Iteration 10, loss = 0.32930211\n",
      "Iteration 11, loss = 0.32286058\n",
      "Iteration 12, loss = 0.31691345\n",
      "Iteration 13, loss = 0.31207950\n",
      "Iteration 14, loss = 0.30741836\n",
      "Iteration 15, loss = 0.30338309\n",
      "Iteration 16, loss = 0.29975564\n",
      "Iteration 17, loss = 0.29658503\n",
      "Iteration 18, loss = 0.29368370\n",
      "Iteration 19, loss = 0.29101928\n",
      "Iteration 20, loss = 0.28868472\n",
      "Iteration 21, loss = 0.28660584\n",
      "Iteration 22, loss = 0.28474807\n",
      "Iteration 23, loss = 0.28274006\n",
      "Iteration 24, loss = 0.28104690\n",
      "Iteration 25, loss = 0.27941747\n",
      "Iteration 26, loss = 0.27793230\n",
      "Iteration 27, loss = 0.27661438\n",
      "Iteration 28, loss = 0.27513331\n",
      "Iteration 29, loss = 0.27400748\n",
      "Iteration 30, loss = 0.27263071\n",
      "Iteration 31, loss = 0.27135183\n",
      "Iteration 32, loss = 0.27021436\n",
      "Iteration 33, loss = 0.26912907\n",
      "Iteration 34, loss = 0.26809986\n",
      "Iteration 35, loss = 0.26695258\n",
      "Iteration 36, loss = 0.26597713\n",
      "Iteration 37, loss = 0.26499840\n",
      "Iteration 38, loss = 0.26392152\n",
      "Iteration 39, loss = 0.26307799\n",
      "Iteration 40, loss = 0.26191459\n",
      "Iteration 41, loss = 0.26087057\n",
      "Iteration 42, loss = 0.26005342\n",
      "Iteration 43, loss = 0.25920633\n",
      "Iteration 44, loss = 0.25837496\n",
      "Iteration 45, loss = 0.25751481\n",
      "Iteration 46, loss = 0.25683645\n",
      "Iteration 47, loss = 0.25597529\n",
      "Iteration 48, loss = 0.25486454\n",
      "Iteration 49, loss = 0.25440167\n",
      "Iteration 50, loss = 0.25347003\n",
      "Iteration 51, loss = 0.25296749\n",
      "Iteration 52, loss = 0.25226367\n",
      "Iteration 53, loss = 0.25139348\n",
      "Iteration 54, loss = 0.25067558\n",
      "Iteration 55, loss = 0.25001642\n",
      "Iteration 56, loss = 0.24939132\n",
      "Iteration 57, loss = 0.24889178\n",
      "Iteration 58, loss = 0.24837785\n",
      "Iteration 59, loss = 0.24780688\n",
      "Iteration 60, loss = 0.24729588\n",
      "Iteration 61, loss = 0.24683617\n",
      "Iteration 62, loss = 0.24621830\n",
      "Iteration 63, loss = 0.24586855\n",
      "Iteration 64, loss = 0.24540774\n",
      "Iteration 65, loss = 0.24531083\n",
      "Iteration 66, loss = 0.24459887\n",
      "Iteration 67, loss = 0.24433754\n",
      "Iteration 68, loss = 0.24388537\n",
      "Iteration 69, loss = 0.24358579\n",
      "Iteration 70, loss = 0.24330427\n",
      "Iteration 71, loss = 0.24302854\n",
      "Iteration 72, loss = 0.24249375\n",
      "Iteration 73, loss = 0.24228840\n",
      "Iteration 74, loss = 0.24201100\n",
      "Iteration 75, loss = 0.24176155\n",
      "Iteration 76, loss = 0.24144204\n",
      "Iteration 77, loss = 0.24125054\n",
      "Iteration 78, loss = 0.24106302\n",
      "Iteration 79, loss = 0.24065746\n",
      "Iteration 80, loss = 0.24057729\n",
      "Iteration 81, loss = 0.24029515\n",
      "Iteration 82, loss = 0.24036901\n",
      "Iteration 83, loss = 0.24007637\n",
      "Iteration 84, loss = 0.23960930\n",
      "Iteration 85, loss = 0.23906347\n",
      "Iteration 86, loss = 0.23922565\n",
      "Iteration 87, loss = 0.23877600\n",
      "Iteration 88, loss = 0.23863221\n",
      "Iteration 89, loss = 0.23830953\n",
      "Iteration 90, loss = 0.23805885\n",
      "Iteration 91, loss = 0.23790855\n",
      "Iteration 92, loss = 0.23763368\n",
      "Iteration 93, loss = 0.23734068\n",
      "Iteration 94, loss = 0.23722048\n",
      "Iteration 95, loss = 0.23703374\n",
      "Iteration 96, loss = 0.23707591\n",
      "Iteration 97, loss = 0.23648602\n",
      "Iteration 98, loss = 0.23614754\n",
      "Iteration 99, loss = 0.23592373\n",
      "Iteration 100, loss = 0.23578892\n",
      "Iteration 101, loss = 0.23585021\n",
      "Iteration 102, loss = 0.23549096\n",
      "Iteration 103, loss = 0.23486710\n",
      "Iteration 104, loss = 0.23441954\n",
      "Iteration 105, loss = 0.23428927\n",
      "Iteration 106, loss = 0.23396805\n",
      "Iteration 107, loss = 0.23372888\n",
      "Iteration 108, loss = 0.23338285\n",
      "Iteration 109, loss = 0.23299476\n",
      "Iteration 110, loss = 0.23270212\n",
      "Iteration 111, loss = 0.23255749\n",
      "Iteration 112, loss = 0.23209645\n",
      "Iteration 113, loss = 0.23178684\n",
      "Iteration 114, loss = 0.23157421\n",
      "Iteration 115, loss = 0.23098249\n",
      "Iteration 116, loss = 0.23080481\n",
      "Iteration 117, loss = 0.23026174\n",
      "Iteration 118, loss = 0.22978880\n",
      "Iteration 119, loss = 0.22958599\n",
      "Iteration 120, loss = 0.22909280\n",
      "Iteration 121, loss = 0.22881809\n",
      "Iteration 122, loss = 0.22845896\n",
      "Iteration 123, loss = 0.22811396\n",
      "Iteration 124, loss = 0.22763961\n",
      "Iteration 125, loss = 0.22724104\n",
      "Iteration 126, loss = 0.22708805\n",
      "Iteration 127, loss = 0.22647031\n",
      "Iteration 128, loss = 0.22599340\n",
      "Iteration 129, loss = 0.22550158\n",
      "Iteration 130, loss = 0.22529329\n",
      "Iteration 131, loss = 0.22450357\n",
      "Iteration 132, loss = 0.22432596\n",
      "Iteration 133, loss = 0.22361257\n",
      "Iteration 134, loss = 0.22344507\n",
      "Iteration 135, loss = 0.22265498\n",
      "Iteration 136, loss = 0.22240845\n",
      "Iteration 137, loss = 0.22170257\n",
      "Iteration 138, loss = 0.22132604\n",
      "Iteration 139, loss = 0.22081828\n",
      "Iteration 140, loss = 0.22057292\n",
      "Iteration 141, loss = 0.21974391\n",
      "Iteration 142, loss = 0.21946698\n",
      "Iteration 143, loss = 0.21909439\n",
      "Iteration 144, loss = 0.21836497\n",
      "Iteration 145, loss = 0.21793510\n",
      "Iteration 146, loss = 0.21725574\n",
      "Iteration 147, loss = 0.21681361\n",
      "Iteration 148, loss = 0.21638189\n",
      "Iteration 149, loss = 0.21576025\n",
      "Iteration 150, loss = 0.21543255\n",
      "Iteration 151, loss = 0.21526873\n",
      "Iteration 152, loss = 0.21484103\n",
      "Iteration 153, loss = 0.21348700\n",
      "Iteration 154, loss = 0.21342252\n",
      "Iteration 155, loss = 0.21243403\n",
      "Iteration 156, loss = 0.21187814\n",
      "Iteration 157, loss = 0.21144864\n",
      "Iteration 158, loss = 0.21083789\n",
      "Iteration 159, loss = 0.21031512\n",
      "Iteration 160, loss = 0.20960599\n",
      "Iteration 161, loss = 0.20928902\n",
      "Iteration 162, loss = 0.20908542\n",
      "Iteration 163, loss = 0.20810159\n",
      "Iteration 164, loss = 0.20747404\n",
      "Iteration 165, loss = 0.20703775\n",
      "Iteration 166, loss = 0.20610855\n",
      "Iteration 167, loss = 0.20562506\n",
      "Iteration 168, loss = 0.20518848\n",
      "Iteration 169, loss = 0.20456416\n",
      "Iteration 170, loss = 0.20386925\n",
      "Iteration 171, loss = 0.20350637\n",
      "Iteration 172, loss = 0.20276251\n",
      "Iteration 173, loss = 0.20232039\n",
      "Iteration 174, loss = 0.20154939\n",
      "Iteration 175, loss = 0.20150482\n",
      "Iteration 176, loss = 0.20045770\n",
      "Iteration 177, loss = 0.19984471\n",
      "Iteration 178, loss = 0.19934223\n",
      "Iteration 179, loss = 0.19851297\n",
      "Iteration 180, loss = 0.19803359\n",
      "Iteration 181, loss = 0.19765901\n",
      "Iteration 182, loss = 0.19669598\n",
      "Iteration 183, loss = 0.19640214\n",
      "Iteration 184, loss = 0.19560484\n",
      "Iteration 185, loss = 0.19492170\n",
      "Iteration 186, loss = 0.19448547\n",
      "Iteration 187, loss = 0.19395393\n",
      "Iteration 188, loss = 0.19309344\n",
      "Iteration 189, loss = 0.19248910\n",
      "Iteration 190, loss = 0.19233110\n",
      "Iteration 191, loss = 0.19223414\n",
      "Iteration 192, loss = 0.19152930\n",
      "Iteration 193, loss = 0.19008065\n",
      "Iteration 194, loss = 0.18985411\n",
      "Iteration 195, loss = 0.18935751\n",
      "Iteration 196, loss = 0.18843006\n",
      "Iteration 197, loss = 0.18785551\n",
      "Iteration 198, loss = 0.18724516\n",
      "Iteration 199, loss = 0.18652389\n",
      "Iteration 200, loss = 0.18621455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75836684\n",
      "Iteration 2, loss = 0.69663936\n",
      "Iteration 3, loss = 0.63348707\n",
      "Iteration 4, loss = 0.58050372\n",
      "Iteration 5, loss = 0.53781968\n",
      "Iteration 6, loss = 0.50241508\n",
      "Iteration 7, loss = 0.47362173\n",
      "Iteration 8, loss = 0.44951623\n",
      "Iteration 9, loss = 0.42929321\n",
      "Iteration 10, loss = 0.41221097\n",
      "Iteration 11, loss = 0.39761203\n",
      "Iteration 12, loss = 0.38484070\n",
      "Iteration 13, loss = 0.37403819\n",
      "Iteration 14, loss = 0.36438332\n",
      "Iteration 15, loss = 0.35592398\n",
      "Iteration 16, loss = 0.34849725\n",
      "Iteration 17, loss = 0.34177531\n",
      "Iteration 18, loss = 0.33583815\n",
      "Iteration 19, loss = 0.33038866\n",
      "Iteration 20, loss = 0.32551205\n",
      "Iteration 21, loss = 0.32116187\n",
      "Iteration 22, loss = 0.31711695\n",
      "Iteration 23, loss = 0.31340113\n",
      "Iteration 24, loss = 0.31002918\n",
      "Iteration 25, loss = 0.30691749\n",
      "Iteration 26, loss = 0.30400165\n",
      "Iteration 27, loss = 0.30136945\n",
      "Iteration 28, loss = 0.29888933\n",
      "Iteration 29, loss = 0.29657350\n",
      "Iteration 30, loss = 0.29442347\n",
      "Iteration 31, loss = 0.29240886\n",
      "Iteration 32, loss = 0.29053419\n",
      "Iteration 33, loss = 0.28876725\n",
      "Iteration 34, loss = 0.28712494\n",
      "Iteration 35, loss = 0.28555328\n",
      "Iteration 36, loss = 0.28408354\n",
      "Iteration 37, loss = 0.28264948\n",
      "Iteration 38, loss = 0.28135611\n",
      "Iteration 39, loss = 0.28009537\n",
      "Iteration 40, loss = 0.27889937\n",
      "Iteration 41, loss = 0.27778676\n",
      "Iteration 42, loss = 0.27670527\n",
      "Iteration 43, loss = 0.27566066\n",
      "Iteration 44, loss = 0.27469095\n",
      "Iteration 45, loss = 0.27376334\n",
      "Iteration 46, loss = 0.27286025\n",
      "Iteration 47, loss = 0.27203274\n",
      "Iteration 48, loss = 0.27117962\n",
      "Iteration 49, loss = 0.27039591\n",
      "Iteration 50, loss = 0.26962318\n",
      "Iteration 51, loss = 0.26889992\n",
      "Iteration 52, loss = 0.26820973\n",
      "Iteration 53, loss = 0.26751231\n",
      "Iteration 54, loss = 0.26687192\n",
      "Iteration 55, loss = 0.26622854\n",
      "Iteration 56, loss = 0.26562834\n",
      "Iteration 57, loss = 0.26506179\n",
      "Iteration 58, loss = 0.26447132\n",
      "Iteration 59, loss = 0.26390686\n",
      "Iteration 60, loss = 0.26339269\n",
      "Iteration 61, loss = 0.26286861\n",
      "Iteration 62, loss = 0.26236339\n",
      "Iteration 63, loss = 0.26187657\n",
      "Iteration 64, loss = 0.26140917\n",
      "Iteration 65, loss = 0.26094401\n",
      "Iteration 66, loss = 0.26051625\n",
      "Iteration 67, loss = 0.26004431\n",
      "Iteration 68, loss = 0.25961456\n",
      "Iteration 69, loss = 0.25923009\n",
      "Iteration 70, loss = 0.25879308\n",
      "Iteration 71, loss = 0.25840753\n",
      "Iteration 72, loss = 0.25802808\n",
      "Iteration 73, loss = 0.25764386\n",
      "Iteration 74, loss = 0.25727029\n",
      "Iteration 75, loss = 0.25691584\n",
      "Iteration 76, loss = 0.25657338\n",
      "Iteration 77, loss = 0.25624102\n",
      "Iteration 78, loss = 0.25588609\n",
      "Iteration 79, loss = 0.25554447\n",
      "Iteration 80, loss = 0.25522664\n",
      "Iteration 81, loss = 0.25490957\n",
      "Iteration 82, loss = 0.25461334\n",
      "Iteration 83, loss = 0.25426589\n",
      "Iteration 84, loss = 0.25397231\n",
      "Iteration 85, loss = 0.25366965\n",
      "Iteration 86, loss = 0.25338997\n",
      "Iteration 87, loss = 0.25310855\n",
      "Iteration 88, loss = 0.25281602\n",
      "Iteration 89, loss = 0.25253870\n",
      "Iteration 90, loss = 0.25227696\n",
      "Iteration 91, loss = 0.25199435\n",
      "Iteration 92, loss = 0.25175184\n",
      "Iteration 93, loss = 0.25146023\n",
      "Iteration 94, loss = 0.25121866\n",
      "Iteration 95, loss = 0.25096455\n",
      "Iteration 96, loss = 0.25070730\n",
      "Iteration 97, loss = 0.25046455\n",
      "Iteration 98, loss = 0.25021366\n",
      "Iteration 99, loss = 0.24996979\n",
      "Iteration 100, loss = 0.24977511\n",
      "Iteration 101, loss = 0.24952388\n",
      "Iteration 102, loss = 0.24926315\n",
      "Iteration 103, loss = 0.24905316\n",
      "Iteration 104, loss = 0.24883228\n",
      "Iteration 105, loss = 0.24861070\n",
      "Iteration 106, loss = 0.24838152\n",
      "Iteration 107, loss = 0.24816565\n",
      "Iteration 108, loss = 0.24794917\n",
      "Iteration 109, loss = 0.24774409\n",
      "Iteration 110, loss = 0.24754868\n",
      "Iteration 111, loss = 0.24732880\n",
      "Iteration 112, loss = 0.24711356\n",
      "Iteration 113, loss = 0.24692975\n",
      "Iteration 114, loss = 0.24671731\n",
      "Iteration 115, loss = 0.24653892\n",
      "Iteration 116, loss = 0.24634449\n",
      "Iteration 117, loss = 0.24615684\n",
      "Iteration 118, loss = 0.24592619\n",
      "Iteration 119, loss = 0.24575411\n",
      "Iteration 120, loss = 0.24557199\n",
      "Iteration 121, loss = 0.24537398\n",
      "Iteration 122, loss = 0.24520006\n",
      "Iteration 123, loss = 0.24500559\n",
      "Iteration 124, loss = 0.24481647\n",
      "Iteration 125, loss = 0.24464802\n",
      "Iteration 126, loss = 0.24447817\n",
      "Iteration 127, loss = 0.24430526\n",
      "Iteration 128, loss = 0.24412100\n",
      "Iteration 129, loss = 0.24396323\n",
      "Iteration 130, loss = 0.24377453\n",
      "Iteration 131, loss = 0.24361235\n",
      "Iteration 132, loss = 0.24345865\n",
      "Iteration 133, loss = 0.24328041\n",
      "Iteration 134, loss = 0.24311418\n",
      "Iteration 135, loss = 0.24295123\n",
      "Iteration 136, loss = 0.24282490\n",
      "Iteration 137, loss = 0.24263788\n",
      "Iteration 138, loss = 0.24246328\n",
      "Iteration 139, loss = 0.24231840\n",
      "Iteration 140, loss = 0.24215425\n",
      "Iteration 141, loss = 0.24198158\n",
      "Iteration 142, loss = 0.24184545\n",
      "Iteration 143, loss = 0.24169910\n",
      "Iteration 144, loss = 0.24154246\n",
      "Iteration 145, loss = 0.24138776\n",
      "Iteration 146, loss = 0.24123881\n",
      "Iteration 147, loss = 0.24110562\n",
      "Iteration 148, loss = 0.24096142\n",
      "Iteration 149, loss = 0.24079299\n",
      "Iteration 150, loss = 0.24064313\n",
      "Iteration 151, loss = 0.24051814\n",
      "Iteration 152, loss = 0.24038144\n",
      "Iteration 153, loss = 0.24023744\n",
      "Iteration 154, loss = 0.24008672\n",
      "Iteration 155, loss = 0.23996227\n",
      "Iteration 156, loss = 0.23980917\n",
      "Iteration 157, loss = 0.23966810\n",
      "Iteration 158, loss = 0.23952545\n",
      "Iteration 159, loss = 0.23940456\n",
      "Iteration 160, loss = 0.23929205\n",
      "Iteration 161, loss = 0.23914465\n",
      "Iteration 162, loss = 0.23900635\n",
      "Iteration 163, loss = 0.23886086\n",
      "Iteration 164, loss = 0.23873525\n",
      "Iteration 165, loss = 0.23861256\n",
      "Iteration 166, loss = 0.23847411\n",
      "Iteration 167, loss = 0.23836960\n",
      "Iteration 168, loss = 0.23822056\n",
      "Iteration 169, loss = 0.23810349\n",
      "Iteration 170, loss = 0.23799417\n",
      "Iteration 171, loss = 0.23786051\n",
      "Iteration 172, loss = 0.23773126\n",
      "Iteration 173, loss = 0.23760966\n",
      "Iteration 174, loss = 0.23749714\n",
      "Iteration 175, loss = 0.23735812\n",
      "Iteration 176, loss = 0.23725837\n",
      "Iteration 177, loss = 0.23714358\n",
      "Iteration 178, loss = 0.23700043\n",
      "Iteration 179, loss = 0.23692435\n",
      "Iteration 180, loss = 0.23677021\n",
      "Iteration 181, loss = 0.23667094\n",
      "Iteration 182, loss = 0.23654381\n",
      "Iteration 183, loss = 0.23641429\n",
      "Iteration 184, loss = 0.23632284\n",
      "Iteration 185, loss = 0.23621293\n",
      "Iteration 186, loss = 0.23608558\n",
      "Iteration 187, loss = 0.23596604\n",
      "Iteration 188, loss = 0.23586063\n",
      "Iteration 189, loss = 0.23575277\n",
      "Iteration 190, loss = 0.23565025\n",
      "Iteration 191, loss = 0.23551604\n",
      "Iteration 192, loss = 0.23543558\n",
      "Iteration 193, loss = 0.23530161\n",
      "Iteration 194, loss = 0.23520770\n",
      "Iteration 195, loss = 0.23509870\n",
      "Iteration 196, loss = 0.23499451\n",
      "Iteration 197, loss = 0.23490230\n",
      "Iteration 198, loss = 0.23480174\n",
      "Iteration 199, loss = 0.23468792\n",
      "Iteration 200, loss = 0.23458799\n",
      "Iteration 1, loss = 0.75305843\n",
      "Iteration 2, loss = 0.69270918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.63016102\n",
      "Iteration 4, loss = 0.57819971\n",
      "Iteration 5, loss = 0.53593335\n",
      "Iteration 6, loss = 0.50094335\n",
      "Iteration 7, loss = 0.47246897\n",
      "Iteration 8, loss = 0.44843428\n",
      "Iteration 9, loss = 0.42836758\n",
      "Iteration 10, loss = 0.41144248\n",
      "Iteration 11, loss = 0.39678262\n",
      "Iteration 12, loss = 0.38416780\n",
      "Iteration 13, loss = 0.37327355\n",
      "Iteration 14, loss = 0.36363775\n",
      "Iteration 15, loss = 0.35518673\n",
      "Iteration 16, loss = 0.34768401\n",
      "Iteration 17, loss = 0.34104231\n",
      "Iteration 18, loss = 0.33503474\n",
      "Iteration 19, loss = 0.32957865\n",
      "Iteration 20, loss = 0.32471295\n",
      "Iteration 21, loss = 0.32029263\n",
      "Iteration 22, loss = 0.31622168\n",
      "Iteration 23, loss = 0.31250851\n",
      "Iteration 24, loss = 0.30911916\n",
      "Iteration 25, loss = 0.30597643\n",
      "Iteration 26, loss = 0.30308092\n",
      "Iteration 27, loss = 0.30040508\n",
      "Iteration 28, loss = 0.29792674\n",
      "Iteration 29, loss = 0.29555802\n",
      "Iteration 30, loss = 0.29344561\n",
      "Iteration 31, loss = 0.29138438\n",
      "Iteration 32, loss = 0.28949996\n",
      "Iteration 33, loss = 0.28772999\n",
      "Iteration 34, loss = 0.28608150\n",
      "Iteration 35, loss = 0.28448381\n",
      "Iteration 36, loss = 0.28298670\n",
      "Iteration 37, loss = 0.28156546\n",
      "Iteration 38, loss = 0.28026034\n",
      "Iteration 39, loss = 0.27900386\n",
      "Iteration 40, loss = 0.27780472\n",
      "Iteration 41, loss = 0.27666596\n",
      "Iteration 42, loss = 0.27557900\n",
      "Iteration 43, loss = 0.27453852\n",
      "Iteration 44, loss = 0.27355467\n",
      "Iteration 45, loss = 0.27260615\n",
      "Iteration 46, loss = 0.27173061\n",
      "Iteration 47, loss = 0.27087408\n",
      "Iteration 48, loss = 0.27003107\n",
      "Iteration 49, loss = 0.26924358\n",
      "Iteration 50, loss = 0.26846282\n",
      "Iteration 51, loss = 0.26773094\n",
      "Iteration 52, loss = 0.26703211\n",
      "Iteration 53, loss = 0.26634857\n",
      "Iteration 54, loss = 0.26570262\n",
      "Iteration 55, loss = 0.26506378\n",
      "Iteration 56, loss = 0.26444999\n",
      "Iteration 57, loss = 0.26387909\n",
      "Iteration 58, loss = 0.26333002\n",
      "Iteration 59, loss = 0.26274373\n",
      "Iteration 60, loss = 0.26221424\n",
      "Iteration 61, loss = 0.26170172\n",
      "Iteration 62, loss = 0.26119881\n",
      "Iteration 63, loss = 0.26069557\n",
      "Iteration 64, loss = 0.26022319\n",
      "Iteration 65, loss = 0.25976380\n",
      "Iteration 66, loss = 0.25934357\n",
      "Iteration 67, loss = 0.25886933\n",
      "Iteration 68, loss = 0.25845832\n",
      "Iteration 69, loss = 0.25804442\n",
      "Iteration 70, loss = 0.25762960\n",
      "Iteration 71, loss = 0.25723963\n",
      "Iteration 72, loss = 0.25688890\n",
      "Iteration 73, loss = 0.25647496\n",
      "Iteration 74, loss = 0.25611160\n",
      "Iteration 75, loss = 0.25576975\n",
      "Iteration 76, loss = 0.25540386\n",
      "Iteration 77, loss = 0.25507669\n",
      "Iteration 78, loss = 0.25473699\n",
      "Iteration 79, loss = 0.25441072\n",
      "Iteration 80, loss = 0.25407018\n",
      "Iteration 81, loss = 0.25374937\n",
      "Iteration 82, loss = 0.25346527\n",
      "Iteration 83, loss = 0.25314433\n",
      "Iteration 84, loss = 0.25284271\n",
      "Iteration 85, loss = 0.25253865\n",
      "Iteration 86, loss = 0.25224885\n",
      "Iteration 87, loss = 0.25198380\n",
      "Iteration 88, loss = 0.25170214\n",
      "Iteration 89, loss = 0.25143747\n",
      "Iteration 90, loss = 0.25116271\n",
      "Iteration 91, loss = 0.25088483\n",
      "Iteration 92, loss = 0.25062604\n",
      "Iteration 93, loss = 0.25036338\n",
      "Iteration 94, loss = 0.25010551\n",
      "Iteration 95, loss = 0.24986777\n",
      "Iteration 96, loss = 0.24960538\n",
      "Iteration 97, loss = 0.24937655\n",
      "Iteration 98, loss = 0.24914639\n",
      "Iteration 99, loss = 0.24889152\n",
      "Iteration 100, loss = 0.24868056\n",
      "Iteration 101, loss = 0.24843354\n",
      "Iteration 102, loss = 0.24819389\n",
      "Iteration 103, loss = 0.24797085\n",
      "Iteration 104, loss = 0.24775564\n",
      "Iteration 105, loss = 0.24755898\n",
      "Iteration 106, loss = 0.24732494\n",
      "Iteration 107, loss = 0.24711527\n",
      "Iteration 108, loss = 0.24690337\n",
      "Iteration 109, loss = 0.24669740\n",
      "Iteration 110, loss = 0.24648829\n",
      "Iteration 111, loss = 0.24628817\n",
      "Iteration 112, loss = 0.24605317\n",
      "Iteration 113, loss = 0.24590008\n",
      "Iteration 114, loss = 0.24569551\n",
      "Iteration 115, loss = 0.24550118\n",
      "Iteration 116, loss = 0.24530103\n",
      "Iteration 117, loss = 0.24510948\n",
      "Iteration 118, loss = 0.24491567\n",
      "Iteration 119, loss = 0.24473895\n",
      "Iteration 120, loss = 0.24455989\n",
      "Iteration 121, loss = 0.24435343\n",
      "Iteration 122, loss = 0.24418986\n",
      "Iteration 123, loss = 0.24399761\n",
      "Iteration 124, loss = 0.24381329\n",
      "Iteration 125, loss = 0.24366261\n",
      "Iteration 126, loss = 0.24346603\n",
      "Iteration 127, loss = 0.24330477\n",
      "Iteration 128, loss = 0.24312127\n",
      "Iteration 129, loss = 0.24296156\n",
      "Iteration 130, loss = 0.24279052\n",
      "Iteration 131, loss = 0.24263452\n",
      "Iteration 132, loss = 0.24245852\n",
      "Iteration 133, loss = 0.24231734\n",
      "Iteration 134, loss = 0.24214555\n",
      "Iteration 135, loss = 0.24198071\n",
      "Iteration 136, loss = 0.24180663\n",
      "Iteration 137, loss = 0.24167154\n",
      "Iteration 138, loss = 0.24150401\n",
      "Iteration 139, loss = 0.24133765\n",
      "Iteration 140, loss = 0.24116997\n",
      "Iteration 141, loss = 0.24103793\n",
      "Iteration 142, loss = 0.24088930\n",
      "Iteration 143, loss = 0.24076103\n",
      "Iteration 144, loss = 0.24058763\n",
      "Iteration 145, loss = 0.24044637\n",
      "Iteration 146, loss = 0.24028438\n",
      "Iteration 147, loss = 0.24015157\n",
      "Iteration 148, loss = 0.24001516\n",
      "Iteration 149, loss = 0.23984580\n",
      "Iteration 150, loss = 0.23972227\n",
      "Iteration 151, loss = 0.23957461\n",
      "Iteration 152, loss = 0.23944600\n",
      "Iteration 153, loss = 0.23930317\n",
      "Iteration 154, loss = 0.23914218\n",
      "Iteration 155, loss = 0.23902353\n",
      "Iteration 156, loss = 0.23887434\n",
      "Iteration 157, loss = 0.23874107\n",
      "Iteration 158, loss = 0.23860310\n",
      "Iteration 159, loss = 0.23849499\n",
      "Iteration 160, loss = 0.23837108\n",
      "Iteration 161, loss = 0.23820157\n",
      "Iteration 162, loss = 0.23808955\n",
      "Iteration 163, loss = 0.23794062\n",
      "Iteration 164, loss = 0.23782290\n",
      "Iteration 165, loss = 0.23768652\n",
      "Iteration 166, loss = 0.23755820\n",
      "Iteration 167, loss = 0.23743579\n",
      "Iteration 168, loss = 0.23730930\n",
      "Iteration 169, loss = 0.23719665\n",
      "Iteration 170, loss = 0.23708173\n",
      "Iteration 171, loss = 0.23695511\n",
      "Iteration 172, loss = 0.23683345\n",
      "Iteration 173, loss = 0.23670391\n",
      "Iteration 174, loss = 0.23658278\n",
      "Iteration 175, loss = 0.23646378\n",
      "Iteration 176, loss = 0.23635872\n",
      "Iteration 177, loss = 0.23624631\n",
      "Iteration 178, loss = 0.23610614\n",
      "Iteration 179, loss = 0.23603380\n",
      "Iteration 180, loss = 0.23589693\n",
      "Iteration 181, loss = 0.23579985\n",
      "Iteration 182, loss = 0.23566155\n",
      "Iteration 183, loss = 0.23555122\n",
      "Iteration 184, loss = 0.23544107\n",
      "Iteration 185, loss = 0.23532907\n",
      "Iteration 186, loss = 0.23522508\n",
      "Iteration 187, loss = 0.23510153\n",
      "Iteration 188, loss = 0.23499737\n",
      "Iteration 189, loss = 0.23489114\n",
      "Iteration 190, loss = 0.23478135\n",
      "Iteration 191, loss = 0.23466507\n",
      "Iteration 192, loss = 0.23455234\n",
      "Iteration 193, loss = 0.23445765\n",
      "Iteration 194, loss = 0.23435313\n",
      "Iteration 195, loss = 0.23424098\n",
      "Iteration 196, loss = 0.23413876\n",
      "Iteration 197, loss = 0.23403517\n",
      "Iteration 198, loss = 0.23393961\n",
      "Iteration 199, loss = 0.23383741\n",
      "Iteration 200, loss = 0.23372632\n",
      "Iteration 1, loss = 0.75555773\n",
      "Iteration 2, loss = 0.69490179\n",
      "Iteration 3, loss = 0.63169384\n",
      "Iteration 4, loss = 0.57972760\n",
      "Iteration 5, loss = 0.53704198\n",
      "Iteration 6, loss = 0.50188891\n",
      "Iteration 7, loss = 0.47335841\n",
      "Iteration 8, loss = 0.44940894\n",
      "Iteration 9, loss = 0.42919563\n",
      "Iteration 10, loss = 0.41232130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39774628\n",
      "Iteration 12, loss = 0.38509314\n",
      "Iteration 13, loss = 0.37429261\n",
      "Iteration 14, loss = 0.36471050\n",
      "Iteration 15, loss = 0.35628659\n",
      "Iteration 16, loss = 0.34884420\n",
      "Iteration 17, loss = 0.34229370\n",
      "Iteration 18, loss = 0.33633124\n",
      "Iteration 19, loss = 0.33092019\n",
      "Iteration 20, loss = 0.32612756\n",
      "Iteration 21, loss = 0.32176808\n",
      "Iteration 22, loss = 0.31774802\n",
      "Iteration 23, loss = 0.31409352\n",
      "Iteration 24, loss = 0.31077941\n",
      "Iteration 25, loss = 0.30767585\n",
      "Iteration 26, loss = 0.30484868\n",
      "Iteration 27, loss = 0.30221122\n",
      "Iteration 28, loss = 0.29976813\n",
      "Iteration 29, loss = 0.29748157\n",
      "Iteration 30, loss = 0.29538050\n",
      "Iteration 31, loss = 0.29342134\n",
      "Iteration 32, loss = 0.29154386\n",
      "Iteration 33, loss = 0.28983638\n",
      "Iteration 34, loss = 0.28821777\n",
      "Iteration 35, loss = 0.28668140\n",
      "Iteration 36, loss = 0.28523724\n",
      "Iteration 37, loss = 0.28384956\n",
      "Iteration 38, loss = 0.28259410\n",
      "Iteration 39, loss = 0.28135701\n",
      "Iteration 40, loss = 0.28019693\n",
      "Iteration 41, loss = 0.27911992\n",
      "Iteration 42, loss = 0.27805525\n",
      "Iteration 43, loss = 0.27706007\n",
      "Iteration 44, loss = 0.27611990\n",
      "Iteration 45, loss = 0.27518640\n",
      "Iteration 46, loss = 0.27436979\n",
      "Iteration 47, loss = 0.27350948\n",
      "Iteration 48, loss = 0.27272295\n",
      "Iteration 49, loss = 0.27195425\n",
      "Iteration 50, loss = 0.27123805\n",
      "Iteration 51, loss = 0.27053180\n",
      "Iteration 52, loss = 0.26984710\n",
      "Iteration 53, loss = 0.26921007\n",
      "Iteration 54, loss = 0.26861674\n",
      "Iteration 55, loss = 0.26796229\n",
      "Iteration 56, loss = 0.26739248\n",
      "Iteration 57, loss = 0.26687066\n",
      "Iteration 58, loss = 0.26631542\n",
      "Iteration 59, loss = 0.26577498\n",
      "Iteration 60, loss = 0.26528272\n",
      "Iteration 61, loss = 0.26479366\n",
      "Iteration 62, loss = 0.26431901\n",
      "Iteration 63, loss = 0.26384557\n",
      "Iteration 64, loss = 0.26339253\n",
      "Iteration 65, loss = 0.26297213\n",
      "Iteration 66, loss = 0.26257571\n",
      "Iteration 67, loss = 0.26213120\n",
      "Iteration 68, loss = 0.26174070\n",
      "Iteration 69, loss = 0.26133956\n",
      "Iteration 70, loss = 0.26095801\n",
      "Iteration 71, loss = 0.26060950\n",
      "Iteration 72, loss = 0.26025213\n",
      "Iteration 73, loss = 0.25988454\n",
      "Iteration 74, loss = 0.25955189\n",
      "Iteration 75, loss = 0.25922591\n",
      "Iteration 76, loss = 0.25886722\n",
      "Iteration 77, loss = 0.25856251\n",
      "Iteration 78, loss = 0.25823845\n",
      "Iteration 79, loss = 0.25792096\n",
      "Iteration 80, loss = 0.25763888\n",
      "Iteration 81, loss = 0.25732636\n",
      "Iteration 82, loss = 0.25705934\n",
      "Iteration 83, loss = 0.25676544\n",
      "Iteration 84, loss = 0.25646617\n",
      "Iteration 85, loss = 0.25620553\n",
      "Iteration 86, loss = 0.25592829\n",
      "Iteration 87, loss = 0.25568431\n",
      "Iteration 88, loss = 0.25543555\n",
      "Iteration 89, loss = 0.25518892\n",
      "Iteration 90, loss = 0.25492564\n",
      "Iteration 91, loss = 0.25466981\n",
      "Iteration 92, loss = 0.25442246\n",
      "Iteration 93, loss = 0.25417712\n",
      "Iteration 94, loss = 0.25394225\n",
      "Iteration 95, loss = 0.25371140\n",
      "Iteration 96, loss = 0.25348657\n",
      "Iteration 97, loss = 0.25326635\n",
      "Iteration 98, loss = 0.25304913\n",
      "Iteration 99, loss = 0.25281080\n",
      "Iteration 100, loss = 0.25262555\n",
      "Iteration 101, loss = 0.25241558\n",
      "Iteration 102, loss = 0.25218654\n",
      "Iteration 103, loss = 0.25197652\n",
      "Iteration 104, loss = 0.25177435\n",
      "Iteration 105, loss = 0.25157243\n",
      "Iteration 106, loss = 0.25136965\n",
      "Iteration 107, loss = 0.25119977\n",
      "Iteration 108, loss = 0.25099055\n",
      "Iteration 109, loss = 0.25079946\n",
      "Iteration 110, loss = 0.25060694\n",
      "Iteration 111, loss = 0.25042089\n",
      "Iteration 112, loss = 0.25023403\n",
      "Iteration 113, loss = 0.25006859\n",
      "Iteration 114, loss = 0.24986723\n",
      "Iteration 115, loss = 0.24967282\n",
      "Iteration 116, loss = 0.24950521\n",
      "Iteration 117, loss = 0.24932717\n",
      "Iteration 118, loss = 0.24915192\n",
      "Iteration 119, loss = 0.24899564\n",
      "Iteration 120, loss = 0.24882076\n",
      "Iteration 121, loss = 0.24863544\n",
      "Iteration 122, loss = 0.24849788\n",
      "Iteration 123, loss = 0.24831031\n",
      "Iteration 124, loss = 0.24812726\n",
      "Iteration 125, loss = 0.24800666\n",
      "Iteration 126, loss = 0.24782697\n",
      "Iteration 127, loss = 0.24768595\n",
      "Iteration 128, loss = 0.24750684\n",
      "Iteration 129, loss = 0.24734636\n",
      "Iteration 130, loss = 0.24718644\n",
      "Iteration 131, loss = 0.24704936\n",
      "Iteration 132, loss = 0.24689409\n",
      "Iteration 133, loss = 0.24673492\n",
      "Iteration 134, loss = 0.24658373\n",
      "Iteration 135, loss = 0.24643647\n",
      "Iteration 136, loss = 0.24628248\n",
      "Iteration 137, loss = 0.24613604\n",
      "Iteration 138, loss = 0.24600400\n",
      "Iteration 139, loss = 0.24585520\n",
      "Iteration 140, loss = 0.24570989\n",
      "Iteration 141, loss = 0.24558214\n",
      "Iteration 142, loss = 0.24544601\n",
      "Iteration 143, loss = 0.24531580\n",
      "Iteration 144, loss = 0.24516513\n",
      "Iteration 145, loss = 0.24505071\n",
      "Iteration 146, loss = 0.24486827\n",
      "Iteration 147, loss = 0.24475286\n",
      "Iteration 148, loss = 0.24462538\n",
      "Iteration 149, loss = 0.24446958\n",
      "Iteration 150, loss = 0.24436018\n",
      "Iteration 151, loss = 0.24421549\n",
      "Iteration 152, loss = 0.24408677\n",
      "Iteration 153, loss = 0.24396484\n",
      "Iteration 154, loss = 0.24381905\n",
      "Iteration 155, loss = 0.24373833\n",
      "Iteration 156, loss = 0.24356818\n",
      "Iteration 157, loss = 0.24345285\n",
      "Iteration 158, loss = 0.24333826\n",
      "Iteration 159, loss = 0.24322672\n",
      "Iteration 160, loss = 0.24308669\n",
      "Iteration 161, loss = 0.24294490\n",
      "Iteration 162, loss = 0.24283062\n",
      "Iteration 163, loss = 0.24272713\n",
      "Iteration 164, loss = 0.24259120\n",
      "Iteration 165, loss = 0.24248786\n",
      "Iteration 166, loss = 0.24235373\n",
      "Iteration 167, loss = 0.24223424\n",
      "Iteration 168, loss = 0.24212101\n",
      "Iteration 169, loss = 0.24200536\n",
      "Iteration 170, loss = 0.24189499\n",
      "Iteration 171, loss = 0.24177654\n",
      "Iteration 172, loss = 0.24166500\n",
      "Iteration 173, loss = 0.24156845\n",
      "Iteration 174, loss = 0.24144477\n",
      "Iteration 175, loss = 0.24132729\n",
      "Iteration 176, loss = 0.24121739\n",
      "Iteration 177, loss = 0.24111397\n",
      "Iteration 178, loss = 0.24102958\n",
      "Iteration 179, loss = 0.24092679\n",
      "Iteration 180, loss = 0.24080990\n",
      "Iteration 181, loss = 0.24069760\n",
      "Iteration 182, loss = 0.24058033\n",
      "Iteration 183, loss = 0.24047293\n",
      "Iteration 184, loss = 0.24036403\n",
      "Iteration 185, loss = 0.24027942\n",
      "Iteration 186, loss = 0.24018013\n",
      "Iteration 187, loss = 0.24005550\n",
      "Iteration 188, loss = 0.23996440\n",
      "Iteration 189, loss = 0.23987160\n",
      "Iteration 190, loss = 0.23974767\n",
      "Iteration 191, loss = 0.23963981\n",
      "Iteration 192, loss = 0.23955946\n",
      "Iteration 193, loss = 0.23943970\n",
      "Iteration 194, loss = 0.23936825\n",
      "Iteration 195, loss = 0.23924973\n",
      "Iteration 196, loss = 0.23913711\n",
      "Iteration 197, loss = 0.23905095\n",
      "Iteration 198, loss = 0.23895444\n",
      "Iteration 199, loss = 0.23887655\n",
      "Iteration 200, loss = 0.23877052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75739948\n",
      "Iteration 2, loss = 0.69643549\n",
      "Iteration 3, loss = 0.63394971\n",
      "Iteration 4, loss = 0.58144165\n",
      "Iteration 5, loss = 0.53862016\n",
      "Iteration 6, loss = 0.50386836\n",
      "Iteration 7, loss = 0.47518753\n",
      "Iteration 8, loss = 0.45120597\n",
      "Iteration 9, loss = 0.43100889\n",
      "Iteration 10, loss = 0.41406384\n",
      "Iteration 11, loss = 0.39949487\n",
      "Iteration 12, loss = 0.38679323\n",
      "Iteration 13, loss = 0.37591695\n",
      "Iteration 14, loss = 0.36624482\n",
      "Iteration 15, loss = 0.35784739\n",
      "Iteration 16, loss = 0.35024032\n",
      "Iteration 17, loss = 0.34359388\n",
      "Iteration 18, loss = 0.33765860\n",
      "Iteration 19, loss = 0.33216762\n",
      "Iteration 20, loss = 0.32730015\n",
      "Iteration 21, loss = 0.32285661\n",
      "Iteration 22, loss = 0.31878686\n",
      "Iteration 23, loss = 0.31505591\n",
      "Iteration 24, loss = 0.31163383\n",
      "Iteration 25, loss = 0.30851890\n",
      "Iteration 26, loss = 0.30558613\n",
      "Iteration 27, loss = 0.30291448\n",
      "Iteration 28, loss = 0.30041482\n",
      "Iteration 29, loss = 0.29810771\n",
      "Iteration 30, loss = 0.29586230\n",
      "Iteration 31, loss = 0.29388970\n",
      "Iteration 32, loss = 0.29190726\n",
      "Iteration 33, loss = 0.29017032\n",
      "Iteration 34, loss = 0.28846448\n",
      "Iteration 35, loss = 0.28687830\n",
      "Iteration 36, loss = 0.28537830\n",
      "Iteration 37, loss = 0.28393616\n",
      "Iteration 38, loss = 0.28264608\n",
      "Iteration 39, loss = 0.28131546\n",
      "Iteration 40, loss = 0.28010452\n",
      "Iteration 41, loss = 0.27899505\n",
      "Iteration 42, loss = 0.27787415\n",
      "Iteration 43, loss = 0.27681257\n",
      "Iteration 44, loss = 0.27584235\n",
      "Iteration 45, loss = 0.27486846\n",
      "Iteration 46, loss = 0.27397435\n",
      "Iteration 47, loss = 0.27309576\n",
      "Iteration 48, loss = 0.27222164\n",
      "Iteration 49, loss = 0.27145615\n",
      "Iteration 50, loss = 0.27064166\n",
      "Iteration 51, loss = 0.26990685\n",
      "Iteration 52, loss = 0.26919599\n",
      "Iteration 53, loss = 0.26849144\n",
      "Iteration 54, loss = 0.26782267\n",
      "Iteration 55, loss = 0.26716069\n",
      "Iteration 56, loss = 0.26655900\n",
      "Iteration 57, loss = 0.26594008\n",
      "Iteration 58, loss = 0.26538163\n",
      "Iteration 59, loss = 0.26479694\n",
      "Iteration 60, loss = 0.26424286\n",
      "Iteration 61, loss = 0.26373347\n",
      "Iteration 62, loss = 0.26319397\n",
      "Iteration 63, loss = 0.26270302\n",
      "Iteration 64, loss = 0.26220386\n",
      "Iteration 65, loss = 0.26175320\n",
      "Iteration 66, loss = 0.26126140\n",
      "Iteration 67, loss = 0.26081550\n",
      "Iteration 68, loss = 0.26038621\n",
      "Iteration 69, loss = 0.25994749\n",
      "Iteration 70, loss = 0.25953798\n",
      "Iteration 71, loss = 0.25913624\n",
      "Iteration 72, loss = 0.25875097\n",
      "Iteration 73, loss = 0.25834404\n",
      "Iteration 74, loss = 0.25794726\n",
      "Iteration 75, loss = 0.25760517\n",
      "Iteration 76, loss = 0.25723264\n",
      "Iteration 77, loss = 0.25685864\n",
      "Iteration 78, loss = 0.25652180\n",
      "Iteration 79, loss = 0.25617910\n",
      "Iteration 80, loss = 0.25582647\n",
      "Iteration 81, loss = 0.25551409\n",
      "Iteration 82, loss = 0.25524352\n",
      "Iteration 83, loss = 0.25486663\n",
      "Iteration 84, loss = 0.25456192\n",
      "Iteration 85, loss = 0.25423359\n",
      "Iteration 86, loss = 0.25395597\n",
      "Iteration 87, loss = 0.25364052\n",
      "Iteration 88, loss = 0.25335190\n",
      "Iteration 89, loss = 0.25306329\n",
      "Iteration 90, loss = 0.25278268\n",
      "Iteration 91, loss = 0.25250126\n",
      "Iteration 92, loss = 0.25225171\n",
      "Iteration 93, loss = 0.25195215\n",
      "Iteration 94, loss = 0.25168863\n",
      "Iteration 95, loss = 0.25142342\n",
      "Iteration 96, loss = 0.25119015\n",
      "Iteration 97, loss = 0.25091922\n",
      "Iteration 98, loss = 0.25064306\n",
      "Iteration 99, loss = 0.25040962\n",
      "Iteration 100, loss = 0.25019267\n",
      "Iteration 101, loss = 0.24992752\n",
      "Iteration 102, loss = 0.24969762\n",
      "Iteration 103, loss = 0.24947677\n",
      "Iteration 104, loss = 0.24920487\n",
      "Iteration 105, loss = 0.24898395\n",
      "Iteration 106, loss = 0.24875841\n",
      "Iteration 107, loss = 0.24853533\n",
      "Iteration 108, loss = 0.24830490\n",
      "Iteration 109, loss = 0.24808514\n",
      "Iteration 110, loss = 0.24787530\n",
      "Iteration 111, loss = 0.24768064\n",
      "Iteration 112, loss = 0.24745069\n",
      "Iteration 113, loss = 0.24723345\n",
      "Iteration 114, loss = 0.24703680\n",
      "Iteration 115, loss = 0.24682629\n",
      "Iteration 116, loss = 0.24662073\n",
      "Iteration 117, loss = 0.24641938\n",
      "Iteration 118, loss = 0.24619958\n",
      "Iteration 119, loss = 0.24603749\n",
      "Iteration 120, loss = 0.24583179\n",
      "Iteration 121, loss = 0.24563661\n",
      "Iteration 122, loss = 0.24544696\n",
      "Iteration 123, loss = 0.24527038\n",
      "Iteration 124, loss = 0.24509059\n",
      "Iteration 125, loss = 0.24488740\n",
      "Iteration 126, loss = 0.24469391\n",
      "Iteration 127, loss = 0.24452458\n",
      "Iteration 128, loss = 0.24434405\n",
      "Iteration 129, loss = 0.24416002\n",
      "Iteration 130, loss = 0.24402464\n",
      "Iteration 131, loss = 0.24380599\n",
      "Iteration 132, loss = 0.24363510\n",
      "Iteration 133, loss = 0.24346937\n",
      "Iteration 134, loss = 0.24334661\n",
      "Iteration 135, loss = 0.24310569\n",
      "Iteration 136, loss = 0.24294730\n",
      "Iteration 137, loss = 0.24279132\n",
      "Iteration 138, loss = 0.24261668\n",
      "Iteration 139, loss = 0.24245235\n",
      "Iteration 140, loss = 0.24230520\n",
      "Iteration 141, loss = 0.24214778\n",
      "Iteration 142, loss = 0.24196627\n",
      "Iteration 143, loss = 0.24183699\n",
      "Iteration 144, loss = 0.24169024\n",
      "Iteration 145, loss = 0.24150999\n",
      "Iteration 146, loss = 0.24135748\n",
      "Iteration 147, loss = 0.24120970\n",
      "Iteration 148, loss = 0.24105957\n",
      "Iteration 149, loss = 0.24091251\n",
      "Iteration 150, loss = 0.24075339\n",
      "Iteration 151, loss = 0.24060514\n",
      "Iteration 152, loss = 0.24051548\n",
      "Iteration 153, loss = 0.24032016\n",
      "Iteration 154, loss = 0.24017404\n",
      "Iteration 155, loss = 0.24003365\n",
      "Iteration 156, loss = 0.23989191\n",
      "Iteration 157, loss = 0.23976587\n",
      "Iteration 158, loss = 0.23959169\n",
      "Iteration 159, loss = 0.23947495\n",
      "Iteration 160, loss = 0.23931860\n",
      "Iteration 161, loss = 0.23919875\n",
      "Iteration 162, loss = 0.23907468\n",
      "Iteration 163, loss = 0.23891129\n",
      "Iteration 164, loss = 0.23879922\n",
      "Iteration 165, loss = 0.23866046\n",
      "Iteration 166, loss = 0.23853777\n",
      "Iteration 167, loss = 0.23839569\n",
      "Iteration 168, loss = 0.23826447\n",
      "Iteration 169, loss = 0.23814582\n",
      "Iteration 170, loss = 0.23800802\n",
      "Iteration 171, loss = 0.23787747\n",
      "Iteration 172, loss = 0.23776744\n",
      "Iteration 173, loss = 0.23762807\n",
      "Iteration 174, loss = 0.23750338\n",
      "Iteration 175, loss = 0.23741765\n",
      "Iteration 176, loss = 0.23725859\n",
      "Iteration 177, loss = 0.23713201\n",
      "Iteration 178, loss = 0.23704424\n",
      "Iteration 179, loss = 0.23691255\n",
      "Iteration 180, loss = 0.23678157\n",
      "Iteration 181, loss = 0.23667732\n",
      "Iteration 182, loss = 0.23653252\n",
      "Iteration 183, loss = 0.23644642\n",
      "Iteration 184, loss = 0.23633722\n",
      "Iteration 185, loss = 0.23620683\n",
      "Iteration 186, loss = 0.23608850\n",
      "Iteration 187, loss = 0.23599925\n",
      "Iteration 188, loss = 0.23586549\n",
      "Iteration 189, loss = 0.23574046\n",
      "Iteration 190, loss = 0.23567605\n",
      "Iteration 191, loss = 0.23553936\n",
      "Iteration 192, loss = 0.23542967\n",
      "Iteration 193, loss = 0.23530978\n",
      "Iteration 194, loss = 0.23522153\n",
      "Iteration 195, loss = 0.23512981\n",
      "Iteration 196, loss = 0.23498865\n",
      "Iteration 197, loss = 0.23488075\n",
      "Iteration 198, loss = 0.23477140\n",
      "Iteration 199, loss = 0.23465517\n",
      "Iteration 200, loss = 0.23456973\n",
      "Iteration 1, loss = 0.76143622\n",
      "Iteration 2, loss = 0.69878427\n",
      "Iteration 3, loss = 0.63444373\n",
      "Iteration 4, loss = 0.58077935\n",
      "Iteration 5, loss = 0.53723248\n",
      "Iteration 6, loss = 0.50171937\n",
      "Iteration 7, loss = 0.47284670\n",
      "Iteration 8, loss = 0.44846892\n",
      "Iteration 9, loss = 0.42830995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.41139901\n",
      "Iteration 11, loss = 0.39680231\n",
      "Iteration 12, loss = 0.38425129\n",
      "Iteration 13, loss = 0.37346258\n",
      "Iteration 14, loss = 0.36389014\n",
      "Iteration 15, loss = 0.35561358\n",
      "Iteration 16, loss = 0.34815153\n",
      "Iteration 17, loss = 0.34163728\n",
      "Iteration 18, loss = 0.33579960\n",
      "Iteration 19, loss = 0.33049669\n",
      "Iteration 20, loss = 0.32572942\n",
      "Iteration 21, loss = 0.32145158\n",
      "Iteration 22, loss = 0.31752925\n",
      "Iteration 23, loss = 0.31388375\n",
      "Iteration 24, loss = 0.31064154\n",
      "Iteration 25, loss = 0.30758953\n",
      "Iteration 26, loss = 0.30483139\n",
      "Iteration 27, loss = 0.30225714\n",
      "Iteration 28, loss = 0.29987374\n",
      "Iteration 29, loss = 0.29768240\n",
      "Iteration 30, loss = 0.29555766\n",
      "Iteration 31, loss = 0.29366329\n",
      "Iteration 32, loss = 0.29181974\n",
      "Iteration 33, loss = 0.29015846\n",
      "Iteration 34, loss = 0.28856018\n",
      "Iteration 35, loss = 0.28707685\n",
      "Iteration 36, loss = 0.28566915\n",
      "Iteration 37, loss = 0.28433596\n",
      "Iteration 38, loss = 0.28308553\n",
      "Iteration 39, loss = 0.28186993\n",
      "Iteration 40, loss = 0.28073216\n",
      "Iteration 41, loss = 0.27968388\n",
      "Iteration 42, loss = 0.27866896\n",
      "Iteration 43, loss = 0.27767855\n",
      "Iteration 44, loss = 0.27678120\n",
      "Iteration 45, loss = 0.27587577\n",
      "Iteration 46, loss = 0.27504569\n",
      "Iteration 47, loss = 0.27425363\n",
      "Iteration 48, loss = 0.27344181\n",
      "Iteration 49, loss = 0.27274711\n",
      "Iteration 50, loss = 0.27200252\n",
      "Iteration 51, loss = 0.27135912\n",
      "Iteration 52, loss = 0.27069171\n",
      "Iteration 53, loss = 0.27004345\n",
      "Iteration 54, loss = 0.26942233\n",
      "Iteration 55, loss = 0.26882955\n",
      "Iteration 56, loss = 0.26827625\n",
      "Iteration 57, loss = 0.26773647\n",
      "Iteration 58, loss = 0.26722090\n",
      "Iteration 59, loss = 0.26668851\n",
      "Iteration 60, loss = 0.26619682\n",
      "Iteration 61, loss = 0.26572248\n",
      "Iteration 62, loss = 0.26524289\n",
      "Iteration 63, loss = 0.26481056\n",
      "Iteration 64, loss = 0.26435235\n",
      "Iteration 65, loss = 0.26396092\n",
      "Iteration 66, loss = 0.26352915\n",
      "Iteration 67, loss = 0.26312554\n",
      "Iteration 68, loss = 0.26273183\n",
      "Iteration 69, loss = 0.26234736\n",
      "Iteration 70, loss = 0.26198787\n",
      "Iteration 71, loss = 0.26161981\n",
      "Iteration 72, loss = 0.26125972\n",
      "Iteration 73, loss = 0.26090728\n",
      "Iteration 74, loss = 0.26057122\n",
      "Iteration 75, loss = 0.26023719\n",
      "Iteration 76, loss = 0.25992466\n",
      "Iteration 77, loss = 0.25961533\n",
      "Iteration 78, loss = 0.25931553\n",
      "Iteration 79, loss = 0.25898562\n",
      "Iteration 80, loss = 0.25870814\n",
      "Iteration 81, loss = 0.25841530\n",
      "Iteration 82, loss = 0.25816484\n",
      "Iteration 83, loss = 0.25785273\n",
      "Iteration 84, loss = 0.25756828\n",
      "Iteration 85, loss = 0.25726876\n",
      "Iteration 86, loss = 0.25703681\n",
      "Iteration 87, loss = 0.25676190\n",
      "Iteration 88, loss = 0.25650302\n",
      "Iteration 89, loss = 0.25624814\n",
      "Iteration 90, loss = 0.25599625\n",
      "Iteration 91, loss = 0.25576798\n",
      "Iteration 92, loss = 0.25551923\n",
      "Iteration 93, loss = 0.25527723\n",
      "Iteration 94, loss = 0.25505698\n",
      "Iteration 95, loss = 0.25482752\n",
      "Iteration 96, loss = 0.25463691\n",
      "Iteration 97, loss = 0.25437260\n",
      "Iteration 98, loss = 0.25415096\n",
      "Iteration 99, loss = 0.25393503\n",
      "Iteration 100, loss = 0.25373654\n",
      "Iteration 101, loss = 0.25353007\n",
      "Iteration 102, loss = 0.25331107\n",
      "Iteration 103, loss = 0.25311842\n",
      "Iteration 104, loss = 0.25288928\n",
      "Iteration 105, loss = 0.25269171\n",
      "Iteration 106, loss = 0.25248966\n",
      "Iteration 107, loss = 0.25229978\n",
      "Iteration 108, loss = 0.25212685\n",
      "Iteration 109, loss = 0.25191634\n",
      "Iteration 110, loss = 0.25173295\n",
      "Iteration 111, loss = 0.25155947\n",
      "Iteration 112, loss = 0.25136381\n",
      "Iteration 113, loss = 0.25117465\n",
      "Iteration 114, loss = 0.25099177\n",
      "Iteration 115, loss = 0.25081917\n",
      "Iteration 116, loss = 0.25064514\n",
      "Iteration 117, loss = 0.25046708\n",
      "Iteration 118, loss = 0.25027578\n",
      "Iteration 119, loss = 0.25012989\n",
      "Iteration 120, loss = 0.24994917\n",
      "Iteration 121, loss = 0.24978693\n",
      "Iteration 122, loss = 0.24963495\n",
      "Iteration 123, loss = 0.24946112\n",
      "Iteration 124, loss = 0.24930377\n",
      "Iteration 125, loss = 0.24913241\n",
      "Iteration 126, loss = 0.24897784\n",
      "Iteration 127, loss = 0.24881858\n",
      "Iteration 128, loss = 0.24866615\n",
      "Iteration 129, loss = 0.24850564\n",
      "Iteration 130, loss = 0.24837725\n",
      "Iteration 131, loss = 0.24818657\n",
      "Iteration 132, loss = 0.24804407\n",
      "Iteration 133, loss = 0.24789214\n",
      "Iteration 134, loss = 0.24777497\n",
      "Iteration 135, loss = 0.24758382\n",
      "Iteration 136, loss = 0.24745232\n",
      "Iteration 137, loss = 0.24729583\n",
      "Iteration 138, loss = 0.24716235\n",
      "Iteration 139, loss = 0.24701431\n",
      "Iteration 140, loss = 0.24689508\n",
      "Iteration 141, loss = 0.24673619\n",
      "Iteration 142, loss = 0.24659960\n",
      "Iteration 143, loss = 0.24648233\n",
      "Iteration 144, loss = 0.24633591\n",
      "Iteration 145, loss = 0.24619347\n",
      "Iteration 146, loss = 0.24604851\n",
      "Iteration 147, loss = 0.24593865\n",
      "Iteration 148, loss = 0.24579854\n",
      "Iteration 149, loss = 0.24566471\n",
      "Iteration 150, loss = 0.24554711\n",
      "Iteration 151, loss = 0.24541412\n",
      "Iteration 152, loss = 0.24531280\n",
      "Iteration 153, loss = 0.24515076\n",
      "Iteration 154, loss = 0.24503599\n",
      "Iteration 155, loss = 0.24490780\n",
      "Iteration 156, loss = 0.24475931\n",
      "Iteration 157, loss = 0.24466930\n",
      "Iteration 158, loss = 0.24451853\n",
      "Iteration 159, loss = 0.24440415\n",
      "Iteration 160, loss = 0.24428180\n",
      "Iteration 161, loss = 0.24418293\n",
      "Iteration 162, loss = 0.24406614\n",
      "Iteration 163, loss = 0.24391897\n",
      "Iteration 164, loss = 0.24381111\n",
      "Iteration 165, loss = 0.24369005\n",
      "Iteration 166, loss = 0.24355916\n",
      "Iteration 167, loss = 0.24344906\n",
      "Iteration 168, loss = 0.24334136\n",
      "Iteration 169, loss = 0.24324170\n",
      "Iteration 170, loss = 0.24311593\n",
      "Iteration 171, loss = 0.24301411\n",
      "Iteration 172, loss = 0.24289412\n",
      "Iteration 173, loss = 0.24278559\n",
      "Iteration 174, loss = 0.24269288\n",
      "Iteration 175, loss = 0.24262021\n",
      "Iteration 176, loss = 0.24244512\n",
      "Iteration 177, loss = 0.24234675\n",
      "Iteration 178, loss = 0.24226227\n",
      "Iteration 179, loss = 0.24214016\n",
      "Iteration 180, loss = 0.24203154\n",
      "Iteration 181, loss = 0.24194757\n",
      "Iteration 182, loss = 0.24182213\n",
      "Iteration 183, loss = 0.24174270\n",
      "Iteration 184, loss = 0.24162181\n",
      "Iteration 185, loss = 0.24150696\n",
      "Iteration 186, loss = 0.24141696\n",
      "Iteration 187, loss = 0.24135001\n",
      "Iteration 188, loss = 0.24121090\n",
      "Iteration 189, loss = 0.24110920\n",
      "Iteration 190, loss = 0.24104330\n",
      "Iteration 191, loss = 0.24094943\n",
      "Iteration 192, loss = 0.24083748\n",
      "Iteration 193, loss = 0.24071309\n",
      "Iteration 194, loss = 0.24064477\n",
      "Iteration 195, loss = 0.24057030\n",
      "Iteration 196, loss = 0.24042488\n",
      "Iteration 197, loss = 0.24032672\n",
      "Iteration 198, loss = 0.24024430\n",
      "Iteration 199, loss = 0.24013267\n",
      "Iteration 200, loss = 0.24005160\n",
      "Iteration 1, loss = 0.68558826\n",
      "Iteration 2, loss = 0.53830094\n",
      "Iteration 3, loss = 0.45087743\n",
      "Iteration 4, loss = 0.39195086\n",
      "Iteration 5, loss = 0.34980838\n",
      "Iteration 6, loss = 0.31956332\n",
      "Iteration 7, loss = 0.29853263\n",
      "Iteration 8, loss = 0.28319724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.27157039\n",
      "Iteration 10, loss = 0.26315379\n",
      "Iteration 11, loss = 0.25642051\n",
      "Iteration 12, loss = 0.25068772\n",
      "Iteration 13, loss = 0.24676908\n",
      "Iteration 14, loss = 0.24304626\n",
      "Iteration 15, loss = 0.23992050\n",
      "Iteration 16, loss = 0.23743372\n",
      "Iteration 17, loss = 0.23455791\n",
      "Iteration 18, loss = 0.23283606\n",
      "Iteration 19, loss = 0.23086640\n",
      "Iteration 20, loss = 0.22942566\n",
      "Iteration 21, loss = 0.22716275\n",
      "Iteration 22, loss = 0.22654750\n",
      "Iteration 23, loss = 0.22445528\n",
      "Iteration 24, loss = 0.22272178\n",
      "Iteration 25, loss = 0.22172490\n",
      "Iteration 26, loss = 0.22032696\n",
      "Iteration 27, loss = 0.21892129\n",
      "Iteration 28, loss = 0.21791463\n",
      "Iteration 29, loss = 0.21576410\n",
      "Iteration 30, loss = 0.21524097\n",
      "Iteration 31, loss = 0.21360184\n",
      "Iteration 32, loss = 0.21204988\n",
      "Iteration 33, loss = 0.21045063\n",
      "Iteration 34, loss = 0.20941028\n",
      "Iteration 35, loss = 0.20865601\n",
      "Iteration 36, loss = 0.20715310\n",
      "Iteration 37, loss = 0.20514715\n",
      "Iteration 38, loss = 0.20364405\n",
      "Iteration 39, loss = 0.20239716\n",
      "Iteration 40, loss = 0.20153636\n",
      "Iteration 41, loss = 0.19966724\n",
      "Iteration 42, loss = 0.19828487\n",
      "Iteration 43, loss = 0.19633722\n",
      "Iteration 44, loss = 0.19532128\n",
      "Iteration 45, loss = 0.19425635\n",
      "Iteration 46, loss = 0.19229255\n",
      "Iteration 47, loss = 0.19164857\n",
      "Iteration 48, loss = 0.18937438\n",
      "Iteration 49, loss = 0.18805228\n",
      "Iteration 50, loss = 0.18552751\n",
      "Iteration 51, loss = 0.18462302\n",
      "Iteration 52, loss = 0.18348090\n",
      "Iteration 53, loss = 0.18151069\n",
      "Iteration 54, loss = 0.17994312\n",
      "Iteration 55, loss = 0.17837778\n",
      "Iteration 56, loss = 0.17648546\n",
      "Iteration 57, loss = 0.17598063\n",
      "Iteration 58, loss = 0.17402915\n",
      "Iteration 59, loss = 0.17205549\n",
      "Iteration 60, loss = 0.17094630\n",
      "Iteration 61, loss = 0.16927280\n",
      "Iteration 62, loss = 0.16783040\n",
      "Iteration 63, loss = 0.16639334\n",
      "Iteration 64, loss = 0.16523745\n",
      "Iteration 65, loss = 0.16344217\n",
      "Iteration 66, loss = 0.16216383\n",
      "Iteration 67, loss = 0.16035210\n",
      "Iteration 68, loss = 0.15897193\n",
      "Iteration 69, loss = 0.15775696\n",
      "Iteration 70, loss = 0.15578715\n",
      "Iteration 71, loss = 0.15445955\n",
      "Iteration 72, loss = 0.15304267\n",
      "Iteration 73, loss = 0.15208235\n",
      "Iteration 74, loss = 0.15019359\n",
      "Iteration 75, loss = 0.14905710\n",
      "Iteration 76, loss = 0.14735166\n",
      "Iteration 77, loss = 0.14674836\n",
      "Iteration 78, loss = 0.14485147\n",
      "Iteration 79, loss = 0.14370599\n",
      "Iteration 80, loss = 0.14189278\n",
      "Iteration 81, loss = 0.14093926\n",
      "Iteration 82, loss = 0.14006476\n",
      "Iteration 83, loss = 0.13828151\n",
      "Iteration 84, loss = 0.13709161\n",
      "Iteration 85, loss = 0.13574872\n",
      "Iteration 86, loss = 0.13412533\n",
      "Iteration 87, loss = 0.13328691\n",
      "Iteration 88, loss = 0.13158651\n",
      "Iteration 89, loss = 0.13026859\n",
      "Iteration 90, loss = 0.12947410\n",
      "Iteration 91, loss = 0.12784468\n",
      "Iteration 92, loss = 0.12692156\n",
      "Iteration 93, loss = 0.12529349\n",
      "Iteration 94, loss = 0.12420306\n",
      "Iteration 95, loss = 0.12296704\n",
      "Iteration 96, loss = 0.12183985\n",
      "Iteration 97, loss = 0.12048183\n",
      "Iteration 98, loss = 0.11945335\n",
      "Iteration 99, loss = 0.11811138\n",
      "Iteration 100, loss = 0.11759647\n",
      "Iteration 101, loss = 0.11640027\n",
      "Iteration 102, loss = 0.11482683\n",
      "Iteration 103, loss = 0.11411140\n",
      "Iteration 104, loss = 0.11228679\n",
      "Iteration 105, loss = 0.11184791\n",
      "Iteration 106, loss = 0.11036775\n",
      "Iteration 107, loss = 0.10980473\n",
      "Iteration 108, loss = 0.10834195\n",
      "Iteration 109, loss = 0.10701289\n",
      "Iteration 110, loss = 0.10638007\n",
      "Iteration 111, loss = 0.10504877\n",
      "Iteration 112, loss = 0.10409894\n",
      "Iteration 113, loss = 0.10296906\n",
      "Iteration 114, loss = 0.10200776\n",
      "Iteration 115, loss = 0.10115204\n",
      "Iteration 116, loss = 0.09962945\n",
      "Iteration 117, loss = 0.09935291\n",
      "Iteration 118, loss = 0.09779492\n",
      "Iteration 119, loss = 0.09689610\n",
      "Iteration 120, loss = 0.09597908\n",
      "Iteration 121, loss = 0.09497081\n",
      "Iteration 122, loss = 0.09375663\n",
      "Iteration 123, loss = 0.09277903\n",
      "Iteration 124, loss = 0.09184879\n",
      "Iteration 125, loss = 0.09141842\n",
      "Iteration 126, loss = 0.09037584\n",
      "Iteration 127, loss = 0.08947886\n",
      "Iteration 128, loss = 0.08844588\n",
      "Iteration 129, loss = 0.08728388\n",
      "Iteration 130, loss = 0.08665142\n",
      "Iteration 131, loss = 0.08575375\n",
      "Iteration 132, loss = 0.08541362\n",
      "Iteration 133, loss = 0.08415828\n",
      "Iteration 134, loss = 0.08296947\n",
      "Iteration 135, loss = 0.08249650\n",
      "Iteration 136, loss = 0.08106192\n",
      "Iteration 137, loss = 0.08073706\n",
      "Iteration 138, loss = 0.07954578\n",
      "Iteration 139, loss = 0.07907819\n",
      "Iteration 140, loss = 0.07794373\n",
      "Iteration 141, loss = 0.07684916\n",
      "Iteration 142, loss = 0.07635764\n",
      "Iteration 143, loss = 0.07563639\n",
      "Iteration 144, loss = 0.07463840\n",
      "Iteration 145, loss = 0.07399203\n",
      "Iteration 146, loss = 0.07279912\n",
      "Iteration 147, loss = 0.07257920\n",
      "Iteration 148, loss = 0.07126854\n",
      "Iteration 149, loss = 0.07087961\n",
      "Iteration 150, loss = 0.06964107\n",
      "Iteration 151, loss = 0.06951104\n",
      "Iteration 152, loss = 0.06823607\n",
      "Iteration 153, loss = 0.06767617\n",
      "Iteration 154, loss = 0.06672280\n",
      "Iteration 155, loss = 0.06622125\n",
      "Iteration 156, loss = 0.06531862\n",
      "Iteration 157, loss = 0.06484910\n",
      "Iteration 158, loss = 0.06429345\n",
      "Iteration 159, loss = 0.06309464\n",
      "Iteration 160, loss = 0.06266881\n",
      "Iteration 161, loss = 0.06196689\n",
      "Iteration 162, loss = 0.06125029\n",
      "Iteration 163, loss = 0.06035011\n",
      "Iteration 164, loss = 0.05981035\n",
      "Iteration 165, loss = 0.05903437\n",
      "Iteration 166, loss = 0.05854965\n",
      "Iteration 167, loss = 0.05808908\n",
      "Iteration 168, loss = 0.05725068\n",
      "Iteration 169, loss = 0.05683533\n",
      "Iteration 170, loss = 0.05649539\n",
      "Iteration 171, loss = 0.05515384\n",
      "Iteration 172, loss = 0.05477337\n",
      "Iteration 173, loss = 0.05375723\n",
      "Iteration 174, loss = 0.05316024\n",
      "Iteration 175, loss = 0.05246565\n",
      "Iteration 176, loss = 0.05228293\n",
      "Iteration 177, loss = 0.05134149\n",
      "Iteration 178, loss = 0.05080468\n",
      "Iteration 179, loss = 0.05012356\n",
      "Iteration 180, loss = 0.04963608\n",
      "Iteration 181, loss = 0.04900272\n",
      "Iteration 182, loss = 0.04854326\n",
      "Iteration 183, loss = 0.04795328\n",
      "Iteration 184, loss = 0.04702953\n",
      "Iteration 185, loss = 0.04684833\n",
      "Iteration 186, loss = 0.04641794\n",
      "Iteration 187, loss = 0.04584967\n",
      "Iteration 188, loss = 0.04478240\n",
      "Iteration 189, loss = 0.04441940\n",
      "Iteration 190, loss = 0.04394259\n",
      "Iteration 191, loss = 0.04334058\n",
      "Iteration 192, loss = 0.04301630\n",
      "Iteration 193, loss = 0.04263042\n",
      "Iteration 194, loss = 0.04196473\n",
      "Iteration 195, loss = 0.04151096\n",
      "Iteration 196, loss = 0.04063460\n",
      "Iteration 197, loss = 0.04048081\n",
      "Iteration 198, loss = 0.03994209\n",
      "Iteration 199, loss = 0.03941559\n",
      "Iteration 200, loss = 0.03890171\n",
      "Iteration 1, loss = 0.67894448\n",
      "Iteration 2, loss = 0.53532958\n",
      "Iteration 3, loss = 0.44792547\n",
      "Iteration 4, loss = 0.38892182\n",
      "Iteration 5, loss = 0.34700180\n",
      "Iteration 6, loss = 0.31721224\n",
      "Iteration 7, loss = 0.29626722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.28083092\n",
      "Iteration 9, loss = 0.26976174\n",
      "Iteration 10, loss = 0.26151261\n",
      "Iteration 11, loss = 0.25484240\n",
      "Iteration 12, loss = 0.24956643\n",
      "Iteration 13, loss = 0.24559034\n",
      "Iteration 14, loss = 0.24196410\n",
      "Iteration 15, loss = 0.23884019\n",
      "Iteration 16, loss = 0.23641547\n",
      "Iteration 17, loss = 0.23446217\n",
      "Iteration 18, loss = 0.23213130\n",
      "Iteration 19, loss = 0.23031198\n",
      "Iteration 20, loss = 0.22847182\n",
      "Iteration 21, loss = 0.22704066\n",
      "Iteration 22, loss = 0.22583203\n",
      "Iteration 23, loss = 0.22426598\n",
      "Iteration 24, loss = 0.22286318\n",
      "Iteration 25, loss = 0.22156565\n",
      "Iteration 26, loss = 0.22033301\n",
      "Iteration 27, loss = 0.21911225\n",
      "Iteration 28, loss = 0.21815091\n",
      "Iteration 29, loss = 0.21660283\n",
      "Iteration 30, loss = 0.21551220\n",
      "Iteration 31, loss = 0.21413305\n",
      "Iteration 32, loss = 0.21285250\n",
      "Iteration 33, loss = 0.21170516\n",
      "Iteration 34, loss = 0.21093214\n",
      "Iteration 35, loss = 0.20966651\n",
      "Iteration 36, loss = 0.20837783\n",
      "Iteration 37, loss = 0.20659657\n",
      "Iteration 38, loss = 0.20507906\n",
      "Iteration 39, loss = 0.20408027\n",
      "Iteration 40, loss = 0.20316906\n",
      "Iteration 41, loss = 0.20173217\n",
      "Iteration 42, loss = 0.20031182\n",
      "Iteration 43, loss = 0.19862226\n",
      "Iteration 44, loss = 0.19743670\n",
      "Iteration 45, loss = 0.19628688\n",
      "Iteration 46, loss = 0.19529239\n",
      "Iteration 47, loss = 0.19335734\n",
      "Iteration 48, loss = 0.19212917\n",
      "Iteration 49, loss = 0.19115343\n",
      "Iteration 50, loss = 0.18863068\n",
      "Iteration 51, loss = 0.18788148\n",
      "Iteration 52, loss = 0.18632811\n",
      "Iteration 53, loss = 0.18456413\n",
      "Iteration 54, loss = 0.18378386\n",
      "Iteration 55, loss = 0.18215420\n",
      "Iteration 56, loss = 0.17996404\n",
      "Iteration 57, loss = 0.17932764\n",
      "Iteration 58, loss = 0.17866948\n",
      "Iteration 59, loss = 0.17642955\n",
      "Iteration 60, loss = 0.17509764\n",
      "Iteration 61, loss = 0.17308070\n",
      "Iteration 62, loss = 0.17181517\n",
      "Iteration 63, loss = 0.17047169\n",
      "Iteration 64, loss = 0.16870099\n",
      "Iteration 65, loss = 0.16724116\n",
      "Iteration 66, loss = 0.16614538\n",
      "Iteration 67, loss = 0.16443073\n",
      "Iteration 68, loss = 0.16329644\n",
      "Iteration 69, loss = 0.16175020\n",
      "Iteration 70, loss = 0.16038673\n",
      "Iteration 71, loss = 0.15878582\n",
      "Iteration 72, loss = 0.15773815\n",
      "Iteration 73, loss = 0.15599359\n",
      "Iteration 74, loss = 0.15443299\n",
      "Iteration 75, loss = 0.15374482\n",
      "Iteration 76, loss = 0.15200701\n",
      "Iteration 77, loss = 0.15075492\n",
      "Iteration 78, loss = 0.14945626\n",
      "Iteration 79, loss = 0.14820810\n",
      "Iteration 80, loss = 0.14642074\n",
      "Iteration 81, loss = 0.14551040\n",
      "Iteration 82, loss = 0.14463239\n",
      "Iteration 83, loss = 0.14289117\n",
      "Iteration 84, loss = 0.14202309\n",
      "Iteration 85, loss = 0.14054200\n",
      "Iteration 86, loss = 0.13871030\n",
      "Iteration 87, loss = 0.13794297\n",
      "Iteration 88, loss = 0.13640200\n",
      "Iteration 89, loss = 0.13549069\n",
      "Iteration 90, loss = 0.13435825\n",
      "Iteration 91, loss = 0.13244274\n",
      "Iteration 92, loss = 0.13126146\n",
      "Iteration 93, loss = 0.13031470\n",
      "Iteration 94, loss = 0.12909291\n",
      "Iteration 95, loss = 0.12782191\n",
      "Iteration 96, loss = 0.12683306\n",
      "Iteration 97, loss = 0.12519411\n",
      "Iteration 98, loss = 0.12436358\n",
      "Iteration 99, loss = 0.12342160\n",
      "Iteration 100, loss = 0.12233386\n",
      "Iteration 101, loss = 0.12106114\n",
      "Iteration 102, loss = 0.11962309\n",
      "Iteration 103, loss = 0.11895802\n",
      "Iteration 104, loss = 0.11727067\n",
      "Iteration 105, loss = 0.11676296\n",
      "Iteration 106, loss = 0.11556260\n",
      "Iteration 107, loss = 0.11451937\n",
      "Iteration 108, loss = 0.11322105\n",
      "Iteration 109, loss = 0.11210835\n",
      "Iteration 110, loss = 0.11121303\n",
      "Iteration 111, loss = 0.10985187\n",
      "Iteration 112, loss = 0.10875719\n",
      "Iteration 113, loss = 0.10812541\n",
      "Iteration 114, loss = 0.10724013\n",
      "Iteration 115, loss = 0.10608366\n",
      "Iteration 116, loss = 0.10463726\n",
      "Iteration 117, loss = 0.10385348\n",
      "Iteration 118, loss = 0.10284047\n",
      "Iteration 119, loss = 0.10198111\n",
      "Iteration 120, loss = 0.10125484\n",
      "Iteration 121, loss = 0.09990793\n",
      "Iteration 122, loss = 0.09870838\n",
      "Iteration 123, loss = 0.09793570\n",
      "Iteration 124, loss = 0.09675032\n",
      "Iteration 125, loss = 0.09686490\n",
      "Iteration 126, loss = 0.09509342\n",
      "Iteration 127, loss = 0.09406963\n",
      "Iteration 128, loss = 0.09317118\n",
      "Iteration 129, loss = 0.09223352\n",
      "Iteration 130, loss = 0.09127254\n",
      "Iteration 131, loss = 0.09021647\n",
      "Iteration 132, loss = 0.08963106\n",
      "Iteration 133, loss = 0.08915024\n",
      "Iteration 134, loss = 0.08777814\n",
      "Iteration 135, loss = 0.08721683\n",
      "Iteration 136, loss = 0.08583578\n",
      "Iteration 137, loss = 0.08529811\n",
      "Iteration 138, loss = 0.08428422\n",
      "Iteration 139, loss = 0.08354293\n",
      "Iteration 140, loss = 0.08221582\n",
      "Iteration 141, loss = 0.08202076\n",
      "Iteration 142, loss = 0.08106836\n",
      "Iteration 143, loss = 0.08068342\n",
      "Iteration 144, loss = 0.07939988\n",
      "Iteration 145, loss = 0.07841968\n",
      "Iteration 146, loss = 0.07752114\n",
      "Iteration 147, loss = 0.07699284\n",
      "Iteration 148, loss = 0.07631889\n",
      "Iteration 149, loss = 0.07559939\n",
      "Iteration 150, loss = 0.07435594\n",
      "Iteration 151, loss = 0.07379533\n",
      "Iteration 152, loss = 0.07297797\n",
      "Iteration 153, loss = 0.07206687\n",
      "Iteration 154, loss = 0.07143030\n",
      "Iteration 155, loss = 0.07078818\n",
      "Iteration 156, loss = 0.06994503\n",
      "Iteration 157, loss = 0.06912067\n",
      "Iteration 158, loss = 0.06835553\n",
      "Iteration 159, loss = 0.06814671\n",
      "Iteration 160, loss = 0.06682196\n",
      "Iteration 161, loss = 0.06622124\n",
      "Iteration 162, loss = 0.06562515\n",
      "Iteration 163, loss = 0.06492736\n",
      "Iteration 164, loss = 0.06436559\n",
      "Iteration 165, loss = 0.06348228\n",
      "Iteration 166, loss = 0.06246994\n",
      "Iteration 167, loss = 0.06207735\n",
      "Iteration 168, loss = 0.06143920\n",
      "Iteration 169, loss = 0.06128724\n",
      "Iteration 170, loss = 0.06040570\n",
      "Iteration 171, loss = 0.05944974\n",
      "Iteration 172, loss = 0.05894719\n",
      "Iteration 173, loss = 0.05820792\n",
      "Iteration 174, loss = 0.05749494\n",
      "Iteration 175, loss = 0.05676899\n",
      "Iteration 176, loss = 0.05628058\n",
      "Iteration 177, loss = 0.05546137\n",
      "Iteration 178, loss = 0.05520733\n",
      "Iteration 179, loss = 0.05451664\n",
      "Iteration 180, loss = 0.05393058\n",
      "Iteration 181, loss = 0.05333400\n",
      "Iteration 182, loss = 0.05288777\n",
      "Iteration 183, loss = 0.05214009\n",
      "Iteration 184, loss = 0.05143804\n",
      "Iteration 185, loss = 0.05114253\n",
      "Iteration 186, loss = 0.05104475\n",
      "Iteration 187, loss = 0.05027120\n",
      "Iteration 188, loss = 0.04893817\n",
      "Iteration 189, loss = 0.04865461\n",
      "Iteration 190, loss = 0.04794427\n",
      "Iteration 191, loss = 0.04785636\n",
      "Iteration 192, loss = 0.04718255\n",
      "Iteration 193, loss = 0.04678287\n",
      "Iteration 194, loss = 0.04635807\n",
      "Iteration 195, loss = 0.04560382\n",
      "Iteration 196, loss = 0.04471136\n",
      "Iteration 197, loss = 0.04466676\n",
      "Iteration 198, loss = 0.04385336\n",
      "Iteration 199, loss = 0.04341022\n",
      "Iteration 200, loss = 0.04295871\n",
      "Iteration 1, loss = 0.67862104\n",
      "Iteration 2, loss = 0.53472209\n",
      "Iteration 3, loss = 0.44769817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.38983499\n",
      "Iteration 5, loss = 0.34843763\n",
      "Iteration 6, loss = 0.31913001\n",
      "Iteration 7, loss = 0.29845484\n",
      "Iteration 8, loss = 0.28390955\n",
      "Iteration 9, loss = 0.27236935\n",
      "Iteration 10, loss = 0.26428791\n",
      "Iteration 11, loss = 0.25817512\n",
      "Iteration 12, loss = 0.25291902\n",
      "Iteration 13, loss = 0.24947717\n",
      "Iteration 14, loss = 0.24594272\n",
      "Iteration 15, loss = 0.24285228\n",
      "Iteration 16, loss = 0.24040914\n",
      "Iteration 17, loss = 0.23890371\n",
      "Iteration 18, loss = 0.23653917\n",
      "Iteration 19, loss = 0.23451458\n",
      "Iteration 20, loss = 0.23279043\n",
      "Iteration 21, loss = 0.23161503\n",
      "Iteration 22, loss = 0.23068591\n",
      "Iteration 23, loss = 0.22884572\n",
      "Iteration 24, loss = 0.22700570\n",
      "Iteration 25, loss = 0.22583812\n",
      "Iteration 26, loss = 0.22477697\n",
      "Iteration 27, loss = 0.22322679\n",
      "Iteration 28, loss = 0.22233032\n",
      "Iteration 29, loss = 0.22058559\n",
      "Iteration 30, loss = 0.21920054\n",
      "Iteration 31, loss = 0.21802727\n",
      "Iteration 32, loss = 0.21725975\n",
      "Iteration 33, loss = 0.21573771\n",
      "Iteration 34, loss = 0.21463396\n",
      "Iteration 35, loss = 0.21312713\n",
      "Iteration 36, loss = 0.21227720\n",
      "Iteration 37, loss = 0.21074899\n",
      "Iteration 38, loss = 0.20927013\n",
      "Iteration 39, loss = 0.20768527\n",
      "Iteration 40, loss = 0.20679692\n",
      "Iteration 41, loss = 0.20553175\n",
      "Iteration 42, loss = 0.20402740\n",
      "Iteration 43, loss = 0.20255760\n",
      "Iteration 44, loss = 0.20118942\n",
      "Iteration 45, loss = 0.19974075\n",
      "Iteration 46, loss = 0.19844735\n",
      "Iteration 47, loss = 0.19661429\n",
      "Iteration 48, loss = 0.19542479\n",
      "Iteration 49, loss = 0.19408072\n",
      "Iteration 50, loss = 0.19266922\n",
      "Iteration 51, loss = 0.19139666\n",
      "Iteration 52, loss = 0.18985368\n",
      "Iteration 53, loss = 0.18805016\n",
      "Iteration 54, loss = 0.18729723\n",
      "Iteration 55, loss = 0.18505645\n",
      "Iteration 56, loss = 0.18330418\n",
      "Iteration 57, loss = 0.18275551\n",
      "Iteration 58, loss = 0.18080273\n",
      "Iteration 59, loss = 0.17940474\n",
      "Iteration 60, loss = 0.17793910\n",
      "Iteration 61, loss = 0.17637302\n",
      "Iteration 62, loss = 0.17465273\n",
      "Iteration 63, loss = 0.17336280\n",
      "Iteration 64, loss = 0.17174482\n",
      "Iteration 65, loss = 0.17041998\n",
      "Iteration 66, loss = 0.16915096\n",
      "Iteration 67, loss = 0.16761130\n",
      "Iteration 68, loss = 0.16654921\n",
      "Iteration 69, loss = 0.16475885\n",
      "Iteration 70, loss = 0.16311781\n",
      "Iteration 71, loss = 0.16190064\n",
      "Iteration 72, loss = 0.16027585\n",
      "Iteration 73, loss = 0.15879940\n",
      "Iteration 74, loss = 0.15726120\n",
      "Iteration 75, loss = 0.15599216\n",
      "Iteration 76, loss = 0.15429708\n",
      "Iteration 77, loss = 0.15292070\n",
      "Iteration 78, loss = 0.15174636\n",
      "Iteration 79, loss = 0.14981786\n",
      "Iteration 80, loss = 0.14847582\n",
      "Iteration 81, loss = 0.14712216\n",
      "Iteration 82, loss = 0.14617342\n",
      "Iteration 83, loss = 0.14450989\n",
      "Iteration 84, loss = 0.14310025\n",
      "Iteration 85, loss = 0.14181706\n",
      "Iteration 86, loss = 0.14003232\n",
      "Iteration 87, loss = 0.13893089\n",
      "Iteration 88, loss = 0.13832971\n",
      "Iteration 89, loss = 0.13733546\n",
      "Iteration 90, loss = 0.13546560\n",
      "Iteration 91, loss = 0.13363243\n",
      "Iteration 92, loss = 0.13245638\n",
      "Iteration 93, loss = 0.13113192\n",
      "Iteration 94, loss = 0.12948276\n",
      "Iteration 95, loss = 0.12859732\n",
      "Iteration 96, loss = 0.12711674\n",
      "Iteration 97, loss = 0.12592877\n",
      "Iteration 98, loss = 0.12471627\n",
      "Iteration 99, loss = 0.12368765\n",
      "Iteration 100, loss = 0.12236491\n",
      "Iteration 101, loss = 0.12154921\n",
      "Iteration 102, loss = 0.11978787\n",
      "Iteration 103, loss = 0.11879899\n",
      "Iteration 104, loss = 0.11756339\n",
      "Iteration 105, loss = 0.11613018\n",
      "Iteration 106, loss = 0.11530163\n",
      "Iteration 107, loss = 0.11449330\n",
      "Iteration 108, loss = 0.11276510\n",
      "Iteration 109, loss = 0.11188351\n",
      "Iteration 110, loss = 0.11054937\n",
      "Iteration 111, loss = 0.10967047\n",
      "Iteration 112, loss = 0.10889141\n",
      "Iteration 113, loss = 0.10729318\n",
      "Iteration 114, loss = 0.10678386\n",
      "Iteration 115, loss = 0.10491359\n",
      "Iteration 116, loss = 0.10362146\n",
      "Iteration 117, loss = 0.10274808\n",
      "Iteration 118, loss = 0.10160430\n",
      "Iteration 119, loss = 0.10083682\n",
      "Iteration 120, loss = 0.09989697\n",
      "Iteration 121, loss = 0.09879450\n",
      "Iteration 122, loss = 0.09779192\n",
      "Iteration 123, loss = 0.09668330\n",
      "Iteration 124, loss = 0.09563706\n",
      "Iteration 125, loss = 0.09569410\n",
      "Iteration 126, loss = 0.09379344\n",
      "Iteration 127, loss = 0.09283407\n",
      "Iteration 128, loss = 0.09186972\n",
      "Iteration 129, loss = 0.09064776\n",
      "Iteration 130, loss = 0.08959753\n",
      "Iteration 131, loss = 0.08887150\n",
      "Iteration 132, loss = 0.08769581\n",
      "Iteration 133, loss = 0.08719657\n",
      "Iteration 134, loss = 0.08638502\n",
      "Iteration 135, loss = 0.08516111\n",
      "Iteration 136, loss = 0.08426922\n",
      "Iteration 137, loss = 0.08344164\n",
      "Iteration 138, loss = 0.08318113\n",
      "Iteration 139, loss = 0.08170772\n",
      "Iteration 140, loss = 0.08071850\n",
      "Iteration 141, loss = 0.08022275\n",
      "Iteration 142, loss = 0.07936088\n",
      "Iteration 143, loss = 0.07882534\n",
      "Iteration 144, loss = 0.07752383\n",
      "Iteration 145, loss = 0.07710032\n",
      "Iteration 146, loss = 0.07570496\n",
      "Iteration 147, loss = 0.07483096\n",
      "Iteration 148, loss = 0.07418031\n",
      "Iteration 149, loss = 0.07376558\n",
      "Iteration 150, loss = 0.07296324\n",
      "Iteration 151, loss = 0.07178130\n",
      "Iteration 152, loss = 0.07125984\n",
      "Iteration 153, loss = 0.07013941\n",
      "Iteration 154, loss = 0.06943564\n",
      "Iteration 155, loss = 0.06939095\n",
      "Iteration 156, loss = 0.06855156\n",
      "Iteration 157, loss = 0.06715438\n",
      "Iteration 158, loss = 0.06699379\n",
      "Iteration 159, loss = 0.06653246\n",
      "Iteration 160, loss = 0.06509656\n",
      "Iteration 161, loss = 0.06443635\n",
      "Iteration 162, loss = 0.06373620\n",
      "Iteration 163, loss = 0.06326265\n",
      "Iteration 164, loss = 0.06256656\n",
      "Iteration 165, loss = 0.06154004\n",
      "Iteration 166, loss = 0.06085444\n",
      "Iteration 167, loss = 0.06012850\n",
      "Iteration 168, loss = 0.05944435\n",
      "Iteration 169, loss = 0.05895927\n",
      "Iteration 170, loss = 0.05804612\n",
      "Iteration 171, loss = 0.05750608\n",
      "Iteration 172, loss = 0.05696435\n",
      "Iteration 173, loss = 0.05665373\n",
      "Iteration 174, loss = 0.05553490\n",
      "Iteration 175, loss = 0.05496671\n",
      "Iteration 176, loss = 0.05440415\n",
      "Iteration 177, loss = 0.05367380\n",
      "Iteration 178, loss = 0.05352184\n",
      "Iteration 179, loss = 0.05248350\n",
      "Iteration 180, loss = 0.05199589\n",
      "Iteration 181, loss = 0.05150953\n",
      "Iteration 182, loss = 0.05076198\n",
      "Iteration 183, loss = 0.05023364\n",
      "Iteration 184, loss = 0.04993243\n",
      "Iteration 185, loss = 0.04926904\n",
      "Iteration 186, loss = 0.04891139\n",
      "Iteration 187, loss = 0.04826866\n",
      "Iteration 188, loss = 0.04735795\n",
      "Iteration 189, loss = 0.04679216\n",
      "Iteration 190, loss = 0.04617532\n",
      "Iteration 191, loss = 0.04582340\n",
      "Iteration 192, loss = 0.04536191\n",
      "Iteration 193, loss = 0.04465312\n",
      "Iteration 194, loss = 0.04438260\n",
      "Iteration 195, loss = 0.04366132\n",
      "Iteration 196, loss = 0.04317043\n",
      "Iteration 197, loss = 0.04265954\n",
      "Iteration 198, loss = 0.04197334\n",
      "Iteration 199, loss = 0.04160303\n",
      "Iteration 200, loss = 0.04120251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68192961\n",
      "Iteration 2, loss = 0.53865094\n",
      "Iteration 3, loss = 0.45181573\n",
      "Iteration 4, loss = 0.39188090\n",
      "Iteration 5, loss = 0.35048724\n",
      "Iteration 6, loss = 0.32024749\n",
      "Iteration 7, loss = 0.29884730\n",
      "Iteration 8, loss = 0.28330087\n",
      "Iteration 9, loss = 0.27243105\n",
      "Iteration 10, loss = 0.26377304\n",
      "Iteration 11, loss = 0.25792210\n",
      "Iteration 12, loss = 0.25201060\n",
      "Iteration 13, loss = 0.24764647\n",
      "Iteration 14, loss = 0.24406972\n",
      "Iteration 15, loss = 0.24069360\n",
      "Iteration 16, loss = 0.23790375\n",
      "Iteration 17, loss = 0.23569326\n",
      "Iteration 18, loss = 0.23351999\n",
      "Iteration 19, loss = 0.23170712\n",
      "Iteration 20, loss = 0.22998032\n",
      "Iteration 21, loss = 0.22893436\n",
      "Iteration 22, loss = 0.22661897\n",
      "Iteration 23, loss = 0.22543519\n",
      "Iteration 24, loss = 0.22384627\n",
      "Iteration 25, loss = 0.22294674\n",
      "Iteration 26, loss = 0.22119569\n",
      "Iteration 27, loss = 0.22065811\n",
      "Iteration 28, loss = 0.21915496\n",
      "Iteration 29, loss = 0.21792941\n",
      "Iteration 30, loss = 0.21642980\n",
      "Iteration 31, loss = 0.21531926\n",
      "Iteration 32, loss = 0.21400483\n",
      "Iteration 33, loss = 0.21252567\n",
      "Iteration 34, loss = 0.21168123\n",
      "Iteration 35, loss = 0.21022270\n",
      "Iteration 36, loss = 0.20854987\n",
      "Iteration 37, loss = 0.20751904\n",
      "Iteration 38, loss = 0.20608717\n",
      "Iteration 39, loss = 0.20491826\n",
      "Iteration 40, loss = 0.20398731\n",
      "Iteration 41, loss = 0.20210521\n",
      "Iteration 42, loss = 0.20103284\n",
      "Iteration 43, loss = 0.19986103\n",
      "Iteration 44, loss = 0.19869328\n",
      "Iteration 45, loss = 0.19637826\n",
      "Iteration 46, loss = 0.19557052\n",
      "Iteration 47, loss = 0.19371964\n",
      "Iteration 48, loss = 0.19228537\n",
      "Iteration 49, loss = 0.19103575\n",
      "Iteration 50, loss = 0.19006914\n",
      "Iteration 51, loss = 0.18834827\n",
      "Iteration 52, loss = 0.18662667\n",
      "Iteration 53, loss = 0.18527775\n",
      "Iteration 54, loss = 0.18402456\n",
      "Iteration 55, loss = 0.18160272\n",
      "Iteration 56, loss = 0.18057417\n",
      "Iteration 57, loss = 0.17905254\n",
      "Iteration 58, loss = 0.17763853\n",
      "Iteration 59, loss = 0.17622401\n",
      "Iteration 60, loss = 0.17444706\n",
      "Iteration 61, loss = 0.17370753\n",
      "Iteration 62, loss = 0.17204565\n",
      "Iteration 63, loss = 0.17052351\n",
      "Iteration 64, loss = 0.16910455\n",
      "Iteration 65, loss = 0.16793947\n",
      "Iteration 66, loss = 0.16541762\n",
      "Iteration 67, loss = 0.16427136\n",
      "Iteration 68, loss = 0.16273604\n",
      "Iteration 69, loss = 0.16132985\n",
      "Iteration 70, loss = 0.15979867\n",
      "Iteration 71, loss = 0.15878436\n",
      "Iteration 72, loss = 0.15740549\n",
      "Iteration 73, loss = 0.15571802\n",
      "Iteration 74, loss = 0.15384016\n",
      "Iteration 75, loss = 0.15331912\n",
      "Iteration 76, loss = 0.15196491\n",
      "Iteration 77, loss = 0.15014969\n",
      "Iteration 78, loss = 0.14907905\n",
      "Iteration 79, loss = 0.14774008\n",
      "Iteration 80, loss = 0.14601719\n",
      "Iteration 81, loss = 0.14526708\n",
      "Iteration 82, loss = 0.14479709\n",
      "Iteration 83, loss = 0.14266258\n",
      "Iteration 84, loss = 0.14129517\n",
      "Iteration 85, loss = 0.13951827\n",
      "Iteration 86, loss = 0.13888556\n",
      "Iteration 87, loss = 0.13709207\n",
      "Iteration 88, loss = 0.13568973\n",
      "Iteration 89, loss = 0.13474353\n",
      "Iteration 90, loss = 0.13402952\n",
      "Iteration 91, loss = 0.13189786\n",
      "Iteration 92, loss = 0.13108726\n",
      "Iteration 93, loss = 0.12963222\n",
      "Iteration 94, loss = 0.12829684\n",
      "Iteration 95, loss = 0.12732068\n",
      "Iteration 96, loss = 0.12683932\n",
      "Iteration 97, loss = 0.12518469\n",
      "Iteration 98, loss = 0.12366067\n",
      "Iteration 99, loss = 0.12287993\n",
      "Iteration 100, loss = 0.12179633\n",
      "Iteration 101, loss = 0.12090980\n",
      "Iteration 102, loss = 0.11965766\n",
      "Iteration 103, loss = 0.11865260\n",
      "Iteration 104, loss = 0.11719138\n",
      "Iteration 105, loss = 0.11619380\n",
      "Iteration 106, loss = 0.11481151\n",
      "Iteration 107, loss = 0.11404281\n",
      "Iteration 108, loss = 0.11278891\n",
      "Iteration 109, loss = 0.11222547\n",
      "Iteration 110, loss = 0.11068647\n",
      "Iteration 111, loss = 0.11003468\n",
      "Iteration 112, loss = 0.10885360\n",
      "Iteration 113, loss = 0.10773763\n",
      "Iteration 114, loss = 0.10736369\n",
      "Iteration 115, loss = 0.10563096\n",
      "Iteration 116, loss = 0.10469756\n",
      "Iteration 117, loss = 0.10327007\n",
      "Iteration 118, loss = 0.10246894\n",
      "Iteration 119, loss = 0.10188732\n",
      "Iteration 120, loss = 0.10090546\n",
      "Iteration 121, loss = 0.09972986\n",
      "Iteration 122, loss = 0.09888351\n",
      "Iteration 123, loss = 0.09772181\n",
      "Iteration 124, loss = 0.09717335\n",
      "Iteration 125, loss = 0.09629585\n",
      "Iteration 126, loss = 0.09516971\n",
      "Iteration 127, loss = 0.09453496\n",
      "Iteration 128, loss = 0.09352077\n",
      "Iteration 129, loss = 0.09249245\n",
      "Iteration 130, loss = 0.09234600\n",
      "Iteration 131, loss = 0.09064546\n",
      "Iteration 132, loss = 0.09001682\n",
      "Iteration 133, loss = 0.08853136\n",
      "Iteration 134, loss = 0.08836177\n",
      "Iteration 135, loss = 0.08671229\n",
      "Iteration 136, loss = 0.08598513\n",
      "Iteration 137, loss = 0.08563862\n",
      "Iteration 138, loss = 0.08469110\n",
      "Iteration 139, loss = 0.08332403\n",
      "Iteration 140, loss = 0.08290850\n",
      "Iteration 141, loss = 0.08210826\n",
      "Iteration 142, loss = 0.08100395\n",
      "Iteration 143, loss = 0.08054927\n",
      "Iteration 144, loss = 0.07978843\n",
      "Iteration 145, loss = 0.07915754\n",
      "Iteration 146, loss = 0.07804289\n",
      "Iteration 147, loss = 0.07781838\n",
      "Iteration 148, loss = 0.07654628\n",
      "Iteration 149, loss = 0.07551152\n",
      "Iteration 150, loss = 0.07482456\n",
      "Iteration 151, loss = 0.07408280\n",
      "Iteration 152, loss = 0.07376589\n",
      "Iteration 153, loss = 0.07290290\n",
      "Iteration 154, loss = 0.07204172\n",
      "Iteration 155, loss = 0.07119484\n",
      "Iteration 156, loss = 0.07020711\n",
      "Iteration 157, loss = 0.06987728\n",
      "Iteration 158, loss = 0.06879228\n",
      "Iteration 159, loss = 0.06827740\n",
      "Iteration 160, loss = 0.06737243\n",
      "Iteration 161, loss = 0.06681581\n",
      "Iteration 162, loss = 0.06645089\n",
      "Iteration 163, loss = 0.06526716\n",
      "Iteration 164, loss = 0.06474866\n",
      "Iteration 165, loss = 0.06424369\n",
      "Iteration 166, loss = 0.06339956\n",
      "Iteration 167, loss = 0.06268592\n",
      "Iteration 168, loss = 0.06219919\n",
      "Iteration 169, loss = 0.06131262\n",
      "Iteration 170, loss = 0.06075921\n",
      "Iteration 171, loss = 0.05983795\n",
      "Iteration 172, loss = 0.05940373\n",
      "Iteration 173, loss = 0.05849893\n",
      "Iteration 174, loss = 0.05808967\n",
      "Iteration 175, loss = 0.05778317\n",
      "Iteration 176, loss = 0.05720926\n",
      "Iteration 177, loss = 0.05632667\n",
      "Iteration 178, loss = 0.05593599\n",
      "Iteration 179, loss = 0.05497611\n",
      "Iteration 180, loss = 0.05437733\n",
      "Iteration 181, loss = 0.05421357\n",
      "Iteration 182, loss = 0.05307762\n",
      "Iteration 183, loss = 0.05290453\n",
      "Iteration 184, loss = 0.05222943\n",
      "Iteration 185, loss = 0.05139913\n",
      "Iteration 186, loss = 0.05083787\n",
      "Iteration 187, loss = 0.05027402\n",
      "Iteration 188, loss = 0.04950949\n",
      "Iteration 189, loss = 0.04900829\n",
      "Iteration 190, loss = 0.04873242\n",
      "Iteration 191, loss = 0.04793772\n",
      "Iteration 192, loss = 0.04734998\n",
      "Iteration 193, loss = 0.04676328\n",
      "Iteration 194, loss = 0.04666046\n",
      "Iteration 195, loss = 0.04606215\n",
      "Iteration 196, loss = 0.04547104\n",
      "Iteration 197, loss = 0.04462709\n",
      "Iteration 198, loss = 0.04425067\n",
      "Iteration 199, loss = 0.04352084\n",
      "Iteration 200, loss = 0.04345022\n",
      "Iteration 1, loss = 0.68078591\n",
      "Iteration 2, loss = 0.53409506\n",
      "Iteration 3, loss = 0.44775951\n",
      "Iteration 4, loss = 0.38920063\n",
      "Iteration 5, loss = 0.34895555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.31948677\n",
      "Iteration 7, loss = 0.29931907\n",
      "Iteration 8, loss = 0.28410767\n",
      "Iteration 9, loss = 0.27403866\n",
      "Iteration 10, loss = 0.26626821\n",
      "Iteration 11, loss = 0.26073036\n",
      "Iteration 12, loss = 0.25530199\n",
      "Iteration 13, loss = 0.25142214\n",
      "Iteration 14, loss = 0.24761489\n",
      "Iteration 15, loss = 0.24471787\n",
      "Iteration 16, loss = 0.24234024\n",
      "Iteration 17, loss = 0.24015049\n",
      "Iteration 18, loss = 0.23835042\n",
      "Iteration 19, loss = 0.23631027\n",
      "Iteration 20, loss = 0.23475289\n",
      "Iteration 21, loss = 0.23361428\n",
      "Iteration 22, loss = 0.23203131\n",
      "Iteration 23, loss = 0.23091515\n",
      "Iteration 24, loss = 0.22921518\n",
      "Iteration 25, loss = 0.22808629\n",
      "Iteration 26, loss = 0.22681784\n",
      "Iteration 27, loss = 0.22604957\n",
      "Iteration 28, loss = 0.22427414\n",
      "Iteration 29, loss = 0.22399774\n",
      "Iteration 30, loss = 0.22256923\n",
      "Iteration 31, loss = 0.22085117\n",
      "Iteration 32, loss = 0.21957090\n",
      "Iteration 33, loss = 0.21802589\n",
      "Iteration 34, loss = 0.21739909\n",
      "Iteration 35, loss = 0.21603020\n",
      "Iteration 36, loss = 0.21432326\n",
      "Iteration 37, loss = 0.21361549\n",
      "Iteration 38, loss = 0.21161603\n",
      "Iteration 39, loss = 0.21051707\n",
      "Iteration 40, loss = 0.20917433\n",
      "Iteration 41, loss = 0.20740076\n",
      "Iteration 42, loss = 0.20663598\n",
      "Iteration 43, loss = 0.20525619\n",
      "Iteration 44, loss = 0.20405085\n",
      "Iteration 45, loss = 0.20212525\n",
      "Iteration 46, loss = 0.20071741\n",
      "Iteration 47, loss = 0.19921326\n",
      "Iteration 48, loss = 0.19761288\n",
      "Iteration 49, loss = 0.19638219\n",
      "Iteration 50, loss = 0.19537143\n",
      "Iteration 51, loss = 0.19422846\n",
      "Iteration 52, loss = 0.19199383\n",
      "Iteration 53, loss = 0.19047117\n",
      "Iteration 54, loss = 0.18899782\n",
      "Iteration 55, loss = 0.18701899\n",
      "Iteration 56, loss = 0.18546756\n",
      "Iteration 57, loss = 0.18430289\n",
      "Iteration 58, loss = 0.18276140\n",
      "Iteration 59, loss = 0.18124165\n",
      "Iteration 60, loss = 0.17933989\n",
      "Iteration 61, loss = 0.17835272\n",
      "Iteration 62, loss = 0.17654438\n",
      "Iteration 63, loss = 0.17478748\n",
      "Iteration 64, loss = 0.17325904\n",
      "Iteration 65, loss = 0.17229113\n",
      "Iteration 66, loss = 0.17021585\n",
      "Iteration 67, loss = 0.16864579\n",
      "Iteration 68, loss = 0.16673597\n",
      "Iteration 69, loss = 0.16572162\n",
      "Iteration 70, loss = 0.16439150\n",
      "Iteration 71, loss = 0.16309977\n",
      "Iteration 72, loss = 0.16077369\n",
      "Iteration 73, loss = 0.15951960\n",
      "Iteration 74, loss = 0.15818154\n",
      "Iteration 75, loss = 0.15653857\n",
      "Iteration 76, loss = 0.15509102\n",
      "Iteration 77, loss = 0.15372426\n",
      "Iteration 78, loss = 0.15262947\n",
      "Iteration 79, loss = 0.15040948\n",
      "Iteration 80, loss = 0.14976889\n",
      "Iteration 81, loss = 0.14835348\n",
      "Iteration 82, loss = 0.14703111\n",
      "Iteration 83, loss = 0.14494479\n",
      "Iteration 84, loss = 0.14378906\n",
      "Iteration 85, loss = 0.14174151\n",
      "Iteration 86, loss = 0.14139050\n",
      "Iteration 87, loss = 0.13961484\n",
      "Iteration 88, loss = 0.13820099\n",
      "Iteration 89, loss = 0.13636609\n",
      "Iteration 90, loss = 0.13566648\n",
      "Iteration 91, loss = 0.13404145\n",
      "Iteration 92, loss = 0.13243051\n",
      "Iteration 93, loss = 0.13121046\n",
      "Iteration 94, loss = 0.13012708\n",
      "Iteration 95, loss = 0.12957116\n",
      "Iteration 96, loss = 0.12851029\n",
      "Iteration 97, loss = 0.12615836\n",
      "Iteration 98, loss = 0.12499761\n",
      "Iteration 99, loss = 0.12397671\n",
      "Iteration 100, loss = 0.12287767\n",
      "Iteration 101, loss = 0.12241548\n",
      "Iteration 102, loss = 0.12085585\n",
      "Iteration 103, loss = 0.11911895\n",
      "Iteration 104, loss = 0.11790021\n",
      "Iteration 105, loss = 0.11713049\n",
      "Iteration 106, loss = 0.11542831\n",
      "Iteration 107, loss = 0.11426253\n",
      "Iteration 108, loss = 0.11331787\n",
      "Iteration 109, loss = 0.11247923\n",
      "Iteration 110, loss = 0.11136593\n",
      "Iteration 111, loss = 0.10980746\n",
      "Iteration 112, loss = 0.10885727\n",
      "Iteration 113, loss = 0.10774131\n",
      "Iteration 114, loss = 0.10696749\n",
      "Iteration 115, loss = 0.10580559\n",
      "Iteration 116, loss = 0.10468186\n",
      "Iteration 117, loss = 0.10333423\n",
      "Iteration 118, loss = 0.10217094\n",
      "Iteration 119, loss = 0.10152200\n",
      "Iteration 120, loss = 0.10049416\n",
      "Iteration 121, loss = 0.09971847\n",
      "Iteration 122, loss = 0.09870235\n",
      "Iteration 123, loss = 0.09758315\n",
      "Iteration 124, loss = 0.09655522\n",
      "Iteration 125, loss = 0.09555859\n",
      "Iteration 126, loss = 0.09507897\n",
      "Iteration 127, loss = 0.09385457\n",
      "Iteration 128, loss = 0.09285199\n",
      "Iteration 129, loss = 0.09204158\n",
      "Iteration 130, loss = 0.09142663\n",
      "Iteration 131, loss = 0.09000816\n",
      "Iteration 132, loss = 0.08919340\n",
      "Iteration 133, loss = 0.08797382\n",
      "Iteration 134, loss = 0.08750278\n",
      "Iteration 135, loss = 0.08610668\n",
      "Iteration 136, loss = 0.08552715\n",
      "Iteration 137, loss = 0.08477803\n",
      "Iteration 138, loss = 0.08408153\n",
      "Iteration 139, loss = 0.08279076\n",
      "Iteration 140, loss = 0.08248266\n",
      "Iteration 141, loss = 0.08117391\n",
      "Iteration 142, loss = 0.08053616\n",
      "Iteration 143, loss = 0.07955833\n",
      "Iteration 144, loss = 0.07882736\n",
      "Iteration 145, loss = 0.07827685\n",
      "Iteration 146, loss = 0.07743090\n",
      "Iteration 147, loss = 0.07695858\n",
      "Iteration 148, loss = 0.07593168\n",
      "Iteration 149, loss = 0.07485664\n",
      "Iteration 150, loss = 0.07464071\n",
      "Iteration 151, loss = 0.07351412\n",
      "Iteration 152, loss = 0.07308350\n",
      "Iteration 153, loss = 0.07162729\n",
      "Iteration 154, loss = 0.07121265\n",
      "Iteration 155, loss = 0.07041705\n",
      "Iteration 156, loss = 0.06972050\n",
      "Iteration 157, loss = 0.06909035\n",
      "Iteration 158, loss = 0.06822289\n",
      "Iteration 159, loss = 0.06751378\n",
      "Iteration 160, loss = 0.06653011\n",
      "Iteration 161, loss = 0.06610325\n",
      "Iteration 162, loss = 0.06544305\n",
      "Iteration 163, loss = 0.06438498\n",
      "Iteration 164, loss = 0.06395221\n",
      "Iteration 165, loss = 0.06340721\n",
      "Iteration 166, loss = 0.06263760\n",
      "Iteration 167, loss = 0.06196473\n",
      "Iteration 168, loss = 0.06219051\n",
      "Iteration 169, loss = 0.06048039\n",
      "Iteration 170, loss = 0.05991816\n",
      "Iteration 171, loss = 0.05932102\n",
      "Iteration 172, loss = 0.05885129\n",
      "Iteration 173, loss = 0.05820954\n",
      "Iteration 174, loss = 0.05733298\n",
      "Iteration 175, loss = 0.05694662\n",
      "Iteration 176, loss = 0.05658556\n",
      "Iteration 177, loss = 0.05572848\n",
      "Iteration 178, loss = 0.05497286\n",
      "Iteration 179, loss = 0.05474894\n",
      "Iteration 180, loss = 0.05383973\n",
      "Iteration 181, loss = 0.05324224\n",
      "Iteration 182, loss = 0.05255048\n",
      "Iteration 183, loss = 0.05212416\n",
      "Iteration 184, loss = 0.05122060\n",
      "Iteration 185, loss = 0.05080458\n",
      "Iteration 186, loss = 0.05013444\n",
      "Iteration 187, loss = 0.05004396\n",
      "Iteration 188, loss = 0.04895952\n",
      "Iteration 189, loss = 0.04840677\n",
      "Iteration 190, loss = 0.04820334\n",
      "Iteration 191, loss = 0.04774758\n",
      "Iteration 192, loss = 0.04691222\n",
      "Iteration 193, loss = 0.04642722\n",
      "Iteration 194, loss = 0.04612720\n",
      "Iteration 195, loss = 0.04550880\n",
      "Iteration 196, loss = 0.04480277\n",
      "Iteration 197, loss = 0.04429078\n",
      "Iteration 198, loss = 0.04390954\n",
      "Iteration 199, loss = 0.04323919\n",
      "Iteration 200, loss = 0.04265237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75836684\n",
      "Iteration 2, loss = 0.69663936\n",
      "Iteration 3, loss = 0.63348707\n",
      "Iteration 4, loss = 0.58050372\n",
      "Iteration 5, loss = 0.53781968\n",
      "Iteration 6, loss = 0.50241508\n",
      "Iteration 7, loss = 0.47362173\n",
      "Iteration 8, loss = 0.44951623\n",
      "Iteration 9, loss = 0.42929321\n",
      "Iteration 10, loss = 0.41221097\n",
      "Iteration 11, loss = 0.39761203\n",
      "Iteration 12, loss = 0.38484070\n",
      "Iteration 13, loss = 0.37403819\n",
      "Iteration 14, loss = 0.36438332\n",
      "Iteration 15, loss = 0.35592398\n",
      "Iteration 16, loss = 0.34849725\n",
      "Iteration 17, loss = 0.34177531\n",
      "Iteration 18, loss = 0.33583815\n",
      "Iteration 19, loss = 0.33038866\n",
      "Iteration 20, loss = 0.32551205\n",
      "Iteration 21, loss = 0.32116187\n",
      "Iteration 22, loss = 0.31711695\n",
      "Iteration 23, loss = 0.31340113\n",
      "Iteration 24, loss = 0.31002918\n",
      "Iteration 25, loss = 0.30691749\n",
      "Iteration 26, loss = 0.30400165\n",
      "Iteration 27, loss = 0.30136945\n",
      "Iteration 28, loss = 0.29888933\n",
      "Iteration 29, loss = 0.29657350\n",
      "Iteration 30, loss = 0.29442347\n",
      "Iteration 31, loss = 0.29240886\n",
      "Iteration 32, loss = 0.29053419\n",
      "Iteration 33, loss = 0.28876725\n",
      "Iteration 34, loss = 0.28712494\n",
      "Iteration 35, loss = 0.28555328\n",
      "Iteration 36, loss = 0.28408354\n",
      "Iteration 37, loss = 0.28264948\n",
      "Iteration 38, loss = 0.28135611\n",
      "Iteration 39, loss = 0.28009537\n",
      "Iteration 40, loss = 0.27889937\n",
      "Iteration 41, loss = 0.27778676\n",
      "Iteration 42, loss = 0.27670527\n",
      "Iteration 43, loss = 0.27566066\n",
      "Iteration 44, loss = 0.27469095\n",
      "Iteration 45, loss = 0.27376334\n",
      "Iteration 46, loss = 0.27286025\n",
      "Iteration 47, loss = 0.27203274\n",
      "Iteration 48, loss = 0.27117962\n",
      "Iteration 49, loss = 0.27039591\n",
      "Iteration 50, loss = 0.26962318\n",
      "Iteration 51, loss = 0.26889992\n",
      "Iteration 52, loss = 0.26820973\n",
      "Iteration 53, loss = 0.26751231\n",
      "Iteration 54, loss = 0.26687192\n",
      "Iteration 55, loss = 0.26622854\n",
      "Iteration 56, loss = 0.26562834\n",
      "Iteration 57, loss = 0.26506179\n",
      "Iteration 58, loss = 0.26447132\n",
      "Iteration 59, loss = 0.26390686\n",
      "Iteration 60, loss = 0.26339269\n",
      "Iteration 61, loss = 0.26286861\n",
      "Iteration 62, loss = 0.26236339\n",
      "Iteration 63, loss = 0.26187657\n",
      "Iteration 64, loss = 0.26140917\n",
      "Iteration 65, loss = 0.26094401\n",
      "Iteration 66, loss = 0.26051625\n",
      "Iteration 67, loss = 0.26004431\n",
      "Iteration 68, loss = 0.25961456\n",
      "Iteration 69, loss = 0.25923009\n",
      "Iteration 70, loss = 0.25879308\n",
      "Iteration 71, loss = 0.25840753\n",
      "Iteration 72, loss = 0.25802808\n",
      "Iteration 73, loss = 0.25764386\n",
      "Iteration 74, loss = 0.25727029\n",
      "Iteration 75, loss = 0.25691584\n",
      "Iteration 76, loss = 0.25657338\n",
      "Iteration 77, loss = 0.25624102\n",
      "Iteration 78, loss = 0.25588609\n",
      "Iteration 79, loss = 0.25554447\n",
      "Iteration 80, loss = 0.25522664\n",
      "Iteration 81, loss = 0.25490957\n",
      "Iteration 82, loss = 0.25461334\n",
      "Iteration 83, loss = 0.25426589\n",
      "Iteration 84, loss = 0.25397231\n",
      "Iteration 85, loss = 0.25366965\n",
      "Iteration 86, loss = 0.25338997\n",
      "Iteration 87, loss = 0.25310855\n",
      "Iteration 88, loss = 0.25281602\n",
      "Iteration 89, loss = 0.25253870\n",
      "Iteration 90, loss = 0.25227696\n",
      "Iteration 91, loss = 0.25199435\n",
      "Iteration 92, loss = 0.25175184\n",
      "Iteration 93, loss = 0.25146023\n",
      "Iteration 94, loss = 0.25121866\n",
      "Iteration 95, loss = 0.25096455\n",
      "Iteration 96, loss = 0.25070730\n",
      "Iteration 97, loss = 0.25046455\n",
      "Iteration 98, loss = 0.25021366\n",
      "Iteration 99, loss = 0.24996979\n",
      "Iteration 100, loss = 0.24977511\n",
      "Iteration 101, loss = 0.24952388\n",
      "Iteration 102, loss = 0.24926315\n",
      "Iteration 103, loss = 0.24905316\n",
      "Iteration 104, loss = 0.24883228\n",
      "Iteration 105, loss = 0.24861070\n",
      "Iteration 106, loss = 0.24838152\n",
      "Iteration 107, loss = 0.24816565\n",
      "Iteration 108, loss = 0.24794917\n",
      "Iteration 109, loss = 0.24774409\n",
      "Iteration 110, loss = 0.24754868\n",
      "Iteration 111, loss = 0.24732880\n",
      "Iteration 112, loss = 0.24711356\n",
      "Iteration 113, loss = 0.24692975\n",
      "Iteration 114, loss = 0.24671731\n",
      "Iteration 115, loss = 0.24653892\n",
      "Iteration 116, loss = 0.24634449\n",
      "Iteration 117, loss = 0.24615684\n",
      "Iteration 118, loss = 0.24592619\n",
      "Iteration 119, loss = 0.24575411\n",
      "Iteration 120, loss = 0.24557199\n",
      "Iteration 121, loss = 0.24537398\n",
      "Iteration 122, loss = 0.24520006\n",
      "Iteration 123, loss = 0.24500559\n",
      "Iteration 124, loss = 0.24481647\n",
      "Iteration 125, loss = 0.24464802\n",
      "Iteration 126, loss = 0.24447817\n",
      "Iteration 127, loss = 0.24430526\n",
      "Iteration 128, loss = 0.24412100\n",
      "Iteration 129, loss = 0.24396323\n",
      "Iteration 130, loss = 0.24377453\n",
      "Iteration 131, loss = 0.24361235\n",
      "Iteration 132, loss = 0.24345865\n",
      "Iteration 133, loss = 0.24328041\n",
      "Iteration 134, loss = 0.24311418\n",
      "Iteration 135, loss = 0.24295123\n",
      "Iteration 136, loss = 0.24282490\n",
      "Iteration 137, loss = 0.24263788\n",
      "Iteration 138, loss = 0.24246328\n",
      "Iteration 139, loss = 0.24231840\n",
      "Iteration 140, loss = 0.24215425\n",
      "Iteration 141, loss = 0.24198158\n",
      "Iteration 142, loss = 0.24184545\n",
      "Iteration 143, loss = 0.24169910\n",
      "Iteration 144, loss = 0.24154246\n",
      "Iteration 145, loss = 0.24138776\n",
      "Iteration 146, loss = 0.24123881\n",
      "Iteration 147, loss = 0.24110562\n",
      "Iteration 148, loss = 0.24096142\n",
      "Iteration 149, loss = 0.24079299\n",
      "Iteration 150, loss = 0.24064313\n",
      "Iteration 151, loss = 0.24051814\n",
      "Iteration 152, loss = 0.24038144\n",
      "Iteration 153, loss = 0.24023744\n",
      "Iteration 154, loss = 0.24008672\n",
      "Iteration 155, loss = 0.23996227\n",
      "Iteration 156, loss = 0.23980917\n",
      "Iteration 157, loss = 0.23966810\n",
      "Iteration 158, loss = 0.23952545\n",
      "Iteration 159, loss = 0.23940456\n",
      "Iteration 160, loss = 0.23929205\n",
      "Iteration 161, loss = 0.23914465\n",
      "Iteration 162, loss = 0.23900635\n",
      "Iteration 163, loss = 0.23886086\n",
      "Iteration 164, loss = 0.23873525\n",
      "Iteration 165, loss = 0.23861256\n",
      "Iteration 166, loss = 0.23847411\n",
      "Iteration 167, loss = 0.23836960\n",
      "Iteration 168, loss = 0.23822056\n",
      "Iteration 169, loss = 0.23810349\n",
      "Iteration 170, loss = 0.23799417\n",
      "Iteration 171, loss = 0.23786051\n",
      "Iteration 172, loss = 0.23773126\n",
      "Iteration 173, loss = 0.23760966\n",
      "Iteration 174, loss = 0.23749714\n",
      "Iteration 175, loss = 0.23735812\n",
      "Iteration 176, loss = 0.23725837\n",
      "Iteration 177, loss = 0.23714358\n",
      "Iteration 178, loss = 0.23700043\n",
      "Iteration 179, loss = 0.23692435\n",
      "Iteration 180, loss = 0.23677021\n",
      "Iteration 181, loss = 0.23667094\n",
      "Iteration 182, loss = 0.23654381\n",
      "Iteration 183, loss = 0.23641429\n",
      "Iteration 184, loss = 0.23632284\n",
      "Iteration 185, loss = 0.23621293\n",
      "Iteration 186, loss = 0.23608558\n",
      "Iteration 187, loss = 0.23596604\n",
      "Iteration 188, loss = 0.23586063\n",
      "Iteration 189, loss = 0.23575277\n",
      "Iteration 190, loss = 0.23565025\n",
      "Iteration 191, loss = 0.23551604\n",
      "Iteration 192, loss = 0.23543558\n",
      "Iteration 193, loss = 0.23530161\n",
      "Iteration 194, loss = 0.23520770\n",
      "Iteration 195, loss = 0.23509870\n",
      "Iteration 196, loss = 0.23499451\n",
      "Iteration 197, loss = 0.23490230\n",
      "Iteration 198, loss = 0.23480174\n",
      "Iteration 199, loss = 0.23468792\n",
      "Iteration 200, loss = 0.23458799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75305843\n",
      "Iteration 2, loss = 0.69270918\n",
      "Iteration 3, loss = 0.63016102\n",
      "Iteration 4, loss = 0.57819971\n",
      "Iteration 5, loss = 0.53593335\n",
      "Iteration 6, loss = 0.50094335\n",
      "Iteration 7, loss = 0.47246897\n",
      "Iteration 8, loss = 0.44843428\n",
      "Iteration 9, loss = 0.42836758\n",
      "Iteration 10, loss = 0.41144248\n",
      "Iteration 11, loss = 0.39678262\n",
      "Iteration 12, loss = 0.38416780\n",
      "Iteration 13, loss = 0.37327355\n",
      "Iteration 14, loss = 0.36363775\n",
      "Iteration 15, loss = 0.35518673\n",
      "Iteration 16, loss = 0.34768401\n",
      "Iteration 17, loss = 0.34104231\n",
      "Iteration 18, loss = 0.33503474\n",
      "Iteration 19, loss = 0.32957865\n",
      "Iteration 20, loss = 0.32471295\n",
      "Iteration 21, loss = 0.32029263\n",
      "Iteration 22, loss = 0.31622168\n",
      "Iteration 23, loss = 0.31250851\n",
      "Iteration 24, loss = 0.30911916\n",
      "Iteration 25, loss = 0.30597643\n",
      "Iteration 26, loss = 0.30308092\n",
      "Iteration 27, loss = 0.30040508\n",
      "Iteration 28, loss = 0.29792674\n",
      "Iteration 29, loss = 0.29555802\n",
      "Iteration 30, loss = 0.29344561\n",
      "Iteration 31, loss = 0.29138438\n",
      "Iteration 32, loss = 0.28949996\n",
      "Iteration 33, loss = 0.28772999\n",
      "Iteration 34, loss = 0.28608150\n",
      "Iteration 35, loss = 0.28448381\n",
      "Iteration 36, loss = 0.28298670\n",
      "Iteration 37, loss = 0.28156546\n",
      "Iteration 38, loss = 0.28026034\n",
      "Iteration 39, loss = 0.27900386\n",
      "Iteration 40, loss = 0.27780472\n",
      "Iteration 41, loss = 0.27666596\n",
      "Iteration 42, loss = 0.27557900\n",
      "Iteration 43, loss = 0.27453852\n",
      "Iteration 44, loss = 0.27355467\n",
      "Iteration 45, loss = 0.27260615\n",
      "Iteration 46, loss = 0.27173061\n",
      "Iteration 47, loss = 0.27087408\n",
      "Iteration 48, loss = 0.27003107\n",
      "Iteration 49, loss = 0.26924358\n",
      "Iteration 50, loss = 0.26846282\n",
      "Iteration 51, loss = 0.26773094\n",
      "Iteration 52, loss = 0.26703211\n",
      "Iteration 53, loss = 0.26634857\n",
      "Iteration 54, loss = 0.26570262\n",
      "Iteration 55, loss = 0.26506378\n",
      "Iteration 56, loss = 0.26444999\n",
      "Iteration 57, loss = 0.26387909\n",
      "Iteration 58, loss = 0.26333002\n",
      "Iteration 59, loss = 0.26274373\n",
      "Iteration 60, loss = 0.26221424\n",
      "Iteration 61, loss = 0.26170172\n",
      "Iteration 62, loss = 0.26119881\n",
      "Iteration 63, loss = 0.26069557\n",
      "Iteration 64, loss = 0.26022319\n",
      "Iteration 65, loss = 0.25976380\n",
      "Iteration 66, loss = 0.25934357\n",
      "Iteration 67, loss = 0.25886933\n",
      "Iteration 68, loss = 0.25845832\n",
      "Iteration 69, loss = 0.25804442\n",
      "Iteration 70, loss = 0.25762960\n",
      "Iteration 71, loss = 0.25723963\n",
      "Iteration 72, loss = 0.25688890\n",
      "Iteration 73, loss = 0.25647496\n",
      "Iteration 74, loss = 0.25611160\n",
      "Iteration 75, loss = 0.25576975\n",
      "Iteration 76, loss = 0.25540386\n",
      "Iteration 77, loss = 0.25507669\n",
      "Iteration 78, loss = 0.25473699\n",
      "Iteration 79, loss = 0.25441072\n",
      "Iteration 80, loss = 0.25407018\n",
      "Iteration 81, loss = 0.25374937\n",
      "Iteration 82, loss = 0.25346527\n",
      "Iteration 83, loss = 0.25314433\n",
      "Iteration 84, loss = 0.25284271\n",
      "Iteration 85, loss = 0.25253865\n",
      "Iteration 86, loss = 0.25224885\n",
      "Iteration 87, loss = 0.25198380\n",
      "Iteration 88, loss = 0.25170214\n",
      "Iteration 89, loss = 0.25143747\n",
      "Iteration 90, loss = 0.25116271\n",
      "Iteration 91, loss = 0.25088483\n",
      "Iteration 92, loss = 0.25062604\n",
      "Iteration 93, loss = 0.25036338\n",
      "Iteration 94, loss = 0.25010551\n",
      "Iteration 95, loss = 0.24986777\n",
      "Iteration 96, loss = 0.24960538\n",
      "Iteration 97, loss = 0.24937655\n",
      "Iteration 98, loss = 0.24914639\n",
      "Iteration 99, loss = 0.24889152\n",
      "Iteration 100, loss = 0.24868056\n",
      "Iteration 101, loss = 0.24843354\n",
      "Iteration 102, loss = 0.24819389\n",
      "Iteration 103, loss = 0.24797085\n",
      "Iteration 104, loss = 0.24775564\n",
      "Iteration 105, loss = 0.24755898\n",
      "Iteration 106, loss = 0.24732494\n",
      "Iteration 107, loss = 0.24711527\n",
      "Iteration 108, loss = 0.24690337\n",
      "Iteration 109, loss = 0.24669740\n",
      "Iteration 110, loss = 0.24648829\n",
      "Iteration 111, loss = 0.24628817\n",
      "Iteration 112, loss = 0.24605317\n",
      "Iteration 113, loss = 0.24590008\n",
      "Iteration 114, loss = 0.24569551\n",
      "Iteration 115, loss = 0.24550118\n",
      "Iteration 116, loss = 0.24530103\n",
      "Iteration 117, loss = 0.24510948\n",
      "Iteration 118, loss = 0.24491567\n",
      "Iteration 119, loss = 0.24473895\n",
      "Iteration 120, loss = 0.24455989\n",
      "Iteration 121, loss = 0.24435343\n",
      "Iteration 122, loss = 0.24418986\n",
      "Iteration 123, loss = 0.24399761\n",
      "Iteration 124, loss = 0.24381329\n",
      "Iteration 125, loss = 0.24366261\n",
      "Iteration 126, loss = 0.24346603\n",
      "Iteration 127, loss = 0.24330477\n",
      "Iteration 128, loss = 0.24312127\n",
      "Iteration 129, loss = 0.24296156\n",
      "Iteration 130, loss = 0.24279052\n",
      "Iteration 131, loss = 0.24263452\n",
      "Iteration 132, loss = 0.24245852\n",
      "Iteration 133, loss = 0.24231734\n",
      "Iteration 134, loss = 0.24214555\n",
      "Iteration 135, loss = 0.24198071\n",
      "Iteration 136, loss = 0.24180663\n",
      "Iteration 137, loss = 0.24167154\n",
      "Iteration 138, loss = 0.24150401\n",
      "Iteration 139, loss = 0.24133765\n",
      "Iteration 140, loss = 0.24116997\n",
      "Iteration 141, loss = 0.24103793\n",
      "Iteration 142, loss = 0.24088930\n",
      "Iteration 143, loss = 0.24076103\n",
      "Iteration 144, loss = 0.24058763\n",
      "Iteration 145, loss = 0.24044637\n",
      "Iteration 146, loss = 0.24028438\n",
      "Iteration 147, loss = 0.24015157\n",
      "Iteration 148, loss = 0.24001516\n",
      "Iteration 149, loss = 0.23984580\n",
      "Iteration 150, loss = 0.23972227\n",
      "Iteration 151, loss = 0.23957461\n",
      "Iteration 152, loss = 0.23944600\n",
      "Iteration 153, loss = 0.23930317\n",
      "Iteration 154, loss = 0.23914218\n",
      "Iteration 155, loss = 0.23902353\n",
      "Iteration 156, loss = 0.23887434\n",
      "Iteration 157, loss = 0.23874107\n",
      "Iteration 158, loss = 0.23860310\n",
      "Iteration 159, loss = 0.23849499\n",
      "Iteration 160, loss = 0.23837108\n",
      "Iteration 161, loss = 0.23820157\n",
      "Iteration 162, loss = 0.23808955\n",
      "Iteration 163, loss = 0.23794062\n",
      "Iteration 164, loss = 0.23782290\n",
      "Iteration 165, loss = 0.23768652\n",
      "Iteration 166, loss = 0.23755820\n",
      "Iteration 167, loss = 0.23743579\n",
      "Iteration 168, loss = 0.23730930\n",
      "Iteration 169, loss = 0.23719665\n",
      "Iteration 170, loss = 0.23708173\n",
      "Iteration 171, loss = 0.23695511\n",
      "Iteration 172, loss = 0.23683345\n",
      "Iteration 173, loss = 0.23670391\n",
      "Iteration 174, loss = 0.23658278\n",
      "Iteration 175, loss = 0.23646378\n",
      "Iteration 176, loss = 0.23635872\n",
      "Iteration 177, loss = 0.23624631\n",
      "Iteration 178, loss = 0.23610614\n",
      "Iteration 179, loss = 0.23603380\n",
      "Iteration 180, loss = 0.23589693\n",
      "Iteration 181, loss = 0.23579985\n",
      "Iteration 182, loss = 0.23566155\n",
      "Iteration 183, loss = 0.23555122\n",
      "Iteration 184, loss = 0.23544107\n",
      "Iteration 185, loss = 0.23532907\n",
      "Iteration 186, loss = 0.23522508\n",
      "Iteration 187, loss = 0.23510153\n",
      "Iteration 188, loss = 0.23499737\n",
      "Iteration 189, loss = 0.23489114\n",
      "Iteration 190, loss = 0.23478135\n",
      "Iteration 191, loss = 0.23466507\n",
      "Iteration 192, loss = 0.23455234\n",
      "Iteration 193, loss = 0.23445765\n",
      "Iteration 194, loss = 0.23435313\n",
      "Iteration 195, loss = 0.23424098\n",
      "Iteration 196, loss = 0.23413876\n",
      "Iteration 197, loss = 0.23403517\n",
      "Iteration 198, loss = 0.23393961\n",
      "Iteration 199, loss = 0.23383741\n",
      "Iteration 200, loss = 0.23372632\n",
      "Iteration 1, loss = 0.75555773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69490179\n",
      "Iteration 3, loss = 0.63169384\n",
      "Iteration 4, loss = 0.57972760\n",
      "Iteration 5, loss = 0.53704198\n",
      "Iteration 6, loss = 0.50188891\n",
      "Iteration 7, loss = 0.47335841\n",
      "Iteration 8, loss = 0.44940894\n",
      "Iteration 9, loss = 0.42919563\n",
      "Iteration 10, loss = 0.41232130\n",
      "Iteration 11, loss = 0.39774628\n",
      "Iteration 12, loss = 0.38509314\n",
      "Iteration 13, loss = 0.37429261\n",
      "Iteration 14, loss = 0.36471050\n",
      "Iteration 15, loss = 0.35628659\n",
      "Iteration 16, loss = 0.34884420\n",
      "Iteration 17, loss = 0.34229370\n",
      "Iteration 18, loss = 0.33633124\n",
      "Iteration 19, loss = 0.33092019\n",
      "Iteration 20, loss = 0.32612756\n",
      "Iteration 21, loss = 0.32176808\n",
      "Iteration 22, loss = 0.31774802\n",
      "Iteration 23, loss = 0.31409352\n",
      "Iteration 24, loss = 0.31077941\n",
      "Iteration 25, loss = 0.30767585\n",
      "Iteration 26, loss = 0.30484868\n",
      "Iteration 27, loss = 0.30221122\n",
      "Iteration 28, loss = 0.29976813\n",
      "Iteration 29, loss = 0.29748157\n",
      "Iteration 30, loss = 0.29538050\n",
      "Iteration 31, loss = 0.29342134\n",
      "Iteration 32, loss = 0.29154386\n",
      "Iteration 33, loss = 0.28983638\n",
      "Iteration 34, loss = 0.28821777\n",
      "Iteration 35, loss = 0.28668140\n",
      "Iteration 36, loss = 0.28523724\n",
      "Iteration 37, loss = 0.28384956\n",
      "Iteration 38, loss = 0.28259410\n",
      "Iteration 39, loss = 0.28135701\n",
      "Iteration 40, loss = 0.28019693\n",
      "Iteration 41, loss = 0.27911992\n",
      "Iteration 42, loss = 0.27805525\n",
      "Iteration 43, loss = 0.27706007\n",
      "Iteration 44, loss = 0.27611990\n",
      "Iteration 45, loss = 0.27518640\n",
      "Iteration 46, loss = 0.27436979\n",
      "Iteration 47, loss = 0.27350948\n",
      "Iteration 48, loss = 0.27272295\n",
      "Iteration 49, loss = 0.27195425\n",
      "Iteration 50, loss = 0.27123805\n",
      "Iteration 51, loss = 0.27053180\n",
      "Iteration 52, loss = 0.26984710\n",
      "Iteration 53, loss = 0.26921007\n",
      "Iteration 54, loss = 0.26861674\n",
      "Iteration 55, loss = 0.26796229\n",
      "Iteration 56, loss = 0.26739248\n",
      "Iteration 57, loss = 0.26687066\n",
      "Iteration 58, loss = 0.26631542\n",
      "Iteration 59, loss = 0.26577498\n",
      "Iteration 60, loss = 0.26528272\n",
      "Iteration 61, loss = 0.26479366\n",
      "Iteration 62, loss = 0.26431901\n",
      "Iteration 63, loss = 0.26384557\n",
      "Iteration 64, loss = 0.26339253\n",
      "Iteration 65, loss = 0.26297213\n",
      "Iteration 66, loss = 0.26257571\n",
      "Iteration 67, loss = 0.26213120\n",
      "Iteration 68, loss = 0.26174070\n",
      "Iteration 69, loss = 0.26133956\n",
      "Iteration 70, loss = 0.26095801\n",
      "Iteration 71, loss = 0.26060950\n",
      "Iteration 72, loss = 0.26025213\n",
      "Iteration 73, loss = 0.25988454\n",
      "Iteration 74, loss = 0.25955189\n",
      "Iteration 75, loss = 0.25922591\n",
      "Iteration 76, loss = 0.25886722\n",
      "Iteration 77, loss = 0.25856251\n",
      "Iteration 78, loss = 0.25823845\n",
      "Iteration 79, loss = 0.25792096\n",
      "Iteration 80, loss = 0.25763888\n",
      "Iteration 81, loss = 0.25732636\n",
      "Iteration 82, loss = 0.25705934\n",
      "Iteration 83, loss = 0.25676544\n",
      "Iteration 84, loss = 0.25646617\n",
      "Iteration 85, loss = 0.25620553\n",
      "Iteration 86, loss = 0.25592829\n",
      "Iteration 87, loss = 0.25568431\n",
      "Iteration 88, loss = 0.25543555\n",
      "Iteration 89, loss = 0.25518892\n",
      "Iteration 90, loss = 0.25492564\n",
      "Iteration 91, loss = 0.25466981\n",
      "Iteration 92, loss = 0.25442246\n",
      "Iteration 93, loss = 0.25417712\n",
      "Iteration 94, loss = 0.25394225\n",
      "Iteration 95, loss = 0.25371140\n",
      "Iteration 96, loss = 0.25348657\n",
      "Iteration 97, loss = 0.25326635\n",
      "Iteration 98, loss = 0.25304913\n",
      "Iteration 99, loss = 0.25281080\n",
      "Iteration 100, loss = 0.25262555\n",
      "Iteration 101, loss = 0.25241558\n",
      "Iteration 102, loss = 0.25218654\n",
      "Iteration 103, loss = 0.25197652\n",
      "Iteration 104, loss = 0.25177435\n",
      "Iteration 105, loss = 0.25157243\n",
      "Iteration 106, loss = 0.25136965\n",
      "Iteration 107, loss = 0.25119977\n",
      "Iteration 108, loss = 0.25099055\n",
      "Iteration 109, loss = 0.25079946\n",
      "Iteration 110, loss = 0.25060694\n",
      "Iteration 111, loss = 0.25042089\n",
      "Iteration 112, loss = 0.25023403\n",
      "Iteration 113, loss = 0.25006859\n",
      "Iteration 114, loss = 0.24986723\n",
      "Iteration 115, loss = 0.24967282\n",
      "Iteration 116, loss = 0.24950521\n",
      "Iteration 117, loss = 0.24932717\n",
      "Iteration 118, loss = 0.24915192\n",
      "Iteration 119, loss = 0.24899564\n",
      "Iteration 120, loss = 0.24882076\n",
      "Iteration 121, loss = 0.24863544\n",
      "Iteration 122, loss = 0.24849788\n",
      "Iteration 123, loss = 0.24831031\n",
      "Iteration 124, loss = 0.24812726\n",
      "Iteration 125, loss = 0.24800666\n",
      "Iteration 126, loss = 0.24782697\n",
      "Iteration 127, loss = 0.24768595\n",
      "Iteration 128, loss = 0.24750684\n",
      "Iteration 129, loss = 0.24734636\n",
      "Iteration 130, loss = 0.24718644\n",
      "Iteration 131, loss = 0.24704936\n",
      "Iteration 132, loss = 0.24689409\n",
      "Iteration 133, loss = 0.24673492\n",
      "Iteration 134, loss = 0.24658373\n",
      "Iteration 135, loss = 0.24643647\n",
      "Iteration 136, loss = 0.24628248\n",
      "Iteration 137, loss = 0.24613604\n",
      "Iteration 138, loss = 0.24600400\n",
      "Iteration 139, loss = 0.24585520\n",
      "Iteration 140, loss = 0.24570989\n",
      "Iteration 141, loss = 0.24558214\n",
      "Iteration 142, loss = 0.24544601\n",
      "Iteration 143, loss = 0.24531580\n",
      "Iteration 144, loss = 0.24516513\n",
      "Iteration 145, loss = 0.24505071\n",
      "Iteration 146, loss = 0.24486827\n",
      "Iteration 147, loss = 0.24475286\n",
      "Iteration 148, loss = 0.24462538\n",
      "Iteration 149, loss = 0.24446958\n",
      "Iteration 150, loss = 0.24436018\n",
      "Iteration 151, loss = 0.24421549\n",
      "Iteration 152, loss = 0.24408677\n",
      "Iteration 153, loss = 0.24396484\n",
      "Iteration 154, loss = 0.24381905\n",
      "Iteration 155, loss = 0.24373833\n",
      "Iteration 156, loss = 0.24356818\n",
      "Iteration 157, loss = 0.24345285\n",
      "Iteration 158, loss = 0.24333826\n",
      "Iteration 159, loss = 0.24322672\n",
      "Iteration 160, loss = 0.24308669\n",
      "Iteration 161, loss = 0.24294490\n",
      "Iteration 162, loss = 0.24283062\n",
      "Iteration 163, loss = 0.24272713\n",
      "Iteration 164, loss = 0.24259120\n",
      "Iteration 165, loss = 0.24248786\n",
      "Iteration 166, loss = 0.24235373\n",
      "Iteration 167, loss = 0.24223424\n",
      "Iteration 168, loss = 0.24212101\n",
      "Iteration 169, loss = 0.24200536\n",
      "Iteration 170, loss = 0.24189499\n",
      "Iteration 171, loss = 0.24177654\n",
      "Iteration 172, loss = 0.24166500\n",
      "Iteration 173, loss = 0.24156845\n",
      "Iteration 174, loss = 0.24144477\n",
      "Iteration 175, loss = 0.24132729\n",
      "Iteration 176, loss = 0.24121739\n",
      "Iteration 177, loss = 0.24111397\n",
      "Iteration 178, loss = 0.24102958\n",
      "Iteration 179, loss = 0.24092679\n",
      "Iteration 180, loss = 0.24080990\n",
      "Iteration 181, loss = 0.24069760\n",
      "Iteration 182, loss = 0.24058033\n",
      "Iteration 183, loss = 0.24047293\n",
      "Iteration 184, loss = 0.24036403\n",
      "Iteration 185, loss = 0.24027942\n",
      "Iteration 186, loss = 0.24018013\n",
      "Iteration 187, loss = 0.24005550\n",
      "Iteration 188, loss = 0.23996440\n",
      "Iteration 189, loss = 0.23987160\n",
      "Iteration 190, loss = 0.23974767\n",
      "Iteration 191, loss = 0.23963981\n",
      "Iteration 192, loss = 0.23955946\n",
      "Iteration 193, loss = 0.23943970\n",
      "Iteration 194, loss = 0.23936825\n",
      "Iteration 195, loss = 0.23924973\n",
      "Iteration 196, loss = 0.23913711\n",
      "Iteration 197, loss = 0.23905095\n",
      "Iteration 198, loss = 0.23895444\n",
      "Iteration 199, loss = 0.23887655\n",
      "Iteration 200, loss = 0.23877052\n",
      "Iteration 1, loss = 0.75739948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69643549\n",
      "Iteration 3, loss = 0.63394971\n",
      "Iteration 4, loss = 0.58144165\n",
      "Iteration 5, loss = 0.53862016\n",
      "Iteration 6, loss = 0.50386836\n",
      "Iteration 7, loss = 0.47518753\n",
      "Iteration 8, loss = 0.45120597\n",
      "Iteration 9, loss = 0.43100889\n",
      "Iteration 10, loss = 0.41406384\n",
      "Iteration 11, loss = 0.39949487\n",
      "Iteration 12, loss = 0.38679323\n",
      "Iteration 13, loss = 0.37591695\n",
      "Iteration 14, loss = 0.36624482\n",
      "Iteration 15, loss = 0.35784739\n",
      "Iteration 16, loss = 0.35024032\n",
      "Iteration 17, loss = 0.34359388\n",
      "Iteration 18, loss = 0.33765860\n",
      "Iteration 19, loss = 0.33216762\n",
      "Iteration 20, loss = 0.32730015\n",
      "Iteration 21, loss = 0.32285661\n",
      "Iteration 22, loss = 0.31878686\n",
      "Iteration 23, loss = 0.31505591\n",
      "Iteration 24, loss = 0.31163383\n",
      "Iteration 25, loss = 0.30851890\n",
      "Iteration 26, loss = 0.30558613\n",
      "Iteration 27, loss = 0.30291448\n",
      "Iteration 28, loss = 0.30041482\n",
      "Iteration 29, loss = 0.29810771\n",
      "Iteration 30, loss = 0.29586230\n",
      "Iteration 31, loss = 0.29388970\n",
      "Iteration 32, loss = 0.29190726\n",
      "Iteration 33, loss = 0.29017032\n",
      "Iteration 34, loss = 0.28846448\n",
      "Iteration 35, loss = 0.28687830\n",
      "Iteration 36, loss = 0.28537830\n",
      "Iteration 37, loss = 0.28393616\n",
      "Iteration 38, loss = 0.28264608\n",
      "Iteration 39, loss = 0.28131546\n",
      "Iteration 40, loss = 0.28010452\n",
      "Iteration 41, loss = 0.27899505\n",
      "Iteration 42, loss = 0.27787415\n",
      "Iteration 43, loss = 0.27681257\n",
      "Iteration 44, loss = 0.27584235\n",
      "Iteration 45, loss = 0.27486846\n",
      "Iteration 46, loss = 0.27397435\n",
      "Iteration 47, loss = 0.27309576\n",
      "Iteration 48, loss = 0.27222164\n",
      "Iteration 49, loss = 0.27145615\n",
      "Iteration 50, loss = 0.27064166\n",
      "Iteration 51, loss = 0.26990685\n",
      "Iteration 52, loss = 0.26919599\n",
      "Iteration 53, loss = 0.26849144\n",
      "Iteration 54, loss = 0.26782267\n",
      "Iteration 55, loss = 0.26716069\n",
      "Iteration 56, loss = 0.26655900\n",
      "Iteration 57, loss = 0.26594008\n",
      "Iteration 58, loss = 0.26538163\n",
      "Iteration 59, loss = 0.26479694\n",
      "Iteration 60, loss = 0.26424286\n",
      "Iteration 61, loss = 0.26373347\n",
      "Iteration 62, loss = 0.26319397\n",
      "Iteration 63, loss = 0.26270302\n",
      "Iteration 64, loss = 0.26220386\n",
      "Iteration 65, loss = 0.26175320\n",
      "Iteration 66, loss = 0.26126140\n",
      "Iteration 67, loss = 0.26081550\n",
      "Iteration 68, loss = 0.26038621\n",
      "Iteration 69, loss = 0.25994749\n",
      "Iteration 70, loss = 0.25953798\n",
      "Iteration 71, loss = 0.25913624\n",
      "Iteration 72, loss = 0.25875097\n",
      "Iteration 73, loss = 0.25834404\n",
      "Iteration 74, loss = 0.25794726\n",
      "Iteration 75, loss = 0.25760517\n",
      "Iteration 76, loss = 0.25723264\n",
      "Iteration 77, loss = 0.25685864\n",
      "Iteration 78, loss = 0.25652180\n",
      "Iteration 79, loss = 0.25617910\n",
      "Iteration 80, loss = 0.25582647\n",
      "Iteration 81, loss = 0.25551409\n",
      "Iteration 82, loss = 0.25524352\n",
      "Iteration 83, loss = 0.25486663\n",
      "Iteration 84, loss = 0.25456192\n",
      "Iteration 85, loss = 0.25423359\n",
      "Iteration 86, loss = 0.25395597\n",
      "Iteration 87, loss = 0.25364052\n",
      "Iteration 88, loss = 0.25335190\n",
      "Iteration 89, loss = 0.25306329\n",
      "Iteration 90, loss = 0.25278268\n",
      "Iteration 91, loss = 0.25250126\n",
      "Iteration 92, loss = 0.25225171\n",
      "Iteration 93, loss = 0.25195215\n",
      "Iteration 94, loss = 0.25168863\n",
      "Iteration 95, loss = 0.25142342\n",
      "Iteration 96, loss = 0.25119015\n",
      "Iteration 97, loss = 0.25091922\n",
      "Iteration 98, loss = 0.25064306\n",
      "Iteration 99, loss = 0.25040962\n",
      "Iteration 100, loss = 0.25019267\n",
      "Iteration 101, loss = 0.24992752\n",
      "Iteration 102, loss = 0.24969762\n",
      "Iteration 103, loss = 0.24947677\n",
      "Iteration 104, loss = 0.24920487\n",
      "Iteration 105, loss = 0.24898395\n",
      "Iteration 106, loss = 0.24875841\n",
      "Iteration 107, loss = 0.24853533\n",
      "Iteration 108, loss = 0.24830490\n",
      "Iteration 109, loss = 0.24808514\n",
      "Iteration 110, loss = 0.24787530\n",
      "Iteration 111, loss = 0.24768064\n",
      "Iteration 112, loss = 0.24745069\n",
      "Iteration 113, loss = 0.24723345\n",
      "Iteration 114, loss = 0.24703680\n",
      "Iteration 115, loss = 0.24682629\n",
      "Iteration 116, loss = 0.24662073\n",
      "Iteration 117, loss = 0.24641938\n",
      "Iteration 118, loss = 0.24619958\n",
      "Iteration 119, loss = 0.24603749\n",
      "Iteration 120, loss = 0.24583179\n",
      "Iteration 121, loss = 0.24563661\n",
      "Iteration 122, loss = 0.24544696\n",
      "Iteration 123, loss = 0.24527038\n",
      "Iteration 124, loss = 0.24509059\n",
      "Iteration 125, loss = 0.24488740\n",
      "Iteration 126, loss = 0.24469391\n",
      "Iteration 127, loss = 0.24452458\n",
      "Iteration 128, loss = 0.24434405\n",
      "Iteration 129, loss = 0.24416002\n",
      "Iteration 130, loss = 0.24402464\n",
      "Iteration 131, loss = 0.24380599\n",
      "Iteration 132, loss = 0.24363510\n",
      "Iteration 133, loss = 0.24346937\n",
      "Iteration 134, loss = 0.24334661\n",
      "Iteration 135, loss = 0.24310569\n",
      "Iteration 136, loss = 0.24294730\n",
      "Iteration 137, loss = 0.24279132\n",
      "Iteration 138, loss = 0.24261668\n",
      "Iteration 139, loss = 0.24245235\n",
      "Iteration 140, loss = 0.24230520\n",
      "Iteration 141, loss = 0.24214778\n",
      "Iteration 142, loss = 0.24196627\n",
      "Iteration 143, loss = 0.24183699\n",
      "Iteration 144, loss = 0.24169024\n",
      "Iteration 145, loss = 0.24150999\n",
      "Iteration 146, loss = 0.24135748\n",
      "Iteration 147, loss = 0.24120970\n",
      "Iteration 148, loss = 0.24105957\n",
      "Iteration 149, loss = 0.24091251\n",
      "Iteration 150, loss = 0.24075339\n",
      "Iteration 151, loss = 0.24060514\n",
      "Iteration 152, loss = 0.24051548\n",
      "Iteration 153, loss = 0.24032016\n",
      "Iteration 154, loss = 0.24017404\n",
      "Iteration 155, loss = 0.24003365\n",
      "Iteration 156, loss = 0.23989191\n",
      "Iteration 157, loss = 0.23976587\n",
      "Iteration 158, loss = 0.23959169\n",
      "Iteration 159, loss = 0.23947495\n",
      "Iteration 160, loss = 0.23931860\n",
      "Iteration 161, loss = 0.23919875\n",
      "Iteration 162, loss = 0.23907468\n",
      "Iteration 163, loss = 0.23891129\n",
      "Iteration 164, loss = 0.23879922\n",
      "Iteration 165, loss = 0.23866046\n",
      "Iteration 166, loss = 0.23853777\n",
      "Iteration 167, loss = 0.23839569\n",
      "Iteration 168, loss = 0.23826447\n",
      "Iteration 169, loss = 0.23814582\n",
      "Iteration 170, loss = 0.23800802\n",
      "Iteration 171, loss = 0.23787747\n",
      "Iteration 172, loss = 0.23776744\n",
      "Iteration 173, loss = 0.23762807\n",
      "Iteration 174, loss = 0.23750338\n",
      "Iteration 175, loss = 0.23741765\n",
      "Iteration 176, loss = 0.23725859\n",
      "Iteration 177, loss = 0.23713201\n",
      "Iteration 178, loss = 0.23704424\n",
      "Iteration 179, loss = 0.23691255\n",
      "Iteration 180, loss = 0.23678157\n",
      "Iteration 181, loss = 0.23667732\n",
      "Iteration 182, loss = 0.23653252\n",
      "Iteration 183, loss = 0.23644642\n",
      "Iteration 184, loss = 0.23633722\n",
      "Iteration 185, loss = 0.23620683\n",
      "Iteration 186, loss = 0.23608850\n",
      "Iteration 187, loss = 0.23599925\n",
      "Iteration 188, loss = 0.23586549\n",
      "Iteration 189, loss = 0.23574046\n",
      "Iteration 190, loss = 0.23567605\n",
      "Iteration 191, loss = 0.23553936\n",
      "Iteration 192, loss = 0.23542967\n",
      "Iteration 193, loss = 0.23530978\n",
      "Iteration 194, loss = 0.23522153\n",
      "Iteration 195, loss = 0.23512981\n",
      "Iteration 196, loss = 0.23498865\n",
      "Iteration 197, loss = 0.23488075\n",
      "Iteration 198, loss = 0.23477140\n",
      "Iteration 199, loss = 0.23465517\n",
      "Iteration 200, loss = 0.23456973\n",
      "Iteration 1, loss = 0.76143622\n",
      "Iteration 2, loss = 0.69878427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.63444373\n",
      "Iteration 4, loss = 0.58077935\n",
      "Iteration 5, loss = 0.53723248\n",
      "Iteration 6, loss = 0.50171937\n",
      "Iteration 7, loss = 0.47284670\n",
      "Iteration 8, loss = 0.44846892\n",
      "Iteration 9, loss = 0.42830995\n",
      "Iteration 10, loss = 0.41139901\n",
      "Iteration 11, loss = 0.39680231\n",
      "Iteration 12, loss = 0.38425129\n",
      "Iteration 13, loss = 0.37346258\n",
      "Iteration 14, loss = 0.36389014\n",
      "Iteration 15, loss = 0.35561358\n",
      "Iteration 16, loss = 0.34815153\n",
      "Iteration 17, loss = 0.34163728\n",
      "Iteration 18, loss = 0.33579960\n",
      "Iteration 19, loss = 0.33049669\n",
      "Iteration 20, loss = 0.32572942\n",
      "Iteration 21, loss = 0.32145158\n",
      "Iteration 22, loss = 0.31752925\n",
      "Iteration 23, loss = 0.31388375\n",
      "Iteration 24, loss = 0.31064154\n",
      "Iteration 25, loss = 0.30758953\n",
      "Iteration 26, loss = 0.30483139\n",
      "Iteration 27, loss = 0.30225714\n",
      "Iteration 28, loss = 0.29987374\n",
      "Iteration 29, loss = 0.29768240\n",
      "Iteration 30, loss = 0.29555766\n",
      "Iteration 31, loss = 0.29366329\n",
      "Iteration 32, loss = 0.29181974\n",
      "Iteration 33, loss = 0.29015846\n",
      "Iteration 34, loss = 0.28856018\n",
      "Iteration 35, loss = 0.28707685\n",
      "Iteration 36, loss = 0.28566915\n",
      "Iteration 37, loss = 0.28433596\n",
      "Iteration 38, loss = 0.28308553\n",
      "Iteration 39, loss = 0.28186993\n",
      "Iteration 40, loss = 0.28073216\n",
      "Iteration 41, loss = 0.27968388\n",
      "Iteration 42, loss = 0.27866896\n",
      "Iteration 43, loss = 0.27767855\n",
      "Iteration 44, loss = 0.27678120\n",
      "Iteration 45, loss = 0.27587577\n",
      "Iteration 46, loss = 0.27504569\n",
      "Iteration 47, loss = 0.27425363\n",
      "Iteration 48, loss = 0.27344181\n",
      "Iteration 49, loss = 0.27274711\n",
      "Iteration 50, loss = 0.27200252\n",
      "Iteration 51, loss = 0.27135912\n",
      "Iteration 52, loss = 0.27069171\n",
      "Iteration 53, loss = 0.27004345\n",
      "Iteration 54, loss = 0.26942233\n",
      "Iteration 55, loss = 0.26882955\n",
      "Iteration 56, loss = 0.26827625\n",
      "Iteration 57, loss = 0.26773647\n",
      "Iteration 58, loss = 0.26722090\n",
      "Iteration 59, loss = 0.26668851\n",
      "Iteration 60, loss = 0.26619682\n",
      "Iteration 61, loss = 0.26572248\n",
      "Iteration 62, loss = 0.26524289\n",
      "Iteration 63, loss = 0.26481056\n",
      "Iteration 64, loss = 0.26435235\n",
      "Iteration 65, loss = 0.26396092\n",
      "Iteration 66, loss = 0.26352915\n",
      "Iteration 67, loss = 0.26312554\n",
      "Iteration 68, loss = 0.26273183\n",
      "Iteration 69, loss = 0.26234736\n",
      "Iteration 70, loss = 0.26198787\n",
      "Iteration 71, loss = 0.26161981\n",
      "Iteration 72, loss = 0.26125972\n",
      "Iteration 73, loss = 0.26090728\n",
      "Iteration 74, loss = 0.26057122\n",
      "Iteration 75, loss = 0.26023719\n",
      "Iteration 76, loss = 0.25992466\n",
      "Iteration 77, loss = 0.25961533\n",
      "Iteration 78, loss = 0.25931553\n",
      "Iteration 79, loss = 0.25898562\n",
      "Iteration 80, loss = 0.25870814\n",
      "Iteration 81, loss = 0.25841530\n",
      "Iteration 82, loss = 0.25816484\n",
      "Iteration 83, loss = 0.25785273\n",
      "Iteration 84, loss = 0.25756828\n",
      "Iteration 85, loss = 0.25726876\n",
      "Iteration 86, loss = 0.25703681\n",
      "Iteration 87, loss = 0.25676190\n",
      "Iteration 88, loss = 0.25650302\n",
      "Iteration 89, loss = 0.25624814\n",
      "Iteration 90, loss = 0.25599625\n",
      "Iteration 91, loss = 0.25576798\n",
      "Iteration 92, loss = 0.25551923\n",
      "Iteration 93, loss = 0.25527723\n",
      "Iteration 94, loss = 0.25505698\n",
      "Iteration 95, loss = 0.25482752\n",
      "Iteration 96, loss = 0.25463691\n",
      "Iteration 97, loss = 0.25437260\n",
      "Iteration 98, loss = 0.25415096\n",
      "Iteration 99, loss = 0.25393503\n",
      "Iteration 100, loss = 0.25373654\n",
      "Iteration 101, loss = 0.25353007\n",
      "Iteration 102, loss = 0.25331107\n",
      "Iteration 103, loss = 0.25311842\n",
      "Iteration 104, loss = 0.25288928\n",
      "Iteration 105, loss = 0.25269171\n",
      "Iteration 106, loss = 0.25248966\n",
      "Iteration 107, loss = 0.25229978\n",
      "Iteration 108, loss = 0.25212685\n",
      "Iteration 109, loss = 0.25191634\n",
      "Iteration 110, loss = 0.25173295\n",
      "Iteration 111, loss = 0.25155947\n",
      "Iteration 112, loss = 0.25136381\n",
      "Iteration 113, loss = 0.25117465\n",
      "Iteration 114, loss = 0.25099177\n",
      "Iteration 115, loss = 0.25081917\n",
      "Iteration 116, loss = 0.25064514\n",
      "Iteration 117, loss = 0.25046708\n",
      "Iteration 118, loss = 0.25027578\n",
      "Iteration 119, loss = 0.25012989\n",
      "Iteration 120, loss = 0.24994917\n",
      "Iteration 121, loss = 0.24978693\n",
      "Iteration 122, loss = 0.24963495\n",
      "Iteration 123, loss = 0.24946112\n",
      "Iteration 124, loss = 0.24930377\n",
      "Iteration 125, loss = 0.24913241\n",
      "Iteration 126, loss = 0.24897784\n",
      "Iteration 127, loss = 0.24881858\n",
      "Iteration 128, loss = 0.24866615\n",
      "Iteration 129, loss = 0.24850564\n",
      "Iteration 130, loss = 0.24837725\n",
      "Iteration 131, loss = 0.24818657\n",
      "Iteration 132, loss = 0.24804407\n",
      "Iteration 133, loss = 0.24789214\n",
      "Iteration 134, loss = 0.24777497\n",
      "Iteration 135, loss = 0.24758382\n",
      "Iteration 136, loss = 0.24745232\n",
      "Iteration 137, loss = 0.24729583\n",
      "Iteration 138, loss = 0.24716235\n",
      "Iteration 139, loss = 0.24701431\n",
      "Iteration 140, loss = 0.24689508\n",
      "Iteration 141, loss = 0.24673619\n",
      "Iteration 142, loss = 0.24659960\n",
      "Iteration 143, loss = 0.24648233\n",
      "Iteration 144, loss = 0.24633591\n",
      "Iteration 145, loss = 0.24619347\n",
      "Iteration 146, loss = 0.24604851\n",
      "Iteration 147, loss = 0.24593865\n",
      "Iteration 148, loss = 0.24579854\n",
      "Iteration 149, loss = 0.24566471\n",
      "Iteration 150, loss = 0.24554711\n",
      "Iteration 151, loss = 0.24541412\n",
      "Iteration 152, loss = 0.24531280\n",
      "Iteration 153, loss = 0.24515076\n",
      "Iteration 154, loss = 0.24503599\n",
      "Iteration 155, loss = 0.24490780\n",
      "Iteration 156, loss = 0.24475931\n",
      "Iteration 157, loss = 0.24466930\n",
      "Iteration 158, loss = 0.24451853\n",
      "Iteration 159, loss = 0.24440415\n",
      "Iteration 160, loss = 0.24428180\n",
      "Iteration 161, loss = 0.24418293\n",
      "Iteration 162, loss = 0.24406614\n",
      "Iteration 163, loss = 0.24391897\n",
      "Iteration 164, loss = 0.24381111\n",
      "Iteration 165, loss = 0.24369005\n",
      "Iteration 166, loss = 0.24355916\n",
      "Iteration 167, loss = 0.24344906\n",
      "Iteration 168, loss = 0.24334136\n",
      "Iteration 169, loss = 0.24324170\n",
      "Iteration 170, loss = 0.24311593\n",
      "Iteration 171, loss = 0.24301411\n",
      "Iteration 172, loss = 0.24289412\n",
      "Iteration 173, loss = 0.24278559\n",
      "Iteration 174, loss = 0.24269288\n",
      "Iteration 175, loss = 0.24262021\n",
      "Iteration 176, loss = 0.24244512\n",
      "Iteration 177, loss = 0.24234675\n",
      "Iteration 178, loss = 0.24226227\n",
      "Iteration 179, loss = 0.24214016\n",
      "Iteration 180, loss = 0.24203154\n",
      "Iteration 181, loss = 0.24194757\n",
      "Iteration 182, loss = 0.24182213\n",
      "Iteration 183, loss = 0.24174270\n",
      "Iteration 184, loss = 0.24162181\n",
      "Iteration 185, loss = 0.24150696\n",
      "Iteration 186, loss = 0.24141696\n",
      "Iteration 187, loss = 0.24135001\n",
      "Iteration 188, loss = 0.24121090\n",
      "Iteration 189, loss = 0.24110920\n",
      "Iteration 190, loss = 0.24104330\n",
      "Iteration 191, loss = 0.24094943\n",
      "Iteration 192, loss = 0.24083748\n",
      "Iteration 193, loss = 0.24071309\n",
      "Iteration 194, loss = 0.24064477\n",
      "Iteration 195, loss = 0.24057030\n",
      "Iteration 196, loss = 0.24042488\n",
      "Iteration 197, loss = 0.24032672\n",
      "Iteration 198, loss = 0.24024430\n",
      "Iteration 199, loss = 0.24013267\n",
      "Iteration 200, loss = 0.24005160\n",
      "Iteration 1, loss = 0.68558826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53830094\n",
      "Iteration 3, loss = 0.45087743\n",
      "Iteration 4, loss = 0.39195086\n",
      "Iteration 5, loss = 0.34980838\n",
      "Iteration 6, loss = 0.31956332\n",
      "Iteration 7, loss = 0.29853263\n",
      "Iteration 8, loss = 0.28319724\n",
      "Iteration 9, loss = 0.27157039\n",
      "Iteration 10, loss = 0.26315379\n",
      "Iteration 11, loss = 0.25642051\n",
      "Iteration 12, loss = 0.25068772\n",
      "Iteration 13, loss = 0.24676908\n",
      "Iteration 14, loss = 0.24304626\n",
      "Iteration 15, loss = 0.23992050\n",
      "Iteration 16, loss = 0.23743372\n",
      "Iteration 17, loss = 0.23455791\n",
      "Iteration 18, loss = 0.23283606\n",
      "Iteration 19, loss = 0.23086640\n",
      "Iteration 20, loss = 0.22942566\n",
      "Iteration 21, loss = 0.22716275\n",
      "Iteration 22, loss = 0.22654750\n",
      "Iteration 23, loss = 0.22445528\n",
      "Iteration 24, loss = 0.22272178\n",
      "Iteration 25, loss = 0.22172490\n",
      "Iteration 26, loss = 0.22032696\n",
      "Iteration 27, loss = 0.21892129\n",
      "Iteration 28, loss = 0.21791463\n",
      "Iteration 29, loss = 0.21576410\n",
      "Iteration 30, loss = 0.21524097\n",
      "Iteration 31, loss = 0.21360184\n",
      "Iteration 32, loss = 0.21204988\n",
      "Iteration 33, loss = 0.21045063\n",
      "Iteration 34, loss = 0.20941028\n",
      "Iteration 35, loss = 0.20865601\n",
      "Iteration 36, loss = 0.20715310\n",
      "Iteration 37, loss = 0.20514715\n",
      "Iteration 38, loss = 0.20364405\n",
      "Iteration 39, loss = 0.20239716\n",
      "Iteration 40, loss = 0.20153636\n",
      "Iteration 41, loss = 0.19966724\n",
      "Iteration 42, loss = 0.19828487\n",
      "Iteration 43, loss = 0.19633722\n",
      "Iteration 44, loss = 0.19532128\n",
      "Iteration 45, loss = 0.19425635\n",
      "Iteration 46, loss = 0.19229255\n",
      "Iteration 47, loss = 0.19164857\n",
      "Iteration 48, loss = 0.18937438\n",
      "Iteration 49, loss = 0.18805228\n",
      "Iteration 50, loss = 0.18552751\n",
      "Iteration 51, loss = 0.18462302\n",
      "Iteration 52, loss = 0.18348090\n",
      "Iteration 53, loss = 0.18151069\n",
      "Iteration 54, loss = 0.17994312\n",
      "Iteration 55, loss = 0.17837778\n",
      "Iteration 56, loss = 0.17648546\n",
      "Iteration 57, loss = 0.17598063\n",
      "Iteration 58, loss = 0.17402915\n",
      "Iteration 59, loss = 0.17205549\n",
      "Iteration 60, loss = 0.17094630\n",
      "Iteration 61, loss = 0.16927280\n",
      "Iteration 62, loss = 0.16783040\n",
      "Iteration 63, loss = 0.16639334\n",
      "Iteration 64, loss = 0.16523745\n",
      "Iteration 65, loss = 0.16344217\n",
      "Iteration 66, loss = 0.16216383\n",
      "Iteration 67, loss = 0.16035210\n",
      "Iteration 68, loss = 0.15897193\n",
      "Iteration 69, loss = 0.15775696\n",
      "Iteration 70, loss = 0.15578715\n",
      "Iteration 71, loss = 0.15445955\n",
      "Iteration 72, loss = 0.15304267\n",
      "Iteration 73, loss = 0.15208235\n",
      "Iteration 74, loss = 0.15019359\n",
      "Iteration 75, loss = 0.14905710\n",
      "Iteration 76, loss = 0.14735166\n",
      "Iteration 77, loss = 0.14674836\n",
      "Iteration 78, loss = 0.14485147\n",
      "Iteration 79, loss = 0.14370599\n",
      "Iteration 80, loss = 0.14189278\n",
      "Iteration 81, loss = 0.14093926\n",
      "Iteration 82, loss = 0.14006476\n",
      "Iteration 83, loss = 0.13828151\n",
      "Iteration 84, loss = 0.13709161\n",
      "Iteration 85, loss = 0.13574872\n",
      "Iteration 86, loss = 0.13412533\n",
      "Iteration 87, loss = 0.13328691\n",
      "Iteration 88, loss = 0.13158651\n",
      "Iteration 89, loss = 0.13026859\n",
      "Iteration 90, loss = 0.12947410\n",
      "Iteration 91, loss = 0.12784468\n",
      "Iteration 92, loss = 0.12692156\n",
      "Iteration 93, loss = 0.12529349\n",
      "Iteration 94, loss = 0.12420306\n",
      "Iteration 95, loss = 0.12296704\n",
      "Iteration 96, loss = 0.12183985\n",
      "Iteration 97, loss = 0.12048183\n",
      "Iteration 98, loss = 0.11945335\n",
      "Iteration 99, loss = 0.11811138\n",
      "Iteration 100, loss = 0.11759647\n",
      "Iteration 101, loss = 0.11640027\n",
      "Iteration 102, loss = 0.11482683\n",
      "Iteration 103, loss = 0.11411140\n",
      "Iteration 104, loss = 0.11228679\n",
      "Iteration 105, loss = 0.11184791\n",
      "Iteration 106, loss = 0.11036775\n",
      "Iteration 107, loss = 0.10980473\n",
      "Iteration 108, loss = 0.10834195\n",
      "Iteration 109, loss = 0.10701289\n",
      "Iteration 110, loss = 0.10638007\n",
      "Iteration 111, loss = 0.10504877\n",
      "Iteration 112, loss = 0.10409894\n",
      "Iteration 113, loss = 0.10296906\n",
      "Iteration 114, loss = 0.10200776\n",
      "Iteration 115, loss = 0.10115204\n",
      "Iteration 116, loss = 0.09962945\n",
      "Iteration 117, loss = 0.09935291\n",
      "Iteration 118, loss = 0.09779492\n",
      "Iteration 119, loss = 0.09689610\n",
      "Iteration 120, loss = 0.09597908\n",
      "Iteration 121, loss = 0.09497081\n",
      "Iteration 122, loss = 0.09375663\n",
      "Iteration 123, loss = 0.09277903\n",
      "Iteration 124, loss = 0.09184879\n",
      "Iteration 125, loss = 0.09141842\n",
      "Iteration 126, loss = 0.09037584\n",
      "Iteration 127, loss = 0.08947886\n",
      "Iteration 128, loss = 0.08844588\n",
      "Iteration 129, loss = 0.08728388\n",
      "Iteration 130, loss = 0.08665142\n",
      "Iteration 131, loss = 0.08575375\n",
      "Iteration 132, loss = 0.08541362\n",
      "Iteration 133, loss = 0.08415828\n",
      "Iteration 134, loss = 0.08296947\n",
      "Iteration 135, loss = 0.08249650\n",
      "Iteration 136, loss = 0.08106192\n",
      "Iteration 137, loss = 0.08073706\n",
      "Iteration 138, loss = 0.07954578\n",
      "Iteration 139, loss = 0.07907819\n",
      "Iteration 140, loss = 0.07794373\n",
      "Iteration 141, loss = 0.07684916\n",
      "Iteration 142, loss = 0.07635764\n",
      "Iteration 143, loss = 0.07563639\n",
      "Iteration 144, loss = 0.07463840\n",
      "Iteration 145, loss = 0.07399203\n",
      "Iteration 146, loss = 0.07279912\n",
      "Iteration 147, loss = 0.07257920\n",
      "Iteration 148, loss = 0.07126854\n",
      "Iteration 149, loss = 0.07087961\n",
      "Iteration 150, loss = 0.06964107\n",
      "Iteration 151, loss = 0.06951104\n",
      "Iteration 152, loss = 0.06823607\n",
      "Iteration 153, loss = 0.06767617\n",
      "Iteration 154, loss = 0.06672280\n",
      "Iteration 155, loss = 0.06622125\n",
      "Iteration 156, loss = 0.06531862\n",
      "Iteration 157, loss = 0.06484910\n",
      "Iteration 158, loss = 0.06429345\n",
      "Iteration 159, loss = 0.06309464\n",
      "Iteration 160, loss = 0.06266881\n",
      "Iteration 161, loss = 0.06196689\n",
      "Iteration 162, loss = 0.06125029\n",
      "Iteration 163, loss = 0.06035011\n",
      "Iteration 164, loss = 0.05981035\n",
      "Iteration 165, loss = 0.05903437\n",
      "Iteration 166, loss = 0.05854965\n",
      "Iteration 167, loss = 0.05808908\n",
      "Iteration 168, loss = 0.05725068\n",
      "Iteration 169, loss = 0.05683533\n",
      "Iteration 170, loss = 0.05649539\n",
      "Iteration 171, loss = 0.05515384\n",
      "Iteration 172, loss = 0.05477337\n",
      "Iteration 173, loss = 0.05375723\n",
      "Iteration 174, loss = 0.05316024\n",
      "Iteration 175, loss = 0.05246565\n",
      "Iteration 176, loss = 0.05228293\n",
      "Iteration 177, loss = 0.05134149\n",
      "Iteration 178, loss = 0.05080468\n",
      "Iteration 179, loss = 0.05012356\n",
      "Iteration 180, loss = 0.04963608\n",
      "Iteration 181, loss = 0.04900272\n",
      "Iteration 182, loss = 0.04854326\n",
      "Iteration 183, loss = 0.04795328\n",
      "Iteration 184, loss = 0.04702953\n",
      "Iteration 185, loss = 0.04684833\n",
      "Iteration 186, loss = 0.04641794\n",
      "Iteration 187, loss = 0.04584967\n",
      "Iteration 188, loss = 0.04478240\n",
      "Iteration 189, loss = 0.04441940\n",
      "Iteration 190, loss = 0.04394259\n",
      "Iteration 191, loss = 0.04334058\n",
      "Iteration 192, loss = 0.04301630\n",
      "Iteration 193, loss = 0.04263042\n",
      "Iteration 194, loss = 0.04196473\n",
      "Iteration 195, loss = 0.04151096\n",
      "Iteration 196, loss = 0.04063460\n",
      "Iteration 197, loss = 0.04048081\n",
      "Iteration 198, loss = 0.03994209\n",
      "Iteration 199, loss = 0.03941559\n",
      "Iteration 200, loss = 0.03890171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67894448\n",
      "Iteration 2, loss = 0.53532958\n",
      "Iteration 3, loss = 0.44792547\n",
      "Iteration 4, loss = 0.38892182\n",
      "Iteration 5, loss = 0.34700180\n",
      "Iteration 6, loss = 0.31721224\n",
      "Iteration 7, loss = 0.29626722\n",
      "Iteration 8, loss = 0.28083092\n",
      "Iteration 9, loss = 0.26976174\n",
      "Iteration 10, loss = 0.26151261\n",
      "Iteration 11, loss = 0.25484240\n",
      "Iteration 12, loss = 0.24956643\n",
      "Iteration 13, loss = 0.24559034\n",
      "Iteration 14, loss = 0.24196410\n",
      "Iteration 15, loss = 0.23884019\n",
      "Iteration 16, loss = 0.23641547\n",
      "Iteration 17, loss = 0.23446217\n",
      "Iteration 18, loss = 0.23213130\n",
      "Iteration 19, loss = 0.23031198\n",
      "Iteration 20, loss = 0.22847182\n",
      "Iteration 21, loss = 0.22704066\n",
      "Iteration 22, loss = 0.22583203\n",
      "Iteration 23, loss = 0.22426598\n",
      "Iteration 24, loss = 0.22286318\n",
      "Iteration 25, loss = 0.22156565\n",
      "Iteration 26, loss = 0.22033301\n",
      "Iteration 27, loss = 0.21911225\n",
      "Iteration 28, loss = 0.21815091\n",
      "Iteration 29, loss = 0.21660283\n",
      "Iteration 30, loss = 0.21551220\n",
      "Iteration 31, loss = 0.21413305\n",
      "Iteration 32, loss = 0.21285250\n",
      "Iteration 33, loss = 0.21170516\n",
      "Iteration 34, loss = 0.21093214\n",
      "Iteration 35, loss = 0.20966651\n",
      "Iteration 36, loss = 0.20837783\n",
      "Iteration 37, loss = 0.20659657\n",
      "Iteration 38, loss = 0.20507906\n",
      "Iteration 39, loss = 0.20408027\n",
      "Iteration 40, loss = 0.20316906\n",
      "Iteration 41, loss = 0.20173217\n",
      "Iteration 42, loss = 0.20031182\n",
      "Iteration 43, loss = 0.19862226\n",
      "Iteration 44, loss = 0.19743670\n",
      "Iteration 45, loss = 0.19628688\n",
      "Iteration 46, loss = 0.19529239\n",
      "Iteration 47, loss = 0.19335734\n",
      "Iteration 48, loss = 0.19212917\n",
      "Iteration 49, loss = 0.19115343\n",
      "Iteration 50, loss = 0.18863068\n",
      "Iteration 51, loss = 0.18788148\n",
      "Iteration 52, loss = 0.18632811\n",
      "Iteration 53, loss = 0.18456413\n",
      "Iteration 54, loss = 0.18378386\n",
      "Iteration 55, loss = 0.18215420\n",
      "Iteration 56, loss = 0.17996404\n",
      "Iteration 57, loss = 0.17932764\n",
      "Iteration 58, loss = 0.17866948\n",
      "Iteration 59, loss = 0.17642955\n",
      "Iteration 60, loss = 0.17509764\n",
      "Iteration 61, loss = 0.17308070\n",
      "Iteration 62, loss = 0.17181517\n",
      "Iteration 63, loss = 0.17047169\n",
      "Iteration 64, loss = 0.16870099\n",
      "Iteration 65, loss = 0.16724116\n",
      "Iteration 66, loss = 0.16614538\n",
      "Iteration 67, loss = 0.16443073\n",
      "Iteration 68, loss = 0.16329644\n",
      "Iteration 69, loss = 0.16175020\n",
      "Iteration 70, loss = 0.16038673\n",
      "Iteration 71, loss = 0.15878582\n",
      "Iteration 72, loss = 0.15773815\n",
      "Iteration 73, loss = 0.15599359\n",
      "Iteration 74, loss = 0.15443299\n",
      "Iteration 75, loss = 0.15374482\n",
      "Iteration 76, loss = 0.15200701\n",
      "Iteration 77, loss = 0.15075492\n",
      "Iteration 78, loss = 0.14945626\n",
      "Iteration 79, loss = 0.14820810\n",
      "Iteration 80, loss = 0.14642074\n",
      "Iteration 81, loss = 0.14551040\n",
      "Iteration 82, loss = 0.14463239\n",
      "Iteration 83, loss = 0.14289117\n",
      "Iteration 84, loss = 0.14202309\n",
      "Iteration 85, loss = 0.14054200\n",
      "Iteration 86, loss = 0.13871030\n",
      "Iteration 87, loss = 0.13794297\n",
      "Iteration 88, loss = 0.13640200\n",
      "Iteration 89, loss = 0.13549069\n",
      "Iteration 90, loss = 0.13435825\n",
      "Iteration 91, loss = 0.13244274\n",
      "Iteration 92, loss = 0.13126146\n",
      "Iteration 93, loss = 0.13031470\n",
      "Iteration 94, loss = 0.12909291\n",
      "Iteration 95, loss = 0.12782191\n",
      "Iteration 96, loss = 0.12683306\n",
      "Iteration 97, loss = 0.12519411\n",
      "Iteration 98, loss = 0.12436358\n",
      "Iteration 99, loss = 0.12342160\n",
      "Iteration 100, loss = 0.12233386\n",
      "Iteration 101, loss = 0.12106114\n",
      "Iteration 102, loss = 0.11962309\n",
      "Iteration 103, loss = 0.11895802\n",
      "Iteration 104, loss = 0.11727067\n",
      "Iteration 105, loss = 0.11676296\n",
      "Iteration 106, loss = 0.11556260\n",
      "Iteration 107, loss = 0.11451937\n",
      "Iteration 108, loss = 0.11322105\n",
      "Iteration 109, loss = 0.11210835\n",
      "Iteration 110, loss = 0.11121303\n",
      "Iteration 111, loss = 0.10985187\n",
      "Iteration 112, loss = 0.10875719\n",
      "Iteration 113, loss = 0.10812541\n",
      "Iteration 114, loss = 0.10724013\n",
      "Iteration 115, loss = 0.10608366\n",
      "Iteration 116, loss = 0.10463726\n",
      "Iteration 117, loss = 0.10385348\n",
      "Iteration 118, loss = 0.10284047\n",
      "Iteration 119, loss = 0.10198111\n",
      "Iteration 120, loss = 0.10125484\n",
      "Iteration 121, loss = 0.09990793\n",
      "Iteration 122, loss = 0.09870838\n",
      "Iteration 123, loss = 0.09793570\n",
      "Iteration 124, loss = 0.09675032\n",
      "Iteration 125, loss = 0.09686490\n",
      "Iteration 126, loss = 0.09509342\n",
      "Iteration 127, loss = 0.09406963\n",
      "Iteration 128, loss = 0.09317118\n",
      "Iteration 129, loss = 0.09223352\n",
      "Iteration 130, loss = 0.09127254\n",
      "Iteration 131, loss = 0.09021647\n",
      "Iteration 132, loss = 0.08963106\n",
      "Iteration 133, loss = 0.08915024\n",
      "Iteration 134, loss = 0.08777814\n",
      "Iteration 135, loss = 0.08721683\n",
      "Iteration 136, loss = 0.08583578\n",
      "Iteration 137, loss = 0.08529811\n",
      "Iteration 138, loss = 0.08428422\n",
      "Iteration 139, loss = 0.08354293\n",
      "Iteration 140, loss = 0.08221582\n",
      "Iteration 141, loss = 0.08202076\n",
      "Iteration 142, loss = 0.08106836\n",
      "Iteration 143, loss = 0.08068342\n",
      "Iteration 144, loss = 0.07939988\n",
      "Iteration 145, loss = 0.07841968\n",
      "Iteration 146, loss = 0.07752114\n",
      "Iteration 147, loss = 0.07699284\n",
      "Iteration 148, loss = 0.07631889\n",
      "Iteration 149, loss = 0.07559939\n",
      "Iteration 150, loss = 0.07435594\n",
      "Iteration 151, loss = 0.07379533\n",
      "Iteration 152, loss = 0.07297797\n",
      "Iteration 153, loss = 0.07206687\n",
      "Iteration 154, loss = 0.07143030\n",
      "Iteration 155, loss = 0.07078818\n",
      "Iteration 156, loss = 0.06994503\n",
      "Iteration 157, loss = 0.06912067\n",
      "Iteration 158, loss = 0.06835553\n",
      "Iteration 159, loss = 0.06814671\n",
      "Iteration 160, loss = 0.06682196\n",
      "Iteration 161, loss = 0.06622124\n",
      "Iteration 162, loss = 0.06562515\n",
      "Iteration 163, loss = 0.06492736\n",
      "Iteration 164, loss = 0.06436559\n",
      "Iteration 165, loss = 0.06348228\n",
      "Iteration 166, loss = 0.06246994\n",
      "Iteration 167, loss = 0.06207735\n",
      "Iteration 168, loss = 0.06143920\n",
      "Iteration 169, loss = 0.06128724\n",
      "Iteration 170, loss = 0.06040570\n",
      "Iteration 171, loss = 0.05944974\n",
      "Iteration 172, loss = 0.05894719\n",
      "Iteration 173, loss = 0.05820792\n",
      "Iteration 174, loss = 0.05749494\n",
      "Iteration 175, loss = 0.05676899\n",
      "Iteration 176, loss = 0.05628058\n",
      "Iteration 177, loss = 0.05546137\n",
      "Iteration 178, loss = 0.05520733\n",
      "Iteration 179, loss = 0.05451664\n",
      "Iteration 180, loss = 0.05393058\n",
      "Iteration 181, loss = 0.05333400\n",
      "Iteration 182, loss = 0.05288777\n",
      "Iteration 183, loss = 0.05214009\n",
      "Iteration 184, loss = 0.05143804\n",
      "Iteration 185, loss = 0.05114253\n",
      "Iteration 186, loss = 0.05104475\n",
      "Iteration 187, loss = 0.05027120\n",
      "Iteration 188, loss = 0.04893817\n",
      "Iteration 189, loss = 0.04865461\n",
      "Iteration 190, loss = 0.04794427\n",
      "Iteration 191, loss = 0.04785636\n",
      "Iteration 192, loss = 0.04718255\n",
      "Iteration 193, loss = 0.04678287\n",
      "Iteration 194, loss = 0.04635807\n",
      "Iteration 195, loss = 0.04560382\n",
      "Iteration 196, loss = 0.04471136\n",
      "Iteration 197, loss = 0.04466676\n",
      "Iteration 198, loss = 0.04385336\n",
      "Iteration 199, loss = 0.04341022\n",
      "Iteration 200, loss = 0.04295871\n",
      "Iteration 1, loss = 0.67862104\n",
      "Iteration 2, loss = 0.53472209\n",
      "Iteration 3, loss = 0.44769817\n",
      "Iteration 4, loss = 0.38983499\n",
      "Iteration 5, loss = 0.34843763\n",
      "Iteration 6, loss = 0.31913001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.29845484\n",
      "Iteration 8, loss = 0.28390955\n",
      "Iteration 9, loss = 0.27236935\n",
      "Iteration 10, loss = 0.26428791\n",
      "Iteration 11, loss = 0.25817512\n",
      "Iteration 12, loss = 0.25291902\n",
      "Iteration 13, loss = 0.24947717\n",
      "Iteration 14, loss = 0.24594272\n",
      "Iteration 15, loss = 0.24285228\n",
      "Iteration 16, loss = 0.24040914\n",
      "Iteration 17, loss = 0.23890371\n",
      "Iteration 18, loss = 0.23653917\n",
      "Iteration 19, loss = 0.23451458\n",
      "Iteration 20, loss = 0.23279043\n",
      "Iteration 21, loss = 0.23161503\n",
      "Iteration 22, loss = 0.23068591\n",
      "Iteration 23, loss = 0.22884572\n",
      "Iteration 24, loss = 0.22700570\n",
      "Iteration 25, loss = 0.22583812\n",
      "Iteration 26, loss = 0.22477697\n",
      "Iteration 27, loss = 0.22322679\n",
      "Iteration 28, loss = 0.22233032\n",
      "Iteration 29, loss = 0.22058559\n",
      "Iteration 30, loss = 0.21920054\n",
      "Iteration 31, loss = 0.21802727\n",
      "Iteration 32, loss = 0.21725975\n",
      "Iteration 33, loss = 0.21573771\n",
      "Iteration 34, loss = 0.21463396\n",
      "Iteration 35, loss = 0.21312713\n",
      "Iteration 36, loss = 0.21227720\n",
      "Iteration 37, loss = 0.21074899\n",
      "Iteration 38, loss = 0.20927013\n",
      "Iteration 39, loss = 0.20768527\n",
      "Iteration 40, loss = 0.20679692\n",
      "Iteration 41, loss = 0.20553175\n",
      "Iteration 42, loss = 0.20402740\n",
      "Iteration 43, loss = 0.20255760\n",
      "Iteration 44, loss = 0.20118942\n",
      "Iteration 45, loss = 0.19974075\n",
      "Iteration 46, loss = 0.19844735\n",
      "Iteration 47, loss = 0.19661429\n",
      "Iteration 48, loss = 0.19542479\n",
      "Iteration 49, loss = 0.19408072\n",
      "Iteration 50, loss = 0.19266922\n",
      "Iteration 51, loss = 0.19139666\n",
      "Iteration 52, loss = 0.18985368\n",
      "Iteration 53, loss = 0.18805016\n",
      "Iteration 54, loss = 0.18729723\n",
      "Iteration 55, loss = 0.18505645\n",
      "Iteration 56, loss = 0.18330418\n",
      "Iteration 57, loss = 0.18275551\n",
      "Iteration 58, loss = 0.18080273\n",
      "Iteration 59, loss = 0.17940474\n",
      "Iteration 60, loss = 0.17793910\n",
      "Iteration 61, loss = 0.17637302\n",
      "Iteration 62, loss = 0.17465273\n",
      "Iteration 63, loss = 0.17336280\n",
      "Iteration 64, loss = 0.17174482\n",
      "Iteration 65, loss = 0.17041998\n",
      "Iteration 66, loss = 0.16915096\n",
      "Iteration 67, loss = 0.16761130\n",
      "Iteration 68, loss = 0.16654921\n",
      "Iteration 69, loss = 0.16475885\n",
      "Iteration 70, loss = 0.16311781\n",
      "Iteration 71, loss = 0.16190064\n",
      "Iteration 72, loss = 0.16027585\n",
      "Iteration 73, loss = 0.15879940\n",
      "Iteration 74, loss = 0.15726120\n",
      "Iteration 75, loss = 0.15599216\n",
      "Iteration 76, loss = 0.15429708\n",
      "Iteration 77, loss = 0.15292070\n",
      "Iteration 78, loss = 0.15174636\n",
      "Iteration 79, loss = 0.14981786\n",
      "Iteration 80, loss = 0.14847582\n",
      "Iteration 81, loss = 0.14712216\n",
      "Iteration 82, loss = 0.14617342\n",
      "Iteration 83, loss = 0.14450989\n",
      "Iteration 84, loss = 0.14310025\n",
      "Iteration 85, loss = 0.14181706\n",
      "Iteration 86, loss = 0.14003232\n",
      "Iteration 87, loss = 0.13893089\n",
      "Iteration 88, loss = 0.13832971\n",
      "Iteration 89, loss = 0.13733546\n",
      "Iteration 90, loss = 0.13546560\n",
      "Iteration 91, loss = 0.13363243\n",
      "Iteration 92, loss = 0.13245638\n",
      "Iteration 93, loss = 0.13113192\n",
      "Iteration 94, loss = 0.12948276\n",
      "Iteration 95, loss = 0.12859732\n",
      "Iteration 96, loss = 0.12711674\n",
      "Iteration 97, loss = 0.12592877\n",
      "Iteration 98, loss = 0.12471627\n",
      "Iteration 99, loss = 0.12368765\n",
      "Iteration 100, loss = 0.12236491\n",
      "Iteration 101, loss = 0.12154921\n",
      "Iteration 102, loss = 0.11978787\n",
      "Iteration 103, loss = 0.11879899\n",
      "Iteration 104, loss = 0.11756339\n",
      "Iteration 105, loss = 0.11613018\n",
      "Iteration 106, loss = 0.11530163\n",
      "Iteration 107, loss = 0.11449330\n",
      "Iteration 108, loss = 0.11276510\n",
      "Iteration 109, loss = 0.11188351\n",
      "Iteration 110, loss = 0.11054937\n",
      "Iteration 111, loss = 0.10967047\n",
      "Iteration 112, loss = 0.10889141\n",
      "Iteration 113, loss = 0.10729318\n",
      "Iteration 114, loss = 0.10678386\n",
      "Iteration 115, loss = 0.10491359\n",
      "Iteration 116, loss = 0.10362146\n",
      "Iteration 117, loss = 0.10274808\n",
      "Iteration 118, loss = 0.10160430\n",
      "Iteration 119, loss = 0.10083682\n",
      "Iteration 120, loss = 0.09989697\n",
      "Iteration 121, loss = 0.09879450\n",
      "Iteration 122, loss = 0.09779192\n",
      "Iteration 123, loss = 0.09668330\n",
      "Iteration 124, loss = 0.09563706\n",
      "Iteration 125, loss = 0.09569410\n",
      "Iteration 126, loss = 0.09379344\n",
      "Iteration 127, loss = 0.09283407\n",
      "Iteration 128, loss = 0.09186972\n",
      "Iteration 129, loss = 0.09064776\n",
      "Iteration 130, loss = 0.08959753\n",
      "Iteration 131, loss = 0.08887150\n",
      "Iteration 132, loss = 0.08769581\n",
      "Iteration 133, loss = 0.08719657\n",
      "Iteration 134, loss = 0.08638502\n",
      "Iteration 135, loss = 0.08516111\n",
      "Iteration 136, loss = 0.08426922\n",
      "Iteration 137, loss = 0.08344164\n",
      "Iteration 138, loss = 0.08318113\n",
      "Iteration 139, loss = 0.08170772\n",
      "Iteration 140, loss = 0.08071850\n",
      "Iteration 141, loss = 0.08022275\n",
      "Iteration 142, loss = 0.07936088\n",
      "Iteration 143, loss = 0.07882534\n",
      "Iteration 144, loss = 0.07752383\n",
      "Iteration 145, loss = 0.07710032\n",
      "Iteration 146, loss = 0.07570496\n",
      "Iteration 147, loss = 0.07483096\n",
      "Iteration 148, loss = 0.07418031\n",
      "Iteration 149, loss = 0.07376558\n",
      "Iteration 150, loss = 0.07296324\n",
      "Iteration 151, loss = 0.07178130\n",
      "Iteration 152, loss = 0.07125984\n",
      "Iteration 153, loss = 0.07013941\n",
      "Iteration 154, loss = 0.06943564\n",
      "Iteration 155, loss = 0.06939095\n",
      "Iteration 156, loss = 0.06855156\n",
      "Iteration 157, loss = 0.06715438\n",
      "Iteration 158, loss = 0.06699379\n",
      "Iteration 159, loss = 0.06653246\n",
      "Iteration 160, loss = 0.06509656\n",
      "Iteration 161, loss = 0.06443635\n",
      "Iteration 162, loss = 0.06373620\n",
      "Iteration 163, loss = 0.06326265\n",
      "Iteration 164, loss = 0.06256656\n",
      "Iteration 165, loss = 0.06154004\n",
      "Iteration 166, loss = 0.06085444\n",
      "Iteration 167, loss = 0.06012850\n",
      "Iteration 168, loss = 0.05944435\n",
      "Iteration 169, loss = 0.05895927\n",
      "Iteration 170, loss = 0.05804612\n",
      "Iteration 171, loss = 0.05750608\n",
      "Iteration 172, loss = 0.05696435\n",
      "Iteration 173, loss = 0.05665373\n",
      "Iteration 174, loss = 0.05553490\n",
      "Iteration 175, loss = 0.05496671\n",
      "Iteration 176, loss = 0.05440415\n",
      "Iteration 177, loss = 0.05367380\n",
      "Iteration 178, loss = 0.05352184\n",
      "Iteration 179, loss = 0.05248350\n",
      "Iteration 180, loss = 0.05199589\n",
      "Iteration 181, loss = 0.05150953\n",
      "Iteration 182, loss = 0.05076198\n",
      "Iteration 183, loss = 0.05023364\n",
      "Iteration 184, loss = 0.04993243\n",
      "Iteration 185, loss = 0.04926904\n",
      "Iteration 186, loss = 0.04891139\n",
      "Iteration 187, loss = 0.04826866\n",
      "Iteration 188, loss = 0.04735795\n",
      "Iteration 189, loss = 0.04679216\n",
      "Iteration 190, loss = 0.04617532\n",
      "Iteration 191, loss = 0.04582340\n",
      "Iteration 192, loss = 0.04536191\n",
      "Iteration 193, loss = 0.04465312\n",
      "Iteration 194, loss = 0.04438260\n",
      "Iteration 195, loss = 0.04366132\n",
      "Iteration 196, loss = 0.04317043\n",
      "Iteration 197, loss = 0.04265954\n",
      "Iteration 198, loss = 0.04197334\n",
      "Iteration 199, loss = 0.04160303\n",
      "Iteration 200, loss = 0.04120251\n",
      "Iteration 1, loss = 0.68192961\n",
      "Iteration 2, loss = 0.53865094\n",
      "Iteration 3, loss = 0.45181573\n",
      "Iteration 4, loss = 0.39188090\n",
      "Iteration 5, loss = 0.35048724\n",
      "Iteration 6, loss = 0.32024749\n",
      "Iteration 7, loss = 0.29884730\n",
      "Iteration 8, loss = 0.28330087\n",
      "Iteration 9, loss = 0.27243105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.26377304\n",
      "Iteration 11, loss = 0.25792210\n",
      "Iteration 12, loss = 0.25201060\n",
      "Iteration 13, loss = 0.24764647\n",
      "Iteration 14, loss = 0.24406972\n",
      "Iteration 15, loss = 0.24069360\n",
      "Iteration 16, loss = 0.23790375\n",
      "Iteration 17, loss = 0.23569326\n",
      "Iteration 18, loss = 0.23351999\n",
      "Iteration 19, loss = 0.23170712\n",
      "Iteration 20, loss = 0.22998032\n",
      "Iteration 21, loss = 0.22893436\n",
      "Iteration 22, loss = 0.22661897\n",
      "Iteration 23, loss = 0.22543519\n",
      "Iteration 24, loss = 0.22384627\n",
      "Iteration 25, loss = 0.22294674\n",
      "Iteration 26, loss = 0.22119569\n",
      "Iteration 27, loss = 0.22065811\n",
      "Iteration 28, loss = 0.21915496\n",
      "Iteration 29, loss = 0.21792941\n",
      "Iteration 30, loss = 0.21642980\n",
      "Iteration 31, loss = 0.21531926\n",
      "Iteration 32, loss = 0.21400483\n",
      "Iteration 33, loss = 0.21252567\n",
      "Iteration 34, loss = 0.21168123\n",
      "Iteration 35, loss = 0.21022270\n",
      "Iteration 36, loss = 0.20854987\n",
      "Iteration 37, loss = 0.20751904\n",
      "Iteration 38, loss = 0.20608717\n",
      "Iteration 39, loss = 0.20491826\n",
      "Iteration 40, loss = 0.20398731\n",
      "Iteration 41, loss = 0.20210521\n",
      "Iteration 42, loss = 0.20103284\n",
      "Iteration 43, loss = 0.19986103\n",
      "Iteration 44, loss = 0.19869328\n",
      "Iteration 45, loss = 0.19637826\n",
      "Iteration 46, loss = 0.19557052\n",
      "Iteration 47, loss = 0.19371964\n",
      "Iteration 48, loss = 0.19228537\n",
      "Iteration 49, loss = 0.19103575\n",
      "Iteration 50, loss = 0.19006914\n",
      "Iteration 51, loss = 0.18834827\n",
      "Iteration 52, loss = 0.18662667\n",
      "Iteration 53, loss = 0.18527775\n",
      "Iteration 54, loss = 0.18402456\n",
      "Iteration 55, loss = 0.18160272\n",
      "Iteration 56, loss = 0.18057417\n",
      "Iteration 57, loss = 0.17905254\n",
      "Iteration 58, loss = 0.17763853\n",
      "Iteration 59, loss = 0.17622401\n",
      "Iteration 60, loss = 0.17444706\n",
      "Iteration 61, loss = 0.17370753\n",
      "Iteration 62, loss = 0.17204565\n",
      "Iteration 63, loss = 0.17052351\n",
      "Iteration 64, loss = 0.16910455\n",
      "Iteration 65, loss = 0.16793947\n",
      "Iteration 66, loss = 0.16541762\n",
      "Iteration 67, loss = 0.16427136\n",
      "Iteration 68, loss = 0.16273604\n",
      "Iteration 69, loss = 0.16132985\n",
      "Iteration 70, loss = 0.15979867\n",
      "Iteration 71, loss = 0.15878436\n",
      "Iteration 72, loss = 0.15740549\n",
      "Iteration 73, loss = 0.15571802\n",
      "Iteration 74, loss = 0.15384016\n",
      "Iteration 75, loss = 0.15331912\n",
      "Iteration 76, loss = 0.15196491\n",
      "Iteration 77, loss = 0.15014969\n",
      "Iteration 78, loss = 0.14907905\n",
      "Iteration 79, loss = 0.14774008\n",
      "Iteration 80, loss = 0.14601719\n",
      "Iteration 81, loss = 0.14526708\n",
      "Iteration 82, loss = 0.14479709\n",
      "Iteration 83, loss = 0.14266258\n",
      "Iteration 84, loss = 0.14129517\n",
      "Iteration 85, loss = 0.13951827\n",
      "Iteration 86, loss = 0.13888556\n",
      "Iteration 87, loss = 0.13709207\n",
      "Iteration 88, loss = 0.13568973\n",
      "Iteration 89, loss = 0.13474353\n",
      "Iteration 90, loss = 0.13402952\n",
      "Iteration 91, loss = 0.13189786\n",
      "Iteration 92, loss = 0.13108726\n",
      "Iteration 93, loss = 0.12963222\n",
      "Iteration 94, loss = 0.12829684\n",
      "Iteration 95, loss = 0.12732068\n",
      "Iteration 96, loss = 0.12683932\n",
      "Iteration 97, loss = 0.12518469\n",
      "Iteration 98, loss = 0.12366067\n",
      "Iteration 99, loss = 0.12287993\n",
      "Iteration 100, loss = 0.12179633\n",
      "Iteration 101, loss = 0.12090980\n",
      "Iteration 102, loss = 0.11965766\n",
      "Iteration 103, loss = 0.11865260\n",
      "Iteration 104, loss = 0.11719138\n",
      "Iteration 105, loss = 0.11619380\n",
      "Iteration 106, loss = 0.11481151\n",
      "Iteration 107, loss = 0.11404281\n",
      "Iteration 108, loss = 0.11278891\n",
      "Iteration 109, loss = 0.11222547\n",
      "Iteration 110, loss = 0.11068647\n",
      "Iteration 111, loss = 0.11003468\n",
      "Iteration 112, loss = 0.10885360\n",
      "Iteration 113, loss = 0.10773763\n",
      "Iteration 114, loss = 0.10736369\n",
      "Iteration 115, loss = 0.10563096\n",
      "Iteration 116, loss = 0.10469756\n",
      "Iteration 117, loss = 0.10327007\n",
      "Iteration 118, loss = 0.10246894\n",
      "Iteration 119, loss = 0.10188732\n",
      "Iteration 120, loss = 0.10090546\n",
      "Iteration 121, loss = 0.09972986\n",
      "Iteration 122, loss = 0.09888351\n",
      "Iteration 123, loss = 0.09772181\n",
      "Iteration 124, loss = 0.09717335\n",
      "Iteration 125, loss = 0.09629585\n",
      "Iteration 126, loss = 0.09516971\n",
      "Iteration 127, loss = 0.09453496\n",
      "Iteration 128, loss = 0.09352077\n",
      "Iteration 129, loss = 0.09249245\n",
      "Iteration 130, loss = 0.09234600\n",
      "Iteration 131, loss = 0.09064546\n",
      "Iteration 132, loss = 0.09001682\n",
      "Iteration 133, loss = 0.08853136\n",
      "Iteration 134, loss = 0.08836177\n",
      "Iteration 135, loss = 0.08671229\n",
      "Iteration 136, loss = 0.08598513\n",
      "Iteration 137, loss = 0.08563862\n",
      "Iteration 138, loss = 0.08469110\n",
      "Iteration 139, loss = 0.08332403\n",
      "Iteration 140, loss = 0.08290850\n",
      "Iteration 141, loss = 0.08210826\n",
      "Iteration 142, loss = 0.08100395\n",
      "Iteration 143, loss = 0.08054927\n",
      "Iteration 144, loss = 0.07978843\n",
      "Iteration 145, loss = 0.07915754\n",
      "Iteration 146, loss = 0.07804289\n",
      "Iteration 147, loss = 0.07781838\n",
      "Iteration 148, loss = 0.07654628\n",
      "Iteration 149, loss = 0.07551152\n",
      "Iteration 150, loss = 0.07482456\n",
      "Iteration 151, loss = 0.07408280\n",
      "Iteration 152, loss = 0.07376589\n",
      "Iteration 153, loss = 0.07290290\n",
      "Iteration 154, loss = 0.07204172\n",
      "Iteration 155, loss = 0.07119484\n",
      "Iteration 156, loss = 0.07020711\n",
      "Iteration 157, loss = 0.06987728\n",
      "Iteration 158, loss = 0.06879228\n",
      "Iteration 159, loss = 0.06827740\n",
      "Iteration 160, loss = 0.06737243\n",
      "Iteration 161, loss = 0.06681581\n",
      "Iteration 162, loss = 0.06645089\n",
      "Iteration 163, loss = 0.06526716\n",
      "Iteration 164, loss = 0.06474866\n",
      "Iteration 165, loss = 0.06424369\n",
      "Iteration 166, loss = 0.06339956\n",
      "Iteration 167, loss = 0.06268592\n",
      "Iteration 168, loss = 0.06219919\n",
      "Iteration 169, loss = 0.06131262\n",
      "Iteration 170, loss = 0.06075921\n",
      "Iteration 171, loss = 0.05983795\n",
      "Iteration 172, loss = 0.05940373\n",
      "Iteration 173, loss = 0.05849893\n",
      "Iteration 174, loss = 0.05808967\n",
      "Iteration 175, loss = 0.05778317\n",
      "Iteration 176, loss = 0.05720926\n",
      "Iteration 177, loss = 0.05632667\n",
      "Iteration 178, loss = 0.05593599\n",
      "Iteration 179, loss = 0.05497611\n",
      "Iteration 180, loss = 0.05437733\n",
      "Iteration 181, loss = 0.05421357\n",
      "Iteration 182, loss = 0.05307762\n",
      "Iteration 183, loss = 0.05290453\n",
      "Iteration 184, loss = 0.05222943\n",
      "Iteration 185, loss = 0.05139913\n",
      "Iteration 186, loss = 0.05083787\n",
      "Iteration 187, loss = 0.05027402\n",
      "Iteration 188, loss = 0.04950949\n",
      "Iteration 189, loss = 0.04900829\n",
      "Iteration 190, loss = 0.04873242\n",
      "Iteration 191, loss = 0.04793772\n",
      "Iteration 192, loss = 0.04734998\n",
      "Iteration 193, loss = 0.04676328\n",
      "Iteration 194, loss = 0.04666046\n",
      "Iteration 195, loss = 0.04606215\n",
      "Iteration 196, loss = 0.04547104\n",
      "Iteration 197, loss = 0.04462709\n",
      "Iteration 198, loss = 0.04425067\n",
      "Iteration 199, loss = 0.04352084\n",
      "Iteration 200, loss = 0.04345022\n",
      "Iteration 1, loss = 0.68078591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53409506\n",
      "Iteration 3, loss = 0.44775951\n",
      "Iteration 4, loss = 0.38920063\n",
      "Iteration 5, loss = 0.34895555\n",
      "Iteration 6, loss = 0.31948677\n",
      "Iteration 7, loss = 0.29931907\n",
      "Iteration 8, loss = 0.28410767\n",
      "Iteration 9, loss = 0.27403866\n",
      "Iteration 10, loss = 0.26626821\n",
      "Iteration 11, loss = 0.26073036\n",
      "Iteration 12, loss = 0.25530199\n",
      "Iteration 13, loss = 0.25142214\n",
      "Iteration 14, loss = 0.24761489\n",
      "Iteration 15, loss = 0.24471787\n",
      "Iteration 16, loss = 0.24234024\n",
      "Iteration 17, loss = 0.24015049\n",
      "Iteration 18, loss = 0.23835042\n",
      "Iteration 19, loss = 0.23631027\n",
      "Iteration 20, loss = 0.23475289\n",
      "Iteration 21, loss = 0.23361428\n",
      "Iteration 22, loss = 0.23203131\n",
      "Iteration 23, loss = 0.23091515\n",
      "Iteration 24, loss = 0.22921518\n",
      "Iteration 25, loss = 0.22808629\n",
      "Iteration 26, loss = 0.22681784\n",
      "Iteration 27, loss = 0.22604957\n",
      "Iteration 28, loss = 0.22427414\n",
      "Iteration 29, loss = 0.22399774\n",
      "Iteration 30, loss = 0.22256923\n",
      "Iteration 31, loss = 0.22085117\n",
      "Iteration 32, loss = 0.21957090\n",
      "Iteration 33, loss = 0.21802589\n",
      "Iteration 34, loss = 0.21739909\n",
      "Iteration 35, loss = 0.21603020\n",
      "Iteration 36, loss = 0.21432326\n",
      "Iteration 37, loss = 0.21361549\n",
      "Iteration 38, loss = 0.21161603\n",
      "Iteration 39, loss = 0.21051707\n",
      "Iteration 40, loss = 0.20917433\n",
      "Iteration 41, loss = 0.20740076\n",
      "Iteration 42, loss = 0.20663598\n",
      "Iteration 43, loss = 0.20525619\n",
      "Iteration 44, loss = 0.20405085\n",
      "Iteration 45, loss = 0.20212525\n",
      "Iteration 46, loss = 0.20071741\n",
      "Iteration 47, loss = 0.19921326\n",
      "Iteration 48, loss = 0.19761288\n",
      "Iteration 49, loss = 0.19638219\n",
      "Iteration 50, loss = 0.19537143\n",
      "Iteration 51, loss = 0.19422846\n",
      "Iteration 52, loss = 0.19199383\n",
      "Iteration 53, loss = 0.19047117\n",
      "Iteration 54, loss = 0.18899782\n",
      "Iteration 55, loss = 0.18701899\n",
      "Iteration 56, loss = 0.18546756\n",
      "Iteration 57, loss = 0.18430289\n",
      "Iteration 58, loss = 0.18276140\n",
      "Iteration 59, loss = 0.18124165\n",
      "Iteration 60, loss = 0.17933989\n",
      "Iteration 61, loss = 0.17835272\n",
      "Iteration 62, loss = 0.17654438\n",
      "Iteration 63, loss = 0.17478748\n",
      "Iteration 64, loss = 0.17325904\n",
      "Iteration 65, loss = 0.17229113\n",
      "Iteration 66, loss = 0.17021585\n",
      "Iteration 67, loss = 0.16864579\n",
      "Iteration 68, loss = 0.16673597\n",
      "Iteration 69, loss = 0.16572162\n",
      "Iteration 70, loss = 0.16439150\n",
      "Iteration 71, loss = 0.16309977\n",
      "Iteration 72, loss = 0.16077369\n",
      "Iteration 73, loss = 0.15951960\n",
      "Iteration 74, loss = 0.15818154\n",
      "Iteration 75, loss = 0.15653857\n",
      "Iteration 76, loss = 0.15509102\n",
      "Iteration 77, loss = 0.15372426\n",
      "Iteration 78, loss = 0.15262947\n",
      "Iteration 79, loss = 0.15040948\n",
      "Iteration 80, loss = 0.14976889\n",
      "Iteration 81, loss = 0.14835348\n",
      "Iteration 82, loss = 0.14703111\n",
      "Iteration 83, loss = 0.14494479\n",
      "Iteration 84, loss = 0.14378906\n",
      "Iteration 85, loss = 0.14174151\n",
      "Iteration 86, loss = 0.14139050\n",
      "Iteration 87, loss = 0.13961484\n",
      "Iteration 88, loss = 0.13820099\n",
      "Iteration 89, loss = 0.13636609\n",
      "Iteration 90, loss = 0.13566648\n",
      "Iteration 91, loss = 0.13404145\n",
      "Iteration 92, loss = 0.13243051\n",
      "Iteration 93, loss = 0.13121046\n",
      "Iteration 94, loss = 0.13012708\n",
      "Iteration 95, loss = 0.12957116\n",
      "Iteration 96, loss = 0.12851029\n",
      "Iteration 97, loss = 0.12615836\n",
      "Iteration 98, loss = 0.12499761\n",
      "Iteration 99, loss = 0.12397671\n",
      "Iteration 100, loss = 0.12287767\n",
      "Iteration 101, loss = 0.12241548\n",
      "Iteration 102, loss = 0.12085585\n",
      "Iteration 103, loss = 0.11911895\n",
      "Iteration 104, loss = 0.11790021\n",
      "Iteration 105, loss = 0.11713049\n",
      "Iteration 106, loss = 0.11542831\n",
      "Iteration 107, loss = 0.11426253\n",
      "Iteration 108, loss = 0.11331787\n",
      "Iteration 109, loss = 0.11247923\n",
      "Iteration 110, loss = 0.11136593\n",
      "Iteration 111, loss = 0.10980746\n",
      "Iteration 112, loss = 0.10885727\n",
      "Iteration 113, loss = 0.10774131\n",
      "Iteration 114, loss = 0.10696749\n",
      "Iteration 115, loss = 0.10580559\n",
      "Iteration 116, loss = 0.10468186\n",
      "Iteration 117, loss = 0.10333423\n",
      "Iteration 118, loss = 0.10217094\n",
      "Iteration 119, loss = 0.10152200\n",
      "Iteration 120, loss = 0.10049416\n",
      "Iteration 121, loss = 0.09971847\n",
      "Iteration 122, loss = 0.09870235\n",
      "Iteration 123, loss = 0.09758315\n",
      "Iteration 124, loss = 0.09655522\n",
      "Iteration 125, loss = 0.09555859\n",
      "Iteration 126, loss = 0.09507897\n",
      "Iteration 127, loss = 0.09385457\n",
      "Iteration 128, loss = 0.09285199\n",
      "Iteration 129, loss = 0.09204158\n",
      "Iteration 130, loss = 0.09142663\n",
      "Iteration 131, loss = 0.09000816\n",
      "Iteration 132, loss = 0.08919340\n",
      "Iteration 133, loss = 0.08797382\n",
      "Iteration 134, loss = 0.08750278\n",
      "Iteration 135, loss = 0.08610668\n",
      "Iteration 136, loss = 0.08552715\n",
      "Iteration 137, loss = 0.08477803\n",
      "Iteration 138, loss = 0.08408153\n",
      "Iteration 139, loss = 0.08279076\n",
      "Iteration 140, loss = 0.08248266\n",
      "Iteration 141, loss = 0.08117391\n",
      "Iteration 142, loss = 0.08053616\n",
      "Iteration 143, loss = 0.07955833\n",
      "Iteration 144, loss = 0.07882736\n",
      "Iteration 145, loss = 0.07827685\n",
      "Iteration 146, loss = 0.07743090\n",
      "Iteration 147, loss = 0.07695858\n",
      "Iteration 148, loss = 0.07593168\n",
      "Iteration 149, loss = 0.07485664\n",
      "Iteration 150, loss = 0.07464071\n",
      "Iteration 151, loss = 0.07351412\n",
      "Iteration 152, loss = 0.07308350\n",
      "Iteration 153, loss = 0.07162729\n",
      "Iteration 154, loss = 0.07121265\n",
      "Iteration 155, loss = 0.07041705\n",
      "Iteration 156, loss = 0.06972050\n",
      "Iteration 157, loss = 0.06909035\n",
      "Iteration 158, loss = 0.06822289\n",
      "Iteration 159, loss = 0.06751378\n",
      "Iteration 160, loss = 0.06653011\n",
      "Iteration 161, loss = 0.06610325\n",
      "Iteration 162, loss = 0.06544305\n",
      "Iteration 163, loss = 0.06438498\n",
      "Iteration 164, loss = 0.06395221\n",
      "Iteration 165, loss = 0.06340721\n",
      "Iteration 166, loss = 0.06263760\n",
      "Iteration 167, loss = 0.06196473\n",
      "Iteration 168, loss = 0.06219051\n",
      "Iteration 169, loss = 0.06048039\n",
      "Iteration 170, loss = 0.05991816\n",
      "Iteration 171, loss = 0.05932102\n",
      "Iteration 172, loss = 0.05885129\n",
      "Iteration 173, loss = 0.05820954\n",
      "Iteration 174, loss = 0.05733298\n",
      "Iteration 175, loss = 0.05694662\n",
      "Iteration 176, loss = 0.05658556\n",
      "Iteration 177, loss = 0.05572848\n",
      "Iteration 178, loss = 0.05497286\n",
      "Iteration 179, loss = 0.05474894\n",
      "Iteration 180, loss = 0.05383973\n",
      "Iteration 181, loss = 0.05324224\n",
      "Iteration 182, loss = 0.05255048\n",
      "Iteration 183, loss = 0.05212416\n",
      "Iteration 184, loss = 0.05122060\n",
      "Iteration 185, loss = 0.05080458\n",
      "Iteration 186, loss = 0.05013444\n",
      "Iteration 187, loss = 0.05004396\n",
      "Iteration 188, loss = 0.04895952\n",
      "Iteration 189, loss = 0.04840677\n",
      "Iteration 190, loss = 0.04820334\n",
      "Iteration 191, loss = 0.04774758\n",
      "Iteration 192, loss = 0.04691222\n",
      "Iteration 193, loss = 0.04642722\n",
      "Iteration 194, loss = 0.04612720\n",
      "Iteration 195, loss = 0.04550880\n",
      "Iteration 196, loss = 0.04480277\n",
      "Iteration 197, loss = 0.04429078\n",
      "Iteration 198, loss = 0.04390954\n",
      "Iteration 199, loss = 0.04323919\n",
      "Iteration 200, loss = 0.04265237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75836684\n",
      "Iteration 2, loss = 0.69663936\n",
      "Iteration 3, loss = 0.63348707\n",
      "Iteration 4, loss = 0.58050372\n",
      "Iteration 5, loss = 0.53781968\n",
      "Iteration 6, loss = 0.50241508\n",
      "Iteration 7, loss = 0.47362173\n",
      "Iteration 8, loss = 0.44951623\n",
      "Iteration 9, loss = 0.42929321\n",
      "Iteration 10, loss = 0.41221097\n",
      "Iteration 11, loss = 0.39761203\n",
      "Iteration 12, loss = 0.38484070\n",
      "Iteration 13, loss = 0.37403819\n",
      "Iteration 14, loss = 0.36438332\n",
      "Iteration 15, loss = 0.35592398\n",
      "Iteration 16, loss = 0.34849725\n",
      "Iteration 17, loss = 0.34177531\n",
      "Iteration 18, loss = 0.33583815\n",
      "Iteration 19, loss = 0.33038866\n",
      "Iteration 20, loss = 0.32551205\n",
      "Iteration 21, loss = 0.32116187\n",
      "Iteration 22, loss = 0.31711695\n",
      "Iteration 23, loss = 0.31340113\n",
      "Iteration 24, loss = 0.31002918\n",
      "Iteration 25, loss = 0.30691749\n",
      "Iteration 26, loss = 0.30400165\n",
      "Iteration 27, loss = 0.30136945\n",
      "Iteration 28, loss = 0.29888933\n",
      "Iteration 29, loss = 0.29657350\n",
      "Iteration 30, loss = 0.29442347\n",
      "Iteration 31, loss = 0.29240886\n",
      "Iteration 32, loss = 0.29053419\n",
      "Iteration 33, loss = 0.28876725\n",
      "Iteration 34, loss = 0.28712494\n",
      "Iteration 35, loss = 0.28555328\n",
      "Iteration 36, loss = 0.28408354\n",
      "Iteration 37, loss = 0.28264948\n",
      "Iteration 38, loss = 0.28135611\n",
      "Iteration 39, loss = 0.28009537\n",
      "Iteration 40, loss = 0.27889937\n",
      "Iteration 41, loss = 0.27778676\n",
      "Iteration 42, loss = 0.27670527\n",
      "Iteration 43, loss = 0.27566066\n",
      "Iteration 44, loss = 0.27469095\n",
      "Iteration 45, loss = 0.27376334\n",
      "Iteration 46, loss = 0.27286025\n",
      "Iteration 47, loss = 0.27203274\n",
      "Iteration 48, loss = 0.27117962\n",
      "Iteration 49, loss = 0.27039591\n",
      "Iteration 50, loss = 0.26962318\n",
      "Iteration 51, loss = 0.26889992\n",
      "Iteration 52, loss = 0.26820973\n",
      "Iteration 53, loss = 0.26751231\n",
      "Iteration 54, loss = 0.26687192\n",
      "Iteration 55, loss = 0.26622854\n",
      "Iteration 56, loss = 0.26562834\n",
      "Iteration 57, loss = 0.26506179\n",
      "Iteration 58, loss = 0.26447132\n",
      "Iteration 59, loss = 0.26390686\n",
      "Iteration 60, loss = 0.26339269\n",
      "Iteration 61, loss = 0.26286861\n",
      "Iteration 62, loss = 0.26236339\n",
      "Iteration 63, loss = 0.26187657\n",
      "Iteration 64, loss = 0.26140917\n",
      "Iteration 65, loss = 0.26094401\n",
      "Iteration 66, loss = 0.26051625\n",
      "Iteration 67, loss = 0.26004431\n",
      "Iteration 68, loss = 0.25961456\n",
      "Iteration 69, loss = 0.25923009\n",
      "Iteration 70, loss = 0.25879308\n",
      "Iteration 71, loss = 0.25840753\n",
      "Iteration 72, loss = 0.25802808\n",
      "Iteration 73, loss = 0.25764386\n",
      "Iteration 74, loss = 0.25727029\n",
      "Iteration 75, loss = 0.25691584\n",
      "Iteration 76, loss = 0.25657338\n",
      "Iteration 77, loss = 0.25624102\n",
      "Iteration 78, loss = 0.25588609\n",
      "Iteration 79, loss = 0.25554447\n",
      "Iteration 80, loss = 0.25522664\n",
      "Iteration 81, loss = 0.25490957\n",
      "Iteration 82, loss = 0.25461334\n",
      "Iteration 83, loss = 0.25426589\n",
      "Iteration 84, loss = 0.25397231\n",
      "Iteration 85, loss = 0.25366965\n",
      "Iteration 86, loss = 0.25338997\n",
      "Iteration 87, loss = 0.25310855\n",
      "Iteration 88, loss = 0.25281602\n",
      "Iteration 89, loss = 0.25253870\n",
      "Iteration 90, loss = 0.25227696\n",
      "Iteration 91, loss = 0.25199435\n",
      "Iteration 92, loss = 0.25175184\n",
      "Iteration 93, loss = 0.25146023\n",
      "Iteration 94, loss = 0.25121866\n",
      "Iteration 95, loss = 0.25096455\n",
      "Iteration 96, loss = 0.25070730\n",
      "Iteration 97, loss = 0.25046455\n",
      "Iteration 98, loss = 0.25021366\n",
      "Iteration 99, loss = 0.24996979\n",
      "Iteration 100, loss = 0.24977511\n",
      "Iteration 101, loss = 0.24952388\n",
      "Iteration 102, loss = 0.24926315\n",
      "Iteration 103, loss = 0.24905316\n",
      "Iteration 104, loss = 0.24883228\n",
      "Iteration 105, loss = 0.24861070\n",
      "Iteration 106, loss = 0.24838152\n",
      "Iteration 107, loss = 0.24816565\n",
      "Iteration 108, loss = 0.24794917\n",
      "Iteration 109, loss = 0.24774409\n",
      "Iteration 110, loss = 0.24754868\n",
      "Iteration 111, loss = 0.24732880\n",
      "Iteration 112, loss = 0.24711356\n",
      "Iteration 113, loss = 0.24692975\n",
      "Iteration 114, loss = 0.24671731\n",
      "Iteration 115, loss = 0.24653892\n",
      "Iteration 116, loss = 0.24634449\n",
      "Iteration 117, loss = 0.24615684\n",
      "Iteration 118, loss = 0.24592619\n",
      "Iteration 119, loss = 0.24575411\n",
      "Iteration 120, loss = 0.24557199\n",
      "Iteration 121, loss = 0.24537398\n",
      "Iteration 122, loss = 0.24520006\n",
      "Iteration 123, loss = 0.24500559\n",
      "Iteration 124, loss = 0.24481647\n",
      "Iteration 125, loss = 0.24464802\n",
      "Iteration 126, loss = 0.24447817\n",
      "Iteration 127, loss = 0.24430526\n",
      "Iteration 128, loss = 0.24412100\n",
      "Iteration 129, loss = 0.24396323\n",
      "Iteration 130, loss = 0.24377453\n",
      "Iteration 131, loss = 0.24361235\n",
      "Iteration 132, loss = 0.24345865\n",
      "Iteration 133, loss = 0.24328041\n",
      "Iteration 134, loss = 0.24311418\n",
      "Iteration 135, loss = 0.24295123\n",
      "Iteration 136, loss = 0.24282490\n",
      "Iteration 137, loss = 0.24263788\n",
      "Iteration 138, loss = 0.24246328\n",
      "Iteration 139, loss = 0.24231840\n",
      "Iteration 140, loss = 0.24215425\n",
      "Iteration 141, loss = 0.24198158\n",
      "Iteration 142, loss = 0.24184545\n",
      "Iteration 143, loss = 0.24169910\n",
      "Iteration 144, loss = 0.24154246\n",
      "Iteration 145, loss = 0.24138776\n",
      "Iteration 146, loss = 0.24123881\n",
      "Iteration 147, loss = 0.24110562\n",
      "Iteration 148, loss = 0.24096142\n",
      "Iteration 149, loss = 0.24079299\n",
      "Iteration 150, loss = 0.24064313\n",
      "Iteration 151, loss = 0.24051814\n",
      "Iteration 152, loss = 0.24038144\n",
      "Iteration 153, loss = 0.24023744\n",
      "Iteration 154, loss = 0.24008672\n",
      "Iteration 155, loss = 0.23996227\n",
      "Iteration 156, loss = 0.23980917\n",
      "Iteration 157, loss = 0.23966810\n",
      "Iteration 158, loss = 0.23952545\n",
      "Iteration 159, loss = 0.23940456\n",
      "Iteration 160, loss = 0.23929205\n",
      "Iteration 161, loss = 0.23914465\n",
      "Iteration 162, loss = 0.23900635\n",
      "Iteration 163, loss = 0.23886086\n",
      "Iteration 164, loss = 0.23873525\n",
      "Iteration 165, loss = 0.23861256\n",
      "Iteration 166, loss = 0.23847411\n",
      "Iteration 167, loss = 0.23836960\n",
      "Iteration 168, loss = 0.23822056\n",
      "Iteration 169, loss = 0.23810349\n",
      "Iteration 170, loss = 0.23799417\n",
      "Iteration 171, loss = 0.23786051\n",
      "Iteration 172, loss = 0.23773126\n",
      "Iteration 173, loss = 0.23760966\n",
      "Iteration 174, loss = 0.23749714\n",
      "Iteration 175, loss = 0.23735812\n",
      "Iteration 176, loss = 0.23725837\n",
      "Iteration 177, loss = 0.23714358\n",
      "Iteration 178, loss = 0.23700043\n",
      "Iteration 179, loss = 0.23692435\n",
      "Iteration 180, loss = 0.23677021\n",
      "Iteration 181, loss = 0.23667094\n",
      "Iteration 182, loss = 0.23654381\n",
      "Iteration 183, loss = 0.23641429\n",
      "Iteration 184, loss = 0.23632284\n",
      "Iteration 185, loss = 0.23621293\n",
      "Iteration 186, loss = 0.23608558\n",
      "Iteration 187, loss = 0.23596604\n",
      "Iteration 188, loss = 0.23586063\n",
      "Iteration 189, loss = 0.23575277\n",
      "Iteration 190, loss = 0.23565025\n",
      "Iteration 191, loss = 0.23551604\n",
      "Iteration 192, loss = 0.23543558\n",
      "Iteration 193, loss = 0.23530161\n",
      "Iteration 194, loss = 0.23520770\n",
      "Iteration 195, loss = 0.23509870\n",
      "Iteration 196, loss = 0.23499451\n",
      "Iteration 197, loss = 0.23490230\n",
      "Iteration 198, loss = 0.23480174\n",
      "Iteration 199, loss = 0.23468792\n",
      "Iteration 200, loss = 0.23458799\n",
      "Iteration 1, loss = 0.75305843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.69270918\n",
      "Iteration 3, loss = 0.63016102\n",
      "Iteration 4, loss = 0.57819971\n",
      "Iteration 5, loss = 0.53593335\n",
      "Iteration 6, loss = 0.50094335\n",
      "Iteration 7, loss = 0.47246897\n",
      "Iteration 8, loss = 0.44843428\n",
      "Iteration 9, loss = 0.42836758\n",
      "Iteration 10, loss = 0.41144248\n",
      "Iteration 11, loss = 0.39678262\n",
      "Iteration 12, loss = 0.38416780\n",
      "Iteration 13, loss = 0.37327355\n",
      "Iteration 14, loss = 0.36363775\n",
      "Iteration 15, loss = 0.35518673\n",
      "Iteration 16, loss = 0.34768401\n",
      "Iteration 17, loss = 0.34104231\n",
      "Iteration 18, loss = 0.33503474\n",
      "Iteration 19, loss = 0.32957865\n",
      "Iteration 20, loss = 0.32471295\n",
      "Iteration 21, loss = 0.32029263\n",
      "Iteration 22, loss = 0.31622168\n",
      "Iteration 23, loss = 0.31250851\n",
      "Iteration 24, loss = 0.30911916\n",
      "Iteration 25, loss = 0.30597643\n",
      "Iteration 26, loss = 0.30308092\n",
      "Iteration 27, loss = 0.30040508\n",
      "Iteration 28, loss = 0.29792674\n",
      "Iteration 29, loss = 0.29555802\n",
      "Iteration 30, loss = 0.29344561\n",
      "Iteration 31, loss = 0.29138438\n",
      "Iteration 32, loss = 0.28949996\n",
      "Iteration 33, loss = 0.28772999\n",
      "Iteration 34, loss = 0.28608150\n",
      "Iteration 35, loss = 0.28448381\n",
      "Iteration 36, loss = 0.28298670\n",
      "Iteration 37, loss = 0.28156546\n",
      "Iteration 38, loss = 0.28026034\n",
      "Iteration 39, loss = 0.27900386\n",
      "Iteration 40, loss = 0.27780472\n",
      "Iteration 41, loss = 0.27666596\n",
      "Iteration 42, loss = 0.27557900\n",
      "Iteration 43, loss = 0.27453852\n",
      "Iteration 44, loss = 0.27355467\n",
      "Iteration 45, loss = 0.27260615\n",
      "Iteration 46, loss = 0.27173061\n",
      "Iteration 47, loss = 0.27087408\n",
      "Iteration 48, loss = 0.27003107\n",
      "Iteration 49, loss = 0.26924358\n",
      "Iteration 50, loss = 0.26846282\n",
      "Iteration 51, loss = 0.26773094\n",
      "Iteration 52, loss = 0.26703211\n",
      "Iteration 53, loss = 0.26634857\n",
      "Iteration 54, loss = 0.26570262\n",
      "Iteration 55, loss = 0.26506378\n",
      "Iteration 56, loss = 0.26444999\n",
      "Iteration 57, loss = 0.26387909\n",
      "Iteration 58, loss = 0.26333002\n",
      "Iteration 59, loss = 0.26274373\n",
      "Iteration 60, loss = 0.26221424\n",
      "Iteration 61, loss = 0.26170172\n",
      "Iteration 62, loss = 0.26119881\n",
      "Iteration 63, loss = 0.26069557\n",
      "Iteration 64, loss = 0.26022319\n",
      "Iteration 65, loss = 0.25976380\n",
      "Iteration 66, loss = 0.25934357\n",
      "Iteration 67, loss = 0.25886933\n",
      "Iteration 68, loss = 0.25845832\n",
      "Iteration 69, loss = 0.25804442\n",
      "Iteration 70, loss = 0.25762960\n",
      "Iteration 71, loss = 0.25723963\n",
      "Iteration 72, loss = 0.25688890\n",
      "Iteration 73, loss = 0.25647496\n",
      "Iteration 74, loss = 0.25611160\n",
      "Iteration 75, loss = 0.25576975\n",
      "Iteration 76, loss = 0.25540386\n",
      "Iteration 77, loss = 0.25507669\n",
      "Iteration 78, loss = 0.25473699\n",
      "Iteration 79, loss = 0.25441072\n",
      "Iteration 80, loss = 0.25407018\n",
      "Iteration 81, loss = 0.25374937\n",
      "Iteration 82, loss = 0.25346527\n",
      "Iteration 83, loss = 0.25314433\n",
      "Iteration 84, loss = 0.25284271\n",
      "Iteration 85, loss = 0.25253865\n",
      "Iteration 86, loss = 0.25224885\n",
      "Iteration 87, loss = 0.25198380\n",
      "Iteration 88, loss = 0.25170214\n",
      "Iteration 89, loss = 0.25143747\n",
      "Iteration 90, loss = 0.25116271\n",
      "Iteration 91, loss = 0.25088483\n",
      "Iteration 92, loss = 0.25062604\n",
      "Iteration 93, loss = 0.25036338\n",
      "Iteration 94, loss = 0.25010551\n",
      "Iteration 95, loss = 0.24986777\n",
      "Iteration 96, loss = 0.24960538\n",
      "Iteration 97, loss = 0.24937655\n",
      "Iteration 98, loss = 0.24914639\n",
      "Iteration 99, loss = 0.24889152\n",
      "Iteration 100, loss = 0.24868056\n",
      "Iteration 101, loss = 0.24843354\n",
      "Iteration 102, loss = 0.24819389\n",
      "Iteration 103, loss = 0.24797085\n",
      "Iteration 104, loss = 0.24775564\n",
      "Iteration 105, loss = 0.24755898\n",
      "Iteration 106, loss = 0.24732494\n",
      "Iteration 107, loss = 0.24711527\n",
      "Iteration 108, loss = 0.24690337\n",
      "Iteration 109, loss = 0.24669740\n",
      "Iteration 110, loss = 0.24648829\n",
      "Iteration 111, loss = 0.24628817\n",
      "Iteration 112, loss = 0.24605317\n",
      "Iteration 113, loss = 0.24590008\n",
      "Iteration 114, loss = 0.24569551\n",
      "Iteration 115, loss = 0.24550118\n",
      "Iteration 116, loss = 0.24530103\n",
      "Iteration 117, loss = 0.24510948\n",
      "Iteration 118, loss = 0.24491567\n",
      "Iteration 119, loss = 0.24473895\n",
      "Iteration 120, loss = 0.24455989\n",
      "Iteration 121, loss = 0.24435343\n",
      "Iteration 122, loss = 0.24418986\n",
      "Iteration 123, loss = 0.24399761\n",
      "Iteration 124, loss = 0.24381329\n",
      "Iteration 125, loss = 0.24366261\n",
      "Iteration 126, loss = 0.24346603\n",
      "Iteration 127, loss = 0.24330477\n",
      "Iteration 128, loss = 0.24312127\n",
      "Iteration 129, loss = 0.24296156\n",
      "Iteration 130, loss = 0.24279052\n",
      "Iteration 131, loss = 0.24263452\n",
      "Iteration 132, loss = 0.24245852\n",
      "Iteration 133, loss = 0.24231734\n",
      "Iteration 134, loss = 0.24214555\n",
      "Iteration 135, loss = 0.24198071\n",
      "Iteration 136, loss = 0.24180663\n",
      "Iteration 137, loss = 0.24167154\n",
      "Iteration 138, loss = 0.24150401\n",
      "Iteration 139, loss = 0.24133765\n",
      "Iteration 140, loss = 0.24116997\n",
      "Iteration 141, loss = 0.24103793\n",
      "Iteration 142, loss = 0.24088930\n",
      "Iteration 143, loss = 0.24076103\n",
      "Iteration 144, loss = 0.24058763\n",
      "Iteration 145, loss = 0.24044637\n",
      "Iteration 146, loss = 0.24028438\n",
      "Iteration 147, loss = 0.24015157\n",
      "Iteration 148, loss = 0.24001516\n",
      "Iteration 149, loss = 0.23984580\n",
      "Iteration 150, loss = 0.23972227\n",
      "Iteration 151, loss = 0.23957461\n",
      "Iteration 152, loss = 0.23944600\n",
      "Iteration 153, loss = 0.23930317\n",
      "Iteration 154, loss = 0.23914218\n",
      "Iteration 155, loss = 0.23902353\n",
      "Iteration 156, loss = 0.23887434\n",
      "Iteration 157, loss = 0.23874107\n",
      "Iteration 158, loss = 0.23860310\n",
      "Iteration 159, loss = 0.23849499\n",
      "Iteration 160, loss = 0.23837108\n",
      "Iteration 161, loss = 0.23820157\n",
      "Iteration 162, loss = 0.23808955\n",
      "Iteration 163, loss = 0.23794062\n",
      "Iteration 164, loss = 0.23782290\n",
      "Iteration 165, loss = 0.23768652\n",
      "Iteration 166, loss = 0.23755820\n",
      "Iteration 167, loss = 0.23743579\n",
      "Iteration 168, loss = 0.23730930\n",
      "Iteration 169, loss = 0.23719665\n",
      "Iteration 170, loss = 0.23708173\n",
      "Iteration 171, loss = 0.23695511\n",
      "Iteration 172, loss = 0.23683345\n",
      "Iteration 173, loss = 0.23670391\n",
      "Iteration 174, loss = 0.23658278\n",
      "Iteration 175, loss = 0.23646378\n",
      "Iteration 176, loss = 0.23635872\n",
      "Iteration 177, loss = 0.23624631\n",
      "Iteration 178, loss = 0.23610614\n",
      "Iteration 179, loss = 0.23603380\n",
      "Iteration 180, loss = 0.23589693\n",
      "Iteration 181, loss = 0.23579985\n",
      "Iteration 182, loss = 0.23566155\n",
      "Iteration 183, loss = 0.23555122\n",
      "Iteration 184, loss = 0.23544107\n",
      "Iteration 185, loss = 0.23532907\n",
      "Iteration 186, loss = 0.23522508\n",
      "Iteration 187, loss = 0.23510153\n",
      "Iteration 188, loss = 0.23499737\n",
      "Iteration 189, loss = 0.23489114\n",
      "Iteration 190, loss = 0.23478135\n",
      "Iteration 191, loss = 0.23466507\n",
      "Iteration 192, loss = 0.23455234\n",
      "Iteration 193, loss = 0.23445765\n",
      "Iteration 194, loss = 0.23435313\n",
      "Iteration 195, loss = 0.23424098\n",
      "Iteration 196, loss = 0.23413876\n",
      "Iteration 197, loss = 0.23403517\n",
      "Iteration 198, loss = 0.23393961\n",
      "Iteration 199, loss = 0.23383741\n",
      "Iteration 200, loss = 0.23372632\n",
      "Iteration 1, loss = 0.75555773\n",
      "Iteration 2, loss = 0.69490179\n",
      "Iteration 3, loss = 0.63169384\n",
      "Iteration 4, loss = 0.57972760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.53704198\n",
      "Iteration 6, loss = 0.50188891\n",
      "Iteration 7, loss = 0.47335841\n",
      "Iteration 8, loss = 0.44940894\n",
      "Iteration 9, loss = 0.42919563\n",
      "Iteration 10, loss = 0.41232130\n",
      "Iteration 11, loss = 0.39774628\n",
      "Iteration 12, loss = 0.38509314\n",
      "Iteration 13, loss = 0.37429261\n",
      "Iteration 14, loss = 0.36471050\n",
      "Iteration 15, loss = 0.35628659\n",
      "Iteration 16, loss = 0.34884420\n",
      "Iteration 17, loss = 0.34229370\n",
      "Iteration 18, loss = 0.33633124\n",
      "Iteration 19, loss = 0.33092019\n",
      "Iteration 20, loss = 0.32612756\n",
      "Iteration 21, loss = 0.32176808\n",
      "Iteration 22, loss = 0.31774802\n",
      "Iteration 23, loss = 0.31409352\n",
      "Iteration 24, loss = 0.31077941\n",
      "Iteration 25, loss = 0.30767585\n",
      "Iteration 26, loss = 0.30484868\n",
      "Iteration 27, loss = 0.30221122\n",
      "Iteration 28, loss = 0.29976813\n",
      "Iteration 29, loss = 0.29748157\n",
      "Iteration 30, loss = 0.29538050\n",
      "Iteration 31, loss = 0.29342134\n",
      "Iteration 32, loss = 0.29154386\n",
      "Iteration 33, loss = 0.28983638\n",
      "Iteration 34, loss = 0.28821777\n",
      "Iteration 35, loss = 0.28668140\n",
      "Iteration 36, loss = 0.28523724\n",
      "Iteration 37, loss = 0.28384956\n",
      "Iteration 38, loss = 0.28259410\n",
      "Iteration 39, loss = 0.28135701\n",
      "Iteration 40, loss = 0.28019693\n",
      "Iteration 41, loss = 0.27911992\n",
      "Iteration 42, loss = 0.27805525\n",
      "Iteration 43, loss = 0.27706007\n",
      "Iteration 44, loss = 0.27611990\n",
      "Iteration 45, loss = 0.27518640\n",
      "Iteration 46, loss = 0.27436979\n",
      "Iteration 47, loss = 0.27350948\n",
      "Iteration 48, loss = 0.27272295\n",
      "Iteration 49, loss = 0.27195425\n",
      "Iteration 50, loss = 0.27123805\n",
      "Iteration 51, loss = 0.27053180\n",
      "Iteration 52, loss = 0.26984710\n",
      "Iteration 53, loss = 0.26921007\n",
      "Iteration 54, loss = 0.26861674\n",
      "Iteration 55, loss = 0.26796229\n",
      "Iteration 56, loss = 0.26739248\n",
      "Iteration 57, loss = 0.26687066\n",
      "Iteration 58, loss = 0.26631542\n",
      "Iteration 59, loss = 0.26577498\n",
      "Iteration 60, loss = 0.26528272\n",
      "Iteration 61, loss = 0.26479366\n",
      "Iteration 62, loss = 0.26431901\n",
      "Iteration 63, loss = 0.26384557\n",
      "Iteration 64, loss = 0.26339253\n",
      "Iteration 65, loss = 0.26297213\n",
      "Iteration 66, loss = 0.26257571\n",
      "Iteration 67, loss = 0.26213120\n",
      "Iteration 68, loss = 0.26174070\n",
      "Iteration 69, loss = 0.26133956\n",
      "Iteration 70, loss = 0.26095801\n",
      "Iteration 71, loss = 0.26060950\n",
      "Iteration 72, loss = 0.26025213\n",
      "Iteration 73, loss = 0.25988454\n",
      "Iteration 74, loss = 0.25955189\n",
      "Iteration 75, loss = 0.25922591\n",
      "Iteration 76, loss = 0.25886722\n",
      "Iteration 77, loss = 0.25856251\n",
      "Iteration 78, loss = 0.25823845\n",
      "Iteration 79, loss = 0.25792096\n",
      "Iteration 80, loss = 0.25763888\n",
      "Iteration 81, loss = 0.25732636\n",
      "Iteration 82, loss = 0.25705934\n",
      "Iteration 83, loss = 0.25676544\n",
      "Iteration 84, loss = 0.25646617\n",
      "Iteration 85, loss = 0.25620553\n",
      "Iteration 86, loss = 0.25592829\n",
      "Iteration 87, loss = 0.25568431\n",
      "Iteration 88, loss = 0.25543555\n",
      "Iteration 89, loss = 0.25518892\n",
      "Iteration 90, loss = 0.25492564\n",
      "Iteration 91, loss = 0.25466981\n",
      "Iteration 92, loss = 0.25442246\n",
      "Iteration 93, loss = 0.25417712\n",
      "Iteration 94, loss = 0.25394225\n",
      "Iteration 95, loss = 0.25371140\n",
      "Iteration 96, loss = 0.25348657\n",
      "Iteration 97, loss = 0.25326635\n",
      "Iteration 98, loss = 0.25304913\n",
      "Iteration 99, loss = 0.25281080\n",
      "Iteration 100, loss = 0.25262555\n",
      "Iteration 101, loss = 0.25241558\n",
      "Iteration 102, loss = 0.25218654\n",
      "Iteration 103, loss = 0.25197652\n",
      "Iteration 104, loss = 0.25177435\n",
      "Iteration 105, loss = 0.25157243\n",
      "Iteration 106, loss = 0.25136965\n",
      "Iteration 107, loss = 0.25119977\n",
      "Iteration 108, loss = 0.25099055\n",
      "Iteration 109, loss = 0.25079946\n",
      "Iteration 110, loss = 0.25060694\n",
      "Iteration 111, loss = 0.25042089\n",
      "Iteration 112, loss = 0.25023403\n",
      "Iteration 113, loss = 0.25006859\n",
      "Iteration 114, loss = 0.24986723\n",
      "Iteration 115, loss = 0.24967282\n",
      "Iteration 116, loss = 0.24950521\n",
      "Iteration 117, loss = 0.24932717\n",
      "Iteration 118, loss = 0.24915192\n",
      "Iteration 119, loss = 0.24899564\n",
      "Iteration 120, loss = 0.24882076\n",
      "Iteration 121, loss = 0.24863544\n",
      "Iteration 122, loss = 0.24849788\n",
      "Iteration 123, loss = 0.24831031\n",
      "Iteration 124, loss = 0.24812726\n",
      "Iteration 125, loss = 0.24800666\n",
      "Iteration 126, loss = 0.24782697\n",
      "Iteration 127, loss = 0.24768595\n",
      "Iteration 128, loss = 0.24750684\n",
      "Iteration 129, loss = 0.24734636\n",
      "Iteration 130, loss = 0.24718644\n",
      "Iteration 131, loss = 0.24704936\n",
      "Iteration 132, loss = 0.24689409\n",
      "Iteration 133, loss = 0.24673492\n",
      "Iteration 134, loss = 0.24658373\n",
      "Iteration 135, loss = 0.24643647\n",
      "Iteration 136, loss = 0.24628248\n",
      "Iteration 137, loss = 0.24613604\n",
      "Iteration 138, loss = 0.24600400\n",
      "Iteration 139, loss = 0.24585520\n",
      "Iteration 140, loss = 0.24570989\n",
      "Iteration 141, loss = 0.24558214\n",
      "Iteration 142, loss = 0.24544601\n",
      "Iteration 143, loss = 0.24531580\n",
      "Iteration 144, loss = 0.24516513\n",
      "Iteration 145, loss = 0.24505071\n",
      "Iteration 146, loss = 0.24486827\n",
      "Iteration 147, loss = 0.24475286\n",
      "Iteration 148, loss = 0.24462538\n",
      "Iteration 149, loss = 0.24446958\n",
      "Iteration 150, loss = 0.24436018\n",
      "Iteration 151, loss = 0.24421549\n",
      "Iteration 152, loss = 0.24408677\n",
      "Iteration 153, loss = 0.24396484\n",
      "Iteration 154, loss = 0.24381905\n",
      "Iteration 155, loss = 0.24373833\n",
      "Iteration 156, loss = 0.24356818\n",
      "Iteration 157, loss = 0.24345285\n",
      "Iteration 158, loss = 0.24333826\n",
      "Iteration 159, loss = 0.24322672\n",
      "Iteration 160, loss = 0.24308669\n",
      "Iteration 161, loss = 0.24294490\n",
      "Iteration 162, loss = 0.24283062\n",
      "Iteration 163, loss = 0.24272713\n",
      "Iteration 164, loss = 0.24259120\n",
      "Iteration 165, loss = 0.24248786\n",
      "Iteration 166, loss = 0.24235373\n",
      "Iteration 167, loss = 0.24223424\n",
      "Iteration 168, loss = 0.24212101\n",
      "Iteration 169, loss = 0.24200536\n",
      "Iteration 170, loss = 0.24189499\n",
      "Iteration 171, loss = 0.24177654\n",
      "Iteration 172, loss = 0.24166500\n",
      "Iteration 173, loss = 0.24156845\n",
      "Iteration 174, loss = 0.24144477\n",
      "Iteration 175, loss = 0.24132729\n",
      "Iteration 176, loss = 0.24121739\n",
      "Iteration 177, loss = 0.24111397\n",
      "Iteration 178, loss = 0.24102958\n",
      "Iteration 179, loss = 0.24092679\n",
      "Iteration 180, loss = 0.24080990\n",
      "Iteration 181, loss = 0.24069760\n",
      "Iteration 182, loss = 0.24058033\n",
      "Iteration 183, loss = 0.24047293\n",
      "Iteration 184, loss = 0.24036403\n",
      "Iteration 185, loss = 0.24027942\n",
      "Iteration 186, loss = 0.24018013\n",
      "Iteration 187, loss = 0.24005550\n",
      "Iteration 188, loss = 0.23996440\n",
      "Iteration 189, loss = 0.23987160\n",
      "Iteration 190, loss = 0.23974767\n",
      "Iteration 191, loss = 0.23963981\n",
      "Iteration 192, loss = 0.23955946\n",
      "Iteration 193, loss = 0.23943970\n",
      "Iteration 194, loss = 0.23936825\n",
      "Iteration 195, loss = 0.23924973\n",
      "Iteration 196, loss = 0.23913711\n",
      "Iteration 197, loss = 0.23905095\n",
      "Iteration 198, loss = 0.23895444\n",
      "Iteration 199, loss = 0.23887655\n",
      "Iteration 200, loss = 0.23877052\n",
      "Iteration 1, loss = 0.75739948\n",
      "Iteration 2, loss = 0.69643549\n",
      "Iteration 3, loss = 0.63394971\n",
      "Iteration 4, loss = 0.58144165\n",
      "Iteration 5, loss = 0.53862016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.50386836\n",
      "Iteration 7, loss = 0.47518753\n",
      "Iteration 8, loss = 0.45120597\n",
      "Iteration 9, loss = 0.43100889\n",
      "Iteration 10, loss = 0.41406384\n",
      "Iteration 11, loss = 0.39949487\n",
      "Iteration 12, loss = 0.38679323\n",
      "Iteration 13, loss = 0.37591695\n",
      "Iteration 14, loss = 0.36624482\n",
      "Iteration 15, loss = 0.35784739\n",
      "Iteration 16, loss = 0.35024032\n",
      "Iteration 17, loss = 0.34359388\n",
      "Iteration 18, loss = 0.33765860\n",
      "Iteration 19, loss = 0.33216762\n",
      "Iteration 20, loss = 0.32730015\n",
      "Iteration 21, loss = 0.32285661\n",
      "Iteration 22, loss = 0.31878686\n",
      "Iteration 23, loss = 0.31505591\n",
      "Iteration 24, loss = 0.31163383\n",
      "Iteration 25, loss = 0.30851890\n",
      "Iteration 26, loss = 0.30558613\n",
      "Iteration 27, loss = 0.30291448\n",
      "Iteration 28, loss = 0.30041482\n",
      "Iteration 29, loss = 0.29810771\n",
      "Iteration 30, loss = 0.29586230\n",
      "Iteration 31, loss = 0.29388970\n",
      "Iteration 32, loss = 0.29190726\n",
      "Iteration 33, loss = 0.29017032\n",
      "Iteration 34, loss = 0.28846448\n",
      "Iteration 35, loss = 0.28687830\n",
      "Iteration 36, loss = 0.28537830\n",
      "Iteration 37, loss = 0.28393616\n",
      "Iteration 38, loss = 0.28264608\n",
      "Iteration 39, loss = 0.28131546\n",
      "Iteration 40, loss = 0.28010452\n",
      "Iteration 41, loss = 0.27899505\n",
      "Iteration 42, loss = 0.27787415\n",
      "Iteration 43, loss = 0.27681257\n",
      "Iteration 44, loss = 0.27584235\n",
      "Iteration 45, loss = 0.27486846\n",
      "Iteration 46, loss = 0.27397435\n",
      "Iteration 47, loss = 0.27309576\n",
      "Iteration 48, loss = 0.27222164\n",
      "Iteration 49, loss = 0.27145615\n",
      "Iteration 50, loss = 0.27064166\n",
      "Iteration 51, loss = 0.26990685\n",
      "Iteration 52, loss = 0.26919599\n",
      "Iteration 53, loss = 0.26849144\n",
      "Iteration 54, loss = 0.26782267\n",
      "Iteration 55, loss = 0.26716069\n",
      "Iteration 56, loss = 0.26655900\n",
      "Iteration 57, loss = 0.26594008\n",
      "Iteration 58, loss = 0.26538163\n",
      "Iteration 59, loss = 0.26479694\n",
      "Iteration 60, loss = 0.26424286\n",
      "Iteration 61, loss = 0.26373347\n",
      "Iteration 62, loss = 0.26319397\n",
      "Iteration 63, loss = 0.26270302\n",
      "Iteration 64, loss = 0.26220386\n",
      "Iteration 65, loss = 0.26175320\n",
      "Iteration 66, loss = 0.26126140\n",
      "Iteration 67, loss = 0.26081550\n",
      "Iteration 68, loss = 0.26038621\n",
      "Iteration 69, loss = 0.25994749\n",
      "Iteration 70, loss = 0.25953798\n",
      "Iteration 71, loss = 0.25913624\n",
      "Iteration 72, loss = 0.25875097\n",
      "Iteration 73, loss = 0.25834404\n",
      "Iteration 74, loss = 0.25794726\n",
      "Iteration 75, loss = 0.25760517\n",
      "Iteration 76, loss = 0.25723264\n",
      "Iteration 77, loss = 0.25685864\n",
      "Iteration 78, loss = 0.25652180\n",
      "Iteration 79, loss = 0.25617910\n",
      "Iteration 80, loss = 0.25582647\n",
      "Iteration 81, loss = 0.25551409\n",
      "Iteration 82, loss = 0.25524352\n",
      "Iteration 83, loss = 0.25486663\n",
      "Iteration 84, loss = 0.25456192\n",
      "Iteration 85, loss = 0.25423359\n",
      "Iteration 86, loss = 0.25395597\n",
      "Iteration 87, loss = 0.25364052\n",
      "Iteration 88, loss = 0.25335190\n",
      "Iteration 89, loss = 0.25306329\n",
      "Iteration 90, loss = 0.25278268\n",
      "Iteration 91, loss = 0.25250126\n",
      "Iteration 92, loss = 0.25225171\n",
      "Iteration 93, loss = 0.25195215\n",
      "Iteration 94, loss = 0.25168863\n",
      "Iteration 95, loss = 0.25142342\n",
      "Iteration 96, loss = 0.25119015\n",
      "Iteration 97, loss = 0.25091922\n",
      "Iteration 98, loss = 0.25064306\n",
      "Iteration 99, loss = 0.25040962\n",
      "Iteration 100, loss = 0.25019267\n",
      "Iteration 101, loss = 0.24992752\n",
      "Iteration 102, loss = 0.24969762\n",
      "Iteration 103, loss = 0.24947677\n",
      "Iteration 104, loss = 0.24920487\n",
      "Iteration 105, loss = 0.24898395\n",
      "Iteration 106, loss = 0.24875841\n",
      "Iteration 107, loss = 0.24853533\n",
      "Iteration 108, loss = 0.24830490\n",
      "Iteration 109, loss = 0.24808514\n",
      "Iteration 110, loss = 0.24787530\n",
      "Iteration 111, loss = 0.24768064\n",
      "Iteration 112, loss = 0.24745069\n",
      "Iteration 113, loss = 0.24723345\n",
      "Iteration 114, loss = 0.24703680\n",
      "Iteration 115, loss = 0.24682629\n",
      "Iteration 116, loss = 0.24662073\n",
      "Iteration 117, loss = 0.24641938\n",
      "Iteration 118, loss = 0.24619958\n",
      "Iteration 119, loss = 0.24603749\n",
      "Iteration 120, loss = 0.24583179\n",
      "Iteration 121, loss = 0.24563661\n",
      "Iteration 122, loss = 0.24544696\n",
      "Iteration 123, loss = 0.24527038\n",
      "Iteration 124, loss = 0.24509059\n",
      "Iteration 125, loss = 0.24488740\n",
      "Iteration 126, loss = 0.24469391\n",
      "Iteration 127, loss = 0.24452458\n",
      "Iteration 128, loss = 0.24434405\n",
      "Iteration 129, loss = 0.24416002\n",
      "Iteration 130, loss = 0.24402464\n",
      "Iteration 131, loss = 0.24380599\n",
      "Iteration 132, loss = 0.24363510\n",
      "Iteration 133, loss = 0.24346937\n",
      "Iteration 134, loss = 0.24334661\n",
      "Iteration 135, loss = 0.24310569\n",
      "Iteration 136, loss = 0.24294730\n",
      "Iteration 137, loss = 0.24279132\n",
      "Iteration 138, loss = 0.24261668\n",
      "Iteration 139, loss = 0.24245235\n",
      "Iteration 140, loss = 0.24230520\n",
      "Iteration 141, loss = 0.24214778\n",
      "Iteration 142, loss = 0.24196627\n",
      "Iteration 143, loss = 0.24183699\n",
      "Iteration 144, loss = 0.24169024\n",
      "Iteration 145, loss = 0.24150999\n",
      "Iteration 146, loss = 0.24135748\n",
      "Iteration 147, loss = 0.24120970\n",
      "Iteration 148, loss = 0.24105957\n",
      "Iteration 149, loss = 0.24091251\n",
      "Iteration 150, loss = 0.24075339\n",
      "Iteration 151, loss = 0.24060514\n",
      "Iteration 152, loss = 0.24051548\n",
      "Iteration 153, loss = 0.24032016\n",
      "Iteration 154, loss = 0.24017404\n",
      "Iteration 155, loss = 0.24003365\n",
      "Iteration 156, loss = 0.23989191\n",
      "Iteration 157, loss = 0.23976587\n",
      "Iteration 158, loss = 0.23959169\n",
      "Iteration 159, loss = 0.23947495\n",
      "Iteration 160, loss = 0.23931860\n",
      "Iteration 161, loss = 0.23919875\n",
      "Iteration 162, loss = 0.23907468\n",
      "Iteration 163, loss = 0.23891129\n",
      "Iteration 164, loss = 0.23879922\n",
      "Iteration 165, loss = 0.23866046\n",
      "Iteration 166, loss = 0.23853777\n",
      "Iteration 167, loss = 0.23839569\n",
      "Iteration 168, loss = 0.23826447\n",
      "Iteration 169, loss = 0.23814582\n",
      "Iteration 170, loss = 0.23800802\n",
      "Iteration 171, loss = 0.23787747\n",
      "Iteration 172, loss = 0.23776744\n",
      "Iteration 173, loss = 0.23762807\n",
      "Iteration 174, loss = 0.23750338\n",
      "Iteration 175, loss = 0.23741765\n",
      "Iteration 176, loss = 0.23725859\n",
      "Iteration 177, loss = 0.23713201\n",
      "Iteration 178, loss = 0.23704424\n",
      "Iteration 179, loss = 0.23691255\n",
      "Iteration 180, loss = 0.23678157\n",
      "Iteration 181, loss = 0.23667732\n",
      "Iteration 182, loss = 0.23653252\n",
      "Iteration 183, loss = 0.23644642\n",
      "Iteration 184, loss = 0.23633722\n",
      "Iteration 185, loss = 0.23620683\n",
      "Iteration 186, loss = 0.23608850\n",
      "Iteration 187, loss = 0.23599925\n",
      "Iteration 188, loss = 0.23586549\n",
      "Iteration 189, loss = 0.23574046\n",
      "Iteration 190, loss = 0.23567605\n",
      "Iteration 191, loss = 0.23553936\n",
      "Iteration 192, loss = 0.23542967\n",
      "Iteration 193, loss = 0.23530978\n",
      "Iteration 194, loss = 0.23522153\n",
      "Iteration 195, loss = 0.23512981\n",
      "Iteration 196, loss = 0.23498865\n",
      "Iteration 197, loss = 0.23488075\n",
      "Iteration 198, loss = 0.23477140\n",
      "Iteration 199, loss = 0.23465517\n",
      "Iteration 200, loss = 0.23456973\n",
      "Iteration 1, loss = 0.76143622\n",
      "Iteration 2, loss = 0.69878427\n",
      "Iteration 3, loss = 0.63444373\n",
      "Iteration 4, loss = 0.58077935\n",
      "Iteration 5, loss = 0.53723248\n",
      "Iteration 6, loss = 0.50171937\n",
      "Iteration 7, loss = 0.47284670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44846892\n",
      "Iteration 9, loss = 0.42830995\n",
      "Iteration 10, loss = 0.41139901\n",
      "Iteration 11, loss = 0.39680231\n",
      "Iteration 12, loss = 0.38425129\n",
      "Iteration 13, loss = 0.37346258\n",
      "Iteration 14, loss = 0.36389014\n",
      "Iteration 15, loss = 0.35561358\n",
      "Iteration 16, loss = 0.34815153\n",
      "Iteration 17, loss = 0.34163728\n",
      "Iteration 18, loss = 0.33579960\n",
      "Iteration 19, loss = 0.33049669\n",
      "Iteration 20, loss = 0.32572942\n",
      "Iteration 21, loss = 0.32145158\n",
      "Iteration 22, loss = 0.31752925\n",
      "Iteration 23, loss = 0.31388375\n",
      "Iteration 24, loss = 0.31064154\n",
      "Iteration 25, loss = 0.30758953\n",
      "Iteration 26, loss = 0.30483139\n",
      "Iteration 27, loss = 0.30225714\n",
      "Iteration 28, loss = 0.29987374\n",
      "Iteration 29, loss = 0.29768240\n",
      "Iteration 30, loss = 0.29555766\n",
      "Iteration 31, loss = 0.29366329\n",
      "Iteration 32, loss = 0.29181974\n",
      "Iteration 33, loss = 0.29015846\n",
      "Iteration 34, loss = 0.28856018\n",
      "Iteration 35, loss = 0.28707685\n",
      "Iteration 36, loss = 0.28566915\n",
      "Iteration 37, loss = 0.28433596\n",
      "Iteration 38, loss = 0.28308553\n",
      "Iteration 39, loss = 0.28186993\n",
      "Iteration 40, loss = 0.28073216\n",
      "Iteration 41, loss = 0.27968388\n",
      "Iteration 42, loss = 0.27866896\n",
      "Iteration 43, loss = 0.27767855\n",
      "Iteration 44, loss = 0.27678120\n",
      "Iteration 45, loss = 0.27587577\n",
      "Iteration 46, loss = 0.27504569\n",
      "Iteration 47, loss = 0.27425363\n",
      "Iteration 48, loss = 0.27344181\n",
      "Iteration 49, loss = 0.27274711\n",
      "Iteration 50, loss = 0.27200252\n",
      "Iteration 51, loss = 0.27135912\n",
      "Iteration 52, loss = 0.27069171\n",
      "Iteration 53, loss = 0.27004345\n",
      "Iteration 54, loss = 0.26942233\n",
      "Iteration 55, loss = 0.26882955\n",
      "Iteration 56, loss = 0.26827625\n",
      "Iteration 57, loss = 0.26773647\n",
      "Iteration 58, loss = 0.26722090\n",
      "Iteration 59, loss = 0.26668851\n",
      "Iteration 60, loss = 0.26619682\n",
      "Iteration 61, loss = 0.26572248\n",
      "Iteration 62, loss = 0.26524289\n",
      "Iteration 63, loss = 0.26481056\n",
      "Iteration 64, loss = 0.26435235\n",
      "Iteration 65, loss = 0.26396092\n",
      "Iteration 66, loss = 0.26352915\n",
      "Iteration 67, loss = 0.26312554\n",
      "Iteration 68, loss = 0.26273183\n",
      "Iteration 69, loss = 0.26234736\n",
      "Iteration 70, loss = 0.26198787\n",
      "Iteration 71, loss = 0.26161981\n",
      "Iteration 72, loss = 0.26125972\n",
      "Iteration 73, loss = 0.26090728\n",
      "Iteration 74, loss = 0.26057122\n",
      "Iteration 75, loss = 0.26023719\n",
      "Iteration 76, loss = 0.25992466\n",
      "Iteration 77, loss = 0.25961533\n",
      "Iteration 78, loss = 0.25931553\n",
      "Iteration 79, loss = 0.25898562\n",
      "Iteration 80, loss = 0.25870814\n",
      "Iteration 81, loss = 0.25841530\n",
      "Iteration 82, loss = 0.25816484\n",
      "Iteration 83, loss = 0.25785273\n",
      "Iteration 84, loss = 0.25756828\n",
      "Iteration 85, loss = 0.25726876\n",
      "Iteration 86, loss = 0.25703681\n",
      "Iteration 87, loss = 0.25676190\n",
      "Iteration 88, loss = 0.25650302\n",
      "Iteration 89, loss = 0.25624814\n",
      "Iteration 90, loss = 0.25599625\n",
      "Iteration 91, loss = 0.25576798\n",
      "Iteration 92, loss = 0.25551923\n",
      "Iteration 93, loss = 0.25527723\n",
      "Iteration 94, loss = 0.25505698\n",
      "Iteration 95, loss = 0.25482752\n",
      "Iteration 96, loss = 0.25463691\n",
      "Iteration 97, loss = 0.25437260\n",
      "Iteration 98, loss = 0.25415096\n",
      "Iteration 99, loss = 0.25393503\n",
      "Iteration 100, loss = 0.25373654\n",
      "Iteration 101, loss = 0.25353007\n",
      "Iteration 102, loss = 0.25331107\n",
      "Iteration 103, loss = 0.25311842\n",
      "Iteration 104, loss = 0.25288928\n",
      "Iteration 105, loss = 0.25269171\n",
      "Iteration 106, loss = 0.25248966\n",
      "Iteration 107, loss = 0.25229978\n",
      "Iteration 108, loss = 0.25212685\n",
      "Iteration 109, loss = 0.25191634\n",
      "Iteration 110, loss = 0.25173295\n",
      "Iteration 111, loss = 0.25155947\n",
      "Iteration 112, loss = 0.25136381\n",
      "Iteration 113, loss = 0.25117465\n",
      "Iteration 114, loss = 0.25099177\n",
      "Iteration 115, loss = 0.25081917\n",
      "Iteration 116, loss = 0.25064514\n",
      "Iteration 117, loss = 0.25046708\n",
      "Iteration 118, loss = 0.25027578\n",
      "Iteration 119, loss = 0.25012989\n",
      "Iteration 120, loss = 0.24994917\n",
      "Iteration 121, loss = 0.24978693\n",
      "Iteration 122, loss = 0.24963495\n",
      "Iteration 123, loss = 0.24946112\n",
      "Iteration 124, loss = 0.24930377\n",
      "Iteration 125, loss = 0.24913241\n",
      "Iteration 126, loss = 0.24897784\n",
      "Iteration 127, loss = 0.24881858\n",
      "Iteration 128, loss = 0.24866615\n",
      "Iteration 129, loss = 0.24850564\n",
      "Iteration 130, loss = 0.24837725\n",
      "Iteration 131, loss = 0.24818657\n",
      "Iteration 132, loss = 0.24804407\n",
      "Iteration 133, loss = 0.24789214\n",
      "Iteration 134, loss = 0.24777497\n",
      "Iteration 135, loss = 0.24758382\n",
      "Iteration 136, loss = 0.24745232\n",
      "Iteration 137, loss = 0.24729583\n",
      "Iteration 138, loss = 0.24716235\n",
      "Iteration 139, loss = 0.24701431\n",
      "Iteration 140, loss = 0.24689508\n",
      "Iteration 141, loss = 0.24673619\n",
      "Iteration 142, loss = 0.24659960\n",
      "Iteration 143, loss = 0.24648233\n",
      "Iteration 144, loss = 0.24633591\n",
      "Iteration 145, loss = 0.24619347\n",
      "Iteration 146, loss = 0.24604851\n",
      "Iteration 147, loss = 0.24593865\n",
      "Iteration 148, loss = 0.24579854\n",
      "Iteration 149, loss = 0.24566471\n",
      "Iteration 150, loss = 0.24554711\n",
      "Iteration 151, loss = 0.24541412\n",
      "Iteration 152, loss = 0.24531280\n",
      "Iteration 153, loss = 0.24515076\n",
      "Iteration 154, loss = 0.24503599\n",
      "Iteration 155, loss = 0.24490780\n",
      "Iteration 156, loss = 0.24475931\n",
      "Iteration 157, loss = 0.24466930\n",
      "Iteration 158, loss = 0.24451853\n",
      "Iteration 159, loss = 0.24440415\n",
      "Iteration 160, loss = 0.24428180\n",
      "Iteration 161, loss = 0.24418293\n",
      "Iteration 162, loss = 0.24406614\n",
      "Iteration 163, loss = 0.24391897\n",
      "Iteration 164, loss = 0.24381111\n",
      "Iteration 165, loss = 0.24369005\n",
      "Iteration 166, loss = 0.24355916\n",
      "Iteration 167, loss = 0.24344906\n",
      "Iteration 168, loss = 0.24334136\n",
      "Iteration 169, loss = 0.24324170\n",
      "Iteration 170, loss = 0.24311593\n",
      "Iteration 171, loss = 0.24301411\n",
      "Iteration 172, loss = 0.24289412\n",
      "Iteration 173, loss = 0.24278559\n",
      "Iteration 174, loss = 0.24269288\n",
      "Iteration 175, loss = 0.24262021\n",
      "Iteration 176, loss = 0.24244512\n",
      "Iteration 177, loss = 0.24234675\n",
      "Iteration 178, loss = 0.24226227\n",
      "Iteration 179, loss = 0.24214016\n",
      "Iteration 180, loss = 0.24203154\n",
      "Iteration 181, loss = 0.24194757\n",
      "Iteration 182, loss = 0.24182213\n",
      "Iteration 183, loss = 0.24174270\n",
      "Iteration 184, loss = 0.24162181\n",
      "Iteration 185, loss = 0.24150696\n",
      "Iteration 186, loss = 0.24141696\n",
      "Iteration 187, loss = 0.24135001\n",
      "Iteration 188, loss = 0.24121090\n",
      "Iteration 189, loss = 0.24110920\n",
      "Iteration 190, loss = 0.24104330\n",
      "Iteration 191, loss = 0.24094943\n",
      "Iteration 192, loss = 0.24083748\n",
      "Iteration 193, loss = 0.24071309\n",
      "Iteration 194, loss = 0.24064477\n",
      "Iteration 195, loss = 0.24057030\n",
      "Iteration 196, loss = 0.24042488\n",
      "Iteration 197, loss = 0.24032672\n",
      "Iteration 198, loss = 0.24024430\n",
      "Iteration 199, loss = 0.24013267\n",
      "Iteration 200, loss = 0.24005160\n",
      "Iteration 1, loss = 0.68558826\n",
      "Iteration 2, loss = 0.53830094\n",
      "Iteration 3, loss = 0.45087743\n",
      "Iteration 4, loss = 0.39195086\n",
      "Iteration 5, loss = 0.34980838\n",
      "Iteration 6, loss = 0.31956332\n",
      "Iteration 7, loss = 0.29853263\n",
      "Iteration 8, loss = 0.28319724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.27157039\n",
      "Iteration 10, loss = 0.26315379\n",
      "Iteration 11, loss = 0.25642051\n",
      "Iteration 12, loss = 0.25068772\n",
      "Iteration 13, loss = 0.24676908\n",
      "Iteration 14, loss = 0.24304626\n",
      "Iteration 15, loss = 0.23992050\n",
      "Iteration 16, loss = 0.23743372\n",
      "Iteration 17, loss = 0.23455791\n",
      "Iteration 18, loss = 0.23283606\n",
      "Iteration 19, loss = 0.23086640\n",
      "Iteration 20, loss = 0.22942566\n",
      "Iteration 21, loss = 0.22716275\n",
      "Iteration 22, loss = 0.22654750\n",
      "Iteration 23, loss = 0.22445528\n",
      "Iteration 24, loss = 0.22272178\n",
      "Iteration 25, loss = 0.22172490\n",
      "Iteration 26, loss = 0.22032696\n",
      "Iteration 27, loss = 0.21892129\n",
      "Iteration 28, loss = 0.21791463\n",
      "Iteration 29, loss = 0.21576410\n",
      "Iteration 30, loss = 0.21524097\n",
      "Iteration 31, loss = 0.21360184\n",
      "Iteration 32, loss = 0.21204988\n",
      "Iteration 33, loss = 0.21045063\n",
      "Iteration 34, loss = 0.20941028\n",
      "Iteration 35, loss = 0.20865601\n",
      "Iteration 36, loss = 0.20715310\n",
      "Iteration 37, loss = 0.20514715\n",
      "Iteration 38, loss = 0.20364405\n",
      "Iteration 39, loss = 0.20239716\n",
      "Iteration 40, loss = 0.20153636\n",
      "Iteration 41, loss = 0.19966724\n",
      "Iteration 42, loss = 0.19828487\n",
      "Iteration 43, loss = 0.19633722\n",
      "Iteration 44, loss = 0.19532128\n",
      "Iteration 45, loss = 0.19425635\n",
      "Iteration 46, loss = 0.19229255\n",
      "Iteration 47, loss = 0.19164857\n",
      "Iteration 48, loss = 0.18937438\n",
      "Iteration 49, loss = 0.18805228\n",
      "Iteration 50, loss = 0.18552751\n",
      "Iteration 51, loss = 0.18462302\n",
      "Iteration 52, loss = 0.18348090\n",
      "Iteration 53, loss = 0.18151069\n",
      "Iteration 54, loss = 0.17994312\n",
      "Iteration 55, loss = 0.17837778\n",
      "Iteration 56, loss = 0.17648546\n",
      "Iteration 57, loss = 0.17598063\n",
      "Iteration 58, loss = 0.17402915\n",
      "Iteration 59, loss = 0.17205549\n",
      "Iteration 60, loss = 0.17094630\n",
      "Iteration 61, loss = 0.16927280\n",
      "Iteration 62, loss = 0.16783040\n",
      "Iteration 63, loss = 0.16639334\n",
      "Iteration 64, loss = 0.16523745\n",
      "Iteration 65, loss = 0.16344217\n",
      "Iteration 66, loss = 0.16216383\n",
      "Iteration 67, loss = 0.16035210\n",
      "Iteration 68, loss = 0.15897193\n",
      "Iteration 69, loss = 0.15775696\n",
      "Iteration 70, loss = 0.15578715\n",
      "Iteration 71, loss = 0.15445955\n",
      "Iteration 72, loss = 0.15304267\n",
      "Iteration 73, loss = 0.15208235\n",
      "Iteration 74, loss = 0.15019359\n",
      "Iteration 75, loss = 0.14905710\n",
      "Iteration 76, loss = 0.14735166\n",
      "Iteration 77, loss = 0.14674836\n",
      "Iteration 78, loss = 0.14485147\n",
      "Iteration 79, loss = 0.14370599\n",
      "Iteration 80, loss = 0.14189278\n",
      "Iteration 81, loss = 0.14093926\n",
      "Iteration 82, loss = 0.14006476\n",
      "Iteration 83, loss = 0.13828151\n",
      "Iteration 84, loss = 0.13709161\n",
      "Iteration 85, loss = 0.13574872\n",
      "Iteration 86, loss = 0.13412533\n",
      "Iteration 87, loss = 0.13328691\n",
      "Iteration 88, loss = 0.13158651\n",
      "Iteration 89, loss = 0.13026859\n",
      "Iteration 90, loss = 0.12947410\n",
      "Iteration 91, loss = 0.12784468\n",
      "Iteration 92, loss = 0.12692156\n",
      "Iteration 93, loss = 0.12529349\n",
      "Iteration 94, loss = 0.12420306\n",
      "Iteration 95, loss = 0.12296704\n",
      "Iteration 96, loss = 0.12183985\n",
      "Iteration 97, loss = 0.12048183\n",
      "Iteration 98, loss = 0.11945335\n",
      "Iteration 99, loss = 0.11811138\n",
      "Iteration 100, loss = 0.11759647\n",
      "Iteration 101, loss = 0.11640027\n",
      "Iteration 102, loss = 0.11482683\n",
      "Iteration 103, loss = 0.11411140\n",
      "Iteration 104, loss = 0.11228679\n",
      "Iteration 105, loss = 0.11184791\n",
      "Iteration 106, loss = 0.11036775\n",
      "Iteration 107, loss = 0.10980473\n",
      "Iteration 108, loss = 0.10834195\n",
      "Iteration 109, loss = 0.10701289\n",
      "Iteration 110, loss = 0.10638007\n",
      "Iteration 111, loss = 0.10504877\n",
      "Iteration 112, loss = 0.10409894\n",
      "Iteration 113, loss = 0.10296906\n",
      "Iteration 114, loss = 0.10200776\n",
      "Iteration 115, loss = 0.10115204\n",
      "Iteration 116, loss = 0.09962945\n",
      "Iteration 117, loss = 0.09935291\n",
      "Iteration 118, loss = 0.09779492\n",
      "Iteration 119, loss = 0.09689610\n",
      "Iteration 120, loss = 0.09597908\n",
      "Iteration 121, loss = 0.09497081\n",
      "Iteration 122, loss = 0.09375663\n",
      "Iteration 123, loss = 0.09277903\n",
      "Iteration 124, loss = 0.09184879\n",
      "Iteration 125, loss = 0.09141842\n",
      "Iteration 126, loss = 0.09037584\n",
      "Iteration 127, loss = 0.08947886\n",
      "Iteration 128, loss = 0.08844588\n",
      "Iteration 129, loss = 0.08728388\n",
      "Iteration 130, loss = 0.08665142\n",
      "Iteration 131, loss = 0.08575375\n",
      "Iteration 132, loss = 0.08541362\n",
      "Iteration 133, loss = 0.08415828\n",
      "Iteration 134, loss = 0.08296947\n",
      "Iteration 135, loss = 0.08249650\n",
      "Iteration 136, loss = 0.08106192\n",
      "Iteration 137, loss = 0.08073706\n",
      "Iteration 138, loss = 0.07954578\n",
      "Iteration 139, loss = 0.07907819\n",
      "Iteration 140, loss = 0.07794373\n",
      "Iteration 141, loss = 0.07684916\n",
      "Iteration 142, loss = 0.07635764\n",
      "Iteration 143, loss = 0.07563639\n",
      "Iteration 144, loss = 0.07463840\n",
      "Iteration 145, loss = 0.07399203\n",
      "Iteration 146, loss = 0.07279912\n",
      "Iteration 147, loss = 0.07257920\n",
      "Iteration 148, loss = 0.07126854\n",
      "Iteration 149, loss = 0.07087961\n",
      "Iteration 150, loss = 0.06964107\n",
      "Iteration 151, loss = 0.06951104\n",
      "Iteration 152, loss = 0.06823607\n",
      "Iteration 153, loss = 0.06767617\n",
      "Iteration 154, loss = 0.06672280\n",
      "Iteration 155, loss = 0.06622125\n",
      "Iteration 156, loss = 0.06531862\n",
      "Iteration 157, loss = 0.06484910\n",
      "Iteration 158, loss = 0.06429345\n",
      "Iteration 159, loss = 0.06309464\n",
      "Iteration 160, loss = 0.06266881\n",
      "Iteration 161, loss = 0.06196689\n",
      "Iteration 162, loss = 0.06125029\n",
      "Iteration 163, loss = 0.06035011\n",
      "Iteration 164, loss = 0.05981035\n",
      "Iteration 165, loss = 0.05903437\n",
      "Iteration 166, loss = 0.05854965\n",
      "Iteration 167, loss = 0.05808908\n",
      "Iteration 168, loss = 0.05725068\n",
      "Iteration 169, loss = 0.05683533\n",
      "Iteration 170, loss = 0.05649539\n",
      "Iteration 171, loss = 0.05515384\n",
      "Iteration 172, loss = 0.05477337\n",
      "Iteration 173, loss = 0.05375723\n",
      "Iteration 174, loss = 0.05316024\n",
      "Iteration 175, loss = 0.05246565\n",
      "Iteration 176, loss = 0.05228293\n",
      "Iteration 177, loss = 0.05134149\n",
      "Iteration 178, loss = 0.05080468\n",
      "Iteration 179, loss = 0.05012356\n",
      "Iteration 180, loss = 0.04963608\n",
      "Iteration 181, loss = 0.04900272\n",
      "Iteration 182, loss = 0.04854326\n",
      "Iteration 183, loss = 0.04795328\n",
      "Iteration 184, loss = 0.04702953\n",
      "Iteration 185, loss = 0.04684833\n",
      "Iteration 186, loss = 0.04641794\n",
      "Iteration 187, loss = 0.04584967\n",
      "Iteration 188, loss = 0.04478240\n",
      "Iteration 189, loss = 0.04441940\n",
      "Iteration 190, loss = 0.04394259\n",
      "Iteration 191, loss = 0.04334058\n",
      "Iteration 192, loss = 0.04301630\n",
      "Iteration 193, loss = 0.04263042\n",
      "Iteration 194, loss = 0.04196473\n",
      "Iteration 195, loss = 0.04151096\n",
      "Iteration 196, loss = 0.04063460\n",
      "Iteration 197, loss = 0.04048081\n",
      "Iteration 198, loss = 0.03994209\n",
      "Iteration 199, loss = 0.03941559\n",
      "Iteration 200, loss = 0.03890171\n",
      "Iteration 1, loss = 0.67894448\n",
      "Iteration 2, loss = 0.53532958\n",
      "Iteration 3, loss = 0.44792547\n",
      "Iteration 4, loss = 0.38892182\n",
      "Iteration 5, loss = 0.34700180\n",
      "Iteration 6, loss = 0.31721224\n",
      "Iteration 7, loss = 0.29626722\n",
      "Iteration 8, loss = 0.28083092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.26976174\n",
      "Iteration 10, loss = 0.26151261\n",
      "Iteration 11, loss = 0.25484240\n",
      "Iteration 12, loss = 0.24956643\n",
      "Iteration 13, loss = 0.24559034\n",
      "Iteration 14, loss = 0.24196410\n",
      "Iteration 15, loss = 0.23884019\n",
      "Iteration 16, loss = 0.23641547\n",
      "Iteration 17, loss = 0.23446217\n",
      "Iteration 18, loss = 0.23213130\n",
      "Iteration 19, loss = 0.23031198\n",
      "Iteration 20, loss = 0.22847182\n",
      "Iteration 21, loss = 0.22704066\n",
      "Iteration 22, loss = 0.22583203\n",
      "Iteration 23, loss = 0.22426598\n",
      "Iteration 24, loss = 0.22286318\n",
      "Iteration 25, loss = 0.22156565\n",
      "Iteration 26, loss = 0.22033301\n",
      "Iteration 27, loss = 0.21911225\n",
      "Iteration 28, loss = 0.21815091\n",
      "Iteration 29, loss = 0.21660283\n",
      "Iteration 30, loss = 0.21551220\n",
      "Iteration 31, loss = 0.21413305\n",
      "Iteration 32, loss = 0.21285250\n",
      "Iteration 33, loss = 0.21170516\n",
      "Iteration 34, loss = 0.21093214\n",
      "Iteration 35, loss = 0.20966651\n",
      "Iteration 36, loss = 0.20837783\n",
      "Iteration 37, loss = 0.20659657\n",
      "Iteration 38, loss = 0.20507906\n",
      "Iteration 39, loss = 0.20408027\n",
      "Iteration 40, loss = 0.20316906\n",
      "Iteration 41, loss = 0.20173217\n",
      "Iteration 42, loss = 0.20031182\n",
      "Iteration 43, loss = 0.19862226\n",
      "Iteration 44, loss = 0.19743670\n",
      "Iteration 45, loss = 0.19628688\n",
      "Iteration 46, loss = 0.19529239\n",
      "Iteration 47, loss = 0.19335734\n",
      "Iteration 48, loss = 0.19212917\n",
      "Iteration 49, loss = 0.19115343\n",
      "Iteration 50, loss = 0.18863068\n",
      "Iteration 51, loss = 0.18788148\n",
      "Iteration 52, loss = 0.18632811\n",
      "Iteration 53, loss = 0.18456413\n",
      "Iteration 54, loss = 0.18378386\n",
      "Iteration 55, loss = 0.18215420\n",
      "Iteration 56, loss = 0.17996404\n",
      "Iteration 57, loss = 0.17932764\n",
      "Iteration 58, loss = 0.17866948\n",
      "Iteration 59, loss = 0.17642955\n",
      "Iteration 60, loss = 0.17509764\n",
      "Iteration 61, loss = 0.17308070\n",
      "Iteration 62, loss = 0.17181517\n",
      "Iteration 63, loss = 0.17047169\n",
      "Iteration 64, loss = 0.16870099\n",
      "Iteration 65, loss = 0.16724116\n",
      "Iteration 66, loss = 0.16614538\n",
      "Iteration 67, loss = 0.16443073\n",
      "Iteration 68, loss = 0.16329644\n",
      "Iteration 69, loss = 0.16175020\n",
      "Iteration 70, loss = 0.16038673\n",
      "Iteration 71, loss = 0.15878582\n",
      "Iteration 72, loss = 0.15773815\n",
      "Iteration 73, loss = 0.15599359\n",
      "Iteration 74, loss = 0.15443299\n",
      "Iteration 75, loss = 0.15374482\n",
      "Iteration 76, loss = 0.15200701\n",
      "Iteration 77, loss = 0.15075492\n",
      "Iteration 78, loss = 0.14945626\n",
      "Iteration 79, loss = 0.14820810\n",
      "Iteration 80, loss = 0.14642074\n",
      "Iteration 81, loss = 0.14551040\n",
      "Iteration 82, loss = 0.14463239\n",
      "Iteration 83, loss = 0.14289117\n",
      "Iteration 84, loss = 0.14202309\n",
      "Iteration 85, loss = 0.14054200\n",
      "Iteration 86, loss = 0.13871030\n",
      "Iteration 87, loss = 0.13794297\n",
      "Iteration 88, loss = 0.13640200\n",
      "Iteration 89, loss = 0.13549069\n",
      "Iteration 90, loss = 0.13435825\n",
      "Iteration 91, loss = 0.13244274\n",
      "Iteration 92, loss = 0.13126146\n",
      "Iteration 93, loss = 0.13031470\n",
      "Iteration 94, loss = 0.12909291\n",
      "Iteration 95, loss = 0.12782191\n",
      "Iteration 96, loss = 0.12683306\n",
      "Iteration 97, loss = 0.12519411\n",
      "Iteration 98, loss = 0.12436358\n",
      "Iteration 99, loss = 0.12342160\n",
      "Iteration 100, loss = 0.12233386\n",
      "Iteration 101, loss = 0.12106114\n",
      "Iteration 102, loss = 0.11962309\n",
      "Iteration 103, loss = 0.11895802\n",
      "Iteration 104, loss = 0.11727067\n",
      "Iteration 105, loss = 0.11676296\n",
      "Iteration 106, loss = 0.11556260\n",
      "Iteration 107, loss = 0.11451937\n",
      "Iteration 108, loss = 0.11322105\n",
      "Iteration 109, loss = 0.11210835\n",
      "Iteration 110, loss = 0.11121303\n",
      "Iteration 111, loss = 0.10985187\n",
      "Iteration 112, loss = 0.10875719\n",
      "Iteration 113, loss = 0.10812541\n",
      "Iteration 114, loss = 0.10724013\n",
      "Iteration 115, loss = 0.10608366\n",
      "Iteration 116, loss = 0.10463726\n",
      "Iteration 117, loss = 0.10385348\n",
      "Iteration 118, loss = 0.10284047\n",
      "Iteration 119, loss = 0.10198111\n",
      "Iteration 120, loss = 0.10125484\n",
      "Iteration 121, loss = 0.09990793\n",
      "Iteration 122, loss = 0.09870838\n",
      "Iteration 123, loss = 0.09793570\n",
      "Iteration 124, loss = 0.09675032\n",
      "Iteration 125, loss = 0.09686490\n",
      "Iteration 126, loss = 0.09509342\n",
      "Iteration 127, loss = 0.09406963\n",
      "Iteration 128, loss = 0.09317118\n",
      "Iteration 129, loss = 0.09223352\n",
      "Iteration 130, loss = 0.09127254\n",
      "Iteration 131, loss = 0.09021647\n",
      "Iteration 132, loss = 0.08963106\n",
      "Iteration 133, loss = 0.08915024\n",
      "Iteration 134, loss = 0.08777814\n",
      "Iteration 135, loss = 0.08721683\n",
      "Iteration 136, loss = 0.08583578\n",
      "Iteration 137, loss = 0.08529811\n",
      "Iteration 138, loss = 0.08428422\n",
      "Iteration 139, loss = 0.08354293\n",
      "Iteration 140, loss = 0.08221582\n",
      "Iteration 141, loss = 0.08202076\n",
      "Iteration 142, loss = 0.08106836\n",
      "Iteration 143, loss = 0.08068342\n",
      "Iteration 144, loss = 0.07939988\n",
      "Iteration 145, loss = 0.07841968\n",
      "Iteration 146, loss = 0.07752114\n",
      "Iteration 147, loss = 0.07699284\n",
      "Iteration 148, loss = 0.07631889\n",
      "Iteration 149, loss = 0.07559939\n",
      "Iteration 150, loss = 0.07435594\n",
      "Iteration 151, loss = 0.07379533\n",
      "Iteration 152, loss = 0.07297797\n",
      "Iteration 153, loss = 0.07206687\n",
      "Iteration 154, loss = 0.07143030\n",
      "Iteration 155, loss = 0.07078818\n",
      "Iteration 156, loss = 0.06994503\n",
      "Iteration 157, loss = 0.06912067\n",
      "Iteration 158, loss = 0.06835553\n",
      "Iteration 159, loss = 0.06814671\n",
      "Iteration 160, loss = 0.06682196\n",
      "Iteration 161, loss = 0.06622124\n",
      "Iteration 162, loss = 0.06562515\n",
      "Iteration 163, loss = 0.06492736\n",
      "Iteration 164, loss = 0.06436559\n",
      "Iteration 165, loss = 0.06348228\n",
      "Iteration 166, loss = 0.06246994\n",
      "Iteration 167, loss = 0.06207735\n",
      "Iteration 168, loss = 0.06143920\n",
      "Iteration 169, loss = 0.06128724\n",
      "Iteration 170, loss = 0.06040570\n",
      "Iteration 171, loss = 0.05944974\n",
      "Iteration 172, loss = 0.05894719\n",
      "Iteration 173, loss = 0.05820792\n",
      "Iteration 174, loss = 0.05749494\n",
      "Iteration 175, loss = 0.05676899\n",
      "Iteration 176, loss = 0.05628058\n",
      "Iteration 177, loss = 0.05546137\n",
      "Iteration 178, loss = 0.05520733\n",
      "Iteration 179, loss = 0.05451664\n",
      "Iteration 180, loss = 0.05393058\n",
      "Iteration 181, loss = 0.05333400\n",
      "Iteration 182, loss = 0.05288777\n",
      "Iteration 183, loss = 0.05214009\n",
      "Iteration 184, loss = 0.05143804\n",
      "Iteration 185, loss = 0.05114253\n",
      "Iteration 186, loss = 0.05104475\n",
      "Iteration 187, loss = 0.05027120\n",
      "Iteration 188, loss = 0.04893817\n",
      "Iteration 189, loss = 0.04865461\n",
      "Iteration 190, loss = 0.04794427\n",
      "Iteration 191, loss = 0.04785636\n",
      "Iteration 192, loss = 0.04718255\n",
      "Iteration 193, loss = 0.04678287\n",
      "Iteration 194, loss = 0.04635807\n",
      "Iteration 195, loss = 0.04560382\n",
      "Iteration 196, loss = 0.04471136\n",
      "Iteration 197, loss = 0.04466676\n",
      "Iteration 198, loss = 0.04385336\n",
      "Iteration 199, loss = 0.04341022\n",
      "Iteration 200, loss = 0.04295871\n",
      "Iteration 1, loss = 0.67862104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53472209\n",
      "Iteration 3, loss = 0.44769817\n",
      "Iteration 4, loss = 0.38983499\n",
      "Iteration 5, loss = 0.34843763\n",
      "Iteration 6, loss = 0.31913001\n",
      "Iteration 7, loss = 0.29845484\n",
      "Iteration 8, loss = 0.28390955\n",
      "Iteration 9, loss = 0.27236935\n",
      "Iteration 10, loss = 0.26428791\n",
      "Iteration 11, loss = 0.25817512\n",
      "Iteration 12, loss = 0.25291902\n",
      "Iteration 13, loss = 0.24947717\n",
      "Iteration 14, loss = 0.24594272\n",
      "Iteration 15, loss = 0.24285228\n",
      "Iteration 16, loss = 0.24040914\n",
      "Iteration 17, loss = 0.23890371\n",
      "Iteration 18, loss = 0.23653917\n",
      "Iteration 19, loss = 0.23451458\n",
      "Iteration 20, loss = 0.23279043\n",
      "Iteration 21, loss = 0.23161503\n",
      "Iteration 22, loss = 0.23068591\n",
      "Iteration 23, loss = 0.22884572\n",
      "Iteration 24, loss = 0.22700570\n",
      "Iteration 25, loss = 0.22583812\n",
      "Iteration 26, loss = 0.22477697\n",
      "Iteration 27, loss = 0.22322679\n",
      "Iteration 28, loss = 0.22233032\n",
      "Iteration 29, loss = 0.22058559\n",
      "Iteration 30, loss = 0.21920054\n",
      "Iteration 31, loss = 0.21802727\n",
      "Iteration 32, loss = 0.21725975\n",
      "Iteration 33, loss = 0.21573771\n",
      "Iteration 34, loss = 0.21463396\n",
      "Iteration 35, loss = 0.21312713\n",
      "Iteration 36, loss = 0.21227720\n",
      "Iteration 37, loss = 0.21074899\n",
      "Iteration 38, loss = 0.20927013\n",
      "Iteration 39, loss = 0.20768527\n",
      "Iteration 40, loss = 0.20679692\n",
      "Iteration 41, loss = 0.20553175\n",
      "Iteration 42, loss = 0.20402740\n",
      "Iteration 43, loss = 0.20255760\n",
      "Iteration 44, loss = 0.20118942\n",
      "Iteration 45, loss = 0.19974075\n",
      "Iteration 46, loss = 0.19844735\n",
      "Iteration 47, loss = 0.19661429\n",
      "Iteration 48, loss = 0.19542479\n",
      "Iteration 49, loss = 0.19408072\n",
      "Iteration 50, loss = 0.19266922\n",
      "Iteration 51, loss = 0.19139666\n",
      "Iteration 52, loss = 0.18985368\n",
      "Iteration 53, loss = 0.18805016\n",
      "Iteration 54, loss = 0.18729723\n",
      "Iteration 55, loss = 0.18505645\n",
      "Iteration 56, loss = 0.18330418\n",
      "Iteration 57, loss = 0.18275551\n",
      "Iteration 58, loss = 0.18080273\n",
      "Iteration 59, loss = 0.17940474\n",
      "Iteration 60, loss = 0.17793910\n",
      "Iteration 61, loss = 0.17637302\n",
      "Iteration 62, loss = 0.17465273\n",
      "Iteration 63, loss = 0.17336280\n",
      "Iteration 64, loss = 0.17174482\n",
      "Iteration 65, loss = 0.17041998\n",
      "Iteration 66, loss = 0.16915096\n",
      "Iteration 67, loss = 0.16761130\n",
      "Iteration 68, loss = 0.16654921\n",
      "Iteration 69, loss = 0.16475885\n",
      "Iteration 70, loss = 0.16311781\n",
      "Iteration 71, loss = 0.16190064\n",
      "Iteration 72, loss = 0.16027585\n",
      "Iteration 73, loss = 0.15879940\n",
      "Iteration 74, loss = 0.15726120\n",
      "Iteration 75, loss = 0.15599216\n",
      "Iteration 76, loss = 0.15429708\n",
      "Iteration 77, loss = 0.15292070\n",
      "Iteration 78, loss = 0.15174636\n",
      "Iteration 79, loss = 0.14981786\n",
      "Iteration 80, loss = 0.14847582\n",
      "Iteration 81, loss = 0.14712216\n",
      "Iteration 82, loss = 0.14617342\n",
      "Iteration 83, loss = 0.14450989\n",
      "Iteration 84, loss = 0.14310025\n",
      "Iteration 85, loss = 0.14181706\n",
      "Iteration 86, loss = 0.14003232\n",
      "Iteration 87, loss = 0.13893089\n",
      "Iteration 88, loss = 0.13832971\n",
      "Iteration 89, loss = 0.13733546\n",
      "Iteration 90, loss = 0.13546560\n",
      "Iteration 91, loss = 0.13363243\n",
      "Iteration 92, loss = 0.13245638\n",
      "Iteration 93, loss = 0.13113192\n",
      "Iteration 94, loss = 0.12948276\n",
      "Iteration 95, loss = 0.12859732\n",
      "Iteration 96, loss = 0.12711674\n",
      "Iteration 97, loss = 0.12592877\n",
      "Iteration 98, loss = 0.12471627\n",
      "Iteration 99, loss = 0.12368765\n",
      "Iteration 100, loss = 0.12236491\n",
      "Iteration 101, loss = 0.12154921\n",
      "Iteration 102, loss = 0.11978787\n",
      "Iteration 103, loss = 0.11879899\n",
      "Iteration 104, loss = 0.11756339\n",
      "Iteration 105, loss = 0.11613018\n",
      "Iteration 106, loss = 0.11530163\n",
      "Iteration 107, loss = 0.11449330\n",
      "Iteration 108, loss = 0.11276510\n",
      "Iteration 109, loss = 0.11188351\n",
      "Iteration 110, loss = 0.11054937\n",
      "Iteration 111, loss = 0.10967047\n",
      "Iteration 112, loss = 0.10889141\n",
      "Iteration 113, loss = 0.10729318\n",
      "Iteration 114, loss = 0.10678386\n",
      "Iteration 115, loss = 0.10491359\n",
      "Iteration 116, loss = 0.10362146\n",
      "Iteration 117, loss = 0.10274808\n",
      "Iteration 118, loss = 0.10160430\n",
      "Iteration 119, loss = 0.10083682\n",
      "Iteration 120, loss = 0.09989697\n",
      "Iteration 121, loss = 0.09879450\n",
      "Iteration 122, loss = 0.09779192\n",
      "Iteration 123, loss = 0.09668330\n",
      "Iteration 124, loss = 0.09563706\n",
      "Iteration 125, loss = 0.09569410\n",
      "Iteration 126, loss = 0.09379344\n",
      "Iteration 127, loss = 0.09283407\n",
      "Iteration 128, loss = 0.09186972\n",
      "Iteration 129, loss = 0.09064776\n",
      "Iteration 130, loss = 0.08959753\n",
      "Iteration 131, loss = 0.08887150\n",
      "Iteration 132, loss = 0.08769581\n",
      "Iteration 133, loss = 0.08719657\n",
      "Iteration 134, loss = 0.08638502\n",
      "Iteration 135, loss = 0.08516111\n",
      "Iteration 136, loss = 0.08426922\n",
      "Iteration 137, loss = 0.08344164\n",
      "Iteration 138, loss = 0.08318113\n",
      "Iteration 139, loss = 0.08170772\n",
      "Iteration 140, loss = 0.08071850\n",
      "Iteration 141, loss = 0.08022275\n",
      "Iteration 142, loss = 0.07936088\n",
      "Iteration 143, loss = 0.07882534\n",
      "Iteration 144, loss = 0.07752383\n",
      "Iteration 145, loss = 0.07710032\n",
      "Iteration 146, loss = 0.07570496\n",
      "Iteration 147, loss = 0.07483096\n",
      "Iteration 148, loss = 0.07418031\n",
      "Iteration 149, loss = 0.07376558\n",
      "Iteration 150, loss = 0.07296324\n",
      "Iteration 151, loss = 0.07178130\n",
      "Iteration 152, loss = 0.07125984\n",
      "Iteration 153, loss = 0.07013941\n",
      "Iteration 154, loss = 0.06943564\n",
      "Iteration 155, loss = 0.06939095\n",
      "Iteration 156, loss = 0.06855156\n",
      "Iteration 157, loss = 0.06715438\n",
      "Iteration 158, loss = 0.06699379\n",
      "Iteration 159, loss = 0.06653246\n",
      "Iteration 160, loss = 0.06509656\n",
      "Iteration 161, loss = 0.06443635\n",
      "Iteration 162, loss = 0.06373620\n",
      "Iteration 163, loss = 0.06326265\n",
      "Iteration 164, loss = 0.06256656\n",
      "Iteration 165, loss = 0.06154004\n",
      "Iteration 166, loss = 0.06085444\n",
      "Iteration 167, loss = 0.06012850\n",
      "Iteration 168, loss = 0.05944435\n",
      "Iteration 169, loss = 0.05895927\n",
      "Iteration 170, loss = 0.05804612\n",
      "Iteration 171, loss = 0.05750608\n",
      "Iteration 172, loss = 0.05696435\n",
      "Iteration 173, loss = 0.05665373\n",
      "Iteration 174, loss = 0.05553490\n",
      "Iteration 175, loss = 0.05496671\n",
      "Iteration 176, loss = 0.05440415\n",
      "Iteration 177, loss = 0.05367380\n",
      "Iteration 178, loss = 0.05352184\n",
      "Iteration 179, loss = 0.05248350\n",
      "Iteration 180, loss = 0.05199589\n",
      "Iteration 181, loss = 0.05150953\n",
      "Iteration 182, loss = 0.05076198\n",
      "Iteration 183, loss = 0.05023364\n",
      "Iteration 184, loss = 0.04993243\n",
      "Iteration 185, loss = 0.04926904\n",
      "Iteration 186, loss = 0.04891139\n",
      "Iteration 187, loss = 0.04826866\n",
      "Iteration 188, loss = 0.04735795\n",
      "Iteration 189, loss = 0.04679216\n",
      "Iteration 190, loss = 0.04617532\n",
      "Iteration 191, loss = 0.04582340\n",
      "Iteration 192, loss = 0.04536191\n",
      "Iteration 193, loss = 0.04465312\n",
      "Iteration 194, loss = 0.04438260\n",
      "Iteration 195, loss = 0.04366132\n",
      "Iteration 196, loss = 0.04317043\n",
      "Iteration 197, loss = 0.04265954\n",
      "Iteration 198, loss = 0.04197334\n",
      "Iteration 199, loss = 0.04160303\n",
      "Iteration 200, loss = 0.04120251\n",
      "Iteration 1, loss = 0.68192961\n",
      "Iteration 2, loss = 0.53865094\n",
      "Iteration 3, loss = 0.45181573\n",
      "Iteration 4, loss = 0.39188090\n",
      "Iteration 5, loss = 0.35048724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.32024749\n",
      "Iteration 7, loss = 0.29884730\n",
      "Iteration 8, loss = 0.28330087\n",
      "Iteration 9, loss = 0.27243105\n",
      "Iteration 10, loss = 0.26377304\n",
      "Iteration 11, loss = 0.25792210\n",
      "Iteration 12, loss = 0.25201060\n",
      "Iteration 13, loss = 0.24764647\n",
      "Iteration 14, loss = 0.24406972\n",
      "Iteration 15, loss = 0.24069360\n",
      "Iteration 16, loss = 0.23790375\n",
      "Iteration 17, loss = 0.23569326\n",
      "Iteration 18, loss = 0.23351999\n",
      "Iteration 19, loss = 0.23170712\n",
      "Iteration 20, loss = 0.22998032\n",
      "Iteration 21, loss = 0.22893436\n",
      "Iteration 22, loss = 0.22661897\n",
      "Iteration 23, loss = 0.22543519\n",
      "Iteration 24, loss = 0.22384627\n",
      "Iteration 25, loss = 0.22294674\n",
      "Iteration 26, loss = 0.22119569\n",
      "Iteration 27, loss = 0.22065811\n",
      "Iteration 28, loss = 0.21915496\n",
      "Iteration 29, loss = 0.21792941\n",
      "Iteration 30, loss = 0.21642980\n",
      "Iteration 31, loss = 0.21531926\n",
      "Iteration 32, loss = 0.21400483\n",
      "Iteration 33, loss = 0.21252567\n",
      "Iteration 34, loss = 0.21168123\n",
      "Iteration 35, loss = 0.21022270\n",
      "Iteration 36, loss = 0.20854987\n",
      "Iteration 37, loss = 0.20751904\n",
      "Iteration 38, loss = 0.20608717\n",
      "Iteration 39, loss = 0.20491826\n",
      "Iteration 40, loss = 0.20398731\n",
      "Iteration 41, loss = 0.20210521\n",
      "Iteration 42, loss = 0.20103284\n",
      "Iteration 43, loss = 0.19986103\n",
      "Iteration 44, loss = 0.19869328\n",
      "Iteration 45, loss = 0.19637826\n",
      "Iteration 46, loss = 0.19557052\n",
      "Iteration 47, loss = 0.19371964\n",
      "Iteration 48, loss = 0.19228537\n",
      "Iteration 49, loss = 0.19103575\n",
      "Iteration 50, loss = 0.19006914\n",
      "Iteration 51, loss = 0.18834827\n",
      "Iteration 52, loss = 0.18662667\n",
      "Iteration 53, loss = 0.18527775\n",
      "Iteration 54, loss = 0.18402456\n",
      "Iteration 55, loss = 0.18160272\n",
      "Iteration 56, loss = 0.18057417\n",
      "Iteration 57, loss = 0.17905254\n",
      "Iteration 58, loss = 0.17763853\n",
      "Iteration 59, loss = 0.17622401\n",
      "Iteration 60, loss = 0.17444706\n",
      "Iteration 61, loss = 0.17370753\n",
      "Iteration 62, loss = 0.17204565\n",
      "Iteration 63, loss = 0.17052351\n",
      "Iteration 64, loss = 0.16910455\n",
      "Iteration 65, loss = 0.16793947\n",
      "Iteration 66, loss = 0.16541762\n",
      "Iteration 67, loss = 0.16427136\n",
      "Iteration 68, loss = 0.16273604\n",
      "Iteration 69, loss = 0.16132985\n",
      "Iteration 70, loss = 0.15979867\n",
      "Iteration 71, loss = 0.15878436\n",
      "Iteration 72, loss = 0.15740549\n",
      "Iteration 73, loss = 0.15571802\n",
      "Iteration 74, loss = 0.15384016\n",
      "Iteration 75, loss = 0.15331912\n",
      "Iteration 76, loss = 0.15196491\n",
      "Iteration 77, loss = 0.15014969\n",
      "Iteration 78, loss = 0.14907905\n",
      "Iteration 79, loss = 0.14774008\n",
      "Iteration 80, loss = 0.14601719\n",
      "Iteration 81, loss = 0.14526708\n",
      "Iteration 82, loss = 0.14479709\n",
      "Iteration 83, loss = 0.14266258\n",
      "Iteration 84, loss = 0.14129517\n",
      "Iteration 85, loss = 0.13951827\n",
      "Iteration 86, loss = 0.13888556\n",
      "Iteration 87, loss = 0.13709207\n",
      "Iteration 88, loss = 0.13568973\n",
      "Iteration 89, loss = 0.13474353\n",
      "Iteration 90, loss = 0.13402952\n",
      "Iteration 91, loss = 0.13189786\n",
      "Iteration 92, loss = 0.13108726\n",
      "Iteration 93, loss = 0.12963222\n",
      "Iteration 94, loss = 0.12829684\n",
      "Iteration 95, loss = 0.12732068\n",
      "Iteration 96, loss = 0.12683932\n",
      "Iteration 97, loss = 0.12518469\n",
      "Iteration 98, loss = 0.12366067\n",
      "Iteration 99, loss = 0.12287993\n",
      "Iteration 100, loss = 0.12179633\n",
      "Iteration 101, loss = 0.12090980\n",
      "Iteration 102, loss = 0.11965766\n",
      "Iteration 103, loss = 0.11865260\n",
      "Iteration 104, loss = 0.11719138\n",
      "Iteration 105, loss = 0.11619380\n",
      "Iteration 106, loss = 0.11481151\n",
      "Iteration 107, loss = 0.11404281\n",
      "Iteration 108, loss = 0.11278891\n",
      "Iteration 109, loss = 0.11222547\n",
      "Iteration 110, loss = 0.11068647\n",
      "Iteration 111, loss = 0.11003468\n",
      "Iteration 112, loss = 0.10885360\n",
      "Iteration 113, loss = 0.10773763\n",
      "Iteration 114, loss = 0.10736369\n",
      "Iteration 115, loss = 0.10563096\n",
      "Iteration 116, loss = 0.10469756\n",
      "Iteration 117, loss = 0.10327007\n",
      "Iteration 118, loss = 0.10246894\n",
      "Iteration 119, loss = 0.10188732\n",
      "Iteration 120, loss = 0.10090546\n",
      "Iteration 121, loss = 0.09972986\n",
      "Iteration 122, loss = 0.09888351\n",
      "Iteration 123, loss = 0.09772181\n",
      "Iteration 124, loss = 0.09717335\n",
      "Iteration 125, loss = 0.09629585\n",
      "Iteration 126, loss = 0.09516971\n",
      "Iteration 127, loss = 0.09453496\n",
      "Iteration 128, loss = 0.09352077\n",
      "Iteration 129, loss = 0.09249245\n",
      "Iteration 130, loss = 0.09234600\n",
      "Iteration 131, loss = 0.09064546\n",
      "Iteration 132, loss = 0.09001682\n",
      "Iteration 133, loss = 0.08853136\n",
      "Iteration 134, loss = 0.08836177\n",
      "Iteration 135, loss = 0.08671229\n",
      "Iteration 136, loss = 0.08598513\n",
      "Iteration 137, loss = 0.08563862\n",
      "Iteration 138, loss = 0.08469110\n",
      "Iteration 139, loss = 0.08332403\n",
      "Iteration 140, loss = 0.08290850\n",
      "Iteration 141, loss = 0.08210826\n",
      "Iteration 142, loss = 0.08100395\n",
      "Iteration 143, loss = 0.08054927\n",
      "Iteration 144, loss = 0.07978843\n",
      "Iteration 145, loss = 0.07915754\n",
      "Iteration 146, loss = 0.07804289\n",
      "Iteration 147, loss = 0.07781838\n",
      "Iteration 148, loss = 0.07654628\n",
      "Iteration 149, loss = 0.07551152\n",
      "Iteration 150, loss = 0.07482456\n",
      "Iteration 151, loss = 0.07408280\n",
      "Iteration 152, loss = 0.07376589\n",
      "Iteration 153, loss = 0.07290290\n",
      "Iteration 154, loss = 0.07204172\n",
      "Iteration 155, loss = 0.07119484\n",
      "Iteration 156, loss = 0.07020711\n",
      "Iteration 157, loss = 0.06987728\n",
      "Iteration 158, loss = 0.06879228\n",
      "Iteration 159, loss = 0.06827740\n",
      "Iteration 160, loss = 0.06737243\n",
      "Iteration 161, loss = 0.06681581\n",
      "Iteration 162, loss = 0.06645089\n",
      "Iteration 163, loss = 0.06526716\n",
      "Iteration 164, loss = 0.06474866\n",
      "Iteration 165, loss = 0.06424369\n",
      "Iteration 166, loss = 0.06339956\n",
      "Iteration 167, loss = 0.06268592\n",
      "Iteration 168, loss = 0.06219919\n",
      "Iteration 169, loss = 0.06131262\n",
      "Iteration 170, loss = 0.06075921\n",
      "Iteration 171, loss = 0.05983795\n",
      "Iteration 172, loss = 0.05940373\n",
      "Iteration 173, loss = 0.05849893\n",
      "Iteration 174, loss = 0.05808967\n",
      "Iteration 175, loss = 0.05778317\n",
      "Iteration 176, loss = 0.05720926\n",
      "Iteration 177, loss = 0.05632667\n",
      "Iteration 178, loss = 0.05593599\n",
      "Iteration 179, loss = 0.05497611\n",
      "Iteration 180, loss = 0.05437733\n",
      "Iteration 181, loss = 0.05421357\n",
      "Iteration 182, loss = 0.05307762\n",
      "Iteration 183, loss = 0.05290453\n",
      "Iteration 184, loss = 0.05222943\n",
      "Iteration 185, loss = 0.05139913\n",
      "Iteration 186, loss = 0.05083787\n",
      "Iteration 187, loss = 0.05027402\n",
      "Iteration 188, loss = 0.04950949\n",
      "Iteration 189, loss = 0.04900829\n",
      "Iteration 190, loss = 0.04873242\n",
      "Iteration 191, loss = 0.04793772\n",
      "Iteration 192, loss = 0.04734998\n",
      "Iteration 193, loss = 0.04676328\n",
      "Iteration 194, loss = 0.04666046\n",
      "Iteration 195, loss = 0.04606215\n",
      "Iteration 196, loss = 0.04547104\n",
      "Iteration 197, loss = 0.04462709\n",
      "Iteration 198, loss = 0.04425067\n",
      "Iteration 199, loss = 0.04352084\n",
      "Iteration 200, loss = 0.04345022\n",
      "Iteration 1, loss = 0.68078591\n",
      "Iteration 2, loss = 0.53409506\n",
      "Iteration 3, loss = 0.44775951\n",
      "Iteration 4, loss = 0.38920063\n",
      "Iteration 5, loss = 0.34895555\n",
      "Iteration 6, loss = 0.31948677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.29931907\n",
      "Iteration 8, loss = 0.28410767\n",
      "Iteration 9, loss = 0.27403866\n",
      "Iteration 10, loss = 0.26626821\n",
      "Iteration 11, loss = 0.26073036\n",
      "Iteration 12, loss = 0.25530199\n",
      "Iteration 13, loss = 0.25142214\n",
      "Iteration 14, loss = 0.24761489\n",
      "Iteration 15, loss = 0.24471787\n",
      "Iteration 16, loss = 0.24234024\n",
      "Iteration 17, loss = 0.24015049\n",
      "Iteration 18, loss = 0.23835042\n",
      "Iteration 19, loss = 0.23631027\n",
      "Iteration 20, loss = 0.23475289\n",
      "Iteration 21, loss = 0.23361428\n",
      "Iteration 22, loss = 0.23203131\n",
      "Iteration 23, loss = 0.23091515\n",
      "Iteration 24, loss = 0.22921518\n",
      "Iteration 25, loss = 0.22808629\n",
      "Iteration 26, loss = 0.22681784\n",
      "Iteration 27, loss = 0.22604957\n",
      "Iteration 28, loss = 0.22427414\n",
      "Iteration 29, loss = 0.22399774\n",
      "Iteration 30, loss = 0.22256923\n",
      "Iteration 31, loss = 0.22085117\n",
      "Iteration 32, loss = 0.21957090\n",
      "Iteration 33, loss = 0.21802589\n",
      "Iteration 34, loss = 0.21739909\n",
      "Iteration 35, loss = 0.21603020\n",
      "Iteration 36, loss = 0.21432326\n",
      "Iteration 37, loss = 0.21361549\n",
      "Iteration 38, loss = 0.21161603\n",
      "Iteration 39, loss = 0.21051707\n",
      "Iteration 40, loss = 0.20917433\n",
      "Iteration 41, loss = 0.20740076\n",
      "Iteration 42, loss = 0.20663598\n",
      "Iteration 43, loss = 0.20525619\n",
      "Iteration 44, loss = 0.20405085\n",
      "Iteration 45, loss = 0.20212525\n",
      "Iteration 46, loss = 0.20071741\n",
      "Iteration 47, loss = 0.19921326\n",
      "Iteration 48, loss = 0.19761288\n",
      "Iteration 49, loss = 0.19638219\n",
      "Iteration 50, loss = 0.19537143\n",
      "Iteration 51, loss = 0.19422846\n",
      "Iteration 52, loss = 0.19199383\n",
      "Iteration 53, loss = 0.19047117\n",
      "Iteration 54, loss = 0.18899782\n",
      "Iteration 55, loss = 0.18701899\n",
      "Iteration 56, loss = 0.18546756\n",
      "Iteration 57, loss = 0.18430289\n",
      "Iteration 58, loss = 0.18276140\n",
      "Iteration 59, loss = 0.18124165\n",
      "Iteration 60, loss = 0.17933989\n",
      "Iteration 61, loss = 0.17835272\n",
      "Iteration 62, loss = 0.17654438\n",
      "Iteration 63, loss = 0.17478748\n",
      "Iteration 64, loss = 0.17325904\n",
      "Iteration 65, loss = 0.17229113\n",
      "Iteration 66, loss = 0.17021585\n",
      "Iteration 67, loss = 0.16864579\n",
      "Iteration 68, loss = 0.16673597\n",
      "Iteration 69, loss = 0.16572162\n",
      "Iteration 70, loss = 0.16439150\n",
      "Iteration 71, loss = 0.16309977\n",
      "Iteration 72, loss = 0.16077369\n",
      "Iteration 73, loss = 0.15951960\n",
      "Iteration 74, loss = 0.15818154\n",
      "Iteration 75, loss = 0.15653857\n",
      "Iteration 76, loss = 0.15509102\n",
      "Iteration 77, loss = 0.15372426\n",
      "Iteration 78, loss = 0.15262947\n",
      "Iteration 79, loss = 0.15040948\n",
      "Iteration 80, loss = 0.14976889\n",
      "Iteration 81, loss = 0.14835348\n",
      "Iteration 82, loss = 0.14703111\n",
      "Iteration 83, loss = 0.14494479\n",
      "Iteration 84, loss = 0.14378906\n",
      "Iteration 85, loss = 0.14174151\n",
      "Iteration 86, loss = 0.14139050\n",
      "Iteration 87, loss = 0.13961484\n",
      "Iteration 88, loss = 0.13820099\n",
      "Iteration 89, loss = 0.13636609\n",
      "Iteration 90, loss = 0.13566648\n",
      "Iteration 91, loss = 0.13404145\n",
      "Iteration 92, loss = 0.13243051\n",
      "Iteration 93, loss = 0.13121046\n",
      "Iteration 94, loss = 0.13012708\n",
      "Iteration 95, loss = 0.12957116\n",
      "Iteration 96, loss = 0.12851029\n",
      "Iteration 97, loss = 0.12615836\n",
      "Iteration 98, loss = 0.12499761\n",
      "Iteration 99, loss = 0.12397671\n",
      "Iteration 100, loss = 0.12287767\n",
      "Iteration 101, loss = 0.12241548\n",
      "Iteration 102, loss = 0.12085585\n",
      "Iteration 103, loss = 0.11911895\n",
      "Iteration 104, loss = 0.11790021\n",
      "Iteration 105, loss = 0.11713049\n",
      "Iteration 106, loss = 0.11542831\n",
      "Iteration 107, loss = 0.11426253\n",
      "Iteration 108, loss = 0.11331787\n",
      "Iteration 109, loss = 0.11247923\n",
      "Iteration 110, loss = 0.11136593\n",
      "Iteration 111, loss = 0.10980746\n",
      "Iteration 112, loss = 0.10885727\n",
      "Iteration 113, loss = 0.10774131\n",
      "Iteration 114, loss = 0.10696749\n",
      "Iteration 115, loss = 0.10580559\n",
      "Iteration 116, loss = 0.10468186\n",
      "Iteration 117, loss = 0.10333423\n",
      "Iteration 118, loss = 0.10217094\n",
      "Iteration 119, loss = 0.10152200\n",
      "Iteration 120, loss = 0.10049416\n",
      "Iteration 121, loss = 0.09971847\n",
      "Iteration 122, loss = 0.09870235\n",
      "Iteration 123, loss = 0.09758315\n",
      "Iteration 124, loss = 0.09655522\n",
      "Iteration 125, loss = 0.09555859\n",
      "Iteration 126, loss = 0.09507897\n",
      "Iteration 127, loss = 0.09385457\n",
      "Iteration 128, loss = 0.09285199\n",
      "Iteration 129, loss = 0.09204158\n",
      "Iteration 130, loss = 0.09142663\n",
      "Iteration 131, loss = 0.09000816\n",
      "Iteration 132, loss = 0.08919340\n",
      "Iteration 133, loss = 0.08797382\n",
      "Iteration 134, loss = 0.08750278\n",
      "Iteration 135, loss = 0.08610668\n",
      "Iteration 136, loss = 0.08552715\n",
      "Iteration 137, loss = 0.08477803\n",
      "Iteration 138, loss = 0.08408153\n",
      "Iteration 139, loss = 0.08279076\n",
      "Iteration 140, loss = 0.08248266\n",
      "Iteration 141, loss = 0.08117391\n",
      "Iteration 142, loss = 0.08053616\n",
      "Iteration 143, loss = 0.07955833\n",
      "Iteration 144, loss = 0.07882736\n",
      "Iteration 145, loss = 0.07827685\n",
      "Iteration 146, loss = 0.07743090\n",
      "Iteration 147, loss = 0.07695858\n",
      "Iteration 148, loss = 0.07593168\n",
      "Iteration 149, loss = 0.07485664\n",
      "Iteration 150, loss = 0.07464071\n",
      "Iteration 151, loss = 0.07351412\n",
      "Iteration 152, loss = 0.07308350\n",
      "Iteration 153, loss = 0.07162729\n",
      "Iteration 154, loss = 0.07121265\n",
      "Iteration 155, loss = 0.07041705\n",
      "Iteration 156, loss = 0.06972050\n",
      "Iteration 157, loss = 0.06909035\n",
      "Iteration 158, loss = 0.06822289\n",
      "Iteration 159, loss = 0.06751378\n",
      "Iteration 160, loss = 0.06653011\n",
      "Iteration 161, loss = 0.06610325\n",
      "Iteration 162, loss = 0.06544305\n",
      "Iteration 163, loss = 0.06438498\n",
      "Iteration 164, loss = 0.06395221\n",
      "Iteration 165, loss = 0.06340721\n",
      "Iteration 166, loss = 0.06263760\n",
      "Iteration 167, loss = 0.06196473\n",
      "Iteration 168, loss = 0.06219051\n",
      "Iteration 169, loss = 0.06048039\n",
      "Iteration 170, loss = 0.05991816\n",
      "Iteration 171, loss = 0.05932102\n",
      "Iteration 172, loss = 0.05885129\n",
      "Iteration 173, loss = 0.05820954\n",
      "Iteration 174, loss = 0.05733298\n",
      "Iteration 175, loss = 0.05694662\n",
      "Iteration 176, loss = 0.05658556\n",
      "Iteration 177, loss = 0.05572848\n",
      "Iteration 178, loss = 0.05497286\n",
      "Iteration 179, loss = 0.05474894\n",
      "Iteration 180, loss = 0.05383973\n",
      "Iteration 181, loss = 0.05324224\n",
      "Iteration 182, loss = 0.05255048\n",
      "Iteration 183, loss = 0.05212416\n",
      "Iteration 184, loss = 0.05122060\n",
      "Iteration 185, loss = 0.05080458\n",
      "Iteration 186, loss = 0.05013444\n",
      "Iteration 187, loss = 0.05004396\n",
      "Iteration 188, loss = 0.04895952\n",
      "Iteration 189, loss = 0.04840677\n",
      "Iteration 190, loss = 0.04820334\n",
      "Iteration 191, loss = 0.04774758\n",
      "Iteration 192, loss = 0.04691222\n",
      "Iteration 193, loss = 0.04642722\n",
      "Iteration 194, loss = 0.04612720\n",
      "Iteration 195, loss = 0.04550880\n",
      "Iteration 196, loss = 0.04480277\n",
      "Iteration 197, loss = 0.04429078\n",
      "Iteration 198, loss = 0.04390954\n",
      "Iteration 199, loss = 0.04323919\n",
      "Iteration 200, loss = 0.04265237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79551265\n",
      "Iteration 2, loss = 0.63890810\n",
      "Iteration 3, loss = 0.53380757\n",
      "Iteration 4, loss = 0.48073250\n",
      "Iteration 5, loss = 0.45325732\n",
      "Iteration 6, loss = 0.43586499\n",
      "Iteration 7, loss = 0.42382638\n",
      "Iteration 8, loss = 0.41427262\n",
      "Iteration 9, loss = 0.40614618\n",
      "Iteration 10, loss = 0.39900902\n",
      "Iteration 11, loss = 0.39258164\n",
      "Iteration 12, loss = 0.38657761\n",
      "Iteration 13, loss = 0.38105741\n",
      "Iteration 14, loss = 0.37582925\n",
      "Iteration 15, loss = 0.37086261\n",
      "Iteration 16, loss = 0.36631737\n",
      "Iteration 17, loss = 0.36193211\n",
      "Iteration 18, loss = 0.35775322\n",
      "Iteration 19, loss = 0.35378599\n",
      "Iteration 20, loss = 0.35011293\n",
      "Iteration 21, loss = 0.34653703\n",
      "Iteration 22, loss = 0.34312586\n",
      "Iteration 23, loss = 0.33990685\n",
      "Iteration 24, loss = 0.33680789\n",
      "Iteration 25, loss = 0.33385751\n",
      "Iteration 26, loss = 0.33103588\n",
      "Iteration 27, loss = 0.32834251\n",
      "Iteration 28, loss = 0.32574114\n",
      "Iteration 29, loss = 0.32329959\n",
      "Iteration 30, loss = 0.32089167\n",
      "Iteration 31, loss = 0.31861721\n",
      "Iteration 32, loss = 0.31642697\n",
      "Iteration 33, loss = 0.31432033\n",
      "Iteration 34, loss = 0.31229975\n",
      "Iteration 35, loss = 0.31033134\n",
      "Iteration 36, loss = 0.30850107\n",
      "Iteration 37, loss = 0.30662230\n",
      "Iteration 38, loss = 0.30489532\n",
      "Iteration 39, loss = 0.30320908\n",
      "Iteration 40, loss = 0.30157316\n",
      "Iteration 41, loss = 0.30001527\n",
      "Iteration 42, loss = 0.29850980\n",
      "Iteration 43, loss = 0.29699412\n",
      "Iteration 44, loss = 0.29562038\n",
      "Iteration 45, loss = 0.29421348\n",
      "Iteration 46, loss = 0.29290195\n",
      "Iteration 47, loss = 0.29159924\n",
      "Iteration 48, loss = 0.29033306\n",
      "Iteration 49, loss = 0.28911061\n",
      "Iteration 50, loss = 0.28790667\n",
      "Iteration 51, loss = 0.28676413\n",
      "Iteration 52, loss = 0.28564627\n",
      "Iteration 53, loss = 0.28454051\n",
      "Iteration 54, loss = 0.28347011\n",
      "Iteration 55, loss = 0.28242703\n",
      "Iteration 56, loss = 0.28143076\n",
      "Iteration 57, loss = 0.28045847\n",
      "Iteration 58, loss = 0.27947809\n",
      "Iteration 59, loss = 0.27853420\n",
      "Iteration 60, loss = 0.27764752\n",
      "Iteration 61, loss = 0.27673812\n",
      "Iteration 62, loss = 0.27586913\n",
      "Iteration 63, loss = 0.27501270\n",
      "Iteration 64, loss = 0.27417736\n",
      "Iteration 65, loss = 0.27337464\n",
      "Iteration 66, loss = 0.27258692\n",
      "Iteration 67, loss = 0.27179303\n",
      "Iteration 68, loss = 0.27104906\n",
      "Iteration 69, loss = 0.27030927\n",
      "Iteration 70, loss = 0.26958779\n",
      "Iteration 71, loss = 0.26886673\n",
      "Iteration 72, loss = 0.26817382\n",
      "Iteration 73, loss = 0.26748854\n",
      "Iteration 74, loss = 0.26680431\n",
      "Iteration 75, loss = 0.26616030\n",
      "Iteration 76, loss = 0.26552189\n",
      "Iteration 77, loss = 0.26489543\n",
      "Iteration 78, loss = 0.26425382\n",
      "Iteration 79, loss = 0.26365256\n",
      "Iteration 80, loss = 0.26305921\n",
      "Iteration 81, loss = 0.26246359\n",
      "Iteration 82, loss = 0.26190790\n",
      "Iteration 83, loss = 0.26129451\n",
      "Iteration 84, loss = 0.26073983\n",
      "Iteration 85, loss = 0.26018255\n",
      "Iteration 86, loss = 0.25965502\n",
      "Iteration 87, loss = 0.25911641\n",
      "Iteration 88, loss = 0.25858044\n",
      "Iteration 89, loss = 0.25805721\n",
      "Iteration 90, loss = 0.25755087\n",
      "Iteration 91, loss = 0.25703887\n",
      "Iteration 92, loss = 0.25655554\n",
      "Iteration 93, loss = 0.25604452\n",
      "Iteration 94, loss = 0.25556104\n",
      "Iteration 95, loss = 0.25508961\n",
      "Iteration 96, loss = 0.25460872\n",
      "Iteration 97, loss = 0.25414616\n",
      "Iteration 98, loss = 0.25367844\n",
      "Iteration 99, loss = 0.25322414\n",
      "Iteration 100, loss = 0.25280192\n",
      "Iteration 101, loss = 0.25235127\n",
      "Iteration 102, loss = 0.25188874\n",
      "Iteration 103, loss = 0.25146831\n",
      "Iteration 104, loss = 0.25104432\n",
      "Iteration 105, loss = 0.25060962\n",
      "Iteration 106, loss = 0.25018816\n",
      "Iteration 107, loss = 0.24977548\n",
      "Iteration 108, loss = 0.24935409\n",
      "Iteration 109, loss = 0.24895624\n",
      "Iteration 110, loss = 0.24856819\n",
      "Iteration 111, loss = 0.24814172\n",
      "Iteration 112, loss = 0.24774493\n",
      "Iteration 113, loss = 0.24735311\n",
      "Iteration 114, loss = 0.24694626\n",
      "Iteration 115, loss = 0.24657166\n",
      "Iteration 116, loss = 0.24619333\n",
      "Iteration 117, loss = 0.24581387\n",
      "Iteration 118, loss = 0.24541839\n",
      "Iteration 119, loss = 0.24505288\n",
      "Iteration 120, loss = 0.24467344\n",
      "Iteration 121, loss = 0.24430388\n",
      "Iteration 122, loss = 0.24394573\n",
      "Iteration 123, loss = 0.24356877\n",
      "Iteration 124, loss = 0.24320547\n",
      "Iteration 125, loss = 0.24284907\n",
      "Iteration 126, loss = 0.24249143\n",
      "Iteration 127, loss = 0.24214318\n",
      "Iteration 128, loss = 0.24178767\n",
      "Iteration 129, loss = 0.24144149\n",
      "Iteration 130, loss = 0.24109226\n",
      "Iteration 131, loss = 0.24074914\n",
      "Iteration 132, loss = 0.24040785\n",
      "Iteration 133, loss = 0.24006166\n",
      "Iteration 134, loss = 0.23972542\n",
      "Iteration 135, loss = 0.23937868\n",
      "Iteration 136, loss = 0.23907690\n",
      "Iteration 137, loss = 0.23871326\n",
      "Iteration 138, loss = 0.23838126\n",
      "Iteration 139, loss = 0.23805949\n",
      "Iteration 140, loss = 0.23772558\n",
      "Iteration 141, loss = 0.23738674\n",
      "Iteration 142, loss = 0.23707951\n",
      "Iteration 143, loss = 0.23675349\n",
      "Iteration 144, loss = 0.23642740\n",
      "Iteration 145, loss = 0.23611046\n",
      "Iteration 146, loss = 0.23579216\n",
      "Iteration 147, loss = 0.23548867\n",
      "Iteration 148, loss = 0.23516896\n",
      "Iteration 149, loss = 0.23485290\n",
      "Iteration 150, loss = 0.23453836\n",
      "Iteration 151, loss = 0.23424086\n",
      "Iteration 152, loss = 0.23393624\n",
      "Iteration 153, loss = 0.23363315\n",
      "Iteration 154, loss = 0.23332114\n",
      "Iteration 155, loss = 0.23302188\n",
      "Iteration 156, loss = 0.23270785\n",
      "Iteration 157, loss = 0.23241294\n",
      "Iteration 158, loss = 0.23211162\n",
      "Iteration 159, loss = 0.23182952\n",
      "Iteration 160, loss = 0.23153510\n",
      "Iteration 161, loss = 0.23123289\n",
      "Iteration 162, loss = 0.23094423\n",
      "Iteration 163, loss = 0.23063992\n",
      "Iteration 164, loss = 0.23035466\n",
      "Iteration 165, loss = 0.23007599\n",
      "Iteration 166, loss = 0.22977510\n",
      "Iteration 167, loss = 0.22951733\n",
      "Iteration 168, loss = 0.22920798\n",
      "Iteration 169, loss = 0.22893050\n",
      "Iteration 170, loss = 0.22865479\n",
      "Iteration 171, loss = 0.22837383\n",
      "Iteration 172, loss = 0.22808617\n",
      "Iteration 173, loss = 0.22780892\n",
      "Iteration 174, loss = 0.22753421\n",
      "Iteration 175, loss = 0.22724622\n",
      "Iteration 176, loss = 0.22697854\n",
      "Iteration 177, loss = 0.22671249\n",
      "Iteration 178, loss = 0.22643634\n",
      "Iteration 179, loss = 0.22617704\n",
      "Iteration 180, loss = 0.22588737\n",
      "Iteration 181, loss = 0.22561500\n",
      "Iteration 182, loss = 0.22535903\n",
      "Iteration 183, loss = 0.22506069\n",
      "Iteration 184, loss = 0.22481785\n",
      "Iteration 185, loss = 0.22454171\n",
      "Iteration 186, loss = 0.22426835\n",
      "Iteration 187, loss = 0.22399276\n",
      "Iteration 188, loss = 0.22373301\n",
      "Iteration 189, loss = 0.22347720\n",
      "Iteration 190, loss = 0.22320614\n",
      "Iteration 191, loss = 0.22293270\n",
      "Iteration 192, loss = 0.22269075\n",
      "Iteration 193, loss = 0.22241298\n",
      "Iteration 194, loss = 0.22217015\n",
      "Iteration 195, loss = 0.22190375\n",
      "Iteration 196, loss = 0.22164380\n",
      "Iteration 197, loss = 0.22139447\n",
      "Iteration 198, loss = 0.22113755\n",
      "Iteration 199, loss = 0.22087586\n",
      "Iteration 200, loss = 0.22062459\n",
      "Iteration 1, loss = 0.79521224\n",
      "Iteration 2, loss = 0.63801335\n",
      "Iteration 3, loss = 0.53237584\n",
      "Iteration 4, loss = 0.47942614\n",
      "Iteration 5, loss = 0.45174219\n",
      "Iteration 6, loss = 0.43431948\n",
      "Iteration 7, loss = 0.42244999\n",
      "Iteration 8, loss = 0.41285625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.40482332\n",
      "Iteration 10, loss = 0.39787768\n",
      "Iteration 11, loss = 0.39144783\n",
      "Iteration 12, loss = 0.38560554\n",
      "Iteration 13, loss = 0.38017180\n",
      "Iteration 14, loss = 0.37502107\n",
      "Iteration 15, loss = 0.37022174\n",
      "Iteration 16, loss = 0.36570427\n",
      "Iteration 17, loss = 0.36145390\n",
      "Iteration 18, loss = 0.35737105\n",
      "Iteration 19, loss = 0.35353919\n",
      "Iteration 20, loss = 0.34991592\n",
      "Iteration 21, loss = 0.34640309\n",
      "Iteration 22, loss = 0.34310602\n",
      "Iteration 23, loss = 0.33995461\n",
      "Iteration 24, loss = 0.33694831\n",
      "Iteration 25, loss = 0.33405728\n",
      "Iteration 26, loss = 0.33131086\n",
      "Iteration 27, loss = 0.32868519\n",
      "Iteration 28, loss = 0.32614950\n",
      "Iteration 29, loss = 0.32371389\n",
      "Iteration 30, loss = 0.32143532\n",
      "Iteration 31, loss = 0.31917582\n",
      "Iteration 32, loss = 0.31705174\n",
      "Iteration 33, loss = 0.31500986\n",
      "Iteration 34, loss = 0.31304255\n",
      "Iteration 35, loss = 0.31113379\n",
      "Iteration 36, loss = 0.30930397\n",
      "Iteration 37, loss = 0.30750414\n",
      "Iteration 38, loss = 0.30581998\n",
      "Iteration 39, loss = 0.30418972\n",
      "Iteration 40, loss = 0.30258951\n",
      "Iteration 41, loss = 0.30105015\n",
      "Iteration 42, loss = 0.29956172\n",
      "Iteration 43, loss = 0.29810363\n",
      "Iteration 44, loss = 0.29674088\n",
      "Iteration 45, loss = 0.29535587\n",
      "Iteration 46, loss = 0.29407378\n",
      "Iteration 47, loss = 0.29280211\n",
      "Iteration 48, loss = 0.29155863\n",
      "Iteration 49, loss = 0.29037098\n",
      "Iteration 50, loss = 0.28917634\n",
      "Iteration 51, loss = 0.28805568\n",
      "Iteration 52, loss = 0.28696901\n",
      "Iteration 53, loss = 0.28589204\n",
      "Iteration 54, loss = 0.28483692\n",
      "Iteration 55, loss = 0.28382363\n",
      "Iteration 56, loss = 0.28285592\n",
      "Iteration 57, loss = 0.28189310\n",
      "Iteration 58, loss = 0.28096650\n",
      "Iteration 59, loss = 0.28002507\n",
      "Iteration 60, loss = 0.27916410\n",
      "Iteration 61, loss = 0.27829569\n",
      "Iteration 62, loss = 0.27744075\n",
      "Iteration 63, loss = 0.27659187\n",
      "Iteration 64, loss = 0.27579301\n",
      "Iteration 65, loss = 0.27500136\n",
      "Iteration 66, loss = 0.27423831\n",
      "Iteration 67, loss = 0.27346628\n",
      "Iteration 68, loss = 0.27274002\n",
      "Iteration 69, loss = 0.27201686\n",
      "Iteration 70, loss = 0.27131384\n",
      "Iteration 71, loss = 0.27059835\n",
      "Iteration 72, loss = 0.26994740\n",
      "Iteration 73, loss = 0.26924064\n",
      "Iteration 74, loss = 0.26858472\n",
      "Iteration 75, loss = 0.26796139\n",
      "Iteration 76, loss = 0.26732924\n",
      "Iteration 77, loss = 0.26672215\n",
      "Iteration 78, loss = 0.26609855\n",
      "Iteration 79, loss = 0.26552682\n",
      "Iteration 80, loss = 0.26494526\n",
      "Iteration 81, loss = 0.26434529\n",
      "Iteration 82, loss = 0.26379724\n",
      "Iteration 83, loss = 0.26323564\n",
      "Iteration 84, loss = 0.26269409\n",
      "Iteration 85, loss = 0.26215024\n",
      "Iteration 86, loss = 0.26163356\n",
      "Iteration 87, loss = 0.26111456\n",
      "Iteration 88, loss = 0.26060692\n",
      "Iteration 89, loss = 0.26010508\n",
      "Iteration 90, loss = 0.25962860\n",
      "Iteration 91, loss = 0.25911585\n",
      "Iteration 92, loss = 0.25862554\n",
      "Iteration 93, loss = 0.25815980\n",
      "Iteration 94, loss = 0.25769064\n",
      "Iteration 95, loss = 0.25723582\n",
      "Iteration 96, loss = 0.25676498\n",
      "Iteration 97, loss = 0.25634255\n",
      "Iteration 98, loss = 0.25590506\n",
      "Iteration 99, loss = 0.25544876\n",
      "Iteration 100, loss = 0.25503228\n",
      "Iteration 101, loss = 0.25459569\n",
      "Iteration 102, loss = 0.25416763\n",
      "Iteration 103, loss = 0.25375654\n",
      "Iteration 104, loss = 0.25334444\n",
      "Iteration 105, loss = 0.25293950\n",
      "Iteration 106, loss = 0.25254020\n",
      "Iteration 107, loss = 0.25215011\n",
      "Iteration 108, loss = 0.25174759\n",
      "Iteration 109, loss = 0.25136427\n",
      "Iteration 110, loss = 0.25098911\n",
      "Iteration 111, loss = 0.25058991\n",
      "Iteration 112, loss = 0.25021057\n",
      "Iteration 113, loss = 0.24985734\n",
      "Iteration 114, loss = 0.24947251\n",
      "Iteration 115, loss = 0.24910210\n",
      "Iteration 116, loss = 0.24874166\n",
      "Iteration 117, loss = 0.24838426\n",
      "Iteration 118, loss = 0.24802787\n",
      "Iteration 119, loss = 0.24766627\n",
      "Iteration 120, loss = 0.24732961\n",
      "Iteration 121, loss = 0.24697650\n",
      "Iteration 122, loss = 0.24663656\n",
      "Iteration 123, loss = 0.24629098\n",
      "Iteration 124, loss = 0.24595617\n",
      "Iteration 125, loss = 0.24563733\n",
      "Iteration 126, loss = 0.24528129\n",
      "Iteration 127, loss = 0.24495413\n",
      "Iteration 128, loss = 0.24461760\n",
      "Iteration 129, loss = 0.24429689\n",
      "Iteration 130, loss = 0.24397344\n",
      "Iteration 131, loss = 0.24366452\n",
      "Iteration 132, loss = 0.24333320\n",
      "Iteration 133, loss = 0.24301873\n",
      "Iteration 134, loss = 0.24270244\n",
      "Iteration 135, loss = 0.24238358\n",
      "Iteration 136, loss = 0.24206268\n",
      "Iteration 137, loss = 0.24177037\n",
      "Iteration 138, loss = 0.24145499\n",
      "Iteration 139, loss = 0.24114133\n",
      "Iteration 140, loss = 0.24083009\n",
      "Iteration 141, loss = 0.24053609\n",
      "Iteration 142, loss = 0.24024408\n",
      "Iteration 143, loss = 0.23995132\n",
      "Iteration 144, loss = 0.23963353\n",
      "Iteration 145, loss = 0.23934388\n",
      "Iteration 146, loss = 0.23905257\n",
      "Iteration 147, loss = 0.23876037\n",
      "Iteration 148, loss = 0.23846660\n",
      "Iteration 149, loss = 0.23816972\n",
      "Iteration 150, loss = 0.23790097\n",
      "Iteration 151, loss = 0.23759416\n",
      "Iteration 152, loss = 0.23732516\n",
      "Iteration 153, loss = 0.23703228\n",
      "Iteration 154, loss = 0.23674188\n",
      "Iteration 155, loss = 0.23646380\n",
      "Iteration 156, loss = 0.23617311\n",
      "Iteration 157, loss = 0.23589683\n",
      "Iteration 158, loss = 0.23563109\n",
      "Iteration 159, loss = 0.23535197\n",
      "Iteration 160, loss = 0.23507677\n",
      "Iteration 161, loss = 0.23478839\n",
      "Iteration 162, loss = 0.23452623\n",
      "Iteration 163, loss = 0.23424053\n",
      "Iteration 164, loss = 0.23397842\n",
      "Iteration 165, loss = 0.23369940\n",
      "Iteration 166, loss = 0.23342939\n",
      "Iteration 167, loss = 0.23317922\n",
      "Iteration 168, loss = 0.23290672\n",
      "Iteration 169, loss = 0.23265269\n",
      "Iteration 170, loss = 0.23238995\n",
      "Iteration 171, loss = 0.23212587\n",
      "Iteration 172, loss = 0.23187327\n",
      "Iteration 173, loss = 0.23160470\n",
      "Iteration 174, loss = 0.23134697\n",
      "Iteration 175, loss = 0.23109068\n",
      "Iteration 176, loss = 0.23083753\n",
      "Iteration 177, loss = 0.23059412\n",
      "Iteration 178, loss = 0.23033729\n",
      "Iteration 179, loss = 0.23009555\n",
      "Iteration 180, loss = 0.22984084\n",
      "Iteration 181, loss = 0.22959699\n",
      "Iteration 182, loss = 0.22935512\n",
      "Iteration 183, loss = 0.22908461\n",
      "Iteration 184, loss = 0.22884486\n",
      "Iteration 185, loss = 0.22858605\n",
      "Iteration 186, loss = 0.22835660\n",
      "Iteration 187, loss = 0.22809470\n",
      "Iteration 188, loss = 0.22784921\n",
      "Iteration 189, loss = 0.22760680\n",
      "Iteration 190, loss = 0.22736099\n",
      "Iteration 191, loss = 0.22711618\n",
      "Iteration 192, loss = 0.22686908\n",
      "Iteration 193, loss = 0.22663063\n",
      "Iteration 194, loss = 0.22638445\n",
      "Iteration 195, loss = 0.22614589\n",
      "Iteration 196, loss = 0.22590638\n",
      "Iteration 197, loss = 0.22566895\n",
      "Iteration 198, loss = 0.22543477\n",
      "Iteration 199, loss = 0.22519536\n",
      "Iteration 200, loss = 0.22495452\n",
      "Iteration 1, loss = 0.79465280\n",
      "Iteration 2, loss = 0.63685224\n",
      "Iteration 3, loss = 0.53032575\n",
      "Iteration 4, loss = 0.47813445\n",
      "Iteration 5, loss = 0.45035274\n",
      "Iteration 6, loss = 0.43304494\n",
      "Iteration 7, loss = 0.42133324\n",
      "Iteration 8, loss = 0.41185160\n",
      "Iteration 9, loss = 0.40375468\n",
      "Iteration 10, loss = 0.39691026\n",
      "Iteration 11, loss = 0.39050212\n",
      "Iteration 12, loss = 0.38462131\n",
      "Iteration 13, loss = 0.37926596\n",
      "Iteration 14, loss = 0.37410170\n",
      "Iteration 15, loss = 0.36930762\n",
      "Iteration 16, loss = 0.36476942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.36059048\n",
      "Iteration 18, loss = 0.35653365\n",
      "Iteration 19, loss = 0.35274573\n",
      "Iteration 20, loss = 0.34911901\n",
      "Iteration 21, loss = 0.34568585\n",
      "Iteration 22, loss = 0.34243811\n",
      "Iteration 23, loss = 0.33930896\n",
      "Iteration 24, loss = 0.33634587\n",
      "Iteration 25, loss = 0.33351031\n",
      "Iteration 26, loss = 0.33083503\n",
      "Iteration 27, loss = 0.32825428\n",
      "Iteration 28, loss = 0.32575015\n",
      "Iteration 29, loss = 0.32340167\n",
      "Iteration 30, loss = 0.32114366\n",
      "Iteration 31, loss = 0.31899122\n",
      "Iteration 32, loss = 0.31686575\n",
      "Iteration 33, loss = 0.31487075\n",
      "Iteration 34, loss = 0.31295580\n",
      "Iteration 35, loss = 0.31112042\n",
      "Iteration 36, loss = 0.30932439\n",
      "Iteration 37, loss = 0.30756829\n",
      "Iteration 38, loss = 0.30594338\n",
      "Iteration 39, loss = 0.30434086\n",
      "Iteration 40, loss = 0.30279222\n",
      "Iteration 41, loss = 0.30132843\n",
      "Iteration 42, loss = 0.29988255\n",
      "Iteration 43, loss = 0.29847445\n",
      "Iteration 44, loss = 0.29716329\n",
      "Iteration 45, loss = 0.29583158\n",
      "Iteration 46, loss = 0.29461769\n",
      "Iteration 47, loss = 0.29336383\n",
      "Iteration 48, loss = 0.29218939\n",
      "Iteration 49, loss = 0.29103546\n",
      "Iteration 50, loss = 0.28991588\n",
      "Iteration 51, loss = 0.28883709\n",
      "Iteration 52, loss = 0.28778475\n",
      "Iteration 53, loss = 0.28678313\n",
      "Iteration 54, loss = 0.28579432\n",
      "Iteration 55, loss = 0.28480550\n",
      "Iteration 56, loss = 0.28387225\n",
      "Iteration 57, loss = 0.28298934\n",
      "Iteration 58, loss = 0.28209033\n",
      "Iteration 59, loss = 0.28121083\n",
      "Iteration 60, loss = 0.28038467\n",
      "Iteration 61, loss = 0.27954439\n",
      "Iteration 62, loss = 0.27874192\n",
      "Iteration 63, loss = 0.27794220\n",
      "Iteration 64, loss = 0.27717521\n",
      "Iteration 65, loss = 0.27643496\n",
      "Iteration 66, loss = 0.27570358\n",
      "Iteration 67, loss = 0.27497556\n",
      "Iteration 68, loss = 0.27429141\n",
      "Iteration 69, loss = 0.27358931\n",
      "Iteration 70, loss = 0.27292355\n",
      "Iteration 71, loss = 0.27226814\n",
      "Iteration 72, loss = 0.27163559\n",
      "Iteration 73, loss = 0.27099966\n",
      "Iteration 74, loss = 0.27037914\n",
      "Iteration 75, loss = 0.26979180\n",
      "Iteration 76, loss = 0.26917853\n",
      "Iteration 77, loss = 0.26861295\n",
      "Iteration 78, loss = 0.26802640\n",
      "Iteration 79, loss = 0.26747386\n",
      "Iteration 80, loss = 0.26695880\n",
      "Iteration 81, loss = 0.26638240\n",
      "Iteration 82, loss = 0.26587229\n",
      "Iteration 83, loss = 0.26535194\n",
      "Iteration 84, loss = 0.26483810\n",
      "Iteration 85, loss = 0.26434186\n",
      "Iteration 86, loss = 0.26384593\n",
      "Iteration 87, loss = 0.26337497\n",
      "Iteration 88, loss = 0.26290555\n",
      "Iteration 89, loss = 0.26243387\n",
      "Iteration 90, loss = 0.26198088\n",
      "Iteration 91, loss = 0.26152024\n",
      "Iteration 92, loss = 0.26105440\n",
      "Iteration 93, loss = 0.26062250\n",
      "Iteration 94, loss = 0.26017878\n",
      "Iteration 95, loss = 0.25975886\n",
      "Iteration 96, loss = 0.25933137\n",
      "Iteration 97, loss = 0.25892939\n",
      "Iteration 98, loss = 0.25852339\n",
      "Iteration 99, loss = 0.25809054\n",
      "Iteration 100, loss = 0.25771344\n",
      "Iteration 101, loss = 0.25730778\n",
      "Iteration 102, loss = 0.25691475\n",
      "Iteration 103, loss = 0.25652457\n",
      "Iteration 104, loss = 0.25614703\n",
      "Iteration 105, loss = 0.25576311\n",
      "Iteration 106, loss = 0.25540094\n",
      "Iteration 107, loss = 0.25504998\n",
      "Iteration 108, loss = 0.25465304\n",
      "Iteration 109, loss = 0.25429185\n",
      "Iteration 110, loss = 0.25393279\n",
      "Iteration 111, loss = 0.25356952\n",
      "Iteration 112, loss = 0.25322787\n",
      "Iteration 113, loss = 0.25287721\n",
      "Iteration 114, loss = 0.25251578\n",
      "Iteration 115, loss = 0.25215688\n",
      "Iteration 116, loss = 0.25184523\n",
      "Iteration 117, loss = 0.25148903\n",
      "Iteration 118, loss = 0.25116075\n",
      "Iteration 119, loss = 0.25082821\n",
      "Iteration 120, loss = 0.25052022\n",
      "Iteration 121, loss = 0.25017687\n",
      "Iteration 122, loss = 0.24986956\n",
      "Iteration 123, loss = 0.24953472\n",
      "Iteration 124, loss = 0.24921065\n",
      "Iteration 125, loss = 0.24891987\n",
      "Iteration 126, loss = 0.24858900\n",
      "Iteration 127, loss = 0.24829326\n",
      "Iteration 128, loss = 0.24797481\n",
      "Iteration 129, loss = 0.24766797\n",
      "Iteration 130, loss = 0.24736039\n",
      "Iteration 131, loss = 0.24709306\n",
      "Iteration 132, loss = 0.24677604\n",
      "Iteration 133, loss = 0.24647478\n",
      "Iteration 134, loss = 0.24618138\n",
      "Iteration 135, loss = 0.24588705\n",
      "Iteration 136, loss = 0.24559656\n",
      "Iteration 137, loss = 0.24531191\n",
      "Iteration 138, loss = 0.24503906\n",
      "Iteration 139, loss = 0.24473937\n",
      "Iteration 140, loss = 0.24446133\n",
      "Iteration 141, loss = 0.24419126\n",
      "Iteration 142, loss = 0.24390340\n",
      "Iteration 143, loss = 0.24363764\n",
      "Iteration 144, loss = 0.24334754\n",
      "Iteration 145, loss = 0.24308423\n",
      "Iteration 146, loss = 0.24279296\n",
      "Iteration 147, loss = 0.24253917\n",
      "Iteration 148, loss = 0.24226102\n",
      "Iteration 149, loss = 0.24199009\n",
      "Iteration 150, loss = 0.24174809\n",
      "Iteration 151, loss = 0.24146135\n",
      "Iteration 152, loss = 0.24120051\n",
      "Iteration 153, loss = 0.24094387\n",
      "Iteration 154, loss = 0.24066482\n",
      "Iteration 155, loss = 0.24043390\n",
      "Iteration 156, loss = 0.24015224\n",
      "Iteration 157, loss = 0.23990702\n",
      "Iteration 158, loss = 0.23965740\n",
      "Iteration 159, loss = 0.23941038\n",
      "Iteration 160, loss = 0.23915344\n",
      "Iteration 161, loss = 0.23888667\n",
      "Iteration 162, loss = 0.23863574\n",
      "Iteration 163, loss = 0.23840315\n",
      "Iteration 164, loss = 0.23814397\n",
      "Iteration 165, loss = 0.23789779\n",
      "Iteration 166, loss = 0.23765373\n",
      "Iteration 167, loss = 0.23740812\n",
      "Iteration 168, loss = 0.23715781\n",
      "Iteration 169, loss = 0.23691499\n",
      "Iteration 170, loss = 0.23667342\n",
      "Iteration 171, loss = 0.23643315\n",
      "Iteration 172, loss = 0.23619624\n",
      "Iteration 173, loss = 0.23596267\n",
      "Iteration 174, loss = 0.23571297\n",
      "Iteration 175, loss = 0.23546996\n",
      "Iteration 176, loss = 0.23522801\n",
      "Iteration 177, loss = 0.23500596\n",
      "Iteration 178, loss = 0.23477584\n",
      "Iteration 179, loss = 0.23455117\n",
      "Iteration 180, loss = 0.23431270\n",
      "Iteration 181, loss = 0.23407337\n",
      "Iteration 182, loss = 0.23384227\n",
      "Iteration 183, loss = 0.23360216\n",
      "Iteration 184, loss = 0.23336762\n",
      "Iteration 185, loss = 0.23314318\n",
      "Iteration 186, loss = 0.23294206\n",
      "Iteration 187, loss = 0.23268682\n",
      "Iteration 188, loss = 0.23246605\n",
      "Iteration 189, loss = 0.23224346\n",
      "Iteration 190, loss = 0.23199042\n",
      "Iteration 191, loss = 0.23176241\n",
      "Iteration 192, loss = 0.23155063\n",
      "Iteration 193, loss = 0.23131044\n",
      "Iteration 194, loss = 0.23109562\n",
      "Iteration 195, loss = 0.23086259\n",
      "Iteration 196, loss = 0.23062357\n",
      "Iteration 197, loss = 0.23041425\n",
      "Iteration 198, loss = 0.23019514\n",
      "Iteration 199, loss = 0.22997079\n",
      "Iteration 200, loss = 0.22973796\n",
      "Iteration 1, loss = 0.79887779\n",
      "Iteration 2, loss = 0.63863307\n",
      "Iteration 3, loss = 0.53305943\n",
      "Iteration 4, loss = 0.48003901\n",
      "Iteration 5, loss = 0.45237280\n",
      "Iteration 6, loss = 0.43555924\n",
      "Iteration 7, loss = 0.42385281\n",
      "Iteration 8, loss = 0.41444578\n",
      "Iteration 9, loss = 0.40657271\n",
      "Iteration 10, loss = 0.39957875\n",
      "Iteration 11, loss = 0.39328105\n",
      "Iteration 12, loss = 0.38738257\n",
      "Iteration 13, loss = 0.38195731\n",
      "Iteration 14, loss = 0.37683936\n",
      "Iteration 15, loss = 0.37197562\n",
      "Iteration 16, loss = 0.36743804\n",
      "Iteration 17, loss = 0.36312730\n",
      "Iteration 18, loss = 0.35908549\n",
      "Iteration 19, loss = 0.35519073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.35152062\n",
      "Iteration 21, loss = 0.34802316\n",
      "Iteration 22, loss = 0.34467515\n",
      "Iteration 23, loss = 0.34147470\n",
      "Iteration 24, loss = 0.33840848\n",
      "Iteration 25, loss = 0.33554662\n",
      "Iteration 26, loss = 0.33277005\n",
      "Iteration 27, loss = 0.33014230\n",
      "Iteration 28, loss = 0.32758916\n",
      "Iteration 29, loss = 0.32514325\n",
      "Iteration 30, loss = 0.32276407\n",
      "Iteration 31, loss = 0.32058107\n",
      "Iteration 32, loss = 0.31836395\n",
      "Iteration 33, loss = 0.31632196\n",
      "Iteration 34, loss = 0.31430616\n",
      "Iteration 35, loss = 0.31238244\n",
      "Iteration 36, loss = 0.31052986\n",
      "Iteration 37, loss = 0.30871456\n",
      "Iteration 38, loss = 0.30701623\n",
      "Iteration 39, loss = 0.30531195\n",
      "Iteration 40, loss = 0.30368988\n",
      "Iteration 41, loss = 0.30217551\n",
      "Iteration 42, loss = 0.30064020\n",
      "Iteration 43, loss = 0.29918849\n",
      "Iteration 44, loss = 0.29778341\n",
      "Iteration 45, loss = 0.29640628\n",
      "Iteration 46, loss = 0.29509365\n",
      "Iteration 47, loss = 0.29383201\n",
      "Iteration 48, loss = 0.29252313\n",
      "Iteration 49, loss = 0.29135796\n",
      "Iteration 50, loss = 0.29014092\n",
      "Iteration 51, loss = 0.28899322\n",
      "Iteration 52, loss = 0.28790931\n",
      "Iteration 53, loss = 0.28679577\n",
      "Iteration 54, loss = 0.28574617\n",
      "Iteration 55, loss = 0.28471713\n",
      "Iteration 56, loss = 0.28373075\n",
      "Iteration 57, loss = 0.28273462\n",
      "Iteration 58, loss = 0.28181006\n",
      "Iteration 59, loss = 0.28087161\n",
      "Iteration 60, loss = 0.27996673\n",
      "Iteration 61, loss = 0.27910440\n",
      "Iteration 62, loss = 0.27821516\n",
      "Iteration 63, loss = 0.27737841\n",
      "Iteration 64, loss = 0.27654445\n",
      "Iteration 65, loss = 0.27576712\n",
      "Iteration 66, loss = 0.27494192\n",
      "Iteration 67, loss = 0.27417052\n",
      "Iteration 68, loss = 0.27342354\n",
      "Iteration 69, loss = 0.27266872\n",
      "Iteration 70, loss = 0.27194592\n",
      "Iteration 71, loss = 0.27124107\n",
      "Iteration 72, loss = 0.27054879\n",
      "Iteration 73, loss = 0.26984852\n",
      "Iteration 74, loss = 0.26919061\n",
      "Iteration 75, loss = 0.26853697\n",
      "Iteration 76, loss = 0.26787913\n",
      "Iteration 77, loss = 0.26723883\n",
      "Iteration 78, loss = 0.26661216\n",
      "Iteration 79, loss = 0.26600888\n",
      "Iteration 80, loss = 0.26538813\n",
      "Iteration 81, loss = 0.26482160\n",
      "Iteration 82, loss = 0.26427881\n",
      "Iteration 83, loss = 0.26366178\n",
      "Iteration 84, loss = 0.26310021\n",
      "Iteration 85, loss = 0.26252952\n",
      "Iteration 86, loss = 0.26199193\n",
      "Iteration 87, loss = 0.26145293\n",
      "Iteration 88, loss = 0.26092045\n",
      "Iteration 89, loss = 0.26039516\n",
      "Iteration 90, loss = 0.25987769\n",
      "Iteration 91, loss = 0.25936695\n",
      "Iteration 92, loss = 0.25887567\n",
      "Iteration 93, loss = 0.25836887\n",
      "Iteration 94, loss = 0.25789076\n",
      "Iteration 95, loss = 0.25739422\n",
      "Iteration 96, loss = 0.25694333\n",
      "Iteration 97, loss = 0.25647275\n",
      "Iteration 98, loss = 0.25596737\n",
      "Iteration 99, loss = 0.25552910\n",
      "Iteration 100, loss = 0.25509508\n",
      "Iteration 101, loss = 0.25461676\n",
      "Iteration 102, loss = 0.25416842\n",
      "Iteration 103, loss = 0.25374955\n",
      "Iteration 104, loss = 0.25328734\n",
      "Iteration 105, loss = 0.25286166\n",
      "Iteration 106, loss = 0.25244033\n",
      "Iteration 107, loss = 0.25201318\n",
      "Iteration 108, loss = 0.25158842\n",
      "Iteration 109, loss = 0.25117486\n",
      "Iteration 110, loss = 0.25076562\n",
      "Iteration 111, loss = 0.25037603\n",
      "Iteration 112, loss = 0.24996230\n",
      "Iteration 113, loss = 0.24956387\n",
      "Iteration 114, loss = 0.24920603\n",
      "Iteration 115, loss = 0.24878229\n",
      "Iteration 116, loss = 0.24838865\n",
      "Iteration 117, loss = 0.24801015\n",
      "Iteration 118, loss = 0.24761639\n",
      "Iteration 119, loss = 0.24725329\n",
      "Iteration 120, loss = 0.24688765\n",
      "Iteration 121, loss = 0.24651203\n",
      "Iteration 122, loss = 0.24613982\n",
      "Iteration 123, loss = 0.24578996\n",
      "Iteration 124, loss = 0.24543576\n",
      "Iteration 125, loss = 0.24506081\n",
      "Iteration 126, loss = 0.24470071\n",
      "Iteration 127, loss = 0.24434940\n",
      "Iteration 128, loss = 0.24399519\n",
      "Iteration 129, loss = 0.24365079\n",
      "Iteration 130, loss = 0.24333181\n",
      "Iteration 131, loss = 0.24295571\n",
      "Iteration 132, loss = 0.24260580\n",
      "Iteration 133, loss = 0.24227908\n",
      "Iteration 134, loss = 0.24196250\n",
      "Iteration 135, loss = 0.24158223\n",
      "Iteration 136, loss = 0.24125122\n",
      "Iteration 137, loss = 0.24092555\n",
      "Iteration 138, loss = 0.24058543\n",
      "Iteration 139, loss = 0.24026168\n",
      "Iteration 140, loss = 0.23994348\n",
      "Iteration 141, loss = 0.23960754\n",
      "Iteration 142, loss = 0.23927251\n",
      "Iteration 143, loss = 0.23897387\n",
      "Iteration 144, loss = 0.23865083\n",
      "Iteration 145, loss = 0.23831203\n",
      "Iteration 146, loss = 0.23799767\n",
      "Iteration 147, loss = 0.23768267\n",
      "Iteration 148, loss = 0.23736722\n",
      "Iteration 149, loss = 0.23706195\n",
      "Iteration 150, loss = 0.23674362\n",
      "Iteration 151, loss = 0.23643413\n",
      "Iteration 152, loss = 0.23614854\n",
      "Iteration 153, loss = 0.23582823\n",
      "Iteration 154, loss = 0.23550913\n",
      "Iteration 155, loss = 0.23519788\n",
      "Iteration 156, loss = 0.23489327\n",
      "Iteration 157, loss = 0.23460577\n",
      "Iteration 158, loss = 0.23428629\n",
      "Iteration 159, loss = 0.23399760\n",
      "Iteration 160, loss = 0.23368477\n",
      "Iteration 161, loss = 0.23339330\n",
      "Iteration 162, loss = 0.23311219\n",
      "Iteration 163, loss = 0.23278966\n",
      "Iteration 164, loss = 0.23251179\n",
      "Iteration 165, loss = 0.23221118\n",
      "Iteration 166, loss = 0.23193754\n",
      "Iteration 167, loss = 0.23162063\n",
      "Iteration 168, loss = 0.23133105\n",
      "Iteration 169, loss = 0.23104270\n",
      "Iteration 170, loss = 0.23074942\n",
      "Iteration 171, loss = 0.23046062\n",
      "Iteration 172, loss = 0.23018866\n",
      "Iteration 173, loss = 0.22990008\n",
      "Iteration 174, loss = 0.22961183\n",
      "Iteration 175, loss = 0.22933458\n",
      "Iteration 176, loss = 0.22905486\n",
      "Iteration 177, loss = 0.22875624\n",
      "Iteration 178, loss = 0.22849390\n",
      "Iteration 179, loss = 0.22820895\n",
      "Iteration 180, loss = 0.22792888\n",
      "Iteration 181, loss = 0.22768612\n",
      "Iteration 182, loss = 0.22736655\n",
      "Iteration 183, loss = 0.22711141\n",
      "Iteration 184, loss = 0.22683994\n",
      "Iteration 185, loss = 0.22655424\n",
      "Iteration 186, loss = 0.22628264\n",
      "Iteration 187, loss = 0.22601553\n",
      "Iteration 188, loss = 0.22574698\n",
      "Iteration 189, loss = 0.22545880\n",
      "Iteration 190, loss = 0.22523484\n",
      "Iteration 191, loss = 0.22496073\n",
      "Iteration 192, loss = 0.22466906\n",
      "Iteration 193, loss = 0.22438356\n",
      "Iteration 194, loss = 0.22412615\n",
      "Iteration 195, loss = 0.22386132\n",
      "Iteration 196, loss = 0.22357388\n",
      "Iteration 197, loss = 0.22330574\n",
      "Iteration 198, loss = 0.22304049\n",
      "Iteration 199, loss = 0.22276305\n",
      "Iteration 200, loss = 0.22252713\n",
      "Iteration 1, loss = 0.79865590\n",
      "Iteration 2, loss = 0.63917790\n",
      "Iteration 3, loss = 0.53335930\n",
      "Iteration 4, loss = 0.48019579\n",
      "Iteration 5, loss = 0.45247498\n",
      "Iteration 6, loss = 0.43547400\n",
      "Iteration 7, loss = 0.42378817\n",
      "Iteration 8, loss = 0.41409948\n",
      "Iteration 9, loss = 0.40615577\n",
      "Iteration 10, loss = 0.39917490\n",
      "Iteration 11, loss = 0.39274991\n",
      "Iteration 12, loss = 0.38679241\n",
      "Iteration 13, loss = 0.38136502\n",
      "Iteration 14, loss = 0.37618113\n",
      "Iteration 15, loss = 0.37131939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.36674182\n",
      "Iteration 17, loss = 0.36246995\n",
      "Iteration 18, loss = 0.35840351\n",
      "Iteration 19, loss = 0.35450403\n",
      "Iteration 20, loss = 0.35087537\n",
      "Iteration 21, loss = 0.34740091\n",
      "Iteration 22, loss = 0.34412158\n",
      "Iteration 23, loss = 0.34092702\n",
      "Iteration 24, loss = 0.33793473\n",
      "Iteration 25, loss = 0.33506608\n",
      "Iteration 26, loss = 0.33236822\n",
      "Iteration 27, loss = 0.32977066\n",
      "Iteration 28, loss = 0.32728088\n",
      "Iteration 29, loss = 0.32491484\n",
      "Iteration 30, loss = 0.32259653\n",
      "Iteration 31, loss = 0.32043791\n",
      "Iteration 32, loss = 0.31833018\n",
      "Iteration 33, loss = 0.31632843\n",
      "Iteration 34, loss = 0.31438721\n",
      "Iteration 35, loss = 0.31255369\n",
      "Iteration 36, loss = 0.31074684\n",
      "Iteration 37, loss = 0.30903771\n",
      "Iteration 38, loss = 0.30736312\n",
      "Iteration 39, loss = 0.30576208\n",
      "Iteration 40, loss = 0.30418516\n",
      "Iteration 41, loss = 0.30273030\n",
      "Iteration 42, loss = 0.30127688\n",
      "Iteration 43, loss = 0.29987761\n",
      "Iteration 44, loss = 0.29855719\n",
      "Iteration 45, loss = 0.29723990\n",
      "Iteration 46, loss = 0.29600004\n",
      "Iteration 47, loss = 0.29483951\n",
      "Iteration 48, loss = 0.29355858\n",
      "Iteration 49, loss = 0.29246783\n",
      "Iteration 50, loss = 0.29132040\n",
      "Iteration 51, loss = 0.29026084\n",
      "Iteration 52, loss = 0.28923009\n",
      "Iteration 53, loss = 0.28818051\n",
      "Iteration 54, loss = 0.28718308\n",
      "Iteration 55, loss = 0.28621950\n",
      "Iteration 56, loss = 0.28529437\n",
      "Iteration 57, loss = 0.28437701\n",
      "Iteration 58, loss = 0.28349668\n",
      "Iteration 59, loss = 0.28262578\n",
      "Iteration 60, loss = 0.28178686\n",
      "Iteration 61, loss = 0.28095565\n",
      "Iteration 62, loss = 0.28014169\n",
      "Iteration 63, loss = 0.27937899\n",
      "Iteration 64, loss = 0.27859264\n",
      "Iteration 65, loss = 0.27787112\n",
      "Iteration 66, loss = 0.27711876\n",
      "Iteration 67, loss = 0.27639811\n",
      "Iteration 68, loss = 0.27570196\n",
      "Iteration 69, loss = 0.27501205\n",
      "Iteration 70, loss = 0.27433937\n",
      "Iteration 71, loss = 0.27368219\n",
      "Iteration 72, loss = 0.27303963\n",
      "Iteration 73, loss = 0.27239663\n",
      "Iteration 74, loss = 0.27179160\n",
      "Iteration 75, loss = 0.27116592\n",
      "Iteration 76, loss = 0.27056442\n",
      "Iteration 77, loss = 0.26998747\n",
      "Iteration 78, loss = 0.26940987\n",
      "Iteration 79, loss = 0.26882916\n",
      "Iteration 80, loss = 0.26828173\n",
      "Iteration 81, loss = 0.26774848\n",
      "Iteration 82, loss = 0.26722715\n",
      "Iteration 83, loss = 0.26668598\n",
      "Iteration 84, loss = 0.26616860\n",
      "Iteration 85, loss = 0.26562130\n",
      "Iteration 86, loss = 0.26513989\n",
      "Iteration 87, loss = 0.26463616\n",
      "Iteration 88, loss = 0.26415155\n",
      "Iteration 89, loss = 0.26366602\n",
      "Iteration 90, loss = 0.26318109\n",
      "Iteration 91, loss = 0.26273449\n",
      "Iteration 92, loss = 0.26226625\n",
      "Iteration 93, loss = 0.26183108\n",
      "Iteration 94, loss = 0.26137745\n",
      "Iteration 95, loss = 0.26094481\n",
      "Iteration 96, loss = 0.26054012\n",
      "Iteration 97, loss = 0.26008753\n",
      "Iteration 98, loss = 0.25965470\n",
      "Iteration 99, loss = 0.25925593\n",
      "Iteration 100, loss = 0.25885223\n",
      "Iteration 101, loss = 0.25843906\n",
      "Iteration 102, loss = 0.25802292\n",
      "Iteration 103, loss = 0.25763808\n",
      "Iteration 104, loss = 0.25723054\n",
      "Iteration 105, loss = 0.25684141\n",
      "Iteration 106, loss = 0.25646391\n",
      "Iteration 107, loss = 0.25607827\n",
      "Iteration 108, loss = 0.25570809\n",
      "Iteration 109, loss = 0.25532808\n",
      "Iteration 110, loss = 0.25495774\n",
      "Iteration 111, loss = 0.25460450\n",
      "Iteration 112, loss = 0.25423784\n",
      "Iteration 113, loss = 0.25387420\n",
      "Iteration 114, loss = 0.25352834\n",
      "Iteration 115, loss = 0.25316172\n",
      "Iteration 116, loss = 0.25282950\n",
      "Iteration 117, loss = 0.25246877\n",
      "Iteration 118, loss = 0.25211216\n",
      "Iteration 119, loss = 0.25178293\n",
      "Iteration 120, loss = 0.25144782\n",
      "Iteration 121, loss = 0.25111148\n",
      "Iteration 122, loss = 0.25078253\n",
      "Iteration 123, loss = 0.25045435\n",
      "Iteration 124, loss = 0.25013351\n",
      "Iteration 125, loss = 0.24980752\n",
      "Iteration 126, loss = 0.24948761\n",
      "Iteration 127, loss = 0.24914483\n",
      "Iteration 128, loss = 0.24882361\n",
      "Iteration 129, loss = 0.24851765\n",
      "Iteration 130, loss = 0.24821453\n",
      "Iteration 131, loss = 0.24787856\n",
      "Iteration 132, loss = 0.24757419\n",
      "Iteration 133, loss = 0.24726580\n",
      "Iteration 134, loss = 0.24697817\n",
      "Iteration 135, loss = 0.24665565\n",
      "Iteration 136, loss = 0.24636294\n",
      "Iteration 137, loss = 0.24605600\n",
      "Iteration 138, loss = 0.24576622\n",
      "Iteration 139, loss = 0.24546530\n",
      "Iteration 140, loss = 0.24518400\n",
      "Iteration 141, loss = 0.24487437\n",
      "Iteration 142, loss = 0.24458731\n",
      "Iteration 143, loss = 0.24431932\n",
      "Iteration 144, loss = 0.24401952\n",
      "Iteration 145, loss = 0.24372490\n",
      "Iteration 146, loss = 0.24343196\n",
      "Iteration 147, loss = 0.24317049\n",
      "Iteration 148, loss = 0.24288374\n",
      "Iteration 149, loss = 0.24260728\n",
      "Iteration 150, loss = 0.24233819\n",
      "Iteration 151, loss = 0.24207747\n",
      "Iteration 152, loss = 0.24179243\n",
      "Iteration 153, loss = 0.24151126\n",
      "Iteration 154, loss = 0.24124805\n",
      "Iteration 155, loss = 0.24095829\n",
      "Iteration 156, loss = 0.24067692\n",
      "Iteration 157, loss = 0.24043109\n",
      "Iteration 158, loss = 0.24016067\n",
      "Iteration 159, loss = 0.23988397\n",
      "Iteration 160, loss = 0.23962553\n",
      "Iteration 161, loss = 0.23936976\n",
      "Iteration 162, loss = 0.23911880\n",
      "Iteration 163, loss = 0.23882710\n",
      "Iteration 164, loss = 0.23858159\n",
      "Iteration 165, loss = 0.23831529\n",
      "Iteration 166, loss = 0.23805091\n",
      "Iteration 167, loss = 0.23779023\n",
      "Iteration 168, loss = 0.23754529\n",
      "Iteration 169, loss = 0.23728366\n",
      "Iteration 170, loss = 0.23703406\n",
      "Iteration 171, loss = 0.23678483\n",
      "Iteration 172, loss = 0.23652602\n",
      "Iteration 173, loss = 0.23628222\n",
      "Iteration 174, loss = 0.23603329\n",
      "Iteration 175, loss = 0.23578630\n",
      "Iteration 176, loss = 0.23551944\n",
      "Iteration 177, loss = 0.23528459\n",
      "Iteration 178, loss = 0.23503627\n",
      "Iteration 179, loss = 0.23477986\n",
      "Iteration 180, loss = 0.23453862\n",
      "Iteration 181, loss = 0.23431159\n",
      "Iteration 182, loss = 0.23404499\n",
      "Iteration 183, loss = 0.23381908\n",
      "Iteration 184, loss = 0.23356658\n",
      "Iteration 185, loss = 0.23331366\n",
      "Iteration 186, loss = 0.23307844\n",
      "Iteration 187, loss = 0.23285354\n",
      "Iteration 188, loss = 0.23259705\n",
      "Iteration 189, loss = 0.23234645\n",
      "Iteration 190, loss = 0.23214493\n",
      "Iteration 191, loss = 0.23191799\n",
      "Iteration 192, loss = 0.23164725\n",
      "Iteration 193, loss = 0.23138661\n",
      "Iteration 194, loss = 0.23115786\n",
      "Iteration 195, loss = 0.23092283\n",
      "Iteration 196, loss = 0.23066474\n",
      "Iteration 197, loss = 0.23041683\n",
      "Iteration 198, loss = 0.23019137\n",
      "Iteration 199, loss = 0.22994415\n",
      "Iteration 200, loss = 0.22972552\n",
      "Iteration 1, loss = 0.69911757\n",
      "Iteration 2, loss = 0.48060840\n",
      "Iteration 3, loss = 0.38750640\n",
      "Iteration 4, loss = 0.34560133\n",
      "Iteration 5, loss = 0.32060843\n",
      "Iteration 6, loss = 0.30302012\n",
      "Iteration 7, loss = 0.29001120\n",
      "Iteration 8, loss = 0.27956141\n",
      "Iteration 9, loss = 0.27075632\n",
      "Iteration 10, loss = 0.26325371\n",
      "Iteration 11, loss = 0.25658354\n",
      "Iteration 12, loss = 0.25058881\n",
      "Iteration 13, loss = 0.24517953\n",
      "Iteration 14, loss = 0.24019150\n",
      "Iteration 15, loss = 0.23542526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.23107292\n",
      "Iteration 17, loss = 0.22670121\n",
      "Iteration 18, loss = 0.22281578\n",
      "Iteration 19, loss = 0.21893181\n",
      "Iteration 20, loss = 0.21525069\n",
      "Iteration 21, loss = 0.21144223\n",
      "Iteration 22, loss = 0.20824743\n",
      "Iteration 23, loss = 0.20475182\n",
      "Iteration 24, loss = 0.20127410\n",
      "Iteration 25, loss = 0.19828248\n",
      "Iteration 26, loss = 0.19517758\n",
      "Iteration 27, loss = 0.19214683\n",
      "Iteration 28, loss = 0.18938015\n",
      "Iteration 29, loss = 0.18633537\n",
      "Iteration 30, loss = 0.18372352\n",
      "Iteration 31, loss = 0.18101691\n",
      "Iteration 32, loss = 0.17828711\n",
      "Iteration 33, loss = 0.17582458\n",
      "Iteration 34, loss = 0.17349773\n",
      "Iteration 35, loss = 0.17160807\n",
      "Iteration 36, loss = 0.16920385\n",
      "Iteration 37, loss = 0.16665074\n",
      "Iteration 38, loss = 0.16442053\n",
      "Iteration 39, loss = 0.16246524\n",
      "Iteration 40, loss = 0.16076531\n",
      "Iteration 41, loss = 0.15839839\n",
      "Iteration 42, loss = 0.15676597\n",
      "Iteration 43, loss = 0.15439100\n",
      "Iteration 44, loss = 0.15301735\n",
      "Iteration 45, loss = 0.15123833\n",
      "Iteration 46, loss = 0.14926694\n",
      "Iteration 47, loss = 0.14795141\n",
      "Iteration 48, loss = 0.14599673\n",
      "Iteration 49, loss = 0.14438352\n",
      "Iteration 50, loss = 0.14238709\n",
      "Iteration 51, loss = 0.14112823\n",
      "Iteration 52, loss = 0.14018358\n",
      "Iteration 53, loss = 0.13803346\n",
      "Iteration 54, loss = 0.13672829\n",
      "Iteration 55, loss = 0.13528992\n",
      "Iteration 56, loss = 0.13373191\n",
      "Iteration 57, loss = 0.13283372\n",
      "Iteration 58, loss = 0.13138833\n",
      "Iteration 59, loss = 0.13001477\n",
      "Iteration 60, loss = 0.12892932\n",
      "Iteration 61, loss = 0.12745681\n",
      "Iteration 62, loss = 0.12638688\n",
      "Iteration 63, loss = 0.12515351\n",
      "Iteration 64, loss = 0.12433251\n",
      "Iteration 65, loss = 0.12287426\n",
      "Iteration 66, loss = 0.12176353\n",
      "Iteration 67, loss = 0.12065002\n",
      "Iteration 68, loss = 0.11952466\n",
      "Iteration 69, loss = 0.11885004\n",
      "Iteration 70, loss = 0.11732415\n",
      "Iteration 71, loss = 0.11646595\n",
      "Iteration 72, loss = 0.11547724\n",
      "Iteration 73, loss = 0.11477712\n",
      "Iteration 74, loss = 0.11342055\n",
      "Iteration 75, loss = 0.11250611\n",
      "Iteration 76, loss = 0.11154854\n",
      "Iteration 77, loss = 0.11082275\n",
      "Iteration 78, loss = 0.10991482\n",
      "Iteration 79, loss = 0.10907707\n",
      "Iteration 80, loss = 0.10792383\n",
      "Iteration 81, loss = 0.10736636\n",
      "Iteration 82, loss = 0.10673594\n",
      "Iteration 83, loss = 0.10562052\n",
      "Iteration 84, loss = 0.10494311\n",
      "Iteration 85, loss = 0.10410427\n",
      "Iteration 86, loss = 0.10309906\n",
      "Iteration 87, loss = 0.10235499\n",
      "Iteration 88, loss = 0.10163596\n",
      "Iteration 89, loss = 0.10067986\n",
      "Iteration 90, loss = 0.09999151\n",
      "Iteration 91, loss = 0.09890722\n",
      "Iteration 92, loss = 0.09859582\n",
      "Iteration 93, loss = 0.09785662\n",
      "Iteration 94, loss = 0.09689811\n",
      "Iteration 95, loss = 0.09644145\n",
      "Iteration 96, loss = 0.09550407\n",
      "Iteration 97, loss = 0.09489282\n",
      "Iteration 98, loss = 0.09409600\n",
      "Iteration 99, loss = 0.09311529\n",
      "Iteration 100, loss = 0.09286676\n",
      "Iteration 101, loss = 0.09218684\n",
      "Iteration 102, loss = 0.09115403\n",
      "Iteration 103, loss = 0.09076557\n",
      "Iteration 104, loss = 0.09004847\n",
      "Iteration 105, loss = 0.08931415\n",
      "Iteration 106, loss = 0.08852486\n",
      "Iteration 107, loss = 0.08818140\n",
      "Iteration 108, loss = 0.08744580\n",
      "Iteration 109, loss = 0.08652515\n",
      "Iteration 110, loss = 0.08643313\n",
      "Iteration 111, loss = 0.08561703\n",
      "Iteration 112, loss = 0.08490475\n",
      "Iteration 113, loss = 0.08425223\n",
      "Iteration 114, loss = 0.08342955\n",
      "Iteration 115, loss = 0.08301636\n",
      "Iteration 116, loss = 0.08211999\n",
      "Iteration 117, loss = 0.08181416\n",
      "Iteration 118, loss = 0.08103180\n",
      "Iteration 119, loss = 0.08057397\n",
      "Iteration 120, loss = 0.08024235\n",
      "Iteration 121, loss = 0.07965151\n",
      "Iteration 122, loss = 0.07870511\n",
      "Iteration 123, loss = 0.07785015\n",
      "Iteration 124, loss = 0.07773194\n",
      "Iteration 125, loss = 0.07759883\n",
      "Iteration 126, loss = 0.07678821\n",
      "Iteration 127, loss = 0.07646563\n",
      "Iteration 128, loss = 0.07565375\n",
      "Iteration 129, loss = 0.07483058\n",
      "Iteration 130, loss = 0.07450448\n",
      "Iteration 131, loss = 0.07419125\n",
      "Iteration 132, loss = 0.07378848\n",
      "Iteration 133, loss = 0.07300708\n",
      "Iteration 134, loss = 0.07227344\n",
      "Iteration 135, loss = 0.07208767\n",
      "Iteration 136, loss = 0.07106320\n",
      "Iteration 137, loss = 0.07071879\n",
      "Iteration 138, loss = 0.07014221\n",
      "Iteration 139, loss = 0.06989597\n",
      "Iteration 140, loss = 0.06929688\n",
      "Iteration 141, loss = 0.06858612\n",
      "Iteration 142, loss = 0.06857561\n",
      "Iteration 143, loss = 0.06824414\n",
      "Iteration 144, loss = 0.06713979\n",
      "Iteration 145, loss = 0.06675778\n",
      "Iteration 146, loss = 0.06622003\n",
      "Iteration 147, loss = 0.06624080\n",
      "Iteration 148, loss = 0.06547056\n",
      "Iteration 149, loss = 0.06531525\n",
      "Iteration 150, loss = 0.06421132\n",
      "Iteration 151, loss = 0.06466739\n",
      "Iteration 152, loss = 0.06373037\n",
      "Iteration 153, loss = 0.06328246\n",
      "Iteration 154, loss = 0.06268366\n",
      "Iteration 155, loss = 0.06262858\n",
      "Iteration 156, loss = 0.06180954\n",
      "Iteration 157, loss = 0.06123199\n",
      "Iteration 158, loss = 0.06122448\n",
      "Iteration 159, loss = 0.06027111\n",
      "Iteration 160, loss = 0.06017164\n",
      "Iteration 161, loss = 0.05978601\n",
      "Iteration 162, loss = 0.05954763\n",
      "Iteration 163, loss = 0.05895371\n",
      "Iteration 164, loss = 0.05876087\n",
      "Iteration 165, loss = 0.05812464\n",
      "Iteration 166, loss = 0.05783111\n",
      "Iteration 167, loss = 0.05747801\n",
      "Iteration 168, loss = 0.05673743\n",
      "Iteration 169, loss = 0.05668241\n",
      "Iteration 170, loss = 0.05664491\n",
      "Iteration 171, loss = 0.05544272\n",
      "Iteration 172, loss = 0.05545450\n",
      "Iteration 173, loss = 0.05474316\n",
      "Iteration 174, loss = 0.05450389\n",
      "Iteration 175, loss = 0.05382785\n",
      "Iteration 176, loss = 0.05391403\n",
      "Iteration 177, loss = 0.05335309\n",
      "Iteration 178, loss = 0.05268969\n",
      "Iteration 179, loss = 0.05258117\n",
      "Iteration 180, loss = 0.05196590\n",
      "Iteration 181, loss = 0.05169403\n",
      "Iteration 182, loss = 0.05130819\n",
      "Iteration 183, loss = 0.05103996\n",
      "Iteration 184, loss = 0.05044796\n",
      "Iteration 185, loss = 0.05036350\n",
      "Iteration 186, loss = 0.04979173\n",
      "Iteration 187, loss = 0.04950165\n",
      "Iteration 188, loss = 0.04904746\n",
      "Iteration 189, loss = 0.04874314\n",
      "Iteration 190, loss = 0.04817718\n",
      "Iteration 191, loss = 0.04770850\n",
      "Iteration 192, loss = 0.04777510\n",
      "Iteration 193, loss = 0.04726259\n",
      "Iteration 194, loss = 0.04684264\n",
      "Iteration 195, loss = 0.04667826\n",
      "Iteration 196, loss = 0.04585038\n",
      "Iteration 197, loss = 0.04586247\n",
      "Iteration 198, loss = 0.04555245\n",
      "Iteration 199, loss = 0.04518766\n",
      "Iteration 200, loss = 0.04482747\n",
      "Iteration 1, loss = 0.69689933\n",
      "Iteration 2, loss = 0.47814739\n",
      "Iteration 3, loss = 0.38587328\n",
      "Iteration 4, loss = 0.34465455\n",
      "Iteration 5, loss = 0.31980709\n",
      "Iteration 6, loss = 0.30248758\n",
      "Iteration 7, loss = 0.28964087\n",
      "Iteration 8, loss = 0.27923516\n",
      "Iteration 9, loss = 0.27097315\n",
      "Iteration 10, loss = 0.26384437\n",
      "Iteration 11, loss = 0.25746677\n",
      "Iteration 12, loss = 0.25193084\n",
      "Iteration 13, loss = 0.24682277\n",
      "Iteration 14, loss = 0.24210242\n",
      "Iteration 15, loss = 0.23769260\n",
      "Iteration 16, loss = 0.23355578\n",
      "Iteration 17, loss = 0.22957075\n",
      "Iteration 18, loss = 0.22559068\n",
      "Iteration 19, loss = 0.22198391\n",
      "Iteration 20, loss = 0.21822326\n",
      "Iteration 21, loss = 0.21482320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.21156201\n",
      "Iteration 23, loss = 0.20826405\n",
      "Iteration 24, loss = 0.20509163\n",
      "Iteration 25, loss = 0.20198054\n",
      "Iteration 26, loss = 0.19914781\n",
      "Iteration 27, loss = 0.19630755\n",
      "Iteration 28, loss = 0.19368888\n",
      "Iteration 29, loss = 0.19094968\n",
      "Iteration 30, loss = 0.18840098\n",
      "Iteration 31, loss = 0.18581733\n",
      "Iteration 32, loss = 0.18336859\n",
      "Iteration 33, loss = 0.18122965\n",
      "Iteration 34, loss = 0.17890707\n",
      "Iteration 35, loss = 0.17682970\n",
      "Iteration 36, loss = 0.17460318\n",
      "Iteration 37, loss = 0.17223914\n",
      "Iteration 38, loss = 0.17005503\n",
      "Iteration 39, loss = 0.16817287\n",
      "Iteration 40, loss = 0.16657189\n",
      "Iteration 41, loss = 0.16436020\n",
      "Iteration 42, loss = 0.16252033\n",
      "Iteration 43, loss = 0.16045180\n",
      "Iteration 44, loss = 0.15875010\n",
      "Iteration 45, loss = 0.15706684\n",
      "Iteration 46, loss = 0.15544264\n",
      "Iteration 47, loss = 0.15363885\n",
      "Iteration 48, loss = 0.15192009\n",
      "Iteration 49, loss = 0.15047935\n",
      "Iteration 50, loss = 0.14872734\n",
      "Iteration 51, loss = 0.14733425\n",
      "Iteration 52, loss = 0.14598409\n",
      "Iteration 53, loss = 0.14408522\n",
      "Iteration 54, loss = 0.14302381\n",
      "Iteration 55, loss = 0.14150772\n",
      "Iteration 56, loss = 0.13991874\n",
      "Iteration 57, loss = 0.13892789\n",
      "Iteration 58, loss = 0.13787752\n",
      "Iteration 59, loss = 0.13636641\n",
      "Iteration 60, loss = 0.13531650\n",
      "Iteration 61, loss = 0.13381409\n",
      "Iteration 62, loss = 0.13254667\n",
      "Iteration 63, loss = 0.13137218\n",
      "Iteration 64, loss = 0.13016303\n",
      "Iteration 65, loss = 0.12909716\n",
      "Iteration 66, loss = 0.12802225\n",
      "Iteration 67, loss = 0.12686023\n",
      "Iteration 68, loss = 0.12596116\n",
      "Iteration 69, loss = 0.12491111\n",
      "Iteration 70, loss = 0.12379870\n",
      "Iteration 71, loss = 0.12274265\n",
      "Iteration 72, loss = 0.12192661\n",
      "Iteration 73, loss = 0.12085791\n",
      "Iteration 74, loss = 0.11976845\n",
      "Iteration 75, loss = 0.11933185\n",
      "Iteration 76, loss = 0.11797089\n",
      "Iteration 77, loss = 0.11698524\n",
      "Iteration 78, loss = 0.11600255\n",
      "Iteration 79, loss = 0.11539460\n",
      "Iteration 80, loss = 0.11435956\n",
      "Iteration 81, loss = 0.11357352\n",
      "Iteration 82, loss = 0.11262305\n",
      "Iteration 83, loss = 0.11178085\n",
      "Iteration 84, loss = 0.11111514\n",
      "Iteration 85, loss = 0.11009914\n",
      "Iteration 86, loss = 0.10923460\n",
      "Iteration 87, loss = 0.10848323\n",
      "Iteration 88, loss = 0.10776416\n",
      "Iteration 89, loss = 0.10691142\n",
      "Iteration 90, loss = 0.10649755\n",
      "Iteration 91, loss = 0.10525380\n",
      "Iteration 92, loss = 0.10428951\n",
      "Iteration 93, loss = 0.10386008\n",
      "Iteration 94, loss = 0.10282189\n",
      "Iteration 95, loss = 0.10246230\n",
      "Iteration 96, loss = 0.10149806\n",
      "Iteration 97, loss = 0.10077667\n",
      "Iteration 98, loss = 0.10042671\n",
      "Iteration 99, loss = 0.09938743\n",
      "Iteration 100, loss = 0.09890899\n",
      "Iteration 101, loss = 0.09848911\n",
      "Iteration 102, loss = 0.09729947\n",
      "Iteration 103, loss = 0.09673066\n",
      "Iteration 104, loss = 0.09601207\n",
      "Iteration 105, loss = 0.09554312\n",
      "Iteration 106, loss = 0.09481650\n",
      "Iteration 107, loss = 0.09414036\n",
      "Iteration 108, loss = 0.09349318\n",
      "Iteration 109, loss = 0.09278566\n",
      "Iteration 110, loss = 0.09239120\n",
      "Iteration 111, loss = 0.09157006\n",
      "Iteration 112, loss = 0.09084357\n",
      "Iteration 113, loss = 0.09032948\n",
      "Iteration 114, loss = 0.08978276\n",
      "Iteration 115, loss = 0.08905398\n",
      "Iteration 116, loss = 0.08828594\n",
      "Iteration 117, loss = 0.08764756\n",
      "Iteration 118, loss = 0.08721455\n",
      "Iteration 119, loss = 0.08665508\n",
      "Iteration 120, loss = 0.08631926\n",
      "Iteration 121, loss = 0.08534335\n",
      "Iteration 122, loss = 0.08486618\n",
      "Iteration 123, loss = 0.08414361\n",
      "Iteration 124, loss = 0.08379103\n",
      "Iteration 125, loss = 0.08438162\n",
      "Iteration 126, loss = 0.08264659\n",
      "Iteration 127, loss = 0.08199091\n",
      "Iteration 128, loss = 0.08158991\n",
      "Iteration 129, loss = 0.08107902\n",
      "Iteration 130, loss = 0.08035572\n",
      "Iteration 131, loss = 0.07989772\n",
      "Iteration 132, loss = 0.07956893\n",
      "Iteration 133, loss = 0.07931136\n",
      "Iteration 134, loss = 0.07844662\n",
      "Iteration 135, loss = 0.07793521\n",
      "Iteration 136, loss = 0.07716576\n",
      "Iteration 137, loss = 0.07696347\n",
      "Iteration 138, loss = 0.07659665\n",
      "Iteration 139, loss = 0.07583041\n",
      "Iteration 140, loss = 0.07498644\n",
      "Iteration 141, loss = 0.07463832\n",
      "Iteration 142, loss = 0.07425657\n",
      "Iteration 143, loss = 0.07444798\n",
      "Iteration 144, loss = 0.07394960\n",
      "Iteration 145, loss = 0.07261220\n",
      "Iteration 146, loss = 0.07243575\n",
      "Iteration 147, loss = 0.07202176\n",
      "Iteration 148, loss = 0.07171647\n",
      "Iteration 149, loss = 0.07114475\n",
      "Iteration 150, loss = 0.07024819\n",
      "Iteration 151, loss = 0.06997351\n",
      "Iteration 152, loss = 0.06985174\n",
      "Iteration 153, loss = 0.06899813\n",
      "Iteration 154, loss = 0.06850875\n",
      "Iteration 155, loss = 0.06815726\n",
      "Iteration 156, loss = 0.06743772\n",
      "Iteration 157, loss = 0.06673717\n",
      "Iteration 158, loss = 0.06664894\n",
      "Iteration 159, loss = 0.06649298\n",
      "Iteration 160, loss = 0.06561047\n",
      "Iteration 161, loss = 0.06537444\n",
      "Iteration 162, loss = 0.06507789\n",
      "Iteration 163, loss = 0.06496083\n",
      "Iteration 164, loss = 0.06432612\n",
      "Iteration 165, loss = 0.06347927\n",
      "Iteration 166, loss = 0.06288555\n",
      "Iteration 167, loss = 0.06270228\n",
      "Iteration 168, loss = 0.06231581\n",
      "Iteration 169, loss = 0.06220763\n",
      "Iteration 170, loss = 0.06177053\n",
      "Iteration 171, loss = 0.06091090\n",
      "Iteration 172, loss = 0.06066320\n",
      "Iteration 173, loss = 0.06036358\n",
      "Iteration 174, loss = 0.05977995\n",
      "Iteration 175, loss = 0.05911195\n",
      "Iteration 176, loss = 0.05898173\n",
      "Iteration 177, loss = 0.05846736\n",
      "Iteration 178, loss = 0.05809417\n",
      "Iteration 179, loss = 0.05779518\n",
      "Iteration 180, loss = 0.05757382\n",
      "Iteration 181, loss = 0.05712037\n",
      "Iteration 182, loss = 0.05644151\n",
      "Iteration 183, loss = 0.05615593\n",
      "Iteration 184, loss = 0.05550753\n",
      "Iteration 185, loss = 0.05524647\n",
      "Iteration 186, loss = 0.05517695\n",
      "Iteration 187, loss = 0.05467596\n",
      "Iteration 188, loss = 0.05388790\n",
      "Iteration 189, loss = 0.05361392\n",
      "Iteration 190, loss = 0.05336243\n",
      "Iteration 191, loss = 0.05289162\n",
      "Iteration 192, loss = 0.05276789\n",
      "Iteration 193, loss = 0.05229708\n",
      "Iteration 194, loss = 0.05194087\n",
      "Iteration 195, loss = 0.05138595\n",
      "Iteration 196, loss = 0.05079033\n",
      "Iteration 197, loss = 0.05087704\n",
      "Iteration 198, loss = 0.05042229\n",
      "Iteration 199, loss = 0.04977118\n",
      "Iteration 200, loss = 0.04946531\n",
      "Iteration 1, loss = 0.69559463\n",
      "Iteration 2, loss = 0.47690786\n",
      "Iteration 3, loss = 0.38433310\n",
      "Iteration 4, loss = 0.34427746\n",
      "Iteration 5, loss = 0.31997621\n",
      "Iteration 6, loss = 0.30310257\n",
      "Iteration 7, loss = 0.29069251\n",
      "Iteration 8, loss = 0.28090675\n",
      "Iteration 9, loss = 0.27282989\n",
      "Iteration 10, loss = 0.26589601\n",
      "Iteration 11, loss = 0.25996564\n",
      "Iteration 12, loss = 0.25453699\n",
      "Iteration 13, loss = 0.24989488\n",
      "Iteration 14, loss = 0.24536885\n",
      "Iteration 15, loss = 0.24104377\n",
      "Iteration 16, loss = 0.23708124\n",
      "Iteration 17, loss = 0.23344172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.22968070\n",
      "Iteration 19, loss = 0.22610885\n",
      "Iteration 20, loss = 0.22264207\n",
      "Iteration 21, loss = 0.21929098\n",
      "Iteration 22, loss = 0.21653315\n",
      "Iteration 23, loss = 0.21318323\n",
      "Iteration 24, loss = 0.21008032\n",
      "Iteration 25, loss = 0.20704411\n",
      "Iteration 26, loss = 0.20439643\n",
      "Iteration 27, loss = 0.20152643\n",
      "Iteration 28, loss = 0.19907096\n",
      "Iteration 29, loss = 0.19626659\n",
      "Iteration 30, loss = 0.19375443\n",
      "Iteration 31, loss = 0.19125561\n",
      "Iteration 32, loss = 0.18914207\n",
      "Iteration 33, loss = 0.18685094\n",
      "Iteration 34, loss = 0.18436313\n",
      "Iteration 35, loss = 0.18224321\n",
      "Iteration 36, loss = 0.18014869\n",
      "Iteration 37, loss = 0.17800553\n",
      "Iteration 38, loss = 0.17575163\n",
      "Iteration 39, loss = 0.17370253\n",
      "Iteration 40, loss = 0.17200429\n",
      "Iteration 41, loss = 0.17008607\n",
      "Iteration 42, loss = 0.16809110\n",
      "Iteration 43, loss = 0.16618450\n",
      "Iteration 44, loss = 0.16441001\n",
      "Iteration 45, loss = 0.16256941\n",
      "Iteration 46, loss = 0.16104783\n",
      "Iteration 47, loss = 0.15907005\n",
      "Iteration 48, loss = 0.15761117\n",
      "Iteration 49, loss = 0.15582300\n",
      "Iteration 50, loss = 0.15438765\n",
      "Iteration 51, loss = 0.15300451\n",
      "Iteration 52, loss = 0.15133993\n",
      "Iteration 53, loss = 0.14965122\n",
      "Iteration 54, loss = 0.14848684\n",
      "Iteration 55, loss = 0.14658533\n",
      "Iteration 56, loss = 0.14509203\n",
      "Iteration 57, loss = 0.14412103\n",
      "Iteration 58, loss = 0.14264998\n",
      "Iteration 59, loss = 0.14133235\n",
      "Iteration 60, loss = 0.14000976\n",
      "Iteration 61, loss = 0.13847327\n",
      "Iteration 62, loss = 0.13722386\n",
      "Iteration 63, loss = 0.13611306\n",
      "Iteration 64, loss = 0.13476975\n",
      "Iteration 65, loss = 0.13368984\n",
      "Iteration 66, loss = 0.13263470\n",
      "Iteration 67, loss = 0.13147108\n",
      "Iteration 68, loss = 0.13054068\n",
      "Iteration 69, loss = 0.12923422\n",
      "Iteration 70, loss = 0.12830680\n",
      "Iteration 71, loss = 0.12716432\n",
      "Iteration 72, loss = 0.12589129\n",
      "Iteration 73, loss = 0.12508396\n",
      "Iteration 74, loss = 0.12391931\n",
      "Iteration 75, loss = 0.12304566\n",
      "Iteration 76, loss = 0.12203488\n",
      "Iteration 77, loss = 0.12079658\n",
      "Iteration 78, loss = 0.11996846\n",
      "Iteration 79, loss = 0.11890172\n",
      "Iteration 80, loss = 0.11833106\n",
      "Iteration 81, loss = 0.11701241\n",
      "Iteration 82, loss = 0.11626155\n",
      "Iteration 83, loss = 0.11531318\n",
      "Iteration 84, loss = 0.11435445\n",
      "Iteration 85, loss = 0.11352722\n",
      "Iteration 86, loss = 0.11266280\n",
      "Iteration 87, loss = 0.11163351\n",
      "Iteration 88, loss = 0.11131316\n",
      "Iteration 89, loss = 0.11050203\n",
      "Iteration 90, loss = 0.10988914\n",
      "Iteration 91, loss = 0.10859071\n",
      "Iteration 92, loss = 0.10771061\n",
      "Iteration 93, loss = 0.10707218\n",
      "Iteration 94, loss = 0.10611690\n",
      "Iteration 95, loss = 0.10532915\n",
      "Iteration 96, loss = 0.10467140\n",
      "Iteration 97, loss = 0.10370683\n",
      "Iteration 98, loss = 0.10305764\n",
      "Iteration 99, loss = 0.10221670\n",
      "Iteration 100, loss = 0.10174355\n",
      "Iteration 101, loss = 0.10143403\n",
      "Iteration 102, loss = 0.10007014\n",
      "Iteration 103, loss = 0.09936351\n",
      "Iteration 104, loss = 0.09865535\n",
      "Iteration 105, loss = 0.09783838\n",
      "Iteration 106, loss = 0.09751109\n",
      "Iteration 107, loss = 0.09672832\n",
      "Iteration 108, loss = 0.09578366\n",
      "Iteration 109, loss = 0.09526575\n",
      "Iteration 110, loss = 0.09439126\n",
      "Iteration 111, loss = 0.09363914\n",
      "Iteration 112, loss = 0.09339361\n",
      "Iteration 113, loss = 0.09225484\n",
      "Iteration 114, loss = 0.09184929\n",
      "Iteration 115, loss = 0.09083999\n",
      "Iteration 116, loss = 0.08997979\n",
      "Iteration 117, loss = 0.08945337\n",
      "Iteration 118, loss = 0.08879092\n",
      "Iteration 119, loss = 0.08836610\n",
      "Iteration 120, loss = 0.08819414\n",
      "Iteration 121, loss = 0.08701952\n",
      "Iteration 122, loss = 0.08632621\n",
      "Iteration 123, loss = 0.08545748\n",
      "Iteration 124, loss = 0.08508064\n",
      "Iteration 125, loss = 0.08554997\n",
      "Iteration 126, loss = 0.08367293\n",
      "Iteration 127, loss = 0.08300176\n",
      "Iteration 128, loss = 0.08268966\n",
      "Iteration 129, loss = 0.08196084\n",
      "Iteration 130, loss = 0.08090161\n",
      "Iteration 131, loss = 0.08065153\n",
      "Iteration 132, loss = 0.07992085\n",
      "Iteration 133, loss = 0.07950081\n",
      "Iteration 134, loss = 0.07903664\n",
      "Iteration 135, loss = 0.07808130\n",
      "Iteration 136, loss = 0.07756282\n",
      "Iteration 137, loss = 0.07687365\n",
      "Iteration 138, loss = 0.07709459\n",
      "Iteration 139, loss = 0.07598724\n",
      "Iteration 140, loss = 0.07534977\n",
      "Iteration 141, loss = 0.07488068\n",
      "Iteration 142, loss = 0.07436069\n",
      "Iteration 143, loss = 0.07395097\n",
      "Iteration 144, loss = 0.07327942\n",
      "Iteration 145, loss = 0.07297570\n",
      "Iteration 146, loss = 0.07214001\n",
      "Iteration 147, loss = 0.07141325\n",
      "Iteration 148, loss = 0.07101994\n",
      "Iteration 149, loss = 0.07079925\n",
      "Iteration 150, loss = 0.06995387\n",
      "Iteration 151, loss = 0.06923247\n",
      "Iteration 152, loss = 0.06920443\n",
      "Iteration 153, loss = 0.06828937\n",
      "Iteration 154, loss = 0.06760111\n",
      "Iteration 155, loss = 0.06752355\n",
      "Iteration 156, loss = 0.06677777\n",
      "Iteration 157, loss = 0.06589615\n",
      "Iteration 158, loss = 0.06616082\n",
      "Iteration 159, loss = 0.06545493\n",
      "Iteration 160, loss = 0.06442165\n",
      "Iteration 161, loss = 0.06411531\n",
      "Iteration 162, loss = 0.06396563\n",
      "Iteration 163, loss = 0.06349314\n",
      "Iteration 164, loss = 0.06282998\n",
      "Iteration 165, loss = 0.06211976\n",
      "Iteration 166, loss = 0.06142734\n",
      "Iteration 167, loss = 0.06093183\n",
      "Iteration 168, loss = 0.06066536\n",
      "Iteration 169, loss = 0.06028472\n",
      "Iteration 170, loss = 0.05975258\n",
      "Iteration 171, loss = 0.05906866\n",
      "Iteration 172, loss = 0.05890439\n",
      "Iteration 173, loss = 0.05847157\n",
      "Iteration 174, loss = 0.05786752\n",
      "Iteration 175, loss = 0.05754220\n",
      "Iteration 176, loss = 0.05706028\n",
      "Iteration 177, loss = 0.05684008\n",
      "Iteration 178, loss = 0.05652237\n",
      "Iteration 179, loss = 0.05572313\n",
      "Iteration 180, loss = 0.05542822\n",
      "Iteration 181, loss = 0.05498987\n",
      "Iteration 182, loss = 0.05437191\n",
      "Iteration 183, loss = 0.05443728\n",
      "Iteration 184, loss = 0.05364970\n",
      "Iteration 185, loss = 0.05322966\n",
      "Iteration 186, loss = 0.05279796\n",
      "Iteration 187, loss = 0.05233739\n",
      "Iteration 188, loss = 0.05201681\n",
      "Iteration 189, loss = 0.05142665\n",
      "Iteration 190, loss = 0.05108658\n",
      "Iteration 191, loss = 0.05068223\n",
      "Iteration 192, loss = 0.05047910\n",
      "Iteration 193, loss = 0.04987016\n",
      "Iteration 194, loss = 0.04976648\n",
      "Iteration 195, loss = 0.04925088\n",
      "Iteration 196, loss = 0.04861935\n",
      "Iteration 197, loss = 0.04865465\n",
      "Iteration 198, loss = 0.04788670\n",
      "Iteration 199, loss = 0.04754200\n",
      "Iteration 200, loss = 0.04740941\n",
      "Iteration 1, loss = 0.70085136\n",
      "Iteration 2, loss = 0.48071911\n",
      "Iteration 3, loss = 0.38883208\n",
      "Iteration 4, loss = 0.34749030\n",
      "Iteration 5, loss = 0.32311033\n",
      "Iteration 6, loss = 0.30608499\n",
      "Iteration 7, loss = 0.29269858\n",
      "Iteration 8, loss = 0.28228515\n",
      "Iteration 9, loss = 0.27362027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.26586718\n",
      "Iteration 11, loss = 0.25946422\n",
      "Iteration 12, loss = 0.25318441\n",
      "Iteration 13, loss = 0.24781676\n",
      "Iteration 14, loss = 0.24276767\n",
      "Iteration 15, loss = 0.23771184\n",
      "Iteration 16, loss = 0.23330701\n",
      "Iteration 17, loss = 0.22895920\n",
      "Iteration 18, loss = 0.22471058\n",
      "Iteration 19, loss = 0.22089782\n",
      "Iteration 20, loss = 0.21698731\n",
      "Iteration 21, loss = 0.21340760\n",
      "Iteration 22, loss = 0.20974851\n",
      "Iteration 23, loss = 0.20651449\n",
      "Iteration 24, loss = 0.20289428\n",
      "Iteration 25, loss = 0.19987644\n",
      "Iteration 26, loss = 0.19652596\n",
      "Iteration 27, loss = 0.19378197\n",
      "Iteration 28, loss = 0.19065298\n",
      "Iteration 29, loss = 0.18786330\n",
      "Iteration 30, loss = 0.18528395\n",
      "Iteration 31, loss = 0.18238575\n",
      "Iteration 32, loss = 0.17980919\n",
      "Iteration 33, loss = 0.17728381\n",
      "Iteration 34, loss = 0.17500628\n",
      "Iteration 35, loss = 0.17252046\n",
      "Iteration 36, loss = 0.17009795\n",
      "Iteration 37, loss = 0.16802829\n",
      "Iteration 38, loss = 0.16598183\n",
      "Iteration 39, loss = 0.16381322\n",
      "Iteration 40, loss = 0.16203119\n",
      "Iteration 41, loss = 0.15965320\n",
      "Iteration 42, loss = 0.15785817\n",
      "Iteration 43, loss = 0.15622683\n",
      "Iteration 44, loss = 0.15433083\n",
      "Iteration 45, loss = 0.15211378\n",
      "Iteration 46, loss = 0.15066042\n",
      "Iteration 47, loss = 0.14871062\n",
      "Iteration 48, loss = 0.14712075\n",
      "Iteration 49, loss = 0.14561852\n",
      "Iteration 50, loss = 0.14434723\n",
      "Iteration 51, loss = 0.14242064\n",
      "Iteration 52, loss = 0.14107685\n",
      "Iteration 53, loss = 0.13952217\n",
      "Iteration 54, loss = 0.13822369\n",
      "Iteration 55, loss = 0.13652574\n",
      "Iteration 56, loss = 0.13544374\n",
      "Iteration 57, loss = 0.13417030\n",
      "Iteration 58, loss = 0.13285289\n",
      "Iteration 59, loss = 0.13153077\n",
      "Iteration 60, loss = 0.13039142\n",
      "Iteration 61, loss = 0.12948907\n",
      "Iteration 62, loss = 0.12828921\n",
      "Iteration 63, loss = 0.12733245\n",
      "Iteration 64, loss = 0.12617748\n",
      "Iteration 65, loss = 0.12541118\n",
      "Iteration 66, loss = 0.12347611\n",
      "Iteration 67, loss = 0.12274000\n",
      "Iteration 68, loss = 0.12147472\n",
      "Iteration 69, loss = 0.12053597\n",
      "Iteration 70, loss = 0.11957475\n",
      "Iteration 71, loss = 0.11874452\n",
      "Iteration 72, loss = 0.11765468\n",
      "Iteration 73, loss = 0.11668643\n",
      "Iteration 74, loss = 0.11541702\n",
      "Iteration 75, loss = 0.11499826\n",
      "Iteration 76, loss = 0.11392440\n",
      "Iteration 77, loss = 0.11288302\n",
      "Iteration 78, loss = 0.11187579\n",
      "Iteration 79, loss = 0.11118767\n",
      "Iteration 80, loss = 0.11012606\n",
      "Iteration 81, loss = 0.10939119\n",
      "Iteration 82, loss = 0.10951616\n",
      "Iteration 83, loss = 0.10826809\n",
      "Iteration 84, loss = 0.10720675\n",
      "Iteration 85, loss = 0.10606622\n",
      "Iteration 86, loss = 0.10578480\n",
      "Iteration 87, loss = 0.10440416\n",
      "Iteration 88, loss = 0.10356711\n",
      "Iteration 89, loss = 0.10284871\n",
      "Iteration 90, loss = 0.10265357\n",
      "Iteration 91, loss = 0.10091750\n",
      "Iteration 92, loss = 0.10084887\n",
      "Iteration 93, loss = 0.09955285\n",
      "Iteration 94, loss = 0.09900894\n",
      "Iteration 95, loss = 0.09814596\n",
      "Iteration 96, loss = 0.09824789\n",
      "Iteration 97, loss = 0.09703329\n",
      "Iteration 98, loss = 0.09606056\n",
      "Iteration 99, loss = 0.09567371\n",
      "Iteration 100, loss = 0.09515458\n",
      "Iteration 101, loss = 0.09446616\n",
      "Iteration 102, loss = 0.09390416\n",
      "Iteration 103, loss = 0.09312045\n",
      "Iteration 104, loss = 0.09225517\n",
      "Iteration 105, loss = 0.09189692\n",
      "Iteration 106, loss = 0.09079983\n",
      "Iteration 107, loss = 0.09047376\n",
      "Iteration 108, loss = 0.08964059\n",
      "Iteration 109, loss = 0.08918779\n",
      "Iteration 110, loss = 0.08860658\n",
      "Iteration 111, loss = 0.08812276\n",
      "Iteration 112, loss = 0.08728116\n",
      "Iteration 113, loss = 0.08680847\n",
      "Iteration 114, loss = 0.08657648\n",
      "Iteration 115, loss = 0.08556402\n",
      "Iteration 116, loss = 0.08490854\n",
      "Iteration 117, loss = 0.08405409\n",
      "Iteration 118, loss = 0.08347479\n",
      "Iteration 119, loss = 0.08321746\n",
      "Iteration 120, loss = 0.08287181\n",
      "Iteration 121, loss = 0.08211731\n",
      "Iteration 122, loss = 0.08150936\n",
      "Iteration 123, loss = 0.08066834\n",
      "Iteration 124, loss = 0.08060958\n",
      "Iteration 125, loss = 0.07998754\n",
      "Iteration 126, loss = 0.07903527\n",
      "Iteration 127, loss = 0.07862765\n",
      "Iteration 128, loss = 0.07809167\n",
      "Iteration 129, loss = 0.07751120\n",
      "Iteration 130, loss = 0.07842561\n",
      "Iteration 131, loss = 0.07675474\n",
      "Iteration 132, loss = 0.07627589\n",
      "Iteration 133, loss = 0.07527376\n",
      "Iteration 134, loss = 0.07543992\n",
      "Iteration 135, loss = 0.07441124\n",
      "Iteration 136, loss = 0.07364659\n",
      "Iteration 137, loss = 0.07355263\n",
      "Iteration 138, loss = 0.07302793\n",
      "Iteration 139, loss = 0.07193098\n",
      "Iteration 140, loss = 0.07204441\n",
      "Iteration 141, loss = 0.07139072\n",
      "Iteration 142, loss = 0.07079477\n",
      "Iteration 143, loss = 0.07041496\n",
      "Iteration 144, loss = 0.06990561\n",
      "Iteration 145, loss = 0.06953354\n",
      "Iteration 146, loss = 0.06889663\n",
      "Iteration 147, loss = 0.06861531\n",
      "Iteration 148, loss = 0.06782800\n",
      "Iteration 149, loss = 0.06739387\n",
      "Iteration 150, loss = 0.06712233\n",
      "Iteration 151, loss = 0.06637536\n",
      "Iteration 152, loss = 0.06617210\n",
      "Iteration 153, loss = 0.06547696\n",
      "Iteration 154, loss = 0.06494480\n",
      "Iteration 155, loss = 0.06435954\n",
      "Iteration 156, loss = 0.06405944\n",
      "Iteration 157, loss = 0.06383629\n",
      "Iteration 158, loss = 0.06307615\n",
      "Iteration 159, loss = 0.06304712\n",
      "Iteration 160, loss = 0.06227948\n",
      "Iteration 161, loss = 0.06207780\n",
      "Iteration 162, loss = 0.06153642\n",
      "Iteration 163, loss = 0.06099568\n",
      "Iteration 164, loss = 0.06029534\n",
      "Iteration 165, loss = 0.06017717\n",
      "Iteration 166, loss = 0.05952115\n",
      "Iteration 167, loss = 0.05923023\n",
      "Iteration 168, loss = 0.05897294\n",
      "Iteration 169, loss = 0.05798801\n",
      "Iteration 170, loss = 0.05787002\n",
      "Iteration 171, loss = 0.05729760\n",
      "Iteration 172, loss = 0.05714085\n",
      "Iteration 173, loss = 0.05638546\n",
      "Iteration 174, loss = 0.05605275\n",
      "Iteration 175, loss = 0.05595080\n",
      "Iteration 176, loss = 0.05539067\n",
      "Iteration 177, loss = 0.05498843\n",
      "Iteration 178, loss = 0.05472411\n",
      "Iteration 179, loss = 0.05420156\n",
      "Iteration 180, loss = 0.05363115\n",
      "Iteration 181, loss = 0.05408844\n",
      "Iteration 182, loss = 0.05306655\n",
      "Iteration 183, loss = 0.05287983\n",
      "Iteration 184, loss = 0.05225467\n",
      "Iteration 185, loss = 0.05144876\n",
      "Iteration 186, loss = 0.05136355\n",
      "Iteration 187, loss = 0.05077813\n",
      "Iteration 188, loss = 0.05030896\n",
      "Iteration 189, loss = 0.05007136\n",
      "Iteration 190, loss = 0.05032866\n",
      "Iteration 191, loss = 0.04941566\n",
      "Iteration 192, loss = 0.04883506\n",
      "Iteration 193, loss = 0.04864088\n",
      "Iteration 194, loss = 0.04860116\n",
      "Iteration 195, loss = 0.04820451\n",
      "Iteration 196, loss = 0.04770809\n",
      "Iteration 197, loss = 0.04704418\n",
      "Iteration 198, loss = 0.04664964\n",
      "Iteration 199, loss = 0.04608704\n",
      "Iteration 200, loss = 0.04636430\n",
      "Iteration 1, loss = 0.69846700\n",
      "Iteration 2, loss = 0.47787953\n",
      "Iteration 3, loss = 0.38612170\n",
      "Iteration 4, loss = 0.34514749\n",
      "Iteration 5, loss = 0.32148170\n",
      "Iteration 6, loss = 0.30493880\n",
      "Iteration 7, loss = 0.29243284\n",
      "Iteration 8, loss = 0.28251235\n",
      "Iteration 9, loss = 0.27442259\n",
      "Iteration 10, loss = 0.26749588\n",
      "Iteration 11, loss = 0.26147774\n",
      "Iteration 12, loss = 0.25572134\n",
      "Iteration 13, loss = 0.25089514\n",
      "Iteration 14, loss = 0.24597346\n",
      "Iteration 15, loss = 0.24143691\n",
      "Iteration 16, loss = 0.23730587\n",
      "Iteration 17, loss = 0.23328644\n",
      "Iteration 18, loss = 0.22942978\n",
      "Iteration 19, loss = 0.22563918\n",
      "Iteration 20, loss = 0.22216836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.21877883\n",
      "Iteration 22, loss = 0.21555414\n",
      "Iteration 23, loss = 0.21232675\n",
      "Iteration 24, loss = 0.20885089\n",
      "Iteration 25, loss = 0.20574154\n",
      "Iteration 26, loss = 0.20280808\n",
      "Iteration 27, loss = 0.19999936\n",
      "Iteration 28, loss = 0.19680016\n",
      "Iteration 29, loss = 0.19455797\n",
      "Iteration 30, loss = 0.19195540\n",
      "Iteration 31, loss = 0.18893745\n",
      "Iteration 32, loss = 0.18657822\n",
      "Iteration 33, loss = 0.18403770\n",
      "Iteration 34, loss = 0.18198069\n",
      "Iteration 35, loss = 0.17975209\n",
      "Iteration 36, loss = 0.17715991\n",
      "Iteration 37, loss = 0.17525464\n",
      "Iteration 38, loss = 0.17306269\n",
      "Iteration 39, loss = 0.17092250\n",
      "Iteration 40, loss = 0.16890031\n",
      "Iteration 41, loss = 0.16661873\n",
      "Iteration 42, loss = 0.16486288\n",
      "Iteration 43, loss = 0.16321557\n",
      "Iteration 44, loss = 0.16138112\n",
      "Iteration 45, loss = 0.15953057\n",
      "Iteration 46, loss = 0.15776835\n",
      "Iteration 47, loss = 0.15608674\n",
      "Iteration 48, loss = 0.15436558\n",
      "Iteration 49, loss = 0.15298149\n",
      "Iteration 50, loss = 0.15152954\n",
      "Iteration 51, loss = 0.14998592\n",
      "Iteration 52, loss = 0.14848481\n",
      "Iteration 53, loss = 0.14704792\n",
      "Iteration 54, loss = 0.14548795\n",
      "Iteration 55, loss = 0.14398436\n",
      "Iteration 56, loss = 0.14265029\n",
      "Iteration 57, loss = 0.14168628\n",
      "Iteration 58, loss = 0.14048993\n",
      "Iteration 59, loss = 0.13901077\n",
      "Iteration 60, loss = 0.13783417\n",
      "Iteration 61, loss = 0.13691952\n",
      "Iteration 62, loss = 0.13563859\n",
      "Iteration 63, loss = 0.13428941\n",
      "Iteration 64, loss = 0.13320370\n",
      "Iteration 65, loss = 0.13276823\n",
      "Iteration 66, loss = 0.13108628\n",
      "Iteration 67, loss = 0.13022115\n",
      "Iteration 68, loss = 0.12894043\n",
      "Iteration 69, loss = 0.12812853\n",
      "Iteration 70, loss = 0.12708000\n",
      "Iteration 71, loss = 0.12628761\n",
      "Iteration 72, loss = 0.12483700\n",
      "Iteration 73, loss = 0.12427032\n",
      "Iteration 74, loss = 0.12321030\n",
      "Iteration 75, loss = 0.12246929\n",
      "Iteration 76, loss = 0.12133355\n",
      "Iteration 77, loss = 0.12058633\n",
      "Iteration 78, loss = 0.11985738\n",
      "Iteration 79, loss = 0.11852941\n",
      "Iteration 80, loss = 0.11825558\n",
      "Iteration 81, loss = 0.11727822\n",
      "Iteration 82, loss = 0.11662348\n",
      "Iteration 83, loss = 0.11567595\n",
      "Iteration 84, loss = 0.11444804\n",
      "Iteration 85, loss = 0.11343155\n",
      "Iteration 86, loss = 0.11317493\n",
      "Iteration 87, loss = 0.11195839\n",
      "Iteration 88, loss = 0.11122380\n",
      "Iteration 89, loss = 0.11029590\n",
      "Iteration 90, loss = 0.10983043\n",
      "Iteration 91, loss = 0.10885739\n",
      "Iteration 92, loss = 0.10804383\n",
      "Iteration 93, loss = 0.10712165\n",
      "Iteration 94, loss = 0.10665692\n",
      "Iteration 95, loss = 0.10620734\n",
      "Iteration 96, loss = 0.10605869\n",
      "Iteration 97, loss = 0.10425085\n",
      "Iteration 98, loss = 0.10391637\n",
      "Iteration 99, loss = 0.10323478\n",
      "Iteration 100, loss = 0.10294893\n",
      "Iteration 101, loss = 0.10216603\n",
      "Iteration 102, loss = 0.10165918\n",
      "Iteration 103, loss = 0.10047882\n",
      "Iteration 104, loss = 0.09973564\n",
      "Iteration 105, loss = 0.09929953\n",
      "Iteration 106, loss = 0.09836958\n",
      "Iteration 107, loss = 0.09775348\n",
      "Iteration 108, loss = 0.09728768\n",
      "Iteration 109, loss = 0.09666014\n",
      "Iteration 110, loss = 0.09600634\n",
      "Iteration 111, loss = 0.09496011\n",
      "Iteration 112, loss = 0.09465964\n",
      "Iteration 113, loss = 0.09387905\n",
      "Iteration 114, loss = 0.09373653\n",
      "Iteration 115, loss = 0.09271038\n",
      "Iteration 116, loss = 0.09215550\n",
      "Iteration 117, loss = 0.09142932\n",
      "Iteration 118, loss = 0.09058043\n",
      "Iteration 119, loss = 0.09025555\n",
      "Iteration 120, loss = 0.08982593\n",
      "Iteration 121, loss = 0.08929966\n",
      "Iteration 122, loss = 0.08879049\n",
      "Iteration 123, loss = 0.08821174\n",
      "Iteration 124, loss = 0.08732063\n",
      "Iteration 125, loss = 0.08678022\n",
      "Iteration 126, loss = 0.08657272\n",
      "Iteration 127, loss = 0.08595937\n",
      "Iteration 128, loss = 0.08527365\n",
      "Iteration 129, loss = 0.08474480\n",
      "Iteration 130, loss = 0.08450602\n",
      "Iteration 131, loss = 0.08359905\n",
      "Iteration 132, loss = 0.08299148\n",
      "Iteration 133, loss = 0.08226126\n",
      "Iteration 134, loss = 0.08193759\n",
      "Iteration 135, loss = 0.08109141\n",
      "Iteration 136, loss = 0.08057057\n",
      "Iteration 137, loss = 0.08026247\n",
      "Iteration 138, loss = 0.07974791\n",
      "Iteration 139, loss = 0.07888025\n",
      "Iteration 140, loss = 0.07871179\n",
      "Iteration 141, loss = 0.07806500\n",
      "Iteration 142, loss = 0.07763736\n",
      "Iteration 143, loss = 0.07709613\n",
      "Iteration 144, loss = 0.07653973\n",
      "Iteration 145, loss = 0.07611416\n",
      "Iteration 146, loss = 0.07556843\n",
      "Iteration 147, loss = 0.07537495\n",
      "Iteration 148, loss = 0.07464453\n",
      "Iteration 149, loss = 0.07409793\n",
      "Iteration 150, loss = 0.07403005\n",
      "Iteration 151, loss = 0.07322479\n",
      "Iteration 152, loss = 0.07293383\n",
      "Iteration 153, loss = 0.07182009\n",
      "Iteration 154, loss = 0.07156791\n",
      "Iteration 155, loss = 0.07100500\n",
      "Iteration 156, loss = 0.07041522\n",
      "Iteration 157, loss = 0.07026143\n",
      "Iteration 158, loss = 0.06990398\n",
      "Iteration 159, loss = 0.06926722\n",
      "Iteration 160, loss = 0.06873725\n",
      "Iteration 161, loss = 0.06823467\n",
      "Iteration 162, loss = 0.06772781\n",
      "Iteration 163, loss = 0.06698618\n",
      "Iteration 164, loss = 0.06663618\n",
      "Iteration 165, loss = 0.06639775\n",
      "Iteration 166, loss = 0.06552491\n",
      "Iteration 167, loss = 0.06534213\n",
      "Iteration 168, loss = 0.06593972\n",
      "Iteration 169, loss = 0.06414584\n",
      "Iteration 170, loss = 0.06398283\n",
      "Iteration 171, loss = 0.06346421\n",
      "Iteration 172, loss = 0.06338808\n",
      "Iteration 173, loss = 0.06261036\n",
      "Iteration 174, loss = 0.06187584\n",
      "Iteration 175, loss = 0.06160117\n",
      "Iteration 176, loss = 0.06157301\n",
      "Iteration 177, loss = 0.06100556\n",
      "Iteration 178, loss = 0.06022622\n",
      "Iteration 179, loss = 0.06015298\n",
      "Iteration 180, loss = 0.05955130\n",
      "Iteration 181, loss = 0.05933257\n",
      "Iteration 182, loss = 0.05865306\n",
      "Iteration 183, loss = 0.05855036\n",
      "Iteration 184, loss = 0.05754297\n",
      "Iteration 185, loss = 0.05731307\n",
      "Iteration 186, loss = 0.05684439\n",
      "Iteration 187, loss = 0.05656784\n",
      "Iteration 188, loss = 0.05590794\n",
      "Iteration 189, loss = 0.05550657\n",
      "Iteration 190, loss = 0.05582921\n",
      "Iteration 191, loss = 0.05522903\n",
      "Iteration 192, loss = 0.05467024\n",
      "Iteration 193, loss = 0.05370722\n",
      "Iteration 194, loss = 0.05400399\n",
      "Iteration 195, loss = 0.05341604\n",
      "Iteration 196, loss = 0.05290122\n",
      "Iteration 197, loss = 0.05249112\n",
      "Iteration 198, loss = 0.05179507\n",
      "Iteration 199, loss = 0.05149110\n",
      "Iteration 200, loss = 0.05142554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79551265\n",
      "Iteration 2, loss = 0.63890810\n",
      "Iteration 3, loss = 0.53380757\n",
      "Iteration 4, loss = 0.48073250\n",
      "Iteration 5, loss = 0.45325732\n",
      "Iteration 6, loss = 0.43586499\n",
      "Iteration 7, loss = 0.42382638\n",
      "Iteration 8, loss = 0.41427262\n",
      "Iteration 9, loss = 0.40614618\n",
      "Iteration 10, loss = 0.39900902\n",
      "Iteration 11, loss = 0.39258164\n",
      "Iteration 12, loss = 0.38657761\n",
      "Iteration 13, loss = 0.38105741\n",
      "Iteration 14, loss = 0.37582925\n",
      "Iteration 15, loss = 0.37086261\n",
      "Iteration 16, loss = 0.36631737\n",
      "Iteration 17, loss = 0.36193211\n",
      "Iteration 18, loss = 0.35775322\n",
      "Iteration 19, loss = 0.35378599\n",
      "Iteration 20, loss = 0.35011293\n",
      "Iteration 21, loss = 0.34653703\n",
      "Iteration 22, loss = 0.34312586\n",
      "Iteration 23, loss = 0.33990685\n",
      "Iteration 24, loss = 0.33680789\n",
      "Iteration 25, loss = 0.33385751\n",
      "Iteration 26, loss = 0.33103588\n",
      "Iteration 27, loss = 0.32834251\n",
      "Iteration 28, loss = 0.32574114\n",
      "Iteration 29, loss = 0.32329959\n",
      "Iteration 30, loss = 0.32089167\n",
      "Iteration 31, loss = 0.31861721\n",
      "Iteration 32, loss = 0.31642697\n",
      "Iteration 33, loss = 0.31432033\n",
      "Iteration 34, loss = 0.31229975\n",
      "Iteration 35, loss = 0.31033134\n",
      "Iteration 36, loss = 0.30850107\n",
      "Iteration 37, loss = 0.30662230\n",
      "Iteration 38, loss = 0.30489532\n",
      "Iteration 39, loss = 0.30320908\n",
      "Iteration 40, loss = 0.30157316\n",
      "Iteration 41, loss = 0.30001527\n",
      "Iteration 42, loss = 0.29850980\n",
      "Iteration 43, loss = 0.29699412\n",
      "Iteration 44, loss = 0.29562038\n",
      "Iteration 45, loss = 0.29421348\n",
      "Iteration 46, loss = 0.29290195\n",
      "Iteration 47, loss = 0.29159924\n",
      "Iteration 48, loss = 0.29033306\n",
      "Iteration 49, loss = 0.28911061\n",
      "Iteration 50, loss = 0.28790667\n",
      "Iteration 51, loss = 0.28676413\n",
      "Iteration 52, loss = 0.28564627\n",
      "Iteration 53, loss = 0.28454051\n",
      "Iteration 54, loss = 0.28347011\n",
      "Iteration 55, loss = 0.28242703\n",
      "Iteration 56, loss = 0.28143076\n",
      "Iteration 57, loss = 0.28045847\n",
      "Iteration 58, loss = 0.27947809\n",
      "Iteration 59, loss = 0.27853420\n",
      "Iteration 60, loss = 0.27764752\n",
      "Iteration 61, loss = 0.27673812\n",
      "Iteration 62, loss = 0.27586913\n",
      "Iteration 63, loss = 0.27501270\n",
      "Iteration 64, loss = 0.27417736\n",
      "Iteration 65, loss = 0.27337464\n",
      "Iteration 66, loss = 0.27258692\n",
      "Iteration 67, loss = 0.27179303\n",
      "Iteration 68, loss = 0.27104906\n",
      "Iteration 69, loss = 0.27030927\n",
      "Iteration 70, loss = 0.26958779\n",
      "Iteration 71, loss = 0.26886673\n",
      "Iteration 72, loss = 0.26817382\n",
      "Iteration 73, loss = 0.26748854\n",
      "Iteration 74, loss = 0.26680431\n",
      "Iteration 75, loss = 0.26616030\n",
      "Iteration 76, loss = 0.26552189\n",
      "Iteration 77, loss = 0.26489543\n",
      "Iteration 78, loss = 0.26425382\n",
      "Iteration 79, loss = 0.26365256\n",
      "Iteration 80, loss = 0.26305921\n",
      "Iteration 81, loss = 0.26246359\n",
      "Iteration 82, loss = 0.26190790\n",
      "Iteration 83, loss = 0.26129451\n",
      "Iteration 84, loss = 0.26073983\n",
      "Iteration 85, loss = 0.26018255\n",
      "Iteration 86, loss = 0.25965502\n",
      "Iteration 87, loss = 0.25911641\n",
      "Iteration 88, loss = 0.25858044\n",
      "Iteration 89, loss = 0.25805721\n",
      "Iteration 90, loss = 0.25755087\n",
      "Iteration 91, loss = 0.25703887\n",
      "Iteration 92, loss = 0.25655554\n",
      "Iteration 93, loss = 0.25604452\n",
      "Iteration 94, loss = 0.25556104\n",
      "Iteration 95, loss = 0.25508961\n",
      "Iteration 96, loss = 0.25460872\n",
      "Iteration 97, loss = 0.25414616\n",
      "Iteration 98, loss = 0.25367844\n",
      "Iteration 99, loss = 0.25322414\n",
      "Iteration 100, loss = 0.25280192\n",
      "Iteration 101, loss = 0.25235127\n",
      "Iteration 102, loss = 0.25188874\n",
      "Iteration 103, loss = 0.25146831\n",
      "Iteration 104, loss = 0.25104432\n",
      "Iteration 105, loss = 0.25060962\n",
      "Iteration 106, loss = 0.25018816\n",
      "Iteration 107, loss = 0.24977548\n",
      "Iteration 108, loss = 0.24935409\n",
      "Iteration 109, loss = 0.24895624\n",
      "Iteration 110, loss = 0.24856819\n",
      "Iteration 111, loss = 0.24814172\n",
      "Iteration 112, loss = 0.24774493\n",
      "Iteration 113, loss = 0.24735311\n",
      "Iteration 114, loss = 0.24694626\n",
      "Iteration 115, loss = 0.24657166\n",
      "Iteration 116, loss = 0.24619333\n",
      "Iteration 117, loss = 0.24581387\n",
      "Iteration 118, loss = 0.24541839\n",
      "Iteration 119, loss = 0.24505288\n",
      "Iteration 120, loss = 0.24467344\n",
      "Iteration 121, loss = 0.24430388\n",
      "Iteration 122, loss = 0.24394573\n",
      "Iteration 123, loss = 0.24356877\n",
      "Iteration 124, loss = 0.24320547\n",
      "Iteration 125, loss = 0.24284907\n",
      "Iteration 126, loss = 0.24249143\n",
      "Iteration 127, loss = 0.24214318\n",
      "Iteration 128, loss = 0.24178767\n",
      "Iteration 129, loss = 0.24144149\n",
      "Iteration 130, loss = 0.24109226\n",
      "Iteration 131, loss = 0.24074914\n",
      "Iteration 132, loss = 0.24040785\n",
      "Iteration 133, loss = 0.24006166\n",
      "Iteration 134, loss = 0.23972542\n",
      "Iteration 135, loss = 0.23937868\n",
      "Iteration 136, loss = 0.23907690\n",
      "Iteration 137, loss = 0.23871326\n",
      "Iteration 138, loss = 0.23838126\n",
      "Iteration 139, loss = 0.23805949\n",
      "Iteration 140, loss = 0.23772558\n",
      "Iteration 141, loss = 0.23738674\n",
      "Iteration 142, loss = 0.23707951\n",
      "Iteration 143, loss = 0.23675349\n",
      "Iteration 144, loss = 0.23642740\n",
      "Iteration 145, loss = 0.23611046\n",
      "Iteration 146, loss = 0.23579216\n",
      "Iteration 147, loss = 0.23548867\n",
      "Iteration 148, loss = 0.23516896\n",
      "Iteration 149, loss = 0.23485290\n",
      "Iteration 150, loss = 0.23453836\n",
      "Iteration 151, loss = 0.23424086\n",
      "Iteration 152, loss = 0.23393624\n",
      "Iteration 153, loss = 0.23363315\n",
      "Iteration 154, loss = 0.23332114\n",
      "Iteration 155, loss = 0.23302188\n",
      "Iteration 156, loss = 0.23270785\n",
      "Iteration 157, loss = 0.23241294\n",
      "Iteration 158, loss = 0.23211162\n",
      "Iteration 159, loss = 0.23182952\n",
      "Iteration 160, loss = 0.23153510\n",
      "Iteration 161, loss = 0.23123289\n",
      "Iteration 162, loss = 0.23094423\n",
      "Iteration 163, loss = 0.23063992\n",
      "Iteration 164, loss = 0.23035466\n",
      "Iteration 165, loss = 0.23007599\n",
      "Iteration 166, loss = 0.22977510\n",
      "Iteration 167, loss = 0.22951733\n",
      "Iteration 168, loss = 0.22920798\n",
      "Iteration 169, loss = 0.22893050\n",
      "Iteration 170, loss = 0.22865479\n",
      "Iteration 171, loss = 0.22837383\n",
      "Iteration 172, loss = 0.22808617\n",
      "Iteration 173, loss = 0.22780892\n",
      "Iteration 174, loss = 0.22753421\n",
      "Iteration 175, loss = 0.22724622\n",
      "Iteration 176, loss = 0.22697854\n",
      "Iteration 177, loss = 0.22671249\n",
      "Iteration 178, loss = 0.22643634\n",
      "Iteration 179, loss = 0.22617704\n",
      "Iteration 180, loss = 0.22588737\n",
      "Iteration 181, loss = 0.22561500\n",
      "Iteration 182, loss = 0.22535903\n",
      "Iteration 183, loss = 0.22506069\n",
      "Iteration 184, loss = 0.22481785\n",
      "Iteration 185, loss = 0.22454171\n",
      "Iteration 186, loss = 0.22426835\n",
      "Iteration 187, loss = 0.22399276\n",
      "Iteration 188, loss = 0.22373301\n",
      "Iteration 189, loss = 0.22347720\n",
      "Iteration 190, loss = 0.22320614\n",
      "Iteration 191, loss = 0.22293270\n",
      "Iteration 192, loss = 0.22269075\n",
      "Iteration 193, loss = 0.22241298\n",
      "Iteration 194, loss = 0.22217015\n",
      "Iteration 195, loss = 0.22190375\n",
      "Iteration 196, loss = 0.22164380\n",
      "Iteration 197, loss = 0.22139447\n",
      "Iteration 198, loss = 0.22113755\n",
      "Iteration 199, loss = 0.22087586\n",
      "Iteration 200, loss = 0.22062459\n",
      "Iteration 1, loss = 0.79521224\n",
      "Iteration 2, loss = 0.63801335\n",
      "Iteration 3, loss = 0.53237584\n",
      "Iteration 4, loss = 0.47942614\n",
      "Iteration 5, loss = 0.45174219\n",
      "Iteration 6, loss = 0.43431948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.42244999\n",
      "Iteration 8, loss = 0.41285625\n",
      "Iteration 9, loss = 0.40482332\n",
      "Iteration 10, loss = 0.39787768\n",
      "Iteration 11, loss = 0.39144783\n",
      "Iteration 12, loss = 0.38560554\n",
      "Iteration 13, loss = 0.38017180\n",
      "Iteration 14, loss = 0.37502107\n",
      "Iteration 15, loss = 0.37022174\n",
      "Iteration 16, loss = 0.36570427\n",
      "Iteration 17, loss = 0.36145390\n",
      "Iteration 18, loss = 0.35737105\n",
      "Iteration 19, loss = 0.35353919\n",
      "Iteration 20, loss = 0.34991592\n",
      "Iteration 21, loss = 0.34640309\n",
      "Iteration 22, loss = 0.34310602\n",
      "Iteration 23, loss = 0.33995461\n",
      "Iteration 24, loss = 0.33694831\n",
      "Iteration 25, loss = 0.33405728\n",
      "Iteration 26, loss = 0.33131086\n",
      "Iteration 27, loss = 0.32868519\n",
      "Iteration 28, loss = 0.32614950\n",
      "Iteration 29, loss = 0.32371389\n",
      "Iteration 30, loss = 0.32143532\n",
      "Iteration 31, loss = 0.31917582\n",
      "Iteration 32, loss = 0.31705174\n",
      "Iteration 33, loss = 0.31500986\n",
      "Iteration 34, loss = 0.31304255\n",
      "Iteration 35, loss = 0.31113379\n",
      "Iteration 36, loss = 0.30930397\n",
      "Iteration 37, loss = 0.30750414\n",
      "Iteration 38, loss = 0.30581998\n",
      "Iteration 39, loss = 0.30418972\n",
      "Iteration 40, loss = 0.30258951\n",
      "Iteration 41, loss = 0.30105015\n",
      "Iteration 42, loss = 0.29956172\n",
      "Iteration 43, loss = 0.29810363\n",
      "Iteration 44, loss = 0.29674088\n",
      "Iteration 45, loss = 0.29535587\n",
      "Iteration 46, loss = 0.29407378\n",
      "Iteration 47, loss = 0.29280211\n",
      "Iteration 48, loss = 0.29155863\n",
      "Iteration 49, loss = 0.29037098\n",
      "Iteration 50, loss = 0.28917634\n",
      "Iteration 51, loss = 0.28805568\n",
      "Iteration 52, loss = 0.28696901\n",
      "Iteration 53, loss = 0.28589204\n",
      "Iteration 54, loss = 0.28483692\n",
      "Iteration 55, loss = 0.28382363\n",
      "Iteration 56, loss = 0.28285592\n",
      "Iteration 57, loss = 0.28189310\n",
      "Iteration 58, loss = 0.28096650\n",
      "Iteration 59, loss = 0.28002507\n",
      "Iteration 60, loss = 0.27916410\n",
      "Iteration 61, loss = 0.27829569\n",
      "Iteration 62, loss = 0.27744075\n",
      "Iteration 63, loss = 0.27659187\n",
      "Iteration 64, loss = 0.27579301\n",
      "Iteration 65, loss = 0.27500136\n",
      "Iteration 66, loss = 0.27423831\n",
      "Iteration 67, loss = 0.27346628\n",
      "Iteration 68, loss = 0.27274002\n",
      "Iteration 69, loss = 0.27201686\n",
      "Iteration 70, loss = 0.27131384\n",
      "Iteration 71, loss = 0.27059835\n",
      "Iteration 72, loss = 0.26994740\n",
      "Iteration 73, loss = 0.26924064\n",
      "Iteration 74, loss = 0.26858472\n",
      "Iteration 75, loss = 0.26796139\n",
      "Iteration 76, loss = 0.26732924\n",
      "Iteration 77, loss = 0.26672215\n",
      "Iteration 78, loss = 0.26609855\n",
      "Iteration 79, loss = 0.26552682\n",
      "Iteration 80, loss = 0.26494526\n",
      "Iteration 81, loss = 0.26434529\n",
      "Iteration 82, loss = 0.26379724\n",
      "Iteration 83, loss = 0.26323564\n",
      "Iteration 84, loss = 0.26269409\n",
      "Iteration 85, loss = 0.26215024\n",
      "Iteration 86, loss = 0.26163356\n",
      "Iteration 87, loss = 0.26111456\n",
      "Iteration 88, loss = 0.26060692\n",
      "Iteration 89, loss = 0.26010508\n",
      "Iteration 90, loss = 0.25962860\n",
      "Iteration 91, loss = 0.25911585\n",
      "Iteration 92, loss = 0.25862554\n",
      "Iteration 93, loss = 0.25815980\n",
      "Iteration 94, loss = 0.25769064\n",
      "Iteration 95, loss = 0.25723582\n",
      "Iteration 96, loss = 0.25676498\n",
      "Iteration 97, loss = 0.25634255\n",
      "Iteration 98, loss = 0.25590506\n",
      "Iteration 99, loss = 0.25544876\n",
      "Iteration 100, loss = 0.25503228\n",
      "Iteration 101, loss = 0.25459569\n",
      "Iteration 102, loss = 0.25416763\n",
      "Iteration 103, loss = 0.25375654\n",
      "Iteration 104, loss = 0.25334444\n",
      "Iteration 105, loss = 0.25293950\n",
      "Iteration 106, loss = 0.25254020\n",
      "Iteration 107, loss = 0.25215011\n",
      "Iteration 108, loss = 0.25174759\n",
      "Iteration 109, loss = 0.25136427\n",
      "Iteration 110, loss = 0.25098911\n",
      "Iteration 111, loss = 0.25058991\n",
      "Iteration 112, loss = 0.25021057\n",
      "Iteration 113, loss = 0.24985734\n",
      "Iteration 114, loss = 0.24947251\n",
      "Iteration 115, loss = 0.24910210\n",
      "Iteration 116, loss = 0.24874166\n",
      "Iteration 117, loss = 0.24838426\n",
      "Iteration 118, loss = 0.24802787\n",
      "Iteration 119, loss = 0.24766627\n",
      "Iteration 120, loss = 0.24732961\n",
      "Iteration 121, loss = 0.24697650\n",
      "Iteration 122, loss = 0.24663656\n",
      "Iteration 123, loss = 0.24629098\n",
      "Iteration 124, loss = 0.24595617\n",
      "Iteration 125, loss = 0.24563733\n",
      "Iteration 126, loss = 0.24528129\n",
      "Iteration 127, loss = 0.24495413\n",
      "Iteration 128, loss = 0.24461760\n",
      "Iteration 129, loss = 0.24429689\n",
      "Iteration 130, loss = 0.24397344\n",
      "Iteration 131, loss = 0.24366452\n",
      "Iteration 132, loss = 0.24333320\n",
      "Iteration 133, loss = 0.24301873\n",
      "Iteration 134, loss = 0.24270244\n",
      "Iteration 135, loss = 0.24238358\n",
      "Iteration 136, loss = 0.24206268\n",
      "Iteration 137, loss = 0.24177037\n",
      "Iteration 138, loss = 0.24145499\n",
      "Iteration 139, loss = 0.24114133\n",
      "Iteration 140, loss = 0.24083009\n",
      "Iteration 141, loss = 0.24053609\n",
      "Iteration 142, loss = 0.24024408\n",
      "Iteration 143, loss = 0.23995132\n",
      "Iteration 144, loss = 0.23963353\n",
      "Iteration 145, loss = 0.23934388\n",
      "Iteration 146, loss = 0.23905257\n",
      "Iteration 147, loss = 0.23876037\n",
      "Iteration 148, loss = 0.23846660\n",
      "Iteration 149, loss = 0.23816972\n",
      "Iteration 150, loss = 0.23790097\n",
      "Iteration 151, loss = 0.23759416\n",
      "Iteration 152, loss = 0.23732516\n",
      "Iteration 153, loss = 0.23703228\n",
      "Iteration 154, loss = 0.23674188\n",
      "Iteration 155, loss = 0.23646380\n",
      "Iteration 156, loss = 0.23617311\n",
      "Iteration 157, loss = 0.23589683\n",
      "Iteration 158, loss = 0.23563109\n",
      "Iteration 159, loss = 0.23535197\n",
      "Iteration 160, loss = 0.23507677\n",
      "Iteration 161, loss = 0.23478839\n",
      "Iteration 162, loss = 0.23452623\n",
      "Iteration 163, loss = 0.23424053\n",
      "Iteration 164, loss = 0.23397842\n",
      "Iteration 165, loss = 0.23369940\n",
      "Iteration 166, loss = 0.23342939\n",
      "Iteration 167, loss = 0.23317922\n",
      "Iteration 168, loss = 0.23290672\n",
      "Iteration 169, loss = 0.23265269\n",
      "Iteration 170, loss = 0.23238995\n",
      "Iteration 171, loss = 0.23212587\n",
      "Iteration 172, loss = 0.23187327\n",
      "Iteration 173, loss = 0.23160470\n",
      "Iteration 174, loss = 0.23134697\n",
      "Iteration 175, loss = 0.23109068\n",
      "Iteration 176, loss = 0.23083753\n",
      "Iteration 177, loss = 0.23059412\n",
      "Iteration 178, loss = 0.23033729\n",
      "Iteration 179, loss = 0.23009555\n",
      "Iteration 180, loss = 0.22984084\n",
      "Iteration 181, loss = 0.22959699\n",
      "Iteration 182, loss = 0.22935512\n",
      "Iteration 183, loss = 0.22908461\n",
      "Iteration 184, loss = 0.22884486\n",
      "Iteration 185, loss = 0.22858605\n",
      "Iteration 186, loss = 0.22835660\n",
      "Iteration 187, loss = 0.22809470\n",
      "Iteration 188, loss = 0.22784921\n",
      "Iteration 189, loss = 0.22760680\n",
      "Iteration 190, loss = 0.22736099\n",
      "Iteration 191, loss = 0.22711618\n",
      "Iteration 192, loss = 0.22686908\n",
      "Iteration 193, loss = 0.22663063\n",
      "Iteration 194, loss = 0.22638445\n",
      "Iteration 195, loss = 0.22614589\n",
      "Iteration 196, loss = 0.22590638\n",
      "Iteration 197, loss = 0.22566895\n",
      "Iteration 198, loss = 0.22543477\n",
      "Iteration 199, loss = 0.22519536\n",
      "Iteration 200, loss = 0.22495452\n",
      "Iteration 1, loss = 0.79465280\n",
      "Iteration 2, loss = 0.63685224\n",
      "Iteration 3, loss = 0.53032575\n",
      "Iteration 4, loss = 0.47813445\n",
      "Iteration 5, loss = 0.45035274\n",
      "Iteration 6, loss = 0.43304494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.42133324\n",
      "Iteration 8, loss = 0.41185160\n",
      "Iteration 9, loss = 0.40375468\n",
      "Iteration 10, loss = 0.39691026\n",
      "Iteration 11, loss = 0.39050212\n",
      "Iteration 12, loss = 0.38462131\n",
      "Iteration 13, loss = 0.37926596\n",
      "Iteration 14, loss = 0.37410170\n",
      "Iteration 15, loss = 0.36930762\n",
      "Iteration 16, loss = 0.36476942\n",
      "Iteration 17, loss = 0.36059048\n",
      "Iteration 18, loss = 0.35653365\n",
      "Iteration 19, loss = 0.35274573\n",
      "Iteration 20, loss = 0.34911901\n",
      "Iteration 21, loss = 0.34568585\n",
      "Iteration 22, loss = 0.34243811\n",
      "Iteration 23, loss = 0.33930896\n",
      "Iteration 24, loss = 0.33634587\n",
      "Iteration 25, loss = 0.33351031\n",
      "Iteration 26, loss = 0.33083503\n",
      "Iteration 27, loss = 0.32825428\n",
      "Iteration 28, loss = 0.32575015\n",
      "Iteration 29, loss = 0.32340167\n",
      "Iteration 30, loss = 0.32114366\n",
      "Iteration 31, loss = 0.31899122\n",
      "Iteration 32, loss = 0.31686575\n",
      "Iteration 33, loss = 0.31487075\n",
      "Iteration 34, loss = 0.31295580\n",
      "Iteration 35, loss = 0.31112042\n",
      "Iteration 36, loss = 0.30932439\n",
      "Iteration 37, loss = 0.30756829\n",
      "Iteration 38, loss = 0.30594338\n",
      "Iteration 39, loss = 0.30434086\n",
      "Iteration 40, loss = 0.30279222\n",
      "Iteration 41, loss = 0.30132843\n",
      "Iteration 42, loss = 0.29988255\n",
      "Iteration 43, loss = 0.29847445\n",
      "Iteration 44, loss = 0.29716329\n",
      "Iteration 45, loss = 0.29583158\n",
      "Iteration 46, loss = 0.29461769\n",
      "Iteration 47, loss = 0.29336383\n",
      "Iteration 48, loss = 0.29218939\n",
      "Iteration 49, loss = 0.29103546\n",
      "Iteration 50, loss = 0.28991588\n",
      "Iteration 51, loss = 0.28883709\n",
      "Iteration 52, loss = 0.28778475\n",
      "Iteration 53, loss = 0.28678313\n",
      "Iteration 54, loss = 0.28579432\n",
      "Iteration 55, loss = 0.28480550\n",
      "Iteration 56, loss = 0.28387225\n",
      "Iteration 57, loss = 0.28298934\n",
      "Iteration 58, loss = 0.28209033\n",
      "Iteration 59, loss = 0.28121083\n",
      "Iteration 60, loss = 0.28038467\n",
      "Iteration 61, loss = 0.27954439\n",
      "Iteration 62, loss = 0.27874192\n",
      "Iteration 63, loss = 0.27794220\n",
      "Iteration 64, loss = 0.27717521\n",
      "Iteration 65, loss = 0.27643496\n",
      "Iteration 66, loss = 0.27570358\n",
      "Iteration 67, loss = 0.27497556\n",
      "Iteration 68, loss = 0.27429141\n",
      "Iteration 69, loss = 0.27358931\n",
      "Iteration 70, loss = 0.27292355\n",
      "Iteration 71, loss = 0.27226814\n",
      "Iteration 72, loss = 0.27163559\n",
      "Iteration 73, loss = 0.27099966\n",
      "Iteration 74, loss = 0.27037914\n",
      "Iteration 75, loss = 0.26979180\n",
      "Iteration 76, loss = 0.26917853\n",
      "Iteration 77, loss = 0.26861295\n",
      "Iteration 78, loss = 0.26802640\n",
      "Iteration 79, loss = 0.26747386\n",
      "Iteration 80, loss = 0.26695880\n",
      "Iteration 81, loss = 0.26638240\n",
      "Iteration 82, loss = 0.26587229\n",
      "Iteration 83, loss = 0.26535194\n",
      "Iteration 84, loss = 0.26483810\n",
      "Iteration 85, loss = 0.26434186\n",
      "Iteration 86, loss = 0.26384593\n",
      "Iteration 87, loss = 0.26337497\n",
      "Iteration 88, loss = 0.26290555\n",
      "Iteration 89, loss = 0.26243387\n",
      "Iteration 90, loss = 0.26198088\n",
      "Iteration 91, loss = 0.26152024\n",
      "Iteration 92, loss = 0.26105440\n",
      "Iteration 93, loss = 0.26062250\n",
      "Iteration 94, loss = 0.26017878\n",
      "Iteration 95, loss = 0.25975886\n",
      "Iteration 96, loss = 0.25933137\n",
      "Iteration 97, loss = 0.25892939\n",
      "Iteration 98, loss = 0.25852339\n",
      "Iteration 99, loss = 0.25809054\n",
      "Iteration 100, loss = 0.25771344\n",
      "Iteration 101, loss = 0.25730778\n",
      "Iteration 102, loss = 0.25691475\n",
      "Iteration 103, loss = 0.25652457\n",
      "Iteration 104, loss = 0.25614703\n",
      "Iteration 105, loss = 0.25576311\n",
      "Iteration 106, loss = 0.25540094\n",
      "Iteration 107, loss = 0.25504998\n",
      "Iteration 108, loss = 0.25465304\n",
      "Iteration 109, loss = 0.25429185\n",
      "Iteration 110, loss = 0.25393279\n",
      "Iteration 111, loss = 0.25356952\n",
      "Iteration 112, loss = 0.25322787\n",
      "Iteration 113, loss = 0.25287721\n",
      "Iteration 114, loss = 0.25251578\n",
      "Iteration 115, loss = 0.25215688\n",
      "Iteration 116, loss = 0.25184523\n",
      "Iteration 117, loss = 0.25148903\n",
      "Iteration 118, loss = 0.25116075\n",
      "Iteration 119, loss = 0.25082821\n",
      "Iteration 120, loss = 0.25052022\n",
      "Iteration 121, loss = 0.25017687\n",
      "Iteration 122, loss = 0.24986956\n",
      "Iteration 123, loss = 0.24953472\n",
      "Iteration 124, loss = 0.24921065\n",
      "Iteration 125, loss = 0.24891987\n",
      "Iteration 126, loss = 0.24858900\n",
      "Iteration 127, loss = 0.24829326\n",
      "Iteration 128, loss = 0.24797481\n",
      "Iteration 129, loss = 0.24766797\n",
      "Iteration 130, loss = 0.24736039\n",
      "Iteration 131, loss = 0.24709306\n",
      "Iteration 132, loss = 0.24677604\n",
      "Iteration 133, loss = 0.24647478\n",
      "Iteration 134, loss = 0.24618138\n",
      "Iteration 135, loss = 0.24588705\n",
      "Iteration 136, loss = 0.24559656\n",
      "Iteration 137, loss = 0.24531191\n",
      "Iteration 138, loss = 0.24503906\n",
      "Iteration 139, loss = 0.24473937\n",
      "Iteration 140, loss = 0.24446133\n",
      "Iteration 141, loss = 0.24419126\n",
      "Iteration 142, loss = 0.24390340\n",
      "Iteration 143, loss = 0.24363764\n",
      "Iteration 144, loss = 0.24334754\n",
      "Iteration 145, loss = 0.24308423\n",
      "Iteration 146, loss = 0.24279296\n",
      "Iteration 147, loss = 0.24253917\n",
      "Iteration 148, loss = 0.24226102\n",
      "Iteration 149, loss = 0.24199009\n",
      "Iteration 150, loss = 0.24174809\n",
      "Iteration 151, loss = 0.24146135\n",
      "Iteration 152, loss = 0.24120051\n",
      "Iteration 153, loss = 0.24094387\n",
      "Iteration 154, loss = 0.24066482\n",
      "Iteration 155, loss = 0.24043390\n",
      "Iteration 156, loss = 0.24015224\n",
      "Iteration 157, loss = 0.23990702\n",
      "Iteration 158, loss = 0.23965740\n",
      "Iteration 159, loss = 0.23941038\n",
      "Iteration 160, loss = 0.23915344\n",
      "Iteration 161, loss = 0.23888667\n",
      "Iteration 162, loss = 0.23863574\n",
      "Iteration 163, loss = 0.23840315\n",
      "Iteration 164, loss = 0.23814397\n",
      "Iteration 165, loss = 0.23789779\n",
      "Iteration 166, loss = 0.23765373\n",
      "Iteration 167, loss = 0.23740812\n",
      "Iteration 168, loss = 0.23715781\n",
      "Iteration 169, loss = 0.23691499\n",
      "Iteration 170, loss = 0.23667342\n",
      "Iteration 171, loss = 0.23643315\n",
      "Iteration 172, loss = 0.23619624\n",
      "Iteration 173, loss = 0.23596267\n",
      "Iteration 174, loss = 0.23571297\n",
      "Iteration 175, loss = 0.23546996\n",
      "Iteration 176, loss = 0.23522801\n",
      "Iteration 177, loss = 0.23500596\n",
      "Iteration 178, loss = 0.23477584\n",
      "Iteration 179, loss = 0.23455117\n",
      "Iteration 180, loss = 0.23431270\n",
      "Iteration 181, loss = 0.23407337\n",
      "Iteration 182, loss = 0.23384227\n",
      "Iteration 183, loss = 0.23360216\n",
      "Iteration 184, loss = 0.23336762\n",
      "Iteration 185, loss = 0.23314318\n",
      "Iteration 186, loss = 0.23294206\n",
      "Iteration 187, loss = 0.23268682\n",
      "Iteration 188, loss = 0.23246605\n",
      "Iteration 189, loss = 0.23224346\n",
      "Iteration 190, loss = 0.23199042\n",
      "Iteration 191, loss = 0.23176241\n",
      "Iteration 192, loss = 0.23155063\n",
      "Iteration 193, loss = 0.23131044\n",
      "Iteration 194, loss = 0.23109562\n",
      "Iteration 195, loss = 0.23086259\n",
      "Iteration 196, loss = 0.23062357\n",
      "Iteration 197, loss = 0.23041425\n",
      "Iteration 198, loss = 0.23019514\n",
      "Iteration 199, loss = 0.22997079\n",
      "Iteration 200, loss = 0.22973796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79887779\n",
      "Iteration 2, loss = 0.63863307\n",
      "Iteration 3, loss = 0.53305943\n",
      "Iteration 4, loss = 0.48003901\n",
      "Iteration 5, loss = 0.45237280\n",
      "Iteration 6, loss = 0.43555924\n",
      "Iteration 7, loss = 0.42385281\n",
      "Iteration 8, loss = 0.41444578\n",
      "Iteration 9, loss = 0.40657271\n",
      "Iteration 10, loss = 0.39957875\n",
      "Iteration 11, loss = 0.39328105\n",
      "Iteration 12, loss = 0.38738257\n",
      "Iteration 13, loss = 0.38195731\n",
      "Iteration 14, loss = 0.37683936\n",
      "Iteration 15, loss = 0.37197562\n",
      "Iteration 16, loss = 0.36743804\n",
      "Iteration 17, loss = 0.36312730\n",
      "Iteration 18, loss = 0.35908549\n",
      "Iteration 19, loss = 0.35519073\n",
      "Iteration 20, loss = 0.35152062\n",
      "Iteration 21, loss = 0.34802316\n",
      "Iteration 22, loss = 0.34467515\n",
      "Iteration 23, loss = 0.34147470\n",
      "Iteration 24, loss = 0.33840848\n",
      "Iteration 25, loss = 0.33554662\n",
      "Iteration 26, loss = 0.33277005\n",
      "Iteration 27, loss = 0.33014230\n",
      "Iteration 28, loss = 0.32758916\n",
      "Iteration 29, loss = 0.32514325\n",
      "Iteration 30, loss = 0.32276407\n",
      "Iteration 31, loss = 0.32058107\n",
      "Iteration 32, loss = 0.31836395\n",
      "Iteration 33, loss = 0.31632196\n",
      "Iteration 34, loss = 0.31430616\n",
      "Iteration 35, loss = 0.31238244\n",
      "Iteration 36, loss = 0.31052986\n",
      "Iteration 37, loss = 0.30871456\n",
      "Iteration 38, loss = 0.30701623\n",
      "Iteration 39, loss = 0.30531195\n",
      "Iteration 40, loss = 0.30368988\n",
      "Iteration 41, loss = 0.30217551\n",
      "Iteration 42, loss = 0.30064020\n",
      "Iteration 43, loss = 0.29918849\n",
      "Iteration 44, loss = 0.29778341\n",
      "Iteration 45, loss = 0.29640628\n",
      "Iteration 46, loss = 0.29509365\n",
      "Iteration 47, loss = 0.29383201\n",
      "Iteration 48, loss = 0.29252313\n",
      "Iteration 49, loss = 0.29135796\n",
      "Iteration 50, loss = 0.29014092\n",
      "Iteration 51, loss = 0.28899322\n",
      "Iteration 52, loss = 0.28790931\n",
      "Iteration 53, loss = 0.28679577\n",
      "Iteration 54, loss = 0.28574617\n",
      "Iteration 55, loss = 0.28471713\n",
      "Iteration 56, loss = 0.28373075\n",
      "Iteration 57, loss = 0.28273462\n",
      "Iteration 58, loss = 0.28181006\n",
      "Iteration 59, loss = 0.28087161\n",
      "Iteration 60, loss = 0.27996673\n",
      "Iteration 61, loss = 0.27910440\n",
      "Iteration 62, loss = 0.27821516\n",
      "Iteration 63, loss = 0.27737841\n",
      "Iteration 64, loss = 0.27654445\n",
      "Iteration 65, loss = 0.27576712\n",
      "Iteration 66, loss = 0.27494192\n",
      "Iteration 67, loss = 0.27417052\n",
      "Iteration 68, loss = 0.27342354\n",
      "Iteration 69, loss = 0.27266872\n",
      "Iteration 70, loss = 0.27194592\n",
      "Iteration 71, loss = 0.27124107\n",
      "Iteration 72, loss = 0.27054879\n",
      "Iteration 73, loss = 0.26984852\n",
      "Iteration 74, loss = 0.26919061\n",
      "Iteration 75, loss = 0.26853697\n",
      "Iteration 76, loss = 0.26787913\n",
      "Iteration 77, loss = 0.26723883\n",
      "Iteration 78, loss = 0.26661216\n",
      "Iteration 79, loss = 0.26600888\n",
      "Iteration 80, loss = 0.26538813\n",
      "Iteration 81, loss = 0.26482160\n",
      "Iteration 82, loss = 0.26427881\n",
      "Iteration 83, loss = 0.26366178\n",
      "Iteration 84, loss = 0.26310021\n",
      "Iteration 85, loss = 0.26252952\n",
      "Iteration 86, loss = 0.26199193\n",
      "Iteration 87, loss = 0.26145293\n",
      "Iteration 88, loss = 0.26092045\n",
      "Iteration 89, loss = 0.26039516\n",
      "Iteration 90, loss = 0.25987769\n",
      "Iteration 91, loss = 0.25936695\n",
      "Iteration 92, loss = 0.25887567\n",
      "Iteration 93, loss = 0.25836887\n",
      "Iteration 94, loss = 0.25789076\n",
      "Iteration 95, loss = 0.25739422\n",
      "Iteration 96, loss = 0.25694333\n",
      "Iteration 97, loss = 0.25647275\n",
      "Iteration 98, loss = 0.25596737\n",
      "Iteration 99, loss = 0.25552910\n",
      "Iteration 100, loss = 0.25509508\n",
      "Iteration 101, loss = 0.25461676\n",
      "Iteration 102, loss = 0.25416842\n",
      "Iteration 103, loss = 0.25374955\n",
      "Iteration 104, loss = 0.25328734\n",
      "Iteration 105, loss = 0.25286166\n",
      "Iteration 106, loss = 0.25244033\n",
      "Iteration 107, loss = 0.25201318\n",
      "Iteration 108, loss = 0.25158842\n",
      "Iteration 109, loss = 0.25117486\n",
      "Iteration 110, loss = 0.25076562\n",
      "Iteration 111, loss = 0.25037603\n",
      "Iteration 112, loss = 0.24996230\n",
      "Iteration 113, loss = 0.24956387\n",
      "Iteration 114, loss = 0.24920603\n",
      "Iteration 115, loss = 0.24878229\n",
      "Iteration 116, loss = 0.24838865\n",
      "Iteration 117, loss = 0.24801015\n",
      "Iteration 118, loss = 0.24761639\n",
      "Iteration 119, loss = 0.24725329\n",
      "Iteration 120, loss = 0.24688765\n",
      "Iteration 121, loss = 0.24651203\n",
      "Iteration 122, loss = 0.24613982\n",
      "Iteration 123, loss = 0.24578996\n",
      "Iteration 124, loss = 0.24543576\n",
      "Iteration 125, loss = 0.24506081\n",
      "Iteration 126, loss = 0.24470071\n",
      "Iteration 127, loss = 0.24434940\n",
      "Iteration 128, loss = 0.24399519\n",
      "Iteration 129, loss = 0.24365079\n",
      "Iteration 130, loss = 0.24333181\n",
      "Iteration 131, loss = 0.24295571\n",
      "Iteration 132, loss = 0.24260580\n",
      "Iteration 133, loss = 0.24227908\n",
      "Iteration 134, loss = 0.24196250\n",
      "Iteration 135, loss = 0.24158223\n",
      "Iteration 136, loss = 0.24125122\n",
      "Iteration 137, loss = 0.24092555\n",
      "Iteration 138, loss = 0.24058543\n",
      "Iteration 139, loss = 0.24026168\n",
      "Iteration 140, loss = 0.23994348\n",
      "Iteration 141, loss = 0.23960754\n",
      "Iteration 142, loss = 0.23927251\n",
      "Iteration 143, loss = 0.23897387\n",
      "Iteration 144, loss = 0.23865083\n",
      "Iteration 145, loss = 0.23831203\n",
      "Iteration 146, loss = 0.23799767\n",
      "Iteration 147, loss = 0.23768267\n",
      "Iteration 148, loss = 0.23736722\n",
      "Iteration 149, loss = 0.23706195\n",
      "Iteration 150, loss = 0.23674362\n",
      "Iteration 151, loss = 0.23643413\n",
      "Iteration 152, loss = 0.23614854\n",
      "Iteration 153, loss = 0.23582823\n",
      "Iteration 154, loss = 0.23550913\n",
      "Iteration 155, loss = 0.23519788\n",
      "Iteration 156, loss = 0.23489327\n",
      "Iteration 157, loss = 0.23460577\n",
      "Iteration 158, loss = 0.23428629\n",
      "Iteration 159, loss = 0.23399760\n",
      "Iteration 160, loss = 0.23368477\n",
      "Iteration 161, loss = 0.23339330\n",
      "Iteration 162, loss = 0.23311219\n",
      "Iteration 163, loss = 0.23278966\n",
      "Iteration 164, loss = 0.23251179\n",
      "Iteration 165, loss = 0.23221118\n",
      "Iteration 166, loss = 0.23193754\n",
      "Iteration 167, loss = 0.23162063\n",
      "Iteration 168, loss = 0.23133105\n",
      "Iteration 169, loss = 0.23104270\n",
      "Iteration 170, loss = 0.23074942\n",
      "Iteration 171, loss = 0.23046062\n",
      "Iteration 172, loss = 0.23018866\n",
      "Iteration 173, loss = 0.22990008\n",
      "Iteration 174, loss = 0.22961183\n",
      "Iteration 175, loss = 0.22933458\n",
      "Iteration 176, loss = 0.22905486\n",
      "Iteration 177, loss = 0.22875624\n",
      "Iteration 178, loss = 0.22849390\n",
      "Iteration 179, loss = 0.22820895\n",
      "Iteration 180, loss = 0.22792888\n",
      "Iteration 181, loss = 0.22768612\n",
      "Iteration 182, loss = 0.22736655\n",
      "Iteration 183, loss = 0.22711141\n",
      "Iteration 184, loss = 0.22683994\n",
      "Iteration 185, loss = 0.22655424\n",
      "Iteration 186, loss = 0.22628264\n",
      "Iteration 187, loss = 0.22601553\n",
      "Iteration 188, loss = 0.22574698\n",
      "Iteration 189, loss = 0.22545880\n",
      "Iteration 190, loss = 0.22523484\n",
      "Iteration 191, loss = 0.22496073\n",
      "Iteration 192, loss = 0.22466906\n",
      "Iteration 193, loss = 0.22438356\n",
      "Iteration 194, loss = 0.22412615\n",
      "Iteration 195, loss = 0.22386132\n",
      "Iteration 196, loss = 0.22357388\n",
      "Iteration 197, loss = 0.22330574\n",
      "Iteration 198, loss = 0.22304049\n",
      "Iteration 199, loss = 0.22276305\n",
      "Iteration 200, loss = 0.22252713\n",
      "Iteration 1, loss = 0.79865590\n",
      "Iteration 2, loss = 0.63917790\n",
      "Iteration 3, loss = 0.53335930\n",
      "Iteration 4, loss = 0.48019579\n",
      "Iteration 5, loss = 0.45247498\n",
      "Iteration 6, loss = 0.43547400\n",
      "Iteration 7, loss = 0.42378817\n",
      "Iteration 8, loss = 0.41409948\n",
      "Iteration 9, loss = 0.40615577\n",
      "Iteration 10, loss = 0.39917490\n",
      "Iteration 11, loss = 0.39274991\n",
      "Iteration 12, loss = 0.38679241\n",
      "Iteration 13, loss = 0.38136502\n",
      "Iteration 14, loss = 0.37618113\n",
      "Iteration 15, loss = 0.37131939\n",
      "Iteration 16, loss = 0.36674182\n",
      "Iteration 17, loss = 0.36246995\n",
      "Iteration 18, loss = 0.35840351\n",
      "Iteration 19, loss = 0.35450403\n",
      "Iteration 20, loss = 0.35087537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.34740091\n",
      "Iteration 22, loss = 0.34412158\n",
      "Iteration 23, loss = 0.34092702\n",
      "Iteration 24, loss = 0.33793473\n",
      "Iteration 25, loss = 0.33506608\n",
      "Iteration 26, loss = 0.33236822\n",
      "Iteration 27, loss = 0.32977066\n",
      "Iteration 28, loss = 0.32728088\n",
      "Iteration 29, loss = 0.32491484\n",
      "Iteration 30, loss = 0.32259653\n",
      "Iteration 31, loss = 0.32043791\n",
      "Iteration 32, loss = 0.31833018\n",
      "Iteration 33, loss = 0.31632843\n",
      "Iteration 34, loss = 0.31438721\n",
      "Iteration 35, loss = 0.31255369\n",
      "Iteration 36, loss = 0.31074684\n",
      "Iteration 37, loss = 0.30903771\n",
      "Iteration 38, loss = 0.30736312\n",
      "Iteration 39, loss = 0.30576208\n",
      "Iteration 40, loss = 0.30418516\n",
      "Iteration 41, loss = 0.30273030\n",
      "Iteration 42, loss = 0.30127688\n",
      "Iteration 43, loss = 0.29987761\n",
      "Iteration 44, loss = 0.29855719\n",
      "Iteration 45, loss = 0.29723990\n",
      "Iteration 46, loss = 0.29600004\n",
      "Iteration 47, loss = 0.29483951\n",
      "Iteration 48, loss = 0.29355858\n",
      "Iteration 49, loss = 0.29246783\n",
      "Iteration 50, loss = 0.29132040\n",
      "Iteration 51, loss = 0.29026084\n",
      "Iteration 52, loss = 0.28923009\n",
      "Iteration 53, loss = 0.28818051\n",
      "Iteration 54, loss = 0.28718308\n",
      "Iteration 55, loss = 0.28621950\n",
      "Iteration 56, loss = 0.28529437\n",
      "Iteration 57, loss = 0.28437701\n",
      "Iteration 58, loss = 0.28349668\n",
      "Iteration 59, loss = 0.28262578\n",
      "Iteration 60, loss = 0.28178686\n",
      "Iteration 61, loss = 0.28095565\n",
      "Iteration 62, loss = 0.28014169\n",
      "Iteration 63, loss = 0.27937899\n",
      "Iteration 64, loss = 0.27859264\n",
      "Iteration 65, loss = 0.27787112\n",
      "Iteration 66, loss = 0.27711876\n",
      "Iteration 67, loss = 0.27639811\n",
      "Iteration 68, loss = 0.27570196\n",
      "Iteration 69, loss = 0.27501205\n",
      "Iteration 70, loss = 0.27433937\n",
      "Iteration 71, loss = 0.27368219\n",
      "Iteration 72, loss = 0.27303963\n",
      "Iteration 73, loss = 0.27239663\n",
      "Iteration 74, loss = 0.27179160\n",
      "Iteration 75, loss = 0.27116592\n",
      "Iteration 76, loss = 0.27056442\n",
      "Iteration 77, loss = 0.26998747\n",
      "Iteration 78, loss = 0.26940987\n",
      "Iteration 79, loss = 0.26882916\n",
      "Iteration 80, loss = 0.26828173\n",
      "Iteration 81, loss = 0.26774848\n",
      "Iteration 82, loss = 0.26722715\n",
      "Iteration 83, loss = 0.26668598\n",
      "Iteration 84, loss = 0.26616860\n",
      "Iteration 85, loss = 0.26562130\n",
      "Iteration 86, loss = 0.26513989\n",
      "Iteration 87, loss = 0.26463616\n",
      "Iteration 88, loss = 0.26415155\n",
      "Iteration 89, loss = 0.26366602\n",
      "Iteration 90, loss = 0.26318109\n",
      "Iteration 91, loss = 0.26273449\n",
      "Iteration 92, loss = 0.26226625\n",
      "Iteration 93, loss = 0.26183108\n",
      "Iteration 94, loss = 0.26137745\n",
      "Iteration 95, loss = 0.26094481\n",
      "Iteration 96, loss = 0.26054012\n",
      "Iteration 97, loss = 0.26008753\n",
      "Iteration 98, loss = 0.25965470\n",
      "Iteration 99, loss = 0.25925593\n",
      "Iteration 100, loss = 0.25885223\n",
      "Iteration 101, loss = 0.25843906\n",
      "Iteration 102, loss = 0.25802292\n",
      "Iteration 103, loss = 0.25763808\n",
      "Iteration 104, loss = 0.25723054\n",
      "Iteration 105, loss = 0.25684141\n",
      "Iteration 106, loss = 0.25646391\n",
      "Iteration 107, loss = 0.25607827\n",
      "Iteration 108, loss = 0.25570809\n",
      "Iteration 109, loss = 0.25532808\n",
      "Iteration 110, loss = 0.25495774\n",
      "Iteration 111, loss = 0.25460450\n",
      "Iteration 112, loss = 0.25423784\n",
      "Iteration 113, loss = 0.25387420\n",
      "Iteration 114, loss = 0.25352834\n",
      "Iteration 115, loss = 0.25316172\n",
      "Iteration 116, loss = 0.25282950\n",
      "Iteration 117, loss = 0.25246877\n",
      "Iteration 118, loss = 0.25211216\n",
      "Iteration 119, loss = 0.25178293\n",
      "Iteration 120, loss = 0.25144782\n",
      "Iteration 121, loss = 0.25111148\n",
      "Iteration 122, loss = 0.25078253\n",
      "Iteration 123, loss = 0.25045435\n",
      "Iteration 124, loss = 0.25013351\n",
      "Iteration 125, loss = 0.24980752\n",
      "Iteration 126, loss = 0.24948761\n",
      "Iteration 127, loss = 0.24914483\n",
      "Iteration 128, loss = 0.24882361\n",
      "Iteration 129, loss = 0.24851765\n",
      "Iteration 130, loss = 0.24821453\n",
      "Iteration 131, loss = 0.24787856\n",
      "Iteration 132, loss = 0.24757419\n",
      "Iteration 133, loss = 0.24726580\n",
      "Iteration 134, loss = 0.24697817\n",
      "Iteration 135, loss = 0.24665565\n",
      "Iteration 136, loss = 0.24636294\n",
      "Iteration 137, loss = 0.24605600\n",
      "Iteration 138, loss = 0.24576622\n",
      "Iteration 139, loss = 0.24546530\n",
      "Iteration 140, loss = 0.24518400\n",
      "Iteration 141, loss = 0.24487437\n",
      "Iteration 142, loss = 0.24458731\n",
      "Iteration 143, loss = 0.24431932\n",
      "Iteration 144, loss = 0.24401952\n",
      "Iteration 145, loss = 0.24372490\n",
      "Iteration 146, loss = 0.24343196\n",
      "Iteration 147, loss = 0.24317049\n",
      "Iteration 148, loss = 0.24288374\n",
      "Iteration 149, loss = 0.24260728\n",
      "Iteration 150, loss = 0.24233819\n",
      "Iteration 151, loss = 0.24207747\n",
      "Iteration 152, loss = 0.24179243\n",
      "Iteration 153, loss = 0.24151126\n",
      "Iteration 154, loss = 0.24124805\n",
      "Iteration 155, loss = 0.24095829\n",
      "Iteration 156, loss = 0.24067692\n",
      "Iteration 157, loss = 0.24043109\n",
      "Iteration 158, loss = 0.24016067\n",
      "Iteration 159, loss = 0.23988397\n",
      "Iteration 160, loss = 0.23962553\n",
      "Iteration 161, loss = 0.23936976\n",
      "Iteration 162, loss = 0.23911880\n",
      "Iteration 163, loss = 0.23882710\n",
      "Iteration 164, loss = 0.23858159\n",
      "Iteration 165, loss = 0.23831529\n",
      "Iteration 166, loss = 0.23805091\n",
      "Iteration 167, loss = 0.23779023\n",
      "Iteration 168, loss = 0.23754529\n",
      "Iteration 169, loss = 0.23728366\n",
      "Iteration 170, loss = 0.23703406\n",
      "Iteration 171, loss = 0.23678483\n",
      "Iteration 172, loss = 0.23652602\n",
      "Iteration 173, loss = 0.23628222\n",
      "Iteration 174, loss = 0.23603329\n",
      "Iteration 175, loss = 0.23578630\n",
      "Iteration 176, loss = 0.23551944\n",
      "Iteration 177, loss = 0.23528459\n",
      "Iteration 178, loss = 0.23503627\n",
      "Iteration 179, loss = 0.23477986\n",
      "Iteration 180, loss = 0.23453862\n",
      "Iteration 181, loss = 0.23431159\n",
      "Iteration 182, loss = 0.23404499\n",
      "Iteration 183, loss = 0.23381908\n",
      "Iteration 184, loss = 0.23356658\n",
      "Iteration 185, loss = 0.23331366\n",
      "Iteration 186, loss = 0.23307844\n",
      "Iteration 187, loss = 0.23285354\n",
      "Iteration 188, loss = 0.23259705\n",
      "Iteration 189, loss = 0.23234645\n",
      "Iteration 190, loss = 0.23214493\n",
      "Iteration 191, loss = 0.23191799\n",
      "Iteration 192, loss = 0.23164725\n",
      "Iteration 193, loss = 0.23138661\n",
      "Iteration 194, loss = 0.23115786\n",
      "Iteration 195, loss = 0.23092283\n",
      "Iteration 196, loss = 0.23066474\n",
      "Iteration 197, loss = 0.23041683\n",
      "Iteration 198, loss = 0.23019137\n",
      "Iteration 199, loss = 0.22994415\n",
      "Iteration 200, loss = 0.22972552\n",
      "Iteration 1, loss = 0.69911757\n",
      "Iteration 2, loss = 0.48060840\n",
      "Iteration 3, loss = 0.38750640\n",
      "Iteration 4, loss = 0.34560133\n",
      "Iteration 5, loss = 0.32060843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.30302012\n",
      "Iteration 7, loss = 0.29001120\n",
      "Iteration 8, loss = 0.27956141\n",
      "Iteration 9, loss = 0.27075632\n",
      "Iteration 10, loss = 0.26325371\n",
      "Iteration 11, loss = 0.25658354\n",
      "Iteration 12, loss = 0.25058881\n",
      "Iteration 13, loss = 0.24517953\n",
      "Iteration 14, loss = 0.24019150\n",
      "Iteration 15, loss = 0.23542526\n",
      "Iteration 16, loss = 0.23107292\n",
      "Iteration 17, loss = 0.22670121\n",
      "Iteration 18, loss = 0.22281578\n",
      "Iteration 19, loss = 0.21893181\n",
      "Iteration 20, loss = 0.21525069\n",
      "Iteration 21, loss = 0.21144223\n",
      "Iteration 22, loss = 0.20824743\n",
      "Iteration 23, loss = 0.20475182\n",
      "Iteration 24, loss = 0.20127410\n",
      "Iteration 25, loss = 0.19828248\n",
      "Iteration 26, loss = 0.19517758\n",
      "Iteration 27, loss = 0.19214683\n",
      "Iteration 28, loss = 0.18938015\n",
      "Iteration 29, loss = 0.18633537\n",
      "Iteration 30, loss = 0.18372352\n",
      "Iteration 31, loss = 0.18101691\n",
      "Iteration 32, loss = 0.17828711\n",
      "Iteration 33, loss = 0.17582458\n",
      "Iteration 34, loss = 0.17349773\n",
      "Iteration 35, loss = 0.17160807\n",
      "Iteration 36, loss = 0.16920385\n",
      "Iteration 37, loss = 0.16665074\n",
      "Iteration 38, loss = 0.16442053\n",
      "Iteration 39, loss = 0.16246524\n",
      "Iteration 40, loss = 0.16076531\n",
      "Iteration 41, loss = 0.15839839\n",
      "Iteration 42, loss = 0.15676597\n",
      "Iteration 43, loss = 0.15439100\n",
      "Iteration 44, loss = 0.15301735\n",
      "Iteration 45, loss = 0.15123833\n",
      "Iteration 46, loss = 0.14926694\n",
      "Iteration 47, loss = 0.14795141\n",
      "Iteration 48, loss = 0.14599673\n",
      "Iteration 49, loss = 0.14438352\n",
      "Iteration 50, loss = 0.14238709\n",
      "Iteration 51, loss = 0.14112823\n",
      "Iteration 52, loss = 0.14018358\n",
      "Iteration 53, loss = 0.13803346\n",
      "Iteration 54, loss = 0.13672829\n",
      "Iteration 55, loss = 0.13528992\n",
      "Iteration 56, loss = 0.13373191\n",
      "Iteration 57, loss = 0.13283372\n",
      "Iteration 58, loss = 0.13138833\n",
      "Iteration 59, loss = 0.13001477\n",
      "Iteration 60, loss = 0.12892932\n",
      "Iteration 61, loss = 0.12745681\n",
      "Iteration 62, loss = 0.12638688\n",
      "Iteration 63, loss = 0.12515351\n",
      "Iteration 64, loss = 0.12433251\n",
      "Iteration 65, loss = 0.12287426\n",
      "Iteration 66, loss = 0.12176353\n",
      "Iteration 67, loss = 0.12065002\n",
      "Iteration 68, loss = 0.11952466\n",
      "Iteration 69, loss = 0.11885004\n",
      "Iteration 70, loss = 0.11732415\n",
      "Iteration 71, loss = 0.11646595\n",
      "Iteration 72, loss = 0.11547724\n",
      "Iteration 73, loss = 0.11477712\n",
      "Iteration 74, loss = 0.11342055\n",
      "Iteration 75, loss = 0.11250611\n",
      "Iteration 76, loss = 0.11154854\n",
      "Iteration 77, loss = 0.11082275\n",
      "Iteration 78, loss = 0.10991482\n",
      "Iteration 79, loss = 0.10907707\n",
      "Iteration 80, loss = 0.10792383\n",
      "Iteration 81, loss = 0.10736636\n",
      "Iteration 82, loss = 0.10673594\n",
      "Iteration 83, loss = 0.10562052\n",
      "Iteration 84, loss = 0.10494311\n",
      "Iteration 85, loss = 0.10410427\n",
      "Iteration 86, loss = 0.10309906\n",
      "Iteration 87, loss = 0.10235499\n",
      "Iteration 88, loss = 0.10163596\n",
      "Iteration 89, loss = 0.10067986\n",
      "Iteration 90, loss = 0.09999151\n",
      "Iteration 91, loss = 0.09890722\n",
      "Iteration 92, loss = 0.09859582\n",
      "Iteration 93, loss = 0.09785662\n",
      "Iteration 94, loss = 0.09689811\n",
      "Iteration 95, loss = 0.09644145\n",
      "Iteration 96, loss = 0.09550407\n",
      "Iteration 97, loss = 0.09489282\n",
      "Iteration 98, loss = 0.09409600\n",
      "Iteration 99, loss = 0.09311529\n",
      "Iteration 100, loss = 0.09286676\n",
      "Iteration 101, loss = 0.09218684\n",
      "Iteration 102, loss = 0.09115403\n",
      "Iteration 103, loss = 0.09076557\n",
      "Iteration 104, loss = 0.09004847\n",
      "Iteration 105, loss = 0.08931415\n",
      "Iteration 106, loss = 0.08852486\n",
      "Iteration 107, loss = 0.08818140\n",
      "Iteration 108, loss = 0.08744580\n",
      "Iteration 109, loss = 0.08652515\n",
      "Iteration 110, loss = 0.08643313\n",
      "Iteration 111, loss = 0.08561703\n",
      "Iteration 112, loss = 0.08490475\n",
      "Iteration 113, loss = 0.08425223\n",
      "Iteration 114, loss = 0.08342955\n",
      "Iteration 115, loss = 0.08301636\n",
      "Iteration 116, loss = 0.08211999\n",
      "Iteration 117, loss = 0.08181416\n",
      "Iteration 118, loss = 0.08103180\n",
      "Iteration 119, loss = 0.08057397\n",
      "Iteration 120, loss = 0.08024235\n",
      "Iteration 121, loss = 0.07965151\n",
      "Iteration 122, loss = 0.07870511\n",
      "Iteration 123, loss = 0.07785015\n",
      "Iteration 124, loss = 0.07773194\n",
      "Iteration 125, loss = 0.07759883\n",
      "Iteration 126, loss = 0.07678821\n",
      "Iteration 127, loss = 0.07646563\n",
      "Iteration 128, loss = 0.07565375\n",
      "Iteration 129, loss = 0.07483058\n",
      "Iteration 130, loss = 0.07450448\n",
      "Iteration 131, loss = 0.07419125\n",
      "Iteration 132, loss = 0.07378848\n",
      "Iteration 133, loss = 0.07300708\n",
      "Iteration 134, loss = 0.07227344\n",
      "Iteration 135, loss = 0.07208767\n",
      "Iteration 136, loss = 0.07106320\n",
      "Iteration 137, loss = 0.07071879\n",
      "Iteration 138, loss = 0.07014221\n",
      "Iteration 139, loss = 0.06989597\n",
      "Iteration 140, loss = 0.06929688\n",
      "Iteration 141, loss = 0.06858612\n",
      "Iteration 142, loss = 0.06857561\n",
      "Iteration 143, loss = 0.06824414\n",
      "Iteration 144, loss = 0.06713979\n",
      "Iteration 145, loss = 0.06675778\n",
      "Iteration 146, loss = 0.06622003\n",
      "Iteration 147, loss = 0.06624080\n",
      "Iteration 148, loss = 0.06547056\n",
      "Iteration 149, loss = 0.06531525\n",
      "Iteration 150, loss = 0.06421132\n",
      "Iteration 151, loss = 0.06466739\n",
      "Iteration 152, loss = 0.06373037\n",
      "Iteration 153, loss = 0.06328246\n",
      "Iteration 154, loss = 0.06268366\n",
      "Iteration 155, loss = 0.06262858\n",
      "Iteration 156, loss = 0.06180954\n",
      "Iteration 157, loss = 0.06123199\n",
      "Iteration 158, loss = 0.06122448\n",
      "Iteration 159, loss = 0.06027111\n",
      "Iteration 160, loss = 0.06017164\n",
      "Iteration 161, loss = 0.05978601\n",
      "Iteration 162, loss = 0.05954763\n",
      "Iteration 163, loss = 0.05895371\n",
      "Iteration 164, loss = 0.05876087\n",
      "Iteration 165, loss = 0.05812464\n",
      "Iteration 166, loss = 0.05783111\n",
      "Iteration 167, loss = 0.05747801\n",
      "Iteration 168, loss = 0.05673743\n",
      "Iteration 169, loss = 0.05668241\n",
      "Iteration 170, loss = 0.05664491\n",
      "Iteration 171, loss = 0.05544272\n",
      "Iteration 172, loss = 0.05545450\n",
      "Iteration 173, loss = 0.05474316\n",
      "Iteration 174, loss = 0.05450389\n",
      "Iteration 175, loss = 0.05382785\n",
      "Iteration 176, loss = 0.05391403\n",
      "Iteration 177, loss = 0.05335309\n",
      "Iteration 178, loss = 0.05268969\n",
      "Iteration 179, loss = 0.05258117\n",
      "Iteration 180, loss = 0.05196590\n",
      "Iteration 181, loss = 0.05169403\n",
      "Iteration 182, loss = 0.05130819\n",
      "Iteration 183, loss = 0.05103996\n",
      "Iteration 184, loss = 0.05044796\n",
      "Iteration 185, loss = 0.05036350\n",
      "Iteration 186, loss = 0.04979173\n",
      "Iteration 187, loss = 0.04950165\n",
      "Iteration 188, loss = 0.04904746\n",
      "Iteration 189, loss = 0.04874314\n",
      "Iteration 190, loss = 0.04817718\n",
      "Iteration 191, loss = 0.04770850\n",
      "Iteration 192, loss = 0.04777510\n",
      "Iteration 193, loss = 0.04726259\n",
      "Iteration 194, loss = 0.04684264\n",
      "Iteration 195, loss = 0.04667826\n",
      "Iteration 196, loss = 0.04585038\n",
      "Iteration 197, loss = 0.04586247\n",
      "Iteration 198, loss = 0.04555245\n",
      "Iteration 199, loss = 0.04518766\n",
      "Iteration 200, loss = 0.04482747\n",
      "Iteration 1, loss = 0.69689933\n",
      "Iteration 2, loss = 0.47814739\n",
      "Iteration 3, loss = 0.38587328\n",
      "Iteration 4, loss = 0.34465455\n",
      "Iteration 5, loss = 0.31980709\n",
      "Iteration 6, loss = 0.30248758\n",
      "Iteration 7, loss = 0.28964087\n",
      "Iteration 8, loss = 0.27923516\n",
      "Iteration 9, loss = 0.27097315\n",
      "Iteration 10, loss = 0.26384437\n",
      "Iteration 11, loss = 0.25746677\n",
      "Iteration 12, loss = 0.25193084\n",
      "Iteration 13, loss = 0.24682277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.24210242\n",
      "Iteration 15, loss = 0.23769260\n",
      "Iteration 16, loss = 0.23355578\n",
      "Iteration 17, loss = 0.22957075\n",
      "Iteration 18, loss = 0.22559068\n",
      "Iteration 19, loss = 0.22198391\n",
      "Iteration 20, loss = 0.21822326\n",
      "Iteration 21, loss = 0.21482320\n",
      "Iteration 22, loss = 0.21156201\n",
      "Iteration 23, loss = 0.20826405\n",
      "Iteration 24, loss = 0.20509163\n",
      "Iteration 25, loss = 0.20198054\n",
      "Iteration 26, loss = 0.19914781\n",
      "Iteration 27, loss = 0.19630755\n",
      "Iteration 28, loss = 0.19368888\n",
      "Iteration 29, loss = 0.19094968\n",
      "Iteration 30, loss = 0.18840098\n",
      "Iteration 31, loss = 0.18581733\n",
      "Iteration 32, loss = 0.18336859\n",
      "Iteration 33, loss = 0.18122965\n",
      "Iteration 34, loss = 0.17890707\n",
      "Iteration 35, loss = 0.17682970\n",
      "Iteration 36, loss = 0.17460318\n",
      "Iteration 37, loss = 0.17223914\n",
      "Iteration 38, loss = 0.17005503\n",
      "Iteration 39, loss = 0.16817287\n",
      "Iteration 40, loss = 0.16657189\n",
      "Iteration 41, loss = 0.16436020\n",
      "Iteration 42, loss = 0.16252033\n",
      "Iteration 43, loss = 0.16045180\n",
      "Iteration 44, loss = 0.15875010\n",
      "Iteration 45, loss = 0.15706684\n",
      "Iteration 46, loss = 0.15544264\n",
      "Iteration 47, loss = 0.15363885\n",
      "Iteration 48, loss = 0.15192009\n",
      "Iteration 49, loss = 0.15047935\n",
      "Iteration 50, loss = 0.14872734\n",
      "Iteration 51, loss = 0.14733425\n",
      "Iteration 52, loss = 0.14598409\n",
      "Iteration 53, loss = 0.14408522\n",
      "Iteration 54, loss = 0.14302381\n",
      "Iteration 55, loss = 0.14150772\n",
      "Iteration 56, loss = 0.13991874\n",
      "Iteration 57, loss = 0.13892789\n",
      "Iteration 58, loss = 0.13787752\n",
      "Iteration 59, loss = 0.13636641\n",
      "Iteration 60, loss = 0.13531650\n",
      "Iteration 61, loss = 0.13381409\n",
      "Iteration 62, loss = 0.13254667\n",
      "Iteration 63, loss = 0.13137218\n",
      "Iteration 64, loss = 0.13016303\n",
      "Iteration 65, loss = 0.12909716\n",
      "Iteration 66, loss = 0.12802225\n",
      "Iteration 67, loss = 0.12686023\n",
      "Iteration 68, loss = 0.12596116\n",
      "Iteration 69, loss = 0.12491111\n",
      "Iteration 70, loss = 0.12379870\n",
      "Iteration 71, loss = 0.12274265\n",
      "Iteration 72, loss = 0.12192661\n",
      "Iteration 73, loss = 0.12085791\n",
      "Iteration 74, loss = 0.11976845\n",
      "Iteration 75, loss = 0.11933185\n",
      "Iteration 76, loss = 0.11797089\n",
      "Iteration 77, loss = 0.11698524\n",
      "Iteration 78, loss = 0.11600255\n",
      "Iteration 79, loss = 0.11539460\n",
      "Iteration 80, loss = 0.11435956\n",
      "Iteration 81, loss = 0.11357352\n",
      "Iteration 82, loss = 0.11262305\n",
      "Iteration 83, loss = 0.11178085\n",
      "Iteration 84, loss = 0.11111514\n",
      "Iteration 85, loss = 0.11009914\n",
      "Iteration 86, loss = 0.10923460\n",
      "Iteration 87, loss = 0.10848323\n",
      "Iteration 88, loss = 0.10776416\n",
      "Iteration 89, loss = 0.10691142\n",
      "Iteration 90, loss = 0.10649755\n",
      "Iteration 91, loss = 0.10525380\n",
      "Iteration 92, loss = 0.10428951\n",
      "Iteration 93, loss = 0.10386008\n",
      "Iteration 94, loss = 0.10282189\n",
      "Iteration 95, loss = 0.10246230\n",
      "Iteration 96, loss = 0.10149806\n",
      "Iteration 97, loss = 0.10077667\n",
      "Iteration 98, loss = 0.10042671\n",
      "Iteration 99, loss = 0.09938743\n",
      "Iteration 100, loss = 0.09890899\n",
      "Iteration 101, loss = 0.09848911\n",
      "Iteration 102, loss = 0.09729947\n",
      "Iteration 103, loss = 0.09673066\n",
      "Iteration 104, loss = 0.09601207\n",
      "Iteration 105, loss = 0.09554312\n",
      "Iteration 106, loss = 0.09481650\n",
      "Iteration 107, loss = 0.09414036\n",
      "Iteration 108, loss = 0.09349318\n",
      "Iteration 109, loss = 0.09278566\n",
      "Iteration 110, loss = 0.09239120\n",
      "Iteration 111, loss = 0.09157006\n",
      "Iteration 112, loss = 0.09084357\n",
      "Iteration 113, loss = 0.09032948\n",
      "Iteration 114, loss = 0.08978276\n",
      "Iteration 115, loss = 0.08905398\n",
      "Iteration 116, loss = 0.08828594\n",
      "Iteration 117, loss = 0.08764756\n",
      "Iteration 118, loss = 0.08721455\n",
      "Iteration 119, loss = 0.08665508\n",
      "Iteration 120, loss = 0.08631926\n",
      "Iteration 121, loss = 0.08534335\n",
      "Iteration 122, loss = 0.08486618\n",
      "Iteration 123, loss = 0.08414361\n",
      "Iteration 124, loss = 0.08379103\n",
      "Iteration 125, loss = 0.08438162\n",
      "Iteration 126, loss = 0.08264659\n",
      "Iteration 127, loss = 0.08199091\n",
      "Iteration 128, loss = 0.08158991\n",
      "Iteration 129, loss = 0.08107902\n",
      "Iteration 130, loss = 0.08035572\n",
      "Iteration 131, loss = 0.07989772\n",
      "Iteration 132, loss = 0.07956893\n",
      "Iteration 133, loss = 0.07931136\n",
      "Iteration 134, loss = 0.07844662\n",
      "Iteration 135, loss = 0.07793521\n",
      "Iteration 136, loss = 0.07716576\n",
      "Iteration 137, loss = 0.07696347\n",
      "Iteration 138, loss = 0.07659665\n",
      "Iteration 139, loss = 0.07583041\n",
      "Iteration 140, loss = 0.07498644\n",
      "Iteration 141, loss = 0.07463832\n",
      "Iteration 142, loss = 0.07425657\n",
      "Iteration 143, loss = 0.07444798\n",
      "Iteration 144, loss = 0.07394960\n",
      "Iteration 145, loss = 0.07261220\n",
      "Iteration 146, loss = 0.07243575\n",
      "Iteration 147, loss = 0.07202176\n",
      "Iteration 148, loss = 0.07171647\n",
      "Iteration 149, loss = 0.07114475\n",
      "Iteration 150, loss = 0.07024819\n",
      "Iteration 151, loss = 0.06997351\n",
      "Iteration 152, loss = 0.06985174\n",
      "Iteration 153, loss = 0.06899813\n",
      "Iteration 154, loss = 0.06850875\n",
      "Iteration 155, loss = 0.06815726\n",
      "Iteration 156, loss = 0.06743772\n",
      "Iteration 157, loss = 0.06673717\n",
      "Iteration 158, loss = 0.06664894\n",
      "Iteration 159, loss = 0.06649298\n",
      "Iteration 160, loss = 0.06561047\n",
      "Iteration 161, loss = 0.06537444\n",
      "Iteration 162, loss = 0.06507789\n",
      "Iteration 163, loss = 0.06496083\n",
      "Iteration 164, loss = 0.06432612\n",
      "Iteration 165, loss = 0.06347927\n",
      "Iteration 166, loss = 0.06288555\n",
      "Iteration 167, loss = 0.06270228\n",
      "Iteration 168, loss = 0.06231581\n",
      "Iteration 169, loss = 0.06220763\n",
      "Iteration 170, loss = 0.06177053\n",
      "Iteration 171, loss = 0.06091090\n",
      "Iteration 172, loss = 0.06066320\n",
      "Iteration 173, loss = 0.06036358\n",
      "Iteration 174, loss = 0.05977995\n",
      "Iteration 175, loss = 0.05911195\n",
      "Iteration 176, loss = 0.05898173\n",
      "Iteration 177, loss = 0.05846736\n",
      "Iteration 178, loss = 0.05809417\n",
      "Iteration 179, loss = 0.05779518\n",
      "Iteration 180, loss = 0.05757382\n",
      "Iteration 181, loss = 0.05712037\n",
      "Iteration 182, loss = 0.05644151\n",
      "Iteration 183, loss = 0.05615593\n",
      "Iteration 184, loss = 0.05550753\n",
      "Iteration 185, loss = 0.05524647\n",
      "Iteration 186, loss = 0.05517695\n",
      "Iteration 187, loss = 0.05467596\n",
      "Iteration 188, loss = 0.05388790\n",
      "Iteration 189, loss = 0.05361392\n",
      "Iteration 190, loss = 0.05336243\n",
      "Iteration 191, loss = 0.05289162\n",
      "Iteration 192, loss = 0.05276789\n",
      "Iteration 193, loss = 0.05229708\n",
      "Iteration 194, loss = 0.05194087\n",
      "Iteration 195, loss = 0.05138595\n",
      "Iteration 196, loss = 0.05079033\n",
      "Iteration 197, loss = 0.05087704\n",
      "Iteration 198, loss = 0.05042229\n",
      "Iteration 199, loss = 0.04977118\n",
      "Iteration 200, loss = 0.04946531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69559463\n",
      "Iteration 2, loss = 0.47690786\n",
      "Iteration 3, loss = 0.38433310\n",
      "Iteration 4, loss = 0.34427746\n",
      "Iteration 5, loss = 0.31997621\n",
      "Iteration 6, loss = 0.30310257\n",
      "Iteration 7, loss = 0.29069251\n",
      "Iteration 8, loss = 0.28090675\n",
      "Iteration 9, loss = 0.27282989\n",
      "Iteration 10, loss = 0.26589601\n",
      "Iteration 11, loss = 0.25996564\n",
      "Iteration 12, loss = 0.25453699\n",
      "Iteration 13, loss = 0.24989488\n",
      "Iteration 14, loss = 0.24536885\n",
      "Iteration 15, loss = 0.24104377\n",
      "Iteration 16, loss = 0.23708124\n",
      "Iteration 17, loss = 0.23344172\n",
      "Iteration 18, loss = 0.22968070\n",
      "Iteration 19, loss = 0.22610885\n",
      "Iteration 20, loss = 0.22264207\n",
      "Iteration 21, loss = 0.21929098\n",
      "Iteration 22, loss = 0.21653315\n",
      "Iteration 23, loss = 0.21318323\n",
      "Iteration 24, loss = 0.21008032\n",
      "Iteration 25, loss = 0.20704411\n",
      "Iteration 26, loss = 0.20439643\n",
      "Iteration 27, loss = 0.20152643\n",
      "Iteration 28, loss = 0.19907096\n",
      "Iteration 29, loss = 0.19626659\n",
      "Iteration 30, loss = 0.19375443\n",
      "Iteration 31, loss = 0.19125561\n",
      "Iteration 32, loss = 0.18914207\n",
      "Iteration 33, loss = 0.18685094\n",
      "Iteration 34, loss = 0.18436313\n",
      "Iteration 35, loss = 0.18224321\n",
      "Iteration 36, loss = 0.18014869\n",
      "Iteration 37, loss = 0.17800553\n",
      "Iteration 38, loss = 0.17575163\n",
      "Iteration 39, loss = 0.17370253\n",
      "Iteration 40, loss = 0.17200429\n",
      "Iteration 41, loss = 0.17008607\n",
      "Iteration 42, loss = 0.16809110\n",
      "Iteration 43, loss = 0.16618450\n",
      "Iteration 44, loss = 0.16441001\n",
      "Iteration 45, loss = 0.16256941\n",
      "Iteration 46, loss = 0.16104783\n",
      "Iteration 47, loss = 0.15907005\n",
      "Iteration 48, loss = 0.15761117\n",
      "Iteration 49, loss = 0.15582300\n",
      "Iteration 50, loss = 0.15438765\n",
      "Iteration 51, loss = 0.15300451\n",
      "Iteration 52, loss = 0.15133993\n",
      "Iteration 53, loss = 0.14965122\n",
      "Iteration 54, loss = 0.14848684\n",
      "Iteration 55, loss = 0.14658533\n",
      "Iteration 56, loss = 0.14509203\n",
      "Iteration 57, loss = 0.14412103\n",
      "Iteration 58, loss = 0.14264998\n",
      "Iteration 59, loss = 0.14133235\n",
      "Iteration 60, loss = 0.14000976\n",
      "Iteration 61, loss = 0.13847327\n",
      "Iteration 62, loss = 0.13722386\n",
      "Iteration 63, loss = 0.13611306\n",
      "Iteration 64, loss = 0.13476975\n",
      "Iteration 65, loss = 0.13368984\n",
      "Iteration 66, loss = 0.13263470\n",
      "Iteration 67, loss = 0.13147108\n",
      "Iteration 68, loss = 0.13054068\n",
      "Iteration 69, loss = 0.12923422\n",
      "Iteration 70, loss = 0.12830680\n",
      "Iteration 71, loss = 0.12716432\n",
      "Iteration 72, loss = 0.12589129\n",
      "Iteration 73, loss = 0.12508396\n",
      "Iteration 74, loss = 0.12391931\n",
      "Iteration 75, loss = 0.12304566\n",
      "Iteration 76, loss = 0.12203488\n",
      "Iteration 77, loss = 0.12079658\n",
      "Iteration 78, loss = 0.11996846\n",
      "Iteration 79, loss = 0.11890172\n",
      "Iteration 80, loss = 0.11833106\n",
      "Iteration 81, loss = 0.11701241\n",
      "Iteration 82, loss = 0.11626155\n",
      "Iteration 83, loss = 0.11531318\n",
      "Iteration 84, loss = 0.11435445\n",
      "Iteration 85, loss = 0.11352722\n",
      "Iteration 86, loss = 0.11266280\n",
      "Iteration 87, loss = 0.11163351\n",
      "Iteration 88, loss = 0.11131316\n",
      "Iteration 89, loss = 0.11050203\n",
      "Iteration 90, loss = 0.10988914\n",
      "Iteration 91, loss = 0.10859071\n",
      "Iteration 92, loss = 0.10771061\n",
      "Iteration 93, loss = 0.10707218\n",
      "Iteration 94, loss = 0.10611690\n",
      "Iteration 95, loss = 0.10532915\n",
      "Iteration 96, loss = 0.10467140\n",
      "Iteration 97, loss = 0.10370683\n",
      "Iteration 98, loss = 0.10305764\n",
      "Iteration 99, loss = 0.10221670\n",
      "Iteration 100, loss = 0.10174355\n",
      "Iteration 101, loss = 0.10143403\n",
      "Iteration 102, loss = 0.10007014\n",
      "Iteration 103, loss = 0.09936351\n",
      "Iteration 104, loss = 0.09865535\n",
      "Iteration 105, loss = 0.09783838\n",
      "Iteration 106, loss = 0.09751109\n",
      "Iteration 107, loss = 0.09672832\n",
      "Iteration 108, loss = 0.09578366\n",
      "Iteration 109, loss = 0.09526575\n",
      "Iteration 110, loss = 0.09439126\n",
      "Iteration 111, loss = 0.09363914\n",
      "Iteration 112, loss = 0.09339361\n",
      "Iteration 113, loss = 0.09225484\n",
      "Iteration 114, loss = 0.09184929\n",
      "Iteration 115, loss = 0.09083999\n",
      "Iteration 116, loss = 0.08997979\n",
      "Iteration 117, loss = 0.08945337\n",
      "Iteration 118, loss = 0.08879092\n",
      "Iteration 119, loss = 0.08836610\n",
      "Iteration 120, loss = 0.08819414\n",
      "Iteration 121, loss = 0.08701952\n",
      "Iteration 122, loss = 0.08632621\n",
      "Iteration 123, loss = 0.08545748\n",
      "Iteration 124, loss = 0.08508064\n",
      "Iteration 125, loss = 0.08554997\n",
      "Iteration 126, loss = 0.08367293\n",
      "Iteration 127, loss = 0.08300176\n",
      "Iteration 128, loss = 0.08268966\n",
      "Iteration 129, loss = 0.08196084\n",
      "Iteration 130, loss = 0.08090161\n",
      "Iteration 131, loss = 0.08065153\n",
      "Iteration 132, loss = 0.07992085\n",
      "Iteration 133, loss = 0.07950081\n",
      "Iteration 134, loss = 0.07903664\n",
      "Iteration 135, loss = 0.07808130\n",
      "Iteration 136, loss = 0.07756282\n",
      "Iteration 137, loss = 0.07687365\n",
      "Iteration 138, loss = 0.07709459\n",
      "Iteration 139, loss = 0.07598724\n",
      "Iteration 140, loss = 0.07534977\n",
      "Iteration 141, loss = 0.07488068\n",
      "Iteration 142, loss = 0.07436069\n",
      "Iteration 143, loss = 0.07395097\n",
      "Iteration 144, loss = 0.07327942\n",
      "Iteration 145, loss = 0.07297570\n",
      "Iteration 146, loss = 0.07214001\n",
      "Iteration 147, loss = 0.07141325\n",
      "Iteration 148, loss = 0.07101994\n",
      "Iteration 149, loss = 0.07079925\n",
      "Iteration 150, loss = 0.06995387\n",
      "Iteration 151, loss = 0.06923247\n",
      "Iteration 152, loss = 0.06920443\n",
      "Iteration 153, loss = 0.06828937\n",
      "Iteration 154, loss = 0.06760111\n",
      "Iteration 155, loss = 0.06752355\n",
      "Iteration 156, loss = 0.06677777\n",
      "Iteration 157, loss = 0.06589615\n",
      "Iteration 158, loss = 0.06616082\n",
      "Iteration 159, loss = 0.06545493\n",
      "Iteration 160, loss = 0.06442165\n",
      "Iteration 161, loss = 0.06411531\n",
      "Iteration 162, loss = 0.06396563\n",
      "Iteration 163, loss = 0.06349314\n",
      "Iteration 164, loss = 0.06282998\n",
      "Iteration 165, loss = 0.06211976\n",
      "Iteration 166, loss = 0.06142734\n",
      "Iteration 167, loss = 0.06093183\n",
      "Iteration 168, loss = 0.06066536\n",
      "Iteration 169, loss = 0.06028472\n",
      "Iteration 170, loss = 0.05975258\n",
      "Iteration 171, loss = 0.05906866\n",
      "Iteration 172, loss = 0.05890439\n",
      "Iteration 173, loss = 0.05847157\n",
      "Iteration 174, loss = 0.05786752\n",
      "Iteration 175, loss = 0.05754220\n",
      "Iteration 176, loss = 0.05706028\n",
      "Iteration 177, loss = 0.05684008\n",
      "Iteration 178, loss = 0.05652237\n",
      "Iteration 179, loss = 0.05572313\n",
      "Iteration 180, loss = 0.05542822\n",
      "Iteration 181, loss = 0.05498987\n",
      "Iteration 182, loss = 0.05437191\n",
      "Iteration 183, loss = 0.05443728\n",
      "Iteration 184, loss = 0.05364970\n",
      "Iteration 185, loss = 0.05322966\n",
      "Iteration 186, loss = 0.05279796\n",
      "Iteration 187, loss = 0.05233739\n",
      "Iteration 188, loss = 0.05201681\n",
      "Iteration 189, loss = 0.05142665\n",
      "Iteration 190, loss = 0.05108658\n",
      "Iteration 191, loss = 0.05068223\n",
      "Iteration 192, loss = 0.05047910\n",
      "Iteration 193, loss = 0.04987016\n",
      "Iteration 194, loss = 0.04976648\n",
      "Iteration 195, loss = 0.04925088\n",
      "Iteration 196, loss = 0.04861935\n",
      "Iteration 197, loss = 0.04865465\n",
      "Iteration 198, loss = 0.04788670\n",
      "Iteration 199, loss = 0.04754200\n",
      "Iteration 200, loss = 0.04740941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70085136\n",
      "Iteration 2, loss = 0.48071911\n",
      "Iteration 3, loss = 0.38883208\n",
      "Iteration 4, loss = 0.34749030\n",
      "Iteration 5, loss = 0.32311033\n",
      "Iteration 6, loss = 0.30608499\n",
      "Iteration 7, loss = 0.29269858\n",
      "Iteration 8, loss = 0.28228515\n",
      "Iteration 9, loss = 0.27362027\n",
      "Iteration 10, loss = 0.26586718\n",
      "Iteration 11, loss = 0.25946422\n",
      "Iteration 12, loss = 0.25318441\n",
      "Iteration 13, loss = 0.24781676\n",
      "Iteration 14, loss = 0.24276767\n",
      "Iteration 15, loss = 0.23771184\n",
      "Iteration 16, loss = 0.23330701\n",
      "Iteration 17, loss = 0.22895920\n",
      "Iteration 18, loss = 0.22471058\n",
      "Iteration 19, loss = 0.22089782\n",
      "Iteration 20, loss = 0.21698731\n",
      "Iteration 21, loss = 0.21340760\n",
      "Iteration 22, loss = 0.20974851\n",
      "Iteration 23, loss = 0.20651449\n",
      "Iteration 24, loss = 0.20289428\n",
      "Iteration 25, loss = 0.19987644\n",
      "Iteration 26, loss = 0.19652596\n",
      "Iteration 27, loss = 0.19378197\n",
      "Iteration 28, loss = 0.19065298\n",
      "Iteration 29, loss = 0.18786330\n",
      "Iteration 30, loss = 0.18528395\n",
      "Iteration 31, loss = 0.18238575\n",
      "Iteration 32, loss = 0.17980919\n",
      "Iteration 33, loss = 0.17728381\n",
      "Iteration 34, loss = 0.17500628\n",
      "Iteration 35, loss = 0.17252046\n",
      "Iteration 36, loss = 0.17009795\n",
      "Iteration 37, loss = 0.16802829\n",
      "Iteration 38, loss = 0.16598183\n",
      "Iteration 39, loss = 0.16381322\n",
      "Iteration 40, loss = 0.16203119\n",
      "Iteration 41, loss = 0.15965320\n",
      "Iteration 42, loss = 0.15785817\n",
      "Iteration 43, loss = 0.15622683\n",
      "Iteration 44, loss = 0.15433083\n",
      "Iteration 45, loss = 0.15211378\n",
      "Iteration 46, loss = 0.15066042\n",
      "Iteration 47, loss = 0.14871062\n",
      "Iteration 48, loss = 0.14712075\n",
      "Iteration 49, loss = 0.14561852\n",
      "Iteration 50, loss = 0.14434723\n",
      "Iteration 51, loss = 0.14242064\n",
      "Iteration 52, loss = 0.14107685\n",
      "Iteration 53, loss = 0.13952217\n",
      "Iteration 54, loss = 0.13822369\n",
      "Iteration 55, loss = 0.13652574\n",
      "Iteration 56, loss = 0.13544374\n",
      "Iteration 57, loss = 0.13417030\n",
      "Iteration 58, loss = 0.13285289\n",
      "Iteration 59, loss = 0.13153077\n",
      "Iteration 60, loss = 0.13039142\n",
      "Iteration 61, loss = 0.12948907\n",
      "Iteration 62, loss = 0.12828921\n",
      "Iteration 63, loss = 0.12733245\n",
      "Iteration 64, loss = 0.12617748\n",
      "Iteration 65, loss = 0.12541118\n",
      "Iteration 66, loss = 0.12347611\n",
      "Iteration 67, loss = 0.12274000\n",
      "Iteration 68, loss = 0.12147472\n",
      "Iteration 69, loss = 0.12053597\n",
      "Iteration 70, loss = 0.11957475\n",
      "Iteration 71, loss = 0.11874452\n",
      "Iteration 72, loss = 0.11765468\n",
      "Iteration 73, loss = 0.11668643\n",
      "Iteration 74, loss = 0.11541702\n",
      "Iteration 75, loss = 0.11499826\n",
      "Iteration 76, loss = 0.11392440\n",
      "Iteration 77, loss = 0.11288302\n",
      "Iteration 78, loss = 0.11187579\n",
      "Iteration 79, loss = 0.11118767\n",
      "Iteration 80, loss = 0.11012606\n",
      "Iteration 81, loss = 0.10939119\n",
      "Iteration 82, loss = 0.10951616\n",
      "Iteration 83, loss = 0.10826809\n",
      "Iteration 84, loss = 0.10720675\n",
      "Iteration 85, loss = 0.10606622\n",
      "Iteration 86, loss = 0.10578480\n",
      "Iteration 87, loss = 0.10440416\n",
      "Iteration 88, loss = 0.10356711\n",
      "Iteration 89, loss = 0.10284871\n",
      "Iteration 90, loss = 0.10265357\n",
      "Iteration 91, loss = 0.10091750\n",
      "Iteration 92, loss = 0.10084887\n",
      "Iteration 93, loss = 0.09955285\n",
      "Iteration 94, loss = 0.09900894\n",
      "Iteration 95, loss = 0.09814596\n",
      "Iteration 96, loss = 0.09824789\n",
      "Iteration 97, loss = 0.09703329\n",
      "Iteration 98, loss = 0.09606056\n",
      "Iteration 99, loss = 0.09567371\n",
      "Iteration 100, loss = 0.09515458\n",
      "Iteration 101, loss = 0.09446616\n",
      "Iteration 102, loss = 0.09390416\n",
      "Iteration 103, loss = 0.09312045\n",
      "Iteration 104, loss = 0.09225517\n",
      "Iteration 105, loss = 0.09189692\n",
      "Iteration 106, loss = 0.09079983\n",
      "Iteration 107, loss = 0.09047376\n",
      "Iteration 108, loss = 0.08964059\n",
      "Iteration 109, loss = 0.08918779\n",
      "Iteration 110, loss = 0.08860658\n",
      "Iteration 111, loss = 0.08812276\n",
      "Iteration 112, loss = 0.08728116\n",
      "Iteration 113, loss = 0.08680847\n",
      "Iteration 114, loss = 0.08657648\n",
      "Iteration 115, loss = 0.08556402\n",
      "Iteration 116, loss = 0.08490854\n",
      "Iteration 117, loss = 0.08405409\n",
      "Iteration 118, loss = 0.08347479\n",
      "Iteration 119, loss = 0.08321746\n",
      "Iteration 120, loss = 0.08287181\n",
      "Iteration 121, loss = 0.08211731\n",
      "Iteration 122, loss = 0.08150936\n",
      "Iteration 123, loss = 0.08066834\n",
      "Iteration 124, loss = 0.08060958\n",
      "Iteration 125, loss = 0.07998754\n",
      "Iteration 126, loss = 0.07903527\n",
      "Iteration 127, loss = 0.07862765\n",
      "Iteration 128, loss = 0.07809167\n",
      "Iteration 129, loss = 0.07751120\n",
      "Iteration 130, loss = 0.07842561\n",
      "Iteration 131, loss = 0.07675474\n",
      "Iteration 132, loss = 0.07627589\n",
      "Iteration 133, loss = 0.07527376\n",
      "Iteration 134, loss = 0.07543992\n",
      "Iteration 135, loss = 0.07441124\n",
      "Iteration 136, loss = 0.07364659\n",
      "Iteration 137, loss = 0.07355263\n",
      "Iteration 138, loss = 0.07302793\n",
      "Iteration 139, loss = 0.07193098\n",
      "Iteration 140, loss = 0.07204441\n",
      "Iteration 141, loss = 0.07139072\n",
      "Iteration 142, loss = 0.07079477\n",
      "Iteration 143, loss = 0.07041496\n",
      "Iteration 144, loss = 0.06990561\n",
      "Iteration 145, loss = 0.06953354\n",
      "Iteration 146, loss = 0.06889663\n",
      "Iteration 147, loss = 0.06861531\n",
      "Iteration 148, loss = 0.06782800\n",
      "Iteration 149, loss = 0.06739387\n",
      "Iteration 150, loss = 0.06712233\n",
      "Iteration 151, loss = 0.06637536\n",
      "Iteration 152, loss = 0.06617210\n",
      "Iteration 153, loss = 0.06547696\n",
      "Iteration 154, loss = 0.06494480\n",
      "Iteration 155, loss = 0.06435954\n",
      "Iteration 156, loss = 0.06405944\n",
      "Iteration 157, loss = 0.06383629\n",
      "Iteration 158, loss = 0.06307615\n",
      "Iteration 159, loss = 0.06304712\n",
      "Iteration 160, loss = 0.06227948\n",
      "Iteration 161, loss = 0.06207780\n",
      "Iteration 162, loss = 0.06153642\n",
      "Iteration 163, loss = 0.06099568\n",
      "Iteration 164, loss = 0.06029534\n",
      "Iteration 165, loss = 0.06017717\n",
      "Iteration 166, loss = 0.05952115\n",
      "Iteration 167, loss = 0.05923023\n",
      "Iteration 168, loss = 0.05897294\n",
      "Iteration 169, loss = 0.05798801\n",
      "Iteration 170, loss = 0.05787002\n",
      "Iteration 171, loss = 0.05729760\n",
      "Iteration 172, loss = 0.05714085\n",
      "Iteration 173, loss = 0.05638546\n",
      "Iteration 174, loss = 0.05605275\n",
      "Iteration 175, loss = 0.05595080\n",
      "Iteration 176, loss = 0.05539067\n",
      "Iteration 177, loss = 0.05498843\n",
      "Iteration 178, loss = 0.05472411\n",
      "Iteration 179, loss = 0.05420156\n",
      "Iteration 180, loss = 0.05363115\n",
      "Iteration 181, loss = 0.05408844\n",
      "Iteration 182, loss = 0.05306655\n",
      "Iteration 183, loss = 0.05287983\n",
      "Iteration 184, loss = 0.05225467\n",
      "Iteration 185, loss = 0.05144876\n",
      "Iteration 186, loss = 0.05136355\n",
      "Iteration 187, loss = 0.05077813\n",
      "Iteration 188, loss = 0.05030896\n",
      "Iteration 189, loss = 0.05007136\n",
      "Iteration 190, loss = 0.05032866\n",
      "Iteration 191, loss = 0.04941566\n",
      "Iteration 192, loss = 0.04883506\n",
      "Iteration 193, loss = 0.04864088\n",
      "Iteration 194, loss = 0.04860116\n",
      "Iteration 195, loss = 0.04820451\n",
      "Iteration 196, loss = 0.04770809\n",
      "Iteration 197, loss = 0.04704418\n",
      "Iteration 198, loss = 0.04664964\n",
      "Iteration 199, loss = 0.04608704\n",
      "Iteration 200, loss = 0.04636430\n",
      "Iteration 1, loss = 0.69846700\n",
      "Iteration 2, loss = 0.47787953\n",
      "Iteration 3, loss = 0.38612170\n",
      "Iteration 4, loss = 0.34514749\n",
      "Iteration 5, loss = 0.32148170\n",
      "Iteration 6, loss = 0.30493880\n",
      "Iteration 7, loss = 0.29243284\n",
      "Iteration 8, loss = 0.28251235\n",
      "Iteration 9, loss = 0.27442259\n",
      "Iteration 10, loss = 0.26749588\n",
      "Iteration 11, loss = 0.26147774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.25572134\n",
      "Iteration 13, loss = 0.25089514\n",
      "Iteration 14, loss = 0.24597346\n",
      "Iteration 15, loss = 0.24143691\n",
      "Iteration 16, loss = 0.23730587\n",
      "Iteration 17, loss = 0.23328644\n",
      "Iteration 18, loss = 0.22942978\n",
      "Iteration 19, loss = 0.22563918\n",
      "Iteration 20, loss = 0.22216836\n",
      "Iteration 21, loss = 0.21877883\n",
      "Iteration 22, loss = 0.21555414\n",
      "Iteration 23, loss = 0.21232675\n",
      "Iteration 24, loss = 0.20885089\n",
      "Iteration 25, loss = 0.20574154\n",
      "Iteration 26, loss = 0.20280808\n",
      "Iteration 27, loss = 0.19999936\n",
      "Iteration 28, loss = 0.19680016\n",
      "Iteration 29, loss = 0.19455797\n",
      "Iteration 30, loss = 0.19195540\n",
      "Iteration 31, loss = 0.18893745\n",
      "Iteration 32, loss = 0.18657822\n",
      "Iteration 33, loss = 0.18403770\n",
      "Iteration 34, loss = 0.18198069\n",
      "Iteration 35, loss = 0.17975209\n",
      "Iteration 36, loss = 0.17715991\n",
      "Iteration 37, loss = 0.17525464\n",
      "Iteration 38, loss = 0.17306269\n",
      "Iteration 39, loss = 0.17092250\n",
      "Iteration 40, loss = 0.16890031\n",
      "Iteration 41, loss = 0.16661873\n",
      "Iteration 42, loss = 0.16486288\n",
      "Iteration 43, loss = 0.16321557\n",
      "Iteration 44, loss = 0.16138112\n",
      "Iteration 45, loss = 0.15953057\n",
      "Iteration 46, loss = 0.15776835\n",
      "Iteration 47, loss = 0.15608674\n",
      "Iteration 48, loss = 0.15436558\n",
      "Iteration 49, loss = 0.15298149\n",
      "Iteration 50, loss = 0.15152954\n",
      "Iteration 51, loss = 0.14998592\n",
      "Iteration 52, loss = 0.14848481\n",
      "Iteration 53, loss = 0.14704792\n",
      "Iteration 54, loss = 0.14548795\n",
      "Iteration 55, loss = 0.14398436\n",
      "Iteration 56, loss = 0.14265029\n",
      "Iteration 57, loss = 0.14168628\n",
      "Iteration 58, loss = 0.14048993\n",
      "Iteration 59, loss = 0.13901077\n",
      "Iteration 60, loss = 0.13783417\n",
      "Iteration 61, loss = 0.13691952\n",
      "Iteration 62, loss = 0.13563859\n",
      "Iteration 63, loss = 0.13428941\n",
      "Iteration 64, loss = 0.13320370\n",
      "Iteration 65, loss = 0.13276823\n",
      "Iteration 66, loss = 0.13108628\n",
      "Iteration 67, loss = 0.13022115\n",
      "Iteration 68, loss = 0.12894043\n",
      "Iteration 69, loss = 0.12812853\n",
      "Iteration 70, loss = 0.12708000\n",
      "Iteration 71, loss = 0.12628761\n",
      "Iteration 72, loss = 0.12483700\n",
      "Iteration 73, loss = 0.12427032\n",
      "Iteration 74, loss = 0.12321030\n",
      "Iteration 75, loss = 0.12246929\n",
      "Iteration 76, loss = 0.12133355\n",
      "Iteration 77, loss = 0.12058633\n",
      "Iteration 78, loss = 0.11985738\n",
      "Iteration 79, loss = 0.11852941\n",
      "Iteration 80, loss = 0.11825558\n",
      "Iteration 81, loss = 0.11727822\n",
      "Iteration 82, loss = 0.11662348\n",
      "Iteration 83, loss = 0.11567595\n",
      "Iteration 84, loss = 0.11444804\n",
      "Iteration 85, loss = 0.11343155\n",
      "Iteration 86, loss = 0.11317493\n",
      "Iteration 87, loss = 0.11195839\n",
      "Iteration 88, loss = 0.11122380\n",
      "Iteration 89, loss = 0.11029590\n",
      "Iteration 90, loss = 0.10983043\n",
      "Iteration 91, loss = 0.10885739\n",
      "Iteration 92, loss = 0.10804383\n",
      "Iteration 93, loss = 0.10712165\n",
      "Iteration 94, loss = 0.10665692\n",
      "Iteration 95, loss = 0.10620734\n",
      "Iteration 96, loss = 0.10605869\n",
      "Iteration 97, loss = 0.10425085\n",
      "Iteration 98, loss = 0.10391637\n",
      "Iteration 99, loss = 0.10323478\n",
      "Iteration 100, loss = 0.10294893\n",
      "Iteration 101, loss = 0.10216603\n",
      "Iteration 102, loss = 0.10165918\n",
      "Iteration 103, loss = 0.10047882\n",
      "Iteration 104, loss = 0.09973564\n",
      "Iteration 105, loss = 0.09929953\n",
      "Iteration 106, loss = 0.09836958\n",
      "Iteration 107, loss = 0.09775348\n",
      "Iteration 108, loss = 0.09728768\n",
      "Iteration 109, loss = 0.09666014\n",
      "Iteration 110, loss = 0.09600634\n",
      "Iteration 111, loss = 0.09496011\n",
      "Iteration 112, loss = 0.09465964\n",
      "Iteration 113, loss = 0.09387905\n",
      "Iteration 114, loss = 0.09373653\n",
      "Iteration 115, loss = 0.09271038\n",
      "Iteration 116, loss = 0.09215550\n",
      "Iteration 117, loss = 0.09142932\n",
      "Iteration 118, loss = 0.09058043\n",
      "Iteration 119, loss = 0.09025555\n",
      "Iteration 120, loss = 0.08982593\n",
      "Iteration 121, loss = 0.08929966\n",
      "Iteration 122, loss = 0.08879049\n",
      "Iteration 123, loss = 0.08821174\n",
      "Iteration 124, loss = 0.08732063\n",
      "Iteration 125, loss = 0.08678022\n",
      "Iteration 126, loss = 0.08657272\n",
      "Iteration 127, loss = 0.08595937\n",
      "Iteration 128, loss = 0.08527365\n",
      "Iteration 129, loss = 0.08474480\n",
      "Iteration 130, loss = 0.08450602\n",
      "Iteration 131, loss = 0.08359905\n",
      "Iteration 132, loss = 0.08299148\n",
      "Iteration 133, loss = 0.08226126\n",
      "Iteration 134, loss = 0.08193759\n",
      "Iteration 135, loss = 0.08109141\n",
      "Iteration 136, loss = 0.08057057\n",
      "Iteration 137, loss = 0.08026247\n",
      "Iteration 138, loss = 0.07974791\n",
      "Iteration 139, loss = 0.07888025\n",
      "Iteration 140, loss = 0.07871179\n",
      "Iteration 141, loss = 0.07806500\n",
      "Iteration 142, loss = 0.07763736\n",
      "Iteration 143, loss = 0.07709613\n",
      "Iteration 144, loss = 0.07653973\n",
      "Iteration 145, loss = 0.07611416\n",
      "Iteration 146, loss = 0.07556843\n",
      "Iteration 147, loss = 0.07537495\n",
      "Iteration 148, loss = 0.07464453\n",
      "Iteration 149, loss = 0.07409793\n",
      "Iteration 150, loss = 0.07403005\n",
      "Iteration 151, loss = 0.07322479\n",
      "Iteration 152, loss = 0.07293383\n",
      "Iteration 153, loss = 0.07182009\n",
      "Iteration 154, loss = 0.07156791\n",
      "Iteration 155, loss = 0.07100500\n",
      "Iteration 156, loss = 0.07041522\n",
      "Iteration 157, loss = 0.07026143\n",
      "Iteration 158, loss = 0.06990398\n",
      "Iteration 159, loss = 0.06926722\n",
      "Iteration 160, loss = 0.06873725\n",
      "Iteration 161, loss = 0.06823467\n",
      "Iteration 162, loss = 0.06772781\n",
      "Iteration 163, loss = 0.06698618\n",
      "Iteration 164, loss = 0.06663618\n",
      "Iteration 165, loss = 0.06639775\n",
      "Iteration 166, loss = 0.06552491\n",
      "Iteration 167, loss = 0.06534213\n",
      "Iteration 168, loss = 0.06593972\n",
      "Iteration 169, loss = 0.06414584\n",
      "Iteration 170, loss = 0.06398283\n",
      "Iteration 171, loss = 0.06346421\n",
      "Iteration 172, loss = 0.06338808\n",
      "Iteration 173, loss = 0.06261036\n",
      "Iteration 174, loss = 0.06187584\n",
      "Iteration 175, loss = 0.06160117\n",
      "Iteration 176, loss = 0.06157301\n",
      "Iteration 177, loss = 0.06100556\n",
      "Iteration 178, loss = 0.06022622\n",
      "Iteration 179, loss = 0.06015298\n",
      "Iteration 180, loss = 0.05955130\n",
      "Iteration 181, loss = 0.05933257\n",
      "Iteration 182, loss = 0.05865306\n",
      "Iteration 183, loss = 0.05855036\n",
      "Iteration 184, loss = 0.05754297\n",
      "Iteration 185, loss = 0.05731307\n",
      "Iteration 186, loss = 0.05684439\n",
      "Iteration 187, loss = 0.05656784\n",
      "Iteration 188, loss = 0.05590794\n",
      "Iteration 189, loss = 0.05550657\n",
      "Iteration 190, loss = 0.05582921\n",
      "Iteration 191, loss = 0.05522903\n",
      "Iteration 192, loss = 0.05467024\n",
      "Iteration 193, loss = 0.05370722\n",
      "Iteration 194, loss = 0.05400399\n",
      "Iteration 195, loss = 0.05341604\n",
      "Iteration 196, loss = 0.05290122\n",
      "Iteration 197, loss = 0.05249112\n",
      "Iteration 198, loss = 0.05179507\n",
      "Iteration 199, loss = 0.05149110\n",
      "Iteration 200, loss = 0.05142554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79551265\n",
      "Iteration 2, loss = 0.63890810\n",
      "Iteration 3, loss = 0.53380757\n",
      "Iteration 4, loss = 0.48073250\n",
      "Iteration 5, loss = 0.45325732\n",
      "Iteration 6, loss = 0.43586499\n",
      "Iteration 7, loss = 0.42382638\n",
      "Iteration 8, loss = 0.41427262\n",
      "Iteration 9, loss = 0.40614618\n",
      "Iteration 10, loss = 0.39900902\n",
      "Iteration 11, loss = 0.39258164\n",
      "Iteration 12, loss = 0.38657761\n",
      "Iteration 13, loss = 0.38105741\n",
      "Iteration 14, loss = 0.37582925\n",
      "Iteration 15, loss = 0.37086261\n",
      "Iteration 16, loss = 0.36631737\n",
      "Iteration 17, loss = 0.36193211\n",
      "Iteration 18, loss = 0.35775322\n",
      "Iteration 19, loss = 0.35378599\n",
      "Iteration 20, loss = 0.35011293\n",
      "Iteration 21, loss = 0.34653703\n",
      "Iteration 22, loss = 0.34312586\n",
      "Iteration 23, loss = 0.33990685\n",
      "Iteration 24, loss = 0.33680789\n",
      "Iteration 25, loss = 0.33385751\n",
      "Iteration 26, loss = 0.33103588\n",
      "Iteration 27, loss = 0.32834251\n",
      "Iteration 28, loss = 0.32574114\n",
      "Iteration 29, loss = 0.32329959\n",
      "Iteration 30, loss = 0.32089167\n",
      "Iteration 31, loss = 0.31861721\n",
      "Iteration 32, loss = 0.31642697\n",
      "Iteration 33, loss = 0.31432033\n",
      "Iteration 34, loss = 0.31229975\n",
      "Iteration 35, loss = 0.31033134\n",
      "Iteration 36, loss = 0.30850107\n",
      "Iteration 37, loss = 0.30662230\n",
      "Iteration 38, loss = 0.30489532\n",
      "Iteration 39, loss = 0.30320908\n",
      "Iteration 40, loss = 0.30157316\n",
      "Iteration 41, loss = 0.30001527\n",
      "Iteration 42, loss = 0.29850980\n",
      "Iteration 43, loss = 0.29699412\n",
      "Iteration 44, loss = 0.29562038\n",
      "Iteration 45, loss = 0.29421348\n",
      "Iteration 46, loss = 0.29290195\n",
      "Iteration 47, loss = 0.29159924\n",
      "Iteration 48, loss = 0.29033306\n",
      "Iteration 49, loss = 0.28911061\n",
      "Iteration 50, loss = 0.28790667\n",
      "Iteration 51, loss = 0.28676413\n",
      "Iteration 52, loss = 0.28564627\n",
      "Iteration 53, loss = 0.28454051\n",
      "Iteration 54, loss = 0.28347011\n",
      "Iteration 55, loss = 0.28242703\n",
      "Iteration 56, loss = 0.28143076\n",
      "Iteration 57, loss = 0.28045847\n",
      "Iteration 58, loss = 0.27947809\n",
      "Iteration 59, loss = 0.27853420\n",
      "Iteration 60, loss = 0.27764752\n",
      "Iteration 61, loss = 0.27673812\n",
      "Iteration 62, loss = 0.27586913\n",
      "Iteration 63, loss = 0.27501270\n",
      "Iteration 64, loss = 0.27417736\n",
      "Iteration 65, loss = 0.27337464\n",
      "Iteration 66, loss = 0.27258692\n",
      "Iteration 67, loss = 0.27179303\n",
      "Iteration 68, loss = 0.27104906\n",
      "Iteration 69, loss = 0.27030927\n",
      "Iteration 70, loss = 0.26958779\n",
      "Iteration 71, loss = 0.26886673\n",
      "Iteration 72, loss = 0.26817382\n",
      "Iteration 73, loss = 0.26748854\n",
      "Iteration 74, loss = 0.26680431\n",
      "Iteration 75, loss = 0.26616030\n",
      "Iteration 76, loss = 0.26552189\n",
      "Iteration 77, loss = 0.26489543\n",
      "Iteration 78, loss = 0.26425382\n",
      "Iteration 79, loss = 0.26365256\n",
      "Iteration 80, loss = 0.26305921\n",
      "Iteration 81, loss = 0.26246359\n",
      "Iteration 82, loss = 0.26190790\n",
      "Iteration 83, loss = 0.26129451\n",
      "Iteration 84, loss = 0.26073983\n",
      "Iteration 85, loss = 0.26018255\n",
      "Iteration 86, loss = 0.25965502\n",
      "Iteration 87, loss = 0.25911641\n",
      "Iteration 88, loss = 0.25858044\n",
      "Iteration 89, loss = 0.25805721\n",
      "Iteration 90, loss = 0.25755087\n",
      "Iteration 91, loss = 0.25703887\n",
      "Iteration 92, loss = 0.25655554\n",
      "Iteration 93, loss = 0.25604452\n",
      "Iteration 94, loss = 0.25556104\n",
      "Iteration 95, loss = 0.25508961\n",
      "Iteration 96, loss = 0.25460872\n",
      "Iteration 97, loss = 0.25414616\n",
      "Iteration 98, loss = 0.25367844\n",
      "Iteration 99, loss = 0.25322414\n",
      "Iteration 100, loss = 0.25280192\n",
      "Iteration 101, loss = 0.25235127\n",
      "Iteration 102, loss = 0.25188874\n",
      "Iteration 103, loss = 0.25146831\n",
      "Iteration 104, loss = 0.25104432\n",
      "Iteration 105, loss = 0.25060962\n",
      "Iteration 106, loss = 0.25018816\n",
      "Iteration 107, loss = 0.24977548\n",
      "Iteration 108, loss = 0.24935409\n",
      "Iteration 109, loss = 0.24895624\n",
      "Iteration 110, loss = 0.24856819\n",
      "Iteration 111, loss = 0.24814172\n",
      "Iteration 112, loss = 0.24774493\n",
      "Iteration 113, loss = 0.24735311\n",
      "Iteration 114, loss = 0.24694626\n",
      "Iteration 115, loss = 0.24657166\n",
      "Iteration 116, loss = 0.24619333\n",
      "Iteration 117, loss = 0.24581387\n",
      "Iteration 118, loss = 0.24541839\n",
      "Iteration 119, loss = 0.24505288\n",
      "Iteration 120, loss = 0.24467344\n",
      "Iteration 121, loss = 0.24430388\n",
      "Iteration 122, loss = 0.24394573\n",
      "Iteration 123, loss = 0.24356877\n",
      "Iteration 124, loss = 0.24320547\n",
      "Iteration 125, loss = 0.24284907\n",
      "Iteration 126, loss = 0.24249143\n",
      "Iteration 127, loss = 0.24214318\n",
      "Iteration 128, loss = 0.24178767\n",
      "Iteration 129, loss = 0.24144149\n",
      "Iteration 130, loss = 0.24109226\n",
      "Iteration 131, loss = 0.24074914\n",
      "Iteration 132, loss = 0.24040785\n",
      "Iteration 133, loss = 0.24006166\n",
      "Iteration 134, loss = 0.23972542\n",
      "Iteration 135, loss = 0.23937868\n",
      "Iteration 136, loss = 0.23907690\n",
      "Iteration 137, loss = 0.23871326\n",
      "Iteration 138, loss = 0.23838126\n",
      "Iteration 139, loss = 0.23805949\n",
      "Iteration 140, loss = 0.23772558\n",
      "Iteration 141, loss = 0.23738674\n",
      "Iteration 142, loss = 0.23707951\n",
      "Iteration 143, loss = 0.23675349\n",
      "Iteration 144, loss = 0.23642740\n",
      "Iteration 145, loss = 0.23611046\n",
      "Iteration 146, loss = 0.23579216\n",
      "Iteration 147, loss = 0.23548867\n",
      "Iteration 148, loss = 0.23516896\n",
      "Iteration 149, loss = 0.23485290\n",
      "Iteration 150, loss = 0.23453836\n",
      "Iteration 151, loss = 0.23424086\n",
      "Iteration 152, loss = 0.23393624\n",
      "Iteration 153, loss = 0.23363315\n",
      "Iteration 154, loss = 0.23332114\n",
      "Iteration 155, loss = 0.23302188\n",
      "Iteration 156, loss = 0.23270785\n",
      "Iteration 157, loss = 0.23241294\n",
      "Iteration 158, loss = 0.23211162\n",
      "Iteration 159, loss = 0.23182952\n",
      "Iteration 160, loss = 0.23153510\n",
      "Iteration 161, loss = 0.23123289\n",
      "Iteration 162, loss = 0.23094423\n",
      "Iteration 163, loss = 0.23063992\n",
      "Iteration 164, loss = 0.23035466\n",
      "Iteration 165, loss = 0.23007599\n",
      "Iteration 166, loss = 0.22977510\n",
      "Iteration 167, loss = 0.22951733\n",
      "Iteration 168, loss = 0.22920798\n",
      "Iteration 169, loss = 0.22893050\n",
      "Iteration 170, loss = 0.22865479\n",
      "Iteration 171, loss = 0.22837383\n",
      "Iteration 172, loss = 0.22808617\n",
      "Iteration 173, loss = 0.22780892\n",
      "Iteration 174, loss = 0.22753421\n",
      "Iteration 175, loss = 0.22724622\n",
      "Iteration 176, loss = 0.22697854\n",
      "Iteration 177, loss = 0.22671249\n",
      "Iteration 178, loss = 0.22643634\n",
      "Iteration 179, loss = 0.22617704\n",
      "Iteration 180, loss = 0.22588737\n",
      "Iteration 181, loss = 0.22561500\n",
      "Iteration 182, loss = 0.22535903\n",
      "Iteration 183, loss = 0.22506069\n",
      "Iteration 184, loss = 0.22481785\n",
      "Iteration 185, loss = 0.22454171\n",
      "Iteration 186, loss = 0.22426835\n",
      "Iteration 187, loss = 0.22399276\n",
      "Iteration 188, loss = 0.22373301\n",
      "Iteration 189, loss = 0.22347720\n",
      "Iteration 190, loss = 0.22320614\n",
      "Iteration 191, loss = 0.22293270\n",
      "Iteration 192, loss = 0.22269075\n",
      "Iteration 193, loss = 0.22241298\n",
      "Iteration 194, loss = 0.22217015\n",
      "Iteration 195, loss = 0.22190375\n",
      "Iteration 196, loss = 0.22164380\n",
      "Iteration 197, loss = 0.22139447\n",
      "Iteration 198, loss = 0.22113755\n",
      "Iteration 199, loss = 0.22087586\n",
      "Iteration 200, loss = 0.22062459\n",
      "Iteration 1, loss = 0.79521224\n",
      "Iteration 2, loss = 0.63801335\n",
      "Iteration 3, loss = 0.53237584\n",
      "Iteration 4, loss = 0.47942614\n",
      "Iteration 5, loss = 0.45174219\n",
      "Iteration 6, loss = 0.43431948\n",
      "Iteration 7, loss = 0.42244999\n",
      "Iteration 8, loss = 0.41285625\n",
      "Iteration 9, loss = 0.40482332\n",
      "Iteration 10, loss = 0.39787768\n",
      "Iteration 11, loss = 0.39144783\n",
      "Iteration 12, loss = 0.38560554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.38017180\n",
      "Iteration 14, loss = 0.37502107\n",
      "Iteration 15, loss = 0.37022174\n",
      "Iteration 16, loss = 0.36570427\n",
      "Iteration 17, loss = 0.36145390\n",
      "Iteration 18, loss = 0.35737105\n",
      "Iteration 19, loss = 0.35353919\n",
      "Iteration 20, loss = 0.34991592\n",
      "Iteration 21, loss = 0.34640309\n",
      "Iteration 22, loss = 0.34310602\n",
      "Iteration 23, loss = 0.33995461\n",
      "Iteration 24, loss = 0.33694831\n",
      "Iteration 25, loss = 0.33405728\n",
      "Iteration 26, loss = 0.33131086\n",
      "Iteration 27, loss = 0.32868519\n",
      "Iteration 28, loss = 0.32614950\n",
      "Iteration 29, loss = 0.32371389\n",
      "Iteration 30, loss = 0.32143532\n",
      "Iteration 31, loss = 0.31917582\n",
      "Iteration 32, loss = 0.31705174\n",
      "Iteration 33, loss = 0.31500986\n",
      "Iteration 34, loss = 0.31304255\n",
      "Iteration 35, loss = 0.31113379\n",
      "Iteration 36, loss = 0.30930397\n",
      "Iteration 37, loss = 0.30750414\n",
      "Iteration 38, loss = 0.30581998\n",
      "Iteration 39, loss = 0.30418972\n",
      "Iteration 40, loss = 0.30258951\n",
      "Iteration 41, loss = 0.30105015\n",
      "Iteration 42, loss = 0.29956172\n",
      "Iteration 43, loss = 0.29810363\n",
      "Iteration 44, loss = 0.29674088\n",
      "Iteration 45, loss = 0.29535587\n",
      "Iteration 46, loss = 0.29407378\n",
      "Iteration 47, loss = 0.29280211\n",
      "Iteration 48, loss = 0.29155863\n",
      "Iteration 49, loss = 0.29037098\n",
      "Iteration 50, loss = 0.28917634\n",
      "Iteration 51, loss = 0.28805568\n",
      "Iteration 52, loss = 0.28696901\n",
      "Iteration 53, loss = 0.28589204\n",
      "Iteration 54, loss = 0.28483692\n",
      "Iteration 55, loss = 0.28382363\n",
      "Iteration 56, loss = 0.28285592\n",
      "Iteration 57, loss = 0.28189310\n",
      "Iteration 58, loss = 0.28096650\n",
      "Iteration 59, loss = 0.28002507\n",
      "Iteration 60, loss = 0.27916410\n",
      "Iteration 61, loss = 0.27829569\n",
      "Iteration 62, loss = 0.27744075\n",
      "Iteration 63, loss = 0.27659187\n",
      "Iteration 64, loss = 0.27579301\n",
      "Iteration 65, loss = 0.27500136\n",
      "Iteration 66, loss = 0.27423831\n",
      "Iteration 67, loss = 0.27346628\n",
      "Iteration 68, loss = 0.27274002\n",
      "Iteration 69, loss = 0.27201686\n",
      "Iteration 70, loss = 0.27131384\n",
      "Iteration 71, loss = 0.27059835\n",
      "Iteration 72, loss = 0.26994740\n",
      "Iteration 73, loss = 0.26924064\n",
      "Iteration 74, loss = 0.26858472\n",
      "Iteration 75, loss = 0.26796139\n",
      "Iteration 76, loss = 0.26732924\n",
      "Iteration 77, loss = 0.26672215\n",
      "Iteration 78, loss = 0.26609855\n",
      "Iteration 79, loss = 0.26552682\n",
      "Iteration 80, loss = 0.26494526\n",
      "Iteration 81, loss = 0.26434529\n",
      "Iteration 82, loss = 0.26379724\n",
      "Iteration 83, loss = 0.26323564\n",
      "Iteration 84, loss = 0.26269409\n",
      "Iteration 85, loss = 0.26215024\n",
      "Iteration 86, loss = 0.26163356\n",
      "Iteration 87, loss = 0.26111456\n",
      "Iteration 88, loss = 0.26060692\n",
      "Iteration 89, loss = 0.26010508\n",
      "Iteration 90, loss = 0.25962860\n",
      "Iteration 91, loss = 0.25911585\n",
      "Iteration 92, loss = 0.25862554\n",
      "Iteration 93, loss = 0.25815980\n",
      "Iteration 94, loss = 0.25769064\n",
      "Iteration 95, loss = 0.25723582\n",
      "Iteration 96, loss = 0.25676498\n",
      "Iteration 97, loss = 0.25634255\n",
      "Iteration 98, loss = 0.25590506\n",
      "Iteration 99, loss = 0.25544876\n",
      "Iteration 100, loss = 0.25503228\n",
      "Iteration 101, loss = 0.25459569\n",
      "Iteration 102, loss = 0.25416763\n",
      "Iteration 103, loss = 0.25375654\n",
      "Iteration 104, loss = 0.25334444\n",
      "Iteration 105, loss = 0.25293950\n",
      "Iteration 106, loss = 0.25254020\n",
      "Iteration 107, loss = 0.25215011\n",
      "Iteration 108, loss = 0.25174759\n",
      "Iteration 109, loss = 0.25136427\n",
      "Iteration 110, loss = 0.25098911\n",
      "Iteration 111, loss = 0.25058991\n",
      "Iteration 112, loss = 0.25021057\n",
      "Iteration 113, loss = 0.24985734\n",
      "Iteration 114, loss = 0.24947251\n",
      "Iteration 115, loss = 0.24910210\n",
      "Iteration 116, loss = 0.24874166\n",
      "Iteration 117, loss = 0.24838426\n",
      "Iteration 118, loss = 0.24802787\n",
      "Iteration 119, loss = 0.24766627\n",
      "Iteration 120, loss = 0.24732961\n",
      "Iteration 121, loss = 0.24697650\n",
      "Iteration 122, loss = 0.24663656\n",
      "Iteration 123, loss = 0.24629098\n",
      "Iteration 124, loss = 0.24595617\n",
      "Iteration 125, loss = 0.24563733\n",
      "Iteration 126, loss = 0.24528129\n",
      "Iteration 127, loss = 0.24495413\n",
      "Iteration 128, loss = 0.24461760\n",
      "Iteration 129, loss = 0.24429689\n",
      "Iteration 130, loss = 0.24397344\n",
      "Iteration 131, loss = 0.24366452\n",
      "Iteration 132, loss = 0.24333320\n",
      "Iteration 133, loss = 0.24301873\n",
      "Iteration 134, loss = 0.24270244\n",
      "Iteration 135, loss = 0.24238358\n",
      "Iteration 136, loss = 0.24206268\n",
      "Iteration 137, loss = 0.24177037\n",
      "Iteration 138, loss = 0.24145499\n",
      "Iteration 139, loss = 0.24114133\n",
      "Iteration 140, loss = 0.24083009\n",
      "Iteration 141, loss = 0.24053609\n",
      "Iteration 142, loss = 0.24024408\n",
      "Iteration 143, loss = 0.23995132\n",
      "Iteration 144, loss = 0.23963353\n",
      "Iteration 145, loss = 0.23934388\n",
      "Iteration 146, loss = 0.23905257\n",
      "Iteration 147, loss = 0.23876037\n",
      "Iteration 148, loss = 0.23846660\n",
      "Iteration 149, loss = 0.23816972\n",
      "Iteration 150, loss = 0.23790097\n",
      "Iteration 151, loss = 0.23759416\n",
      "Iteration 152, loss = 0.23732516\n",
      "Iteration 153, loss = 0.23703228\n",
      "Iteration 154, loss = 0.23674188\n",
      "Iteration 155, loss = 0.23646380\n",
      "Iteration 156, loss = 0.23617311\n",
      "Iteration 157, loss = 0.23589683\n",
      "Iteration 158, loss = 0.23563109\n",
      "Iteration 159, loss = 0.23535197\n",
      "Iteration 160, loss = 0.23507677\n",
      "Iteration 161, loss = 0.23478839\n",
      "Iteration 162, loss = 0.23452623\n",
      "Iteration 163, loss = 0.23424053\n",
      "Iteration 164, loss = 0.23397842\n",
      "Iteration 165, loss = 0.23369940\n",
      "Iteration 166, loss = 0.23342939\n",
      "Iteration 167, loss = 0.23317922\n",
      "Iteration 168, loss = 0.23290672\n",
      "Iteration 169, loss = 0.23265269\n",
      "Iteration 170, loss = 0.23238995\n",
      "Iteration 171, loss = 0.23212587\n",
      "Iteration 172, loss = 0.23187327\n",
      "Iteration 173, loss = 0.23160470\n",
      "Iteration 174, loss = 0.23134697\n",
      "Iteration 175, loss = 0.23109068\n",
      "Iteration 176, loss = 0.23083753\n",
      "Iteration 177, loss = 0.23059412\n",
      "Iteration 178, loss = 0.23033729\n",
      "Iteration 179, loss = 0.23009555\n",
      "Iteration 180, loss = 0.22984084\n",
      "Iteration 181, loss = 0.22959699\n",
      "Iteration 182, loss = 0.22935512\n",
      "Iteration 183, loss = 0.22908461\n",
      "Iteration 184, loss = 0.22884486\n",
      "Iteration 185, loss = 0.22858605\n",
      "Iteration 186, loss = 0.22835660\n",
      "Iteration 187, loss = 0.22809470\n",
      "Iteration 188, loss = 0.22784921\n",
      "Iteration 189, loss = 0.22760680\n",
      "Iteration 190, loss = 0.22736099\n",
      "Iteration 191, loss = 0.22711618\n",
      "Iteration 192, loss = 0.22686908\n",
      "Iteration 193, loss = 0.22663063\n",
      "Iteration 194, loss = 0.22638445\n",
      "Iteration 195, loss = 0.22614589\n",
      "Iteration 196, loss = 0.22590638\n",
      "Iteration 197, loss = 0.22566895\n",
      "Iteration 198, loss = 0.22543477\n",
      "Iteration 199, loss = 0.22519536\n",
      "Iteration 200, loss = 0.22495452\n",
      "Iteration 1, loss = 0.79465280\n",
      "Iteration 2, loss = 0.63685224\n",
      "Iteration 3, loss = 0.53032575\n",
      "Iteration 4, loss = 0.47813445\n",
      "Iteration 5, loss = 0.45035274\n",
      "Iteration 6, loss = 0.43304494\n",
      "Iteration 7, loss = 0.42133324\n",
      "Iteration 8, loss = 0.41185160\n",
      "Iteration 9, loss = 0.40375468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.39691026\n",
      "Iteration 11, loss = 0.39050212\n",
      "Iteration 12, loss = 0.38462131\n",
      "Iteration 13, loss = 0.37926596\n",
      "Iteration 14, loss = 0.37410170\n",
      "Iteration 15, loss = 0.36930762\n",
      "Iteration 16, loss = 0.36476942\n",
      "Iteration 17, loss = 0.36059048\n",
      "Iteration 18, loss = 0.35653365\n",
      "Iteration 19, loss = 0.35274573\n",
      "Iteration 20, loss = 0.34911901\n",
      "Iteration 21, loss = 0.34568585\n",
      "Iteration 22, loss = 0.34243811\n",
      "Iteration 23, loss = 0.33930896\n",
      "Iteration 24, loss = 0.33634587\n",
      "Iteration 25, loss = 0.33351031\n",
      "Iteration 26, loss = 0.33083503\n",
      "Iteration 27, loss = 0.32825428\n",
      "Iteration 28, loss = 0.32575015\n",
      "Iteration 29, loss = 0.32340167\n",
      "Iteration 30, loss = 0.32114366\n",
      "Iteration 31, loss = 0.31899122\n",
      "Iteration 32, loss = 0.31686575\n",
      "Iteration 33, loss = 0.31487075\n",
      "Iteration 34, loss = 0.31295580\n",
      "Iteration 35, loss = 0.31112042\n",
      "Iteration 36, loss = 0.30932439\n",
      "Iteration 37, loss = 0.30756829\n",
      "Iteration 38, loss = 0.30594338\n",
      "Iteration 39, loss = 0.30434086\n",
      "Iteration 40, loss = 0.30279222\n",
      "Iteration 41, loss = 0.30132843\n",
      "Iteration 42, loss = 0.29988255\n",
      "Iteration 43, loss = 0.29847445\n",
      "Iteration 44, loss = 0.29716329\n",
      "Iteration 45, loss = 0.29583158\n",
      "Iteration 46, loss = 0.29461769\n",
      "Iteration 47, loss = 0.29336383\n",
      "Iteration 48, loss = 0.29218939\n",
      "Iteration 49, loss = 0.29103546\n",
      "Iteration 50, loss = 0.28991588\n",
      "Iteration 51, loss = 0.28883709\n",
      "Iteration 52, loss = 0.28778475\n",
      "Iteration 53, loss = 0.28678313\n",
      "Iteration 54, loss = 0.28579432\n",
      "Iteration 55, loss = 0.28480550\n",
      "Iteration 56, loss = 0.28387225\n",
      "Iteration 57, loss = 0.28298934\n",
      "Iteration 58, loss = 0.28209033\n",
      "Iteration 59, loss = 0.28121083\n",
      "Iteration 60, loss = 0.28038467\n",
      "Iteration 61, loss = 0.27954439\n",
      "Iteration 62, loss = 0.27874192\n",
      "Iteration 63, loss = 0.27794220\n",
      "Iteration 64, loss = 0.27717521\n",
      "Iteration 65, loss = 0.27643496\n",
      "Iteration 66, loss = 0.27570358\n",
      "Iteration 67, loss = 0.27497556\n",
      "Iteration 68, loss = 0.27429141\n",
      "Iteration 69, loss = 0.27358931\n",
      "Iteration 70, loss = 0.27292355\n",
      "Iteration 71, loss = 0.27226814\n",
      "Iteration 72, loss = 0.27163559\n",
      "Iteration 73, loss = 0.27099966\n",
      "Iteration 74, loss = 0.27037914\n",
      "Iteration 75, loss = 0.26979180\n",
      "Iteration 76, loss = 0.26917853\n",
      "Iteration 77, loss = 0.26861295\n",
      "Iteration 78, loss = 0.26802640\n",
      "Iteration 79, loss = 0.26747386\n",
      "Iteration 80, loss = 0.26695880\n",
      "Iteration 81, loss = 0.26638240\n",
      "Iteration 82, loss = 0.26587229\n",
      "Iteration 83, loss = 0.26535194\n",
      "Iteration 84, loss = 0.26483810\n",
      "Iteration 85, loss = 0.26434186\n",
      "Iteration 86, loss = 0.26384593\n",
      "Iteration 87, loss = 0.26337497\n",
      "Iteration 88, loss = 0.26290555\n",
      "Iteration 89, loss = 0.26243387\n",
      "Iteration 90, loss = 0.26198088\n",
      "Iteration 91, loss = 0.26152024\n",
      "Iteration 92, loss = 0.26105440\n",
      "Iteration 93, loss = 0.26062250\n",
      "Iteration 94, loss = 0.26017878\n",
      "Iteration 95, loss = 0.25975886\n",
      "Iteration 96, loss = 0.25933137\n",
      "Iteration 97, loss = 0.25892939\n",
      "Iteration 98, loss = 0.25852339\n",
      "Iteration 99, loss = 0.25809054\n",
      "Iteration 100, loss = 0.25771344\n",
      "Iteration 101, loss = 0.25730778\n",
      "Iteration 102, loss = 0.25691475\n",
      "Iteration 103, loss = 0.25652457\n",
      "Iteration 104, loss = 0.25614703\n",
      "Iteration 105, loss = 0.25576311\n",
      "Iteration 106, loss = 0.25540094\n",
      "Iteration 107, loss = 0.25504998\n",
      "Iteration 108, loss = 0.25465304\n",
      "Iteration 109, loss = 0.25429185\n",
      "Iteration 110, loss = 0.25393279\n",
      "Iteration 111, loss = 0.25356952\n",
      "Iteration 112, loss = 0.25322787\n",
      "Iteration 113, loss = 0.25287721\n",
      "Iteration 114, loss = 0.25251578\n",
      "Iteration 115, loss = 0.25215688\n",
      "Iteration 116, loss = 0.25184523\n",
      "Iteration 117, loss = 0.25148903\n",
      "Iteration 118, loss = 0.25116075\n",
      "Iteration 119, loss = 0.25082821\n",
      "Iteration 120, loss = 0.25052022\n",
      "Iteration 121, loss = 0.25017687\n",
      "Iteration 122, loss = 0.24986956\n",
      "Iteration 123, loss = 0.24953472\n",
      "Iteration 124, loss = 0.24921065\n",
      "Iteration 125, loss = 0.24891987\n",
      "Iteration 126, loss = 0.24858900\n",
      "Iteration 127, loss = 0.24829326\n",
      "Iteration 128, loss = 0.24797481\n",
      "Iteration 129, loss = 0.24766797\n",
      "Iteration 130, loss = 0.24736039\n",
      "Iteration 131, loss = 0.24709306\n",
      "Iteration 132, loss = 0.24677604\n",
      "Iteration 133, loss = 0.24647478\n",
      "Iteration 134, loss = 0.24618138\n",
      "Iteration 135, loss = 0.24588705\n",
      "Iteration 136, loss = 0.24559656\n",
      "Iteration 137, loss = 0.24531191\n",
      "Iteration 138, loss = 0.24503906\n",
      "Iteration 139, loss = 0.24473937\n",
      "Iteration 140, loss = 0.24446133\n",
      "Iteration 141, loss = 0.24419126\n",
      "Iteration 142, loss = 0.24390340\n",
      "Iteration 143, loss = 0.24363764\n",
      "Iteration 144, loss = 0.24334754\n",
      "Iteration 145, loss = 0.24308423\n",
      "Iteration 146, loss = 0.24279296\n",
      "Iteration 147, loss = 0.24253917\n",
      "Iteration 148, loss = 0.24226102\n",
      "Iteration 149, loss = 0.24199009\n",
      "Iteration 150, loss = 0.24174809\n",
      "Iteration 151, loss = 0.24146135\n",
      "Iteration 152, loss = 0.24120051\n",
      "Iteration 153, loss = 0.24094387\n",
      "Iteration 154, loss = 0.24066482\n",
      "Iteration 155, loss = 0.24043390\n",
      "Iteration 156, loss = 0.24015224\n",
      "Iteration 157, loss = 0.23990702\n",
      "Iteration 158, loss = 0.23965740\n",
      "Iteration 159, loss = 0.23941038\n",
      "Iteration 160, loss = 0.23915344\n",
      "Iteration 161, loss = 0.23888667\n",
      "Iteration 162, loss = 0.23863574\n",
      "Iteration 163, loss = 0.23840315\n",
      "Iteration 164, loss = 0.23814397\n",
      "Iteration 165, loss = 0.23789779\n",
      "Iteration 166, loss = 0.23765373\n",
      "Iteration 167, loss = 0.23740812\n",
      "Iteration 168, loss = 0.23715781\n",
      "Iteration 169, loss = 0.23691499\n",
      "Iteration 170, loss = 0.23667342\n",
      "Iteration 171, loss = 0.23643315\n",
      "Iteration 172, loss = 0.23619624\n",
      "Iteration 173, loss = 0.23596267\n",
      "Iteration 174, loss = 0.23571297\n",
      "Iteration 175, loss = 0.23546996\n",
      "Iteration 176, loss = 0.23522801\n",
      "Iteration 177, loss = 0.23500596\n",
      "Iteration 178, loss = 0.23477584\n",
      "Iteration 179, loss = 0.23455117\n",
      "Iteration 180, loss = 0.23431270\n",
      "Iteration 181, loss = 0.23407337\n",
      "Iteration 182, loss = 0.23384227\n",
      "Iteration 183, loss = 0.23360216\n",
      "Iteration 184, loss = 0.23336762\n",
      "Iteration 185, loss = 0.23314318\n",
      "Iteration 186, loss = 0.23294206\n",
      "Iteration 187, loss = 0.23268682\n",
      "Iteration 188, loss = 0.23246605\n",
      "Iteration 189, loss = 0.23224346\n",
      "Iteration 190, loss = 0.23199042\n",
      "Iteration 191, loss = 0.23176241\n",
      "Iteration 192, loss = 0.23155063\n",
      "Iteration 193, loss = 0.23131044\n",
      "Iteration 194, loss = 0.23109562\n",
      "Iteration 195, loss = 0.23086259\n",
      "Iteration 196, loss = 0.23062357\n",
      "Iteration 197, loss = 0.23041425\n",
      "Iteration 198, loss = 0.23019514\n",
      "Iteration 199, loss = 0.22997079\n",
      "Iteration 200, loss = 0.22973796\n",
      "Iteration 1, loss = 0.79887779\n",
      "Iteration 2, loss = 0.63863307\n",
      "Iteration 3, loss = 0.53305943\n",
      "Iteration 4, loss = 0.48003901\n",
      "Iteration 5, loss = 0.45237280\n",
      "Iteration 6, loss = 0.43555924\n",
      "Iteration 7, loss = 0.42385281\n",
      "Iteration 8, loss = 0.41444578\n",
      "Iteration 9, loss = 0.40657271\n",
      "Iteration 10, loss = 0.39957875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.39328105\n",
      "Iteration 12, loss = 0.38738257\n",
      "Iteration 13, loss = 0.38195731\n",
      "Iteration 14, loss = 0.37683936\n",
      "Iteration 15, loss = 0.37197562\n",
      "Iteration 16, loss = 0.36743804\n",
      "Iteration 17, loss = 0.36312730\n",
      "Iteration 18, loss = 0.35908549\n",
      "Iteration 19, loss = 0.35519073\n",
      "Iteration 20, loss = 0.35152062\n",
      "Iteration 21, loss = 0.34802316\n",
      "Iteration 22, loss = 0.34467515\n",
      "Iteration 23, loss = 0.34147470\n",
      "Iteration 24, loss = 0.33840848\n",
      "Iteration 25, loss = 0.33554662\n",
      "Iteration 26, loss = 0.33277005\n",
      "Iteration 27, loss = 0.33014230\n",
      "Iteration 28, loss = 0.32758916\n",
      "Iteration 29, loss = 0.32514325\n",
      "Iteration 30, loss = 0.32276407\n",
      "Iteration 31, loss = 0.32058107\n",
      "Iteration 32, loss = 0.31836395\n",
      "Iteration 33, loss = 0.31632196\n",
      "Iteration 34, loss = 0.31430616\n",
      "Iteration 35, loss = 0.31238244\n",
      "Iteration 36, loss = 0.31052986\n",
      "Iteration 37, loss = 0.30871456\n",
      "Iteration 38, loss = 0.30701623\n",
      "Iteration 39, loss = 0.30531195\n",
      "Iteration 40, loss = 0.30368988\n",
      "Iteration 41, loss = 0.30217551\n",
      "Iteration 42, loss = 0.30064020\n",
      "Iteration 43, loss = 0.29918849\n",
      "Iteration 44, loss = 0.29778341\n",
      "Iteration 45, loss = 0.29640628\n",
      "Iteration 46, loss = 0.29509365\n",
      "Iteration 47, loss = 0.29383201\n",
      "Iteration 48, loss = 0.29252313\n",
      "Iteration 49, loss = 0.29135796\n",
      "Iteration 50, loss = 0.29014092\n",
      "Iteration 51, loss = 0.28899322\n",
      "Iteration 52, loss = 0.28790931\n",
      "Iteration 53, loss = 0.28679577\n",
      "Iteration 54, loss = 0.28574617\n",
      "Iteration 55, loss = 0.28471713\n",
      "Iteration 56, loss = 0.28373075\n",
      "Iteration 57, loss = 0.28273462\n",
      "Iteration 58, loss = 0.28181006\n",
      "Iteration 59, loss = 0.28087161\n",
      "Iteration 60, loss = 0.27996673\n",
      "Iteration 61, loss = 0.27910440\n",
      "Iteration 62, loss = 0.27821516\n",
      "Iteration 63, loss = 0.27737841\n",
      "Iteration 64, loss = 0.27654445\n",
      "Iteration 65, loss = 0.27576712\n",
      "Iteration 66, loss = 0.27494192\n",
      "Iteration 67, loss = 0.27417052\n",
      "Iteration 68, loss = 0.27342354\n",
      "Iteration 69, loss = 0.27266872\n",
      "Iteration 70, loss = 0.27194592\n",
      "Iteration 71, loss = 0.27124107\n",
      "Iteration 72, loss = 0.27054879\n",
      "Iteration 73, loss = 0.26984852\n",
      "Iteration 74, loss = 0.26919061\n",
      "Iteration 75, loss = 0.26853697\n",
      "Iteration 76, loss = 0.26787913\n",
      "Iteration 77, loss = 0.26723883\n",
      "Iteration 78, loss = 0.26661216\n",
      "Iteration 79, loss = 0.26600888\n",
      "Iteration 80, loss = 0.26538813\n",
      "Iteration 81, loss = 0.26482160\n",
      "Iteration 82, loss = 0.26427881\n",
      "Iteration 83, loss = 0.26366178\n",
      "Iteration 84, loss = 0.26310021\n",
      "Iteration 85, loss = 0.26252952\n",
      "Iteration 86, loss = 0.26199193\n",
      "Iteration 87, loss = 0.26145293\n",
      "Iteration 88, loss = 0.26092045\n",
      "Iteration 89, loss = 0.26039516\n",
      "Iteration 90, loss = 0.25987769\n",
      "Iteration 91, loss = 0.25936695\n",
      "Iteration 92, loss = 0.25887567\n",
      "Iteration 93, loss = 0.25836887\n",
      "Iteration 94, loss = 0.25789076\n",
      "Iteration 95, loss = 0.25739422\n",
      "Iteration 96, loss = 0.25694333\n",
      "Iteration 97, loss = 0.25647275\n",
      "Iteration 98, loss = 0.25596737\n",
      "Iteration 99, loss = 0.25552910\n",
      "Iteration 100, loss = 0.25509508\n",
      "Iteration 101, loss = 0.25461676\n",
      "Iteration 102, loss = 0.25416842\n",
      "Iteration 103, loss = 0.25374955\n",
      "Iteration 104, loss = 0.25328734\n",
      "Iteration 105, loss = 0.25286166\n",
      "Iteration 106, loss = 0.25244033\n",
      "Iteration 107, loss = 0.25201318\n",
      "Iteration 108, loss = 0.25158842\n",
      "Iteration 109, loss = 0.25117486\n",
      "Iteration 110, loss = 0.25076562\n",
      "Iteration 111, loss = 0.25037603\n",
      "Iteration 112, loss = 0.24996230\n",
      "Iteration 113, loss = 0.24956387\n",
      "Iteration 114, loss = 0.24920603\n",
      "Iteration 115, loss = 0.24878229\n",
      "Iteration 116, loss = 0.24838865\n",
      "Iteration 117, loss = 0.24801015\n",
      "Iteration 118, loss = 0.24761639\n",
      "Iteration 119, loss = 0.24725329\n",
      "Iteration 120, loss = 0.24688765\n",
      "Iteration 121, loss = 0.24651203\n",
      "Iteration 122, loss = 0.24613982\n",
      "Iteration 123, loss = 0.24578996\n",
      "Iteration 124, loss = 0.24543576\n",
      "Iteration 125, loss = 0.24506081\n",
      "Iteration 126, loss = 0.24470071\n",
      "Iteration 127, loss = 0.24434940\n",
      "Iteration 128, loss = 0.24399519\n",
      "Iteration 129, loss = 0.24365079\n",
      "Iteration 130, loss = 0.24333181\n",
      "Iteration 131, loss = 0.24295571\n",
      "Iteration 132, loss = 0.24260580\n",
      "Iteration 133, loss = 0.24227908\n",
      "Iteration 134, loss = 0.24196250\n",
      "Iteration 135, loss = 0.24158223\n",
      "Iteration 136, loss = 0.24125122\n",
      "Iteration 137, loss = 0.24092555\n",
      "Iteration 138, loss = 0.24058543\n",
      "Iteration 139, loss = 0.24026168\n",
      "Iteration 140, loss = 0.23994348\n",
      "Iteration 141, loss = 0.23960754\n",
      "Iteration 142, loss = 0.23927251\n",
      "Iteration 143, loss = 0.23897387\n",
      "Iteration 144, loss = 0.23865083\n",
      "Iteration 145, loss = 0.23831203\n",
      "Iteration 146, loss = 0.23799767\n",
      "Iteration 147, loss = 0.23768267\n",
      "Iteration 148, loss = 0.23736722\n",
      "Iteration 149, loss = 0.23706195\n",
      "Iteration 150, loss = 0.23674362\n",
      "Iteration 151, loss = 0.23643413\n",
      "Iteration 152, loss = 0.23614854\n",
      "Iteration 153, loss = 0.23582823\n",
      "Iteration 154, loss = 0.23550913\n",
      "Iteration 155, loss = 0.23519788\n",
      "Iteration 156, loss = 0.23489327\n",
      "Iteration 157, loss = 0.23460577\n",
      "Iteration 158, loss = 0.23428629\n",
      "Iteration 159, loss = 0.23399760\n",
      "Iteration 160, loss = 0.23368477\n",
      "Iteration 161, loss = 0.23339330\n",
      "Iteration 162, loss = 0.23311219\n",
      "Iteration 163, loss = 0.23278966\n",
      "Iteration 164, loss = 0.23251179\n",
      "Iteration 165, loss = 0.23221118\n",
      "Iteration 166, loss = 0.23193754\n",
      "Iteration 167, loss = 0.23162063\n",
      "Iteration 168, loss = 0.23133105\n",
      "Iteration 169, loss = 0.23104270\n",
      "Iteration 170, loss = 0.23074942\n",
      "Iteration 171, loss = 0.23046062\n",
      "Iteration 172, loss = 0.23018866\n",
      "Iteration 173, loss = 0.22990008\n",
      "Iteration 174, loss = 0.22961183\n",
      "Iteration 175, loss = 0.22933458\n",
      "Iteration 176, loss = 0.22905486\n",
      "Iteration 177, loss = 0.22875624\n",
      "Iteration 178, loss = 0.22849390\n",
      "Iteration 179, loss = 0.22820895\n",
      "Iteration 180, loss = 0.22792888\n",
      "Iteration 181, loss = 0.22768612\n",
      "Iteration 182, loss = 0.22736655\n",
      "Iteration 183, loss = 0.22711141\n",
      "Iteration 184, loss = 0.22683994\n",
      "Iteration 185, loss = 0.22655424\n",
      "Iteration 186, loss = 0.22628264\n",
      "Iteration 187, loss = 0.22601553\n",
      "Iteration 188, loss = 0.22574698\n",
      "Iteration 189, loss = 0.22545880\n",
      "Iteration 190, loss = 0.22523484\n",
      "Iteration 191, loss = 0.22496073\n",
      "Iteration 192, loss = 0.22466906\n",
      "Iteration 193, loss = 0.22438356\n",
      "Iteration 194, loss = 0.22412615\n",
      "Iteration 195, loss = 0.22386132\n",
      "Iteration 196, loss = 0.22357388\n",
      "Iteration 197, loss = 0.22330574\n",
      "Iteration 198, loss = 0.22304049\n",
      "Iteration 199, loss = 0.22276305\n",
      "Iteration 200, loss = 0.22252713\n",
      "Iteration 1, loss = 0.79865590\n",
      "Iteration 2, loss = 0.63917790\n",
      "Iteration 3, loss = 0.53335930\n",
      "Iteration 4, loss = 0.48019579\n",
      "Iteration 5, loss = 0.45247498\n",
      "Iteration 6, loss = 0.43547400\n",
      "Iteration 7, loss = 0.42378817\n",
      "Iteration 8, loss = 0.41409948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.40615577\n",
      "Iteration 10, loss = 0.39917490\n",
      "Iteration 11, loss = 0.39274991\n",
      "Iteration 12, loss = 0.38679241\n",
      "Iteration 13, loss = 0.38136502\n",
      "Iteration 14, loss = 0.37618113\n",
      "Iteration 15, loss = 0.37131939\n",
      "Iteration 16, loss = 0.36674182\n",
      "Iteration 17, loss = 0.36246995\n",
      "Iteration 18, loss = 0.35840351\n",
      "Iteration 19, loss = 0.35450403\n",
      "Iteration 20, loss = 0.35087537\n",
      "Iteration 21, loss = 0.34740091\n",
      "Iteration 22, loss = 0.34412158\n",
      "Iteration 23, loss = 0.34092702\n",
      "Iteration 24, loss = 0.33793473\n",
      "Iteration 25, loss = 0.33506608\n",
      "Iteration 26, loss = 0.33236822\n",
      "Iteration 27, loss = 0.32977066\n",
      "Iteration 28, loss = 0.32728088\n",
      "Iteration 29, loss = 0.32491484\n",
      "Iteration 30, loss = 0.32259653\n",
      "Iteration 31, loss = 0.32043791\n",
      "Iteration 32, loss = 0.31833018\n",
      "Iteration 33, loss = 0.31632843\n",
      "Iteration 34, loss = 0.31438721\n",
      "Iteration 35, loss = 0.31255369\n",
      "Iteration 36, loss = 0.31074684\n",
      "Iteration 37, loss = 0.30903771\n",
      "Iteration 38, loss = 0.30736312\n",
      "Iteration 39, loss = 0.30576208\n",
      "Iteration 40, loss = 0.30418516\n",
      "Iteration 41, loss = 0.30273030\n",
      "Iteration 42, loss = 0.30127688\n",
      "Iteration 43, loss = 0.29987761\n",
      "Iteration 44, loss = 0.29855719\n",
      "Iteration 45, loss = 0.29723990\n",
      "Iteration 46, loss = 0.29600004\n",
      "Iteration 47, loss = 0.29483951\n",
      "Iteration 48, loss = 0.29355858\n",
      "Iteration 49, loss = 0.29246783\n",
      "Iteration 50, loss = 0.29132040\n",
      "Iteration 51, loss = 0.29026084\n",
      "Iteration 52, loss = 0.28923009\n",
      "Iteration 53, loss = 0.28818051\n",
      "Iteration 54, loss = 0.28718308\n",
      "Iteration 55, loss = 0.28621950\n",
      "Iteration 56, loss = 0.28529437\n",
      "Iteration 57, loss = 0.28437701\n",
      "Iteration 58, loss = 0.28349668\n",
      "Iteration 59, loss = 0.28262578\n",
      "Iteration 60, loss = 0.28178686\n",
      "Iteration 61, loss = 0.28095565\n",
      "Iteration 62, loss = 0.28014169\n",
      "Iteration 63, loss = 0.27937899\n",
      "Iteration 64, loss = 0.27859264\n",
      "Iteration 65, loss = 0.27787112\n",
      "Iteration 66, loss = 0.27711876\n",
      "Iteration 67, loss = 0.27639811\n",
      "Iteration 68, loss = 0.27570196\n",
      "Iteration 69, loss = 0.27501205\n",
      "Iteration 70, loss = 0.27433937\n",
      "Iteration 71, loss = 0.27368219\n",
      "Iteration 72, loss = 0.27303963\n",
      "Iteration 73, loss = 0.27239663\n",
      "Iteration 74, loss = 0.27179160\n",
      "Iteration 75, loss = 0.27116592\n",
      "Iteration 76, loss = 0.27056442\n",
      "Iteration 77, loss = 0.26998747\n",
      "Iteration 78, loss = 0.26940987\n",
      "Iteration 79, loss = 0.26882916\n",
      "Iteration 80, loss = 0.26828173\n",
      "Iteration 81, loss = 0.26774848\n",
      "Iteration 82, loss = 0.26722715\n",
      "Iteration 83, loss = 0.26668598\n",
      "Iteration 84, loss = 0.26616860\n",
      "Iteration 85, loss = 0.26562130\n",
      "Iteration 86, loss = 0.26513989\n",
      "Iteration 87, loss = 0.26463616\n",
      "Iteration 88, loss = 0.26415155\n",
      "Iteration 89, loss = 0.26366602\n",
      "Iteration 90, loss = 0.26318109\n",
      "Iteration 91, loss = 0.26273449\n",
      "Iteration 92, loss = 0.26226625\n",
      "Iteration 93, loss = 0.26183108\n",
      "Iteration 94, loss = 0.26137745\n",
      "Iteration 95, loss = 0.26094481\n",
      "Iteration 96, loss = 0.26054012\n",
      "Iteration 97, loss = 0.26008753\n",
      "Iteration 98, loss = 0.25965470\n",
      "Iteration 99, loss = 0.25925593\n",
      "Iteration 100, loss = 0.25885223\n",
      "Iteration 101, loss = 0.25843906\n",
      "Iteration 102, loss = 0.25802292\n",
      "Iteration 103, loss = 0.25763808\n",
      "Iteration 104, loss = 0.25723054\n",
      "Iteration 105, loss = 0.25684141\n",
      "Iteration 106, loss = 0.25646391\n",
      "Iteration 107, loss = 0.25607827\n",
      "Iteration 108, loss = 0.25570809\n",
      "Iteration 109, loss = 0.25532808\n",
      "Iteration 110, loss = 0.25495774\n",
      "Iteration 111, loss = 0.25460450\n",
      "Iteration 112, loss = 0.25423784\n",
      "Iteration 113, loss = 0.25387420\n",
      "Iteration 114, loss = 0.25352834\n",
      "Iteration 115, loss = 0.25316172\n",
      "Iteration 116, loss = 0.25282950\n",
      "Iteration 117, loss = 0.25246877\n",
      "Iteration 118, loss = 0.25211216\n",
      "Iteration 119, loss = 0.25178293\n",
      "Iteration 120, loss = 0.25144782\n",
      "Iteration 121, loss = 0.25111148\n",
      "Iteration 122, loss = 0.25078253\n",
      "Iteration 123, loss = 0.25045435\n",
      "Iteration 124, loss = 0.25013351\n",
      "Iteration 125, loss = 0.24980752\n",
      "Iteration 126, loss = 0.24948761\n",
      "Iteration 127, loss = 0.24914483\n",
      "Iteration 128, loss = 0.24882361\n",
      "Iteration 129, loss = 0.24851765\n",
      "Iteration 130, loss = 0.24821453\n",
      "Iteration 131, loss = 0.24787856\n",
      "Iteration 132, loss = 0.24757419\n",
      "Iteration 133, loss = 0.24726580\n",
      "Iteration 134, loss = 0.24697817\n",
      "Iteration 135, loss = 0.24665565\n",
      "Iteration 136, loss = 0.24636294\n",
      "Iteration 137, loss = 0.24605600\n",
      "Iteration 138, loss = 0.24576622\n",
      "Iteration 139, loss = 0.24546530\n",
      "Iteration 140, loss = 0.24518400\n",
      "Iteration 141, loss = 0.24487437\n",
      "Iteration 142, loss = 0.24458731\n",
      "Iteration 143, loss = 0.24431932\n",
      "Iteration 144, loss = 0.24401952\n",
      "Iteration 145, loss = 0.24372490\n",
      "Iteration 146, loss = 0.24343196\n",
      "Iteration 147, loss = 0.24317049\n",
      "Iteration 148, loss = 0.24288374\n",
      "Iteration 149, loss = 0.24260728\n",
      "Iteration 150, loss = 0.24233819\n",
      "Iteration 151, loss = 0.24207747\n",
      "Iteration 152, loss = 0.24179243\n",
      "Iteration 153, loss = 0.24151126\n",
      "Iteration 154, loss = 0.24124805\n",
      "Iteration 155, loss = 0.24095829\n",
      "Iteration 156, loss = 0.24067692\n",
      "Iteration 157, loss = 0.24043109\n",
      "Iteration 158, loss = 0.24016067\n",
      "Iteration 159, loss = 0.23988397\n",
      "Iteration 160, loss = 0.23962553\n",
      "Iteration 161, loss = 0.23936976\n",
      "Iteration 162, loss = 0.23911880\n",
      "Iteration 163, loss = 0.23882710\n",
      "Iteration 164, loss = 0.23858159\n",
      "Iteration 165, loss = 0.23831529\n",
      "Iteration 166, loss = 0.23805091\n",
      "Iteration 167, loss = 0.23779023\n",
      "Iteration 168, loss = 0.23754529\n",
      "Iteration 169, loss = 0.23728366\n",
      "Iteration 170, loss = 0.23703406\n",
      "Iteration 171, loss = 0.23678483\n",
      "Iteration 172, loss = 0.23652602\n",
      "Iteration 173, loss = 0.23628222\n",
      "Iteration 174, loss = 0.23603329\n",
      "Iteration 175, loss = 0.23578630\n",
      "Iteration 176, loss = 0.23551944\n",
      "Iteration 177, loss = 0.23528459\n",
      "Iteration 178, loss = 0.23503627\n",
      "Iteration 179, loss = 0.23477986\n",
      "Iteration 180, loss = 0.23453862\n",
      "Iteration 181, loss = 0.23431159\n",
      "Iteration 182, loss = 0.23404499\n",
      "Iteration 183, loss = 0.23381908\n",
      "Iteration 184, loss = 0.23356658\n",
      "Iteration 185, loss = 0.23331366\n",
      "Iteration 186, loss = 0.23307844\n",
      "Iteration 187, loss = 0.23285354\n",
      "Iteration 188, loss = 0.23259705\n",
      "Iteration 189, loss = 0.23234645\n",
      "Iteration 190, loss = 0.23214493\n",
      "Iteration 191, loss = 0.23191799\n",
      "Iteration 192, loss = 0.23164725\n",
      "Iteration 193, loss = 0.23138661\n",
      "Iteration 194, loss = 0.23115786\n",
      "Iteration 195, loss = 0.23092283\n",
      "Iteration 196, loss = 0.23066474\n",
      "Iteration 197, loss = 0.23041683\n",
      "Iteration 198, loss = 0.23019137\n",
      "Iteration 199, loss = 0.22994415\n",
      "Iteration 200, loss = 0.22972552\n",
      "Iteration 1, loss = 0.69911757\n",
      "Iteration 2, loss = 0.48060840\n",
      "Iteration 3, loss = 0.38750640\n",
      "Iteration 4, loss = 0.34560133\n",
      "Iteration 5, loss = 0.32060843\n",
      "Iteration 6, loss = 0.30302012\n",
      "Iteration 7, loss = 0.29001120\n",
      "Iteration 8, loss = 0.27956141\n",
      "Iteration 9, loss = 0.27075632\n",
      "Iteration 10, loss = 0.26325371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.25658354\n",
      "Iteration 12, loss = 0.25058881\n",
      "Iteration 13, loss = 0.24517953\n",
      "Iteration 14, loss = 0.24019150\n",
      "Iteration 15, loss = 0.23542526\n",
      "Iteration 16, loss = 0.23107292\n",
      "Iteration 17, loss = 0.22670121\n",
      "Iteration 18, loss = 0.22281578\n",
      "Iteration 19, loss = 0.21893181\n",
      "Iteration 20, loss = 0.21525069\n",
      "Iteration 21, loss = 0.21144223\n",
      "Iteration 22, loss = 0.20824743\n",
      "Iteration 23, loss = 0.20475182\n",
      "Iteration 24, loss = 0.20127410\n",
      "Iteration 25, loss = 0.19828248\n",
      "Iteration 26, loss = 0.19517758\n",
      "Iteration 27, loss = 0.19214683\n",
      "Iteration 28, loss = 0.18938015\n",
      "Iteration 29, loss = 0.18633537\n",
      "Iteration 30, loss = 0.18372352\n",
      "Iteration 31, loss = 0.18101691\n",
      "Iteration 32, loss = 0.17828711\n",
      "Iteration 33, loss = 0.17582458\n",
      "Iteration 34, loss = 0.17349773\n",
      "Iteration 35, loss = 0.17160807\n",
      "Iteration 36, loss = 0.16920385\n",
      "Iteration 37, loss = 0.16665074\n",
      "Iteration 38, loss = 0.16442053\n",
      "Iteration 39, loss = 0.16246524\n",
      "Iteration 40, loss = 0.16076531\n",
      "Iteration 41, loss = 0.15839839\n",
      "Iteration 42, loss = 0.15676597\n",
      "Iteration 43, loss = 0.15439100\n",
      "Iteration 44, loss = 0.15301735\n",
      "Iteration 45, loss = 0.15123833\n",
      "Iteration 46, loss = 0.14926694\n",
      "Iteration 47, loss = 0.14795141\n",
      "Iteration 48, loss = 0.14599673\n",
      "Iteration 49, loss = 0.14438352\n",
      "Iteration 50, loss = 0.14238709\n",
      "Iteration 51, loss = 0.14112823\n",
      "Iteration 52, loss = 0.14018358\n",
      "Iteration 53, loss = 0.13803346\n",
      "Iteration 54, loss = 0.13672829\n",
      "Iteration 55, loss = 0.13528992\n",
      "Iteration 56, loss = 0.13373191\n",
      "Iteration 57, loss = 0.13283372\n",
      "Iteration 58, loss = 0.13138833\n",
      "Iteration 59, loss = 0.13001477\n",
      "Iteration 60, loss = 0.12892932\n",
      "Iteration 61, loss = 0.12745681\n",
      "Iteration 62, loss = 0.12638688\n",
      "Iteration 63, loss = 0.12515351\n",
      "Iteration 64, loss = 0.12433251\n",
      "Iteration 65, loss = 0.12287426\n",
      "Iteration 66, loss = 0.12176353\n",
      "Iteration 67, loss = 0.12065002\n",
      "Iteration 68, loss = 0.11952466\n",
      "Iteration 69, loss = 0.11885004\n",
      "Iteration 70, loss = 0.11732415\n",
      "Iteration 71, loss = 0.11646595\n",
      "Iteration 72, loss = 0.11547724\n",
      "Iteration 73, loss = 0.11477712\n",
      "Iteration 74, loss = 0.11342055\n",
      "Iteration 75, loss = 0.11250611\n",
      "Iteration 76, loss = 0.11154854\n",
      "Iteration 77, loss = 0.11082275\n",
      "Iteration 78, loss = 0.10991482\n",
      "Iteration 79, loss = 0.10907707\n",
      "Iteration 80, loss = 0.10792383\n",
      "Iteration 81, loss = 0.10736636\n",
      "Iteration 82, loss = 0.10673594\n",
      "Iteration 83, loss = 0.10562052\n",
      "Iteration 84, loss = 0.10494311\n",
      "Iteration 85, loss = 0.10410427\n",
      "Iteration 86, loss = 0.10309906\n",
      "Iteration 87, loss = 0.10235499\n",
      "Iteration 88, loss = 0.10163596\n",
      "Iteration 89, loss = 0.10067986\n",
      "Iteration 90, loss = 0.09999151\n",
      "Iteration 91, loss = 0.09890722\n",
      "Iteration 92, loss = 0.09859582\n",
      "Iteration 93, loss = 0.09785662\n",
      "Iteration 94, loss = 0.09689811\n",
      "Iteration 95, loss = 0.09644145\n",
      "Iteration 96, loss = 0.09550407\n",
      "Iteration 97, loss = 0.09489282\n",
      "Iteration 98, loss = 0.09409600\n",
      "Iteration 99, loss = 0.09311529\n",
      "Iteration 100, loss = 0.09286676\n",
      "Iteration 101, loss = 0.09218684\n",
      "Iteration 102, loss = 0.09115403\n",
      "Iteration 103, loss = 0.09076557\n",
      "Iteration 104, loss = 0.09004847\n",
      "Iteration 105, loss = 0.08931415\n",
      "Iteration 106, loss = 0.08852486\n",
      "Iteration 107, loss = 0.08818140\n",
      "Iteration 108, loss = 0.08744580\n",
      "Iteration 109, loss = 0.08652515\n",
      "Iteration 110, loss = 0.08643313\n",
      "Iteration 111, loss = 0.08561703\n",
      "Iteration 112, loss = 0.08490475\n",
      "Iteration 113, loss = 0.08425223\n",
      "Iteration 114, loss = 0.08342955\n",
      "Iteration 115, loss = 0.08301636\n",
      "Iteration 116, loss = 0.08211999\n",
      "Iteration 117, loss = 0.08181416\n",
      "Iteration 118, loss = 0.08103180\n",
      "Iteration 119, loss = 0.08057397\n",
      "Iteration 120, loss = 0.08024235\n",
      "Iteration 121, loss = 0.07965151\n",
      "Iteration 122, loss = 0.07870511\n",
      "Iteration 123, loss = 0.07785015\n",
      "Iteration 124, loss = 0.07773194\n",
      "Iteration 125, loss = 0.07759883\n",
      "Iteration 126, loss = 0.07678821\n",
      "Iteration 127, loss = 0.07646563\n",
      "Iteration 128, loss = 0.07565375\n",
      "Iteration 129, loss = 0.07483058\n",
      "Iteration 130, loss = 0.07450448\n",
      "Iteration 131, loss = 0.07419125\n",
      "Iteration 132, loss = 0.07378848\n",
      "Iteration 133, loss = 0.07300708\n",
      "Iteration 134, loss = 0.07227344\n",
      "Iteration 135, loss = 0.07208767\n",
      "Iteration 136, loss = 0.07106320\n",
      "Iteration 137, loss = 0.07071879\n",
      "Iteration 138, loss = 0.07014221\n",
      "Iteration 139, loss = 0.06989597\n",
      "Iteration 140, loss = 0.06929688\n",
      "Iteration 141, loss = 0.06858612\n",
      "Iteration 142, loss = 0.06857561\n",
      "Iteration 143, loss = 0.06824414\n",
      "Iteration 144, loss = 0.06713979\n",
      "Iteration 145, loss = 0.06675778\n",
      "Iteration 146, loss = 0.06622003\n",
      "Iteration 147, loss = 0.06624080\n",
      "Iteration 148, loss = 0.06547056\n",
      "Iteration 149, loss = 0.06531525\n",
      "Iteration 150, loss = 0.06421132\n",
      "Iteration 151, loss = 0.06466739\n",
      "Iteration 152, loss = 0.06373037\n",
      "Iteration 153, loss = 0.06328246\n",
      "Iteration 154, loss = 0.06268366\n",
      "Iteration 155, loss = 0.06262858\n",
      "Iteration 156, loss = 0.06180954\n",
      "Iteration 157, loss = 0.06123199\n",
      "Iteration 158, loss = 0.06122448\n",
      "Iteration 159, loss = 0.06027111\n",
      "Iteration 160, loss = 0.06017164\n",
      "Iteration 161, loss = 0.05978601\n",
      "Iteration 162, loss = 0.05954763\n",
      "Iteration 163, loss = 0.05895371\n",
      "Iteration 164, loss = 0.05876087\n",
      "Iteration 165, loss = 0.05812464\n",
      "Iteration 166, loss = 0.05783111\n",
      "Iteration 167, loss = 0.05747801\n",
      "Iteration 168, loss = 0.05673743\n",
      "Iteration 169, loss = 0.05668241\n",
      "Iteration 170, loss = 0.05664491\n",
      "Iteration 171, loss = 0.05544272\n",
      "Iteration 172, loss = 0.05545450\n",
      "Iteration 173, loss = 0.05474316\n",
      "Iteration 174, loss = 0.05450389\n",
      "Iteration 175, loss = 0.05382785\n",
      "Iteration 176, loss = 0.05391403\n",
      "Iteration 177, loss = 0.05335309\n",
      "Iteration 178, loss = 0.05268969\n",
      "Iteration 179, loss = 0.05258117\n",
      "Iteration 180, loss = 0.05196590\n",
      "Iteration 181, loss = 0.05169403\n",
      "Iteration 182, loss = 0.05130819\n",
      "Iteration 183, loss = 0.05103996\n",
      "Iteration 184, loss = 0.05044796\n",
      "Iteration 185, loss = 0.05036350\n",
      "Iteration 186, loss = 0.04979173\n",
      "Iteration 187, loss = 0.04950165\n",
      "Iteration 188, loss = 0.04904746\n",
      "Iteration 189, loss = 0.04874314\n",
      "Iteration 190, loss = 0.04817718\n",
      "Iteration 191, loss = 0.04770850\n",
      "Iteration 192, loss = 0.04777510\n",
      "Iteration 193, loss = 0.04726259\n",
      "Iteration 194, loss = 0.04684264\n",
      "Iteration 195, loss = 0.04667826\n",
      "Iteration 196, loss = 0.04585038\n",
      "Iteration 197, loss = 0.04586247\n",
      "Iteration 198, loss = 0.04555245\n",
      "Iteration 199, loss = 0.04518766\n",
      "Iteration 200, loss = 0.04482747\n",
      "Iteration 1, loss = 0.69689933\n",
      "Iteration 2, loss = 0.47814739\n",
      "Iteration 3, loss = 0.38587328\n",
      "Iteration 4, loss = 0.34465455\n",
      "Iteration 5, loss = 0.31980709\n",
      "Iteration 6, loss = 0.30248758\n",
      "Iteration 7, loss = 0.28964087\n",
      "Iteration 8, loss = 0.27923516\n",
      "Iteration 9, loss = 0.27097315\n",
      "Iteration 10, loss = 0.26384437\n",
      "Iteration 11, loss = 0.25746677\n",
      "Iteration 12, loss = 0.25193084\n",
      "Iteration 13, loss = 0.24682277\n",
      "Iteration 14, loss = 0.24210242\n",
      "Iteration 15, loss = 0.23769260\n",
      "Iteration 16, loss = 0.23355578\n",
      "Iteration 17, loss = 0.22957075\n",
      "Iteration 18, loss = 0.22559068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.22198391\n",
      "Iteration 20, loss = 0.21822326\n",
      "Iteration 21, loss = 0.21482320\n",
      "Iteration 22, loss = 0.21156201\n",
      "Iteration 23, loss = 0.20826405\n",
      "Iteration 24, loss = 0.20509163\n",
      "Iteration 25, loss = 0.20198054\n",
      "Iteration 26, loss = 0.19914781\n",
      "Iteration 27, loss = 0.19630755\n",
      "Iteration 28, loss = 0.19368888\n",
      "Iteration 29, loss = 0.19094968\n",
      "Iteration 30, loss = 0.18840098\n",
      "Iteration 31, loss = 0.18581733\n",
      "Iteration 32, loss = 0.18336859\n",
      "Iteration 33, loss = 0.18122965\n",
      "Iteration 34, loss = 0.17890707\n",
      "Iteration 35, loss = 0.17682970\n",
      "Iteration 36, loss = 0.17460318\n",
      "Iteration 37, loss = 0.17223914\n",
      "Iteration 38, loss = 0.17005503\n",
      "Iteration 39, loss = 0.16817287\n",
      "Iteration 40, loss = 0.16657189\n",
      "Iteration 41, loss = 0.16436020\n",
      "Iteration 42, loss = 0.16252033\n",
      "Iteration 43, loss = 0.16045180\n",
      "Iteration 44, loss = 0.15875010\n",
      "Iteration 45, loss = 0.15706684\n",
      "Iteration 46, loss = 0.15544264\n",
      "Iteration 47, loss = 0.15363885\n",
      "Iteration 48, loss = 0.15192009\n",
      "Iteration 49, loss = 0.15047935\n",
      "Iteration 50, loss = 0.14872734\n",
      "Iteration 51, loss = 0.14733425\n",
      "Iteration 52, loss = 0.14598409\n",
      "Iteration 53, loss = 0.14408522\n",
      "Iteration 54, loss = 0.14302381\n",
      "Iteration 55, loss = 0.14150772\n",
      "Iteration 56, loss = 0.13991874\n",
      "Iteration 57, loss = 0.13892789\n",
      "Iteration 58, loss = 0.13787752\n",
      "Iteration 59, loss = 0.13636641\n",
      "Iteration 60, loss = 0.13531650\n",
      "Iteration 61, loss = 0.13381409\n",
      "Iteration 62, loss = 0.13254667\n",
      "Iteration 63, loss = 0.13137218\n",
      "Iteration 64, loss = 0.13016303\n",
      "Iteration 65, loss = 0.12909716\n",
      "Iteration 66, loss = 0.12802225\n",
      "Iteration 67, loss = 0.12686023\n",
      "Iteration 68, loss = 0.12596116\n",
      "Iteration 69, loss = 0.12491111\n",
      "Iteration 70, loss = 0.12379870\n",
      "Iteration 71, loss = 0.12274265\n",
      "Iteration 72, loss = 0.12192661\n",
      "Iteration 73, loss = 0.12085791\n",
      "Iteration 74, loss = 0.11976845\n",
      "Iteration 75, loss = 0.11933185\n",
      "Iteration 76, loss = 0.11797089\n",
      "Iteration 77, loss = 0.11698524\n",
      "Iteration 78, loss = 0.11600255\n",
      "Iteration 79, loss = 0.11539460\n",
      "Iteration 80, loss = 0.11435956\n",
      "Iteration 81, loss = 0.11357352\n",
      "Iteration 82, loss = 0.11262305\n",
      "Iteration 83, loss = 0.11178085\n",
      "Iteration 84, loss = 0.11111514\n",
      "Iteration 85, loss = 0.11009914\n",
      "Iteration 86, loss = 0.10923460\n",
      "Iteration 87, loss = 0.10848323\n",
      "Iteration 88, loss = 0.10776416\n",
      "Iteration 89, loss = 0.10691142\n",
      "Iteration 90, loss = 0.10649755\n",
      "Iteration 91, loss = 0.10525380\n",
      "Iteration 92, loss = 0.10428951\n",
      "Iteration 93, loss = 0.10386008\n",
      "Iteration 94, loss = 0.10282189\n",
      "Iteration 95, loss = 0.10246230\n",
      "Iteration 96, loss = 0.10149806\n",
      "Iteration 97, loss = 0.10077667\n",
      "Iteration 98, loss = 0.10042671\n",
      "Iteration 99, loss = 0.09938743\n",
      "Iteration 100, loss = 0.09890899\n",
      "Iteration 101, loss = 0.09848911\n",
      "Iteration 102, loss = 0.09729947\n",
      "Iteration 103, loss = 0.09673066\n",
      "Iteration 104, loss = 0.09601207\n",
      "Iteration 105, loss = 0.09554312\n",
      "Iteration 106, loss = 0.09481650\n",
      "Iteration 107, loss = 0.09414036\n",
      "Iteration 108, loss = 0.09349318\n",
      "Iteration 109, loss = 0.09278566\n",
      "Iteration 110, loss = 0.09239120\n",
      "Iteration 111, loss = 0.09157006\n",
      "Iteration 112, loss = 0.09084357\n",
      "Iteration 113, loss = 0.09032948\n",
      "Iteration 114, loss = 0.08978276\n",
      "Iteration 115, loss = 0.08905398\n",
      "Iteration 116, loss = 0.08828594\n",
      "Iteration 117, loss = 0.08764756\n",
      "Iteration 118, loss = 0.08721455\n",
      "Iteration 119, loss = 0.08665508\n",
      "Iteration 120, loss = 0.08631926\n",
      "Iteration 121, loss = 0.08534335\n",
      "Iteration 122, loss = 0.08486618\n",
      "Iteration 123, loss = 0.08414361\n",
      "Iteration 124, loss = 0.08379103\n",
      "Iteration 125, loss = 0.08438162\n",
      "Iteration 126, loss = 0.08264659\n",
      "Iteration 127, loss = 0.08199091\n",
      "Iteration 128, loss = 0.08158991\n",
      "Iteration 129, loss = 0.08107902\n",
      "Iteration 130, loss = 0.08035572\n",
      "Iteration 131, loss = 0.07989772\n",
      "Iteration 132, loss = 0.07956893\n",
      "Iteration 133, loss = 0.07931136\n",
      "Iteration 134, loss = 0.07844662\n",
      "Iteration 135, loss = 0.07793521\n",
      "Iteration 136, loss = 0.07716576\n",
      "Iteration 137, loss = 0.07696347\n",
      "Iteration 138, loss = 0.07659665\n",
      "Iteration 139, loss = 0.07583041\n",
      "Iteration 140, loss = 0.07498644\n",
      "Iteration 141, loss = 0.07463832\n",
      "Iteration 142, loss = 0.07425657\n",
      "Iteration 143, loss = 0.07444798\n",
      "Iteration 144, loss = 0.07394960\n",
      "Iteration 145, loss = 0.07261220\n",
      "Iteration 146, loss = 0.07243575\n",
      "Iteration 147, loss = 0.07202176\n",
      "Iteration 148, loss = 0.07171647\n",
      "Iteration 149, loss = 0.07114475\n",
      "Iteration 150, loss = 0.07024819\n",
      "Iteration 151, loss = 0.06997351\n",
      "Iteration 152, loss = 0.06985174\n",
      "Iteration 153, loss = 0.06899813\n",
      "Iteration 154, loss = 0.06850875\n",
      "Iteration 155, loss = 0.06815726\n",
      "Iteration 156, loss = 0.06743772\n",
      "Iteration 157, loss = 0.06673717\n",
      "Iteration 158, loss = 0.06664894\n",
      "Iteration 159, loss = 0.06649298\n",
      "Iteration 160, loss = 0.06561047\n",
      "Iteration 161, loss = 0.06537444\n",
      "Iteration 162, loss = 0.06507789\n",
      "Iteration 163, loss = 0.06496083\n",
      "Iteration 164, loss = 0.06432612\n",
      "Iteration 165, loss = 0.06347927\n",
      "Iteration 166, loss = 0.06288555\n",
      "Iteration 167, loss = 0.06270228\n",
      "Iteration 168, loss = 0.06231581\n",
      "Iteration 169, loss = 0.06220763\n",
      "Iteration 170, loss = 0.06177053\n",
      "Iteration 171, loss = 0.06091090\n",
      "Iteration 172, loss = 0.06066320\n",
      "Iteration 173, loss = 0.06036358\n",
      "Iteration 174, loss = 0.05977995\n",
      "Iteration 175, loss = 0.05911195\n",
      "Iteration 176, loss = 0.05898173\n",
      "Iteration 177, loss = 0.05846736\n",
      "Iteration 178, loss = 0.05809417\n",
      "Iteration 179, loss = 0.05779518\n",
      "Iteration 180, loss = 0.05757382\n",
      "Iteration 181, loss = 0.05712037\n",
      "Iteration 182, loss = 0.05644151\n",
      "Iteration 183, loss = 0.05615593\n",
      "Iteration 184, loss = 0.05550753\n",
      "Iteration 185, loss = 0.05524647\n",
      "Iteration 186, loss = 0.05517695\n",
      "Iteration 187, loss = 0.05467596\n",
      "Iteration 188, loss = 0.05388790\n",
      "Iteration 189, loss = 0.05361392\n",
      "Iteration 190, loss = 0.05336243\n",
      "Iteration 191, loss = 0.05289162\n",
      "Iteration 192, loss = 0.05276789\n",
      "Iteration 193, loss = 0.05229708\n",
      "Iteration 194, loss = 0.05194087\n",
      "Iteration 195, loss = 0.05138595\n",
      "Iteration 196, loss = 0.05079033\n",
      "Iteration 197, loss = 0.05087704\n",
      "Iteration 198, loss = 0.05042229\n",
      "Iteration 199, loss = 0.04977118\n",
      "Iteration 200, loss = 0.04946531\n",
      "Iteration 1, loss = 0.69559463\n",
      "Iteration 2, loss = 0.47690786\n",
      "Iteration 3, loss = 0.38433310\n",
      "Iteration 4, loss = 0.34427746\n",
      "Iteration 5, loss = 0.31997621\n",
      "Iteration 6, loss = 0.30310257\n",
      "Iteration 7, loss = 0.29069251\n",
      "Iteration 8, loss = 0.28090675\n",
      "Iteration 9, loss = 0.27282989\n",
      "Iteration 10, loss = 0.26589601\n",
      "Iteration 11, loss = 0.25996564\n",
      "Iteration 12, loss = 0.25453699\n",
      "Iteration 13, loss = 0.24989488\n",
      "Iteration 14, loss = 0.24536885\n",
      "Iteration 15, loss = 0.24104377\n",
      "Iteration 16, loss = 0.23708124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.23344172\n",
      "Iteration 18, loss = 0.22968070\n",
      "Iteration 19, loss = 0.22610885\n",
      "Iteration 20, loss = 0.22264207\n",
      "Iteration 21, loss = 0.21929098\n",
      "Iteration 22, loss = 0.21653315\n",
      "Iteration 23, loss = 0.21318323\n",
      "Iteration 24, loss = 0.21008032\n",
      "Iteration 25, loss = 0.20704411\n",
      "Iteration 26, loss = 0.20439643\n",
      "Iteration 27, loss = 0.20152643\n",
      "Iteration 28, loss = 0.19907096\n",
      "Iteration 29, loss = 0.19626659\n",
      "Iteration 30, loss = 0.19375443\n",
      "Iteration 31, loss = 0.19125561\n",
      "Iteration 32, loss = 0.18914207\n",
      "Iteration 33, loss = 0.18685094\n",
      "Iteration 34, loss = 0.18436313\n",
      "Iteration 35, loss = 0.18224321\n",
      "Iteration 36, loss = 0.18014869\n",
      "Iteration 37, loss = 0.17800553\n",
      "Iteration 38, loss = 0.17575163\n",
      "Iteration 39, loss = 0.17370253\n",
      "Iteration 40, loss = 0.17200429\n",
      "Iteration 41, loss = 0.17008607\n",
      "Iteration 42, loss = 0.16809110\n",
      "Iteration 43, loss = 0.16618450\n",
      "Iteration 44, loss = 0.16441001\n",
      "Iteration 45, loss = 0.16256941\n",
      "Iteration 46, loss = 0.16104783\n",
      "Iteration 47, loss = 0.15907005\n",
      "Iteration 48, loss = 0.15761117\n",
      "Iteration 49, loss = 0.15582300\n",
      "Iteration 50, loss = 0.15438765\n",
      "Iteration 51, loss = 0.15300451\n",
      "Iteration 52, loss = 0.15133993\n",
      "Iteration 53, loss = 0.14965122\n",
      "Iteration 54, loss = 0.14848684\n",
      "Iteration 55, loss = 0.14658533\n",
      "Iteration 56, loss = 0.14509203\n",
      "Iteration 57, loss = 0.14412103\n",
      "Iteration 58, loss = 0.14264998\n",
      "Iteration 59, loss = 0.14133235\n",
      "Iteration 60, loss = 0.14000976\n",
      "Iteration 61, loss = 0.13847327\n",
      "Iteration 62, loss = 0.13722386\n",
      "Iteration 63, loss = 0.13611306\n",
      "Iteration 64, loss = 0.13476975\n",
      "Iteration 65, loss = 0.13368984\n",
      "Iteration 66, loss = 0.13263470\n",
      "Iteration 67, loss = 0.13147108\n",
      "Iteration 68, loss = 0.13054068\n",
      "Iteration 69, loss = 0.12923422\n",
      "Iteration 70, loss = 0.12830680\n",
      "Iteration 71, loss = 0.12716432\n",
      "Iteration 72, loss = 0.12589129\n",
      "Iteration 73, loss = 0.12508396\n",
      "Iteration 74, loss = 0.12391931\n",
      "Iteration 75, loss = 0.12304566\n",
      "Iteration 76, loss = 0.12203488\n",
      "Iteration 77, loss = 0.12079658\n",
      "Iteration 78, loss = 0.11996846\n",
      "Iteration 79, loss = 0.11890172\n",
      "Iteration 80, loss = 0.11833106\n",
      "Iteration 81, loss = 0.11701241\n",
      "Iteration 82, loss = 0.11626155\n",
      "Iteration 83, loss = 0.11531318\n",
      "Iteration 84, loss = 0.11435445\n",
      "Iteration 85, loss = 0.11352722\n",
      "Iteration 86, loss = 0.11266280\n",
      "Iteration 87, loss = 0.11163351\n",
      "Iteration 88, loss = 0.11131316\n",
      "Iteration 89, loss = 0.11050203\n",
      "Iteration 90, loss = 0.10988914\n",
      "Iteration 91, loss = 0.10859071\n",
      "Iteration 92, loss = 0.10771061\n",
      "Iteration 93, loss = 0.10707218\n",
      "Iteration 94, loss = 0.10611690\n",
      "Iteration 95, loss = 0.10532915\n",
      "Iteration 96, loss = 0.10467140\n",
      "Iteration 97, loss = 0.10370683\n",
      "Iteration 98, loss = 0.10305764\n",
      "Iteration 99, loss = 0.10221670\n",
      "Iteration 100, loss = 0.10174355\n",
      "Iteration 101, loss = 0.10143403\n",
      "Iteration 102, loss = 0.10007014\n",
      "Iteration 103, loss = 0.09936351\n",
      "Iteration 104, loss = 0.09865535\n",
      "Iteration 105, loss = 0.09783838\n",
      "Iteration 106, loss = 0.09751109\n",
      "Iteration 107, loss = 0.09672832\n",
      "Iteration 108, loss = 0.09578366\n",
      "Iteration 109, loss = 0.09526575\n",
      "Iteration 110, loss = 0.09439126\n",
      "Iteration 111, loss = 0.09363914\n",
      "Iteration 112, loss = 0.09339361\n",
      "Iteration 113, loss = 0.09225484\n",
      "Iteration 114, loss = 0.09184929\n",
      "Iteration 115, loss = 0.09083999\n",
      "Iteration 116, loss = 0.08997979\n",
      "Iteration 117, loss = 0.08945337\n",
      "Iteration 118, loss = 0.08879092\n",
      "Iteration 119, loss = 0.08836610\n",
      "Iteration 120, loss = 0.08819414\n",
      "Iteration 121, loss = 0.08701952\n",
      "Iteration 122, loss = 0.08632621\n",
      "Iteration 123, loss = 0.08545748\n",
      "Iteration 124, loss = 0.08508064\n",
      "Iteration 125, loss = 0.08554997\n",
      "Iteration 126, loss = 0.08367293\n",
      "Iteration 127, loss = 0.08300176\n",
      "Iteration 128, loss = 0.08268966\n",
      "Iteration 129, loss = 0.08196084\n",
      "Iteration 130, loss = 0.08090161\n",
      "Iteration 131, loss = 0.08065153\n",
      "Iteration 132, loss = 0.07992085\n",
      "Iteration 133, loss = 0.07950081\n",
      "Iteration 134, loss = 0.07903664\n",
      "Iteration 135, loss = 0.07808130\n",
      "Iteration 136, loss = 0.07756282\n",
      "Iteration 137, loss = 0.07687365\n",
      "Iteration 138, loss = 0.07709459\n",
      "Iteration 139, loss = 0.07598724\n",
      "Iteration 140, loss = 0.07534977\n",
      "Iteration 141, loss = 0.07488068\n",
      "Iteration 142, loss = 0.07436069\n",
      "Iteration 143, loss = 0.07395097\n",
      "Iteration 144, loss = 0.07327942\n",
      "Iteration 145, loss = 0.07297570\n",
      "Iteration 146, loss = 0.07214001\n",
      "Iteration 147, loss = 0.07141325\n",
      "Iteration 148, loss = 0.07101994\n",
      "Iteration 149, loss = 0.07079925\n",
      "Iteration 150, loss = 0.06995387\n",
      "Iteration 151, loss = 0.06923247\n",
      "Iteration 152, loss = 0.06920443\n",
      "Iteration 153, loss = 0.06828937\n",
      "Iteration 154, loss = 0.06760111\n",
      "Iteration 155, loss = 0.06752355\n",
      "Iteration 156, loss = 0.06677777\n",
      "Iteration 157, loss = 0.06589615\n",
      "Iteration 158, loss = 0.06616082\n",
      "Iteration 159, loss = 0.06545493\n",
      "Iteration 160, loss = 0.06442165\n",
      "Iteration 161, loss = 0.06411531\n",
      "Iteration 162, loss = 0.06396563\n",
      "Iteration 163, loss = 0.06349314\n",
      "Iteration 164, loss = 0.06282998\n",
      "Iteration 165, loss = 0.06211976\n",
      "Iteration 166, loss = 0.06142734\n",
      "Iteration 167, loss = 0.06093183\n",
      "Iteration 168, loss = 0.06066536\n",
      "Iteration 169, loss = 0.06028472\n",
      "Iteration 170, loss = 0.05975258\n",
      "Iteration 171, loss = 0.05906866\n",
      "Iteration 172, loss = 0.05890439\n",
      "Iteration 173, loss = 0.05847157\n",
      "Iteration 174, loss = 0.05786752\n",
      "Iteration 175, loss = 0.05754220\n",
      "Iteration 176, loss = 0.05706028\n",
      "Iteration 177, loss = 0.05684008\n",
      "Iteration 178, loss = 0.05652237\n",
      "Iteration 179, loss = 0.05572313\n",
      "Iteration 180, loss = 0.05542822\n",
      "Iteration 181, loss = 0.05498987\n",
      "Iteration 182, loss = 0.05437191\n",
      "Iteration 183, loss = 0.05443728\n",
      "Iteration 184, loss = 0.05364970\n",
      "Iteration 185, loss = 0.05322966\n",
      "Iteration 186, loss = 0.05279796\n",
      "Iteration 187, loss = 0.05233739\n",
      "Iteration 188, loss = 0.05201681\n",
      "Iteration 189, loss = 0.05142665\n",
      "Iteration 190, loss = 0.05108658\n",
      "Iteration 191, loss = 0.05068223\n",
      "Iteration 192, loss = 0.05047910\n",
      "Iteration 193, loss = 0.04987016\n",
      "Iteration 194, loss = 0.04976648\n",
      "Iteration 195, loss = 0.04925088\n",
      "Iteration 196, loss = 0.04861935\n",
      "Iteration 197, loss = 0.04865465\n",
      "Iteration 198, loss = 0.04788670\n",
      "Iteration 199, loss = 0.04754200\n",
      "Iteration 200, loss = 0.04740941\n",
      "Iteration 1, loss = 0.70085136\n",
      "Iteration 2, loss = 0.48071911\n",
      "Iteration 3, loss = 0.38883208\n",
      "Iteration 4, loss = 0.34749030\n",
      "Iteration 5, loss = 0.32311033\n",
      "Iteration 6, loss = 0.30608499\n",
      "Iteration 7, loss = 0.29269858\n",
      "Iteration 8, loss = 0.28228515\n",
      "Iteration 9, loss = 0.27362027\n",
      "Iteration 10, loss = 0.26586718\n",
      "Iteration 11, loss = 0.25946422\n",
      "Iteration 12, loss = 0.25318441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.24781676\n",
      "Iteration 14, loss = 0.24276767\n",
      "Iteration 15, loss = 0.23771184\n",
      "Iteration 16, loss = 0.23330701\n",
      "Iteration 17, loss = 0.22895920\n",
      "Iteration 18, loss = 0.22471058\n",
      "Iteration 19, loss = 0.22089782\n",
      "Iteration 20, loss = 0.21698731\n",
      "Iteration 21, loss = 0.21340760\n",
      "Iteration 22, loss = 0.20974851\n",
      "Iteration 23, loss = 0.20651449\n",
      "Iteration 24, loss = 0.20289428\n",
      "Iteration 25, loss = 0.19987644\n",
      "Iteration 26, loss = 0.19652596\n",
      "Iteration 27, loss = 0.19378197\n",
      "Iteration 28, loss = 0.19065298\n",
      "Iteration 29, loss = 0.18786330\n",
      "Iteration 30, loss = 0.18528395\n",
      "Iteration 31, loss = 0.18238575\n",
      "Iteration 32, loss = 0.17980919\n",
      "Iteration 33, loss = 0.17728381\n",
      "Iteration 34, loss = 0.17500628\n",
      "Iteration 35, loss = 0.17252046\n",
      "Iteration 36, loss = 0.17009795\n",
      "Iteration 37, loss = 0.16802829\n",
      "Iteration 38, loss = 0.16598183\n",
      "Iteration 39, loss = 0.16381322\n",
      "Iteration 40, loss = 0.16203119\n",
      "Iteration 41, loss = 0.15965320\n",
      "Iteration 42, loss = 0.15785817\n",
      "Iteration 43, loss = 0.15622683\n",
      "Iteration 44, loss = 0.15433083\n",
      "Iteration 45, loss = 0.15211378\n",
      "Iteration 46, loss = 0.15066042\n",
      "Iteration 47, loss = 0.14871062\n",
      "Iteration 48, loss = 0.14712075\n",
      "Iteration 49, loss = 0.14561852\n",
      "Iteration 50, loss = 0.14434723\n",
      "Iteration 51, loss = 0.14242064\n",
      "Iteration 52, loss = 0.14107685\n",
      "Iteration 53, loss = 0.13952217\n",
      "Iteration 54, loss = 0.13822369\n",
      "Iteration 55, loss = 0.13652574\n",
      "Iteration 56, loss = 0.13544374\n",
      "Iteration 57, loss = 0.13417030\n",
      "Iteration 58, loss = 0.13285289\n",
      "Iteration 59, loss = 0.13153077\n",
      "Iteration 60, loss = 0.13039142\n",
      "Iteration 61, loss = 0.12948907\n",
      "Iteration 62, loss = 0.12828921\n",
      "Iteration 63, loss = 0.12733245\n",
      "Iteration 64, loss = 0.12617748\n",
      "Iteration 65, loss = 0.12541118\n",
      "Iteration 66, loss = 0.12347611\n",
      "Iteration 67, loss = 0.12274000\n",
      "Iteration 68, loss = 0.12147472\n",
      "Iteration 69, loss = 0.12053597\n",
      "Iteration 70, loss = 0.11957475\n",
      "Iteration 71, loss = 0.11874452\n",
      "Iteration 72, loss = 0.11765468\n",
      "Iteration 73, loss = 0.11668643\n",
      "Iteration 74, loss = 0.11541702\n",
      "Iteration 75, loss = 0.11499826\n",
      "Iteration 76, loss = 0.11392440\n",
      "Iteration 77, loss = 0.11288302\n",
      "Iteration 78, loss = 0.11187579\n",
      "Iteration 79, loss = 0.11118767\n",
      "Iteration 80, loss = 0.11012606\n",
      "Iteration 81, loss = 0.10939119\n",
      "Iteration 82, loss = 0.10951616\n",
      "Iteration 83, loss = 0.10826809\n",
      "Iteration 84, loss = 0.10720675\n",
      "Iteration 85, loss = 0.10606622\n",
      "Iteration 86, loss = 0.10578480\n",
      "Iteration 87, loss = 0.10440416\n",
      "Iteration 88, loss = 0.10356711\n",
      "Iteration 89, loss = 0.10284871\n",
      "Iteration 90, loss = 0.10265357\n",
      "Iteration 91, loss = 0.10091750\n",
      "Iteration 92, loss = 0.10084887\n",
      "Iteration 93, loss = 0.09955285\n",
      "Iteration 94, loss = 0.09900894\n",
      "Iteration 95, loss = 0.09814596\n",
      "Iteration 96, loss = 0.09824789\n",
      "Iteration 97, loss = 0.09703329\n",
      "Iteration 98, loss = 0.09606056\n",
      "Iteration 99, loss = 0.09567371\n",
      "Iteration 100, loss = 0.09515458\n",
      "Iteration 101, loss = 0.09446616\n",
      "Iteration 102, loss = 0.09390416\n",
      "Iteration 103, loss = 0.09312045\n",
      "Iteration 104, loss = 0.09225517\n",
      "Iteration 105, loss = 0.09189692\n",
      "Iteration 106, loss = 0.09079983\n",
      "Iteration 107, loss = 0.09047376\n",
      "Iteration 108, loss = 0.08964059\n",
      "Iteration 109, loss = 0.08918779\n",
      "Iteration 110, loss = 0.08860658\n",
      "Iteration 111, loss = 0.08812276\n",
      "Iteration 112, loss = 0.08728116\n",
      "Iteration 113, loss = 0.08680847\n",
      "Iteration 114, loss = 0.08657648\n",
      "Iteration 115, loss = 0.08556402\n",
      "Iteration 116, loss = 0.08490854\n",
      "Iteration 117, loss = 0.08405409\n",
      "Iteration 118, loss = 0.08347479\n",
      "Iteration 119, loss = 0.08321746\n",
      "Iteration 120, loss = 0.08287181\n",
      "Iteration 121, loss = 0.08211731\n",
      "Iteration 122, loss = 0.08150936\n",
      "Iteration 123, loss = 0.08066834\n",
      "Iteration 124, loss = 0.08060958\n",
      "Iteration 125, loss = 0.07998754\n",
      "Iteration 126, loss = 0.07903527\n",
      "Iteration 127, loss = 0.07862765\n",
      "Iteration 128, loss = 0.07809167\n",
      "Iteration 129, loss = 0.07751120\n",
      "Iteration 130, loss = 0.07842561\n",
      "Iteration 131, loss = 0.07675474\n",
      "Iteration 132, loss = 0.07627589\n",
      "Iteration 133, loss = 0.07527376\n",
      "Iteration 134, loss = 0.07543992\n",
      "Iteration 135, loss = 0.07441124\n",
      "Iteration 136, loss = 0.07364659\n",
      "Iteration 137, loss = 0.07355263\n",
      "Iteration 138, loss = 0.07302793\n",
      "Iteration 139, loss = 0.07193098\n",
      "Iteration 140, loss = 0.07204441\n",
      "Iteration 141, loss = 0.07139072\n",
      "Iteration 142, loss = 0.07079477\n",
      "Iteration 143, loss = 0.07041496\n",
      "Iteration 144, loss = 0.06990561\n",
      "Iteration 145, loss = 0.06953354\n",
      "Iteration 146, loss = 0.06889663\n",
      "Iteration 147, loss = 0.06861531\n",
      "Iteration 148, loss = 0.06782800\n",
      "Iteration 149, loss = 0.06739387\n",
      "Iteration 150, loss = 0.06712233\n",
      "Iteration 151, loss = 0.06637536\n",
      "Iteration 152, loss = 0.06617210\n",
      "Iteration 153, loss = 0.06547696\n",
      "Iteration 154, loss = 0.06494480\n",
      "Iteration 155, loss = 0.06435954\n",
      "Iteration 156, loss = 0.06405944\n",
      "Iteration 157, loss = 0.06383629\n",
      "Iteration 158, loss = 0.06307615\n",
      "Iteration 159, loss = 0.06304712\n",
      "Iteration 160, loss = 0.06227948\n",
      "Iteration 161, loss = 0.06207780\n",
      "Iteration 162, loss = 0.06153642\n",
      "Iteration 163, loss = 0.06099568\n",
      "Iteration 164, loss = 0.06029534\n",
      "Iteration 165, loss = 0.06017717\n",
      "Iteration 166, loss = 0.05952115\n",
      "Iteration 167, loss = 0.05923023\n",
      "Iteration 168, loss = 0.05897294\n",
      "Iteration 169, loss = 0.05798801\n",
      "Iteration 170, loss = 0.05787002\n",
      "Iteration 171, loss = 0.05729760\n",
      "Iteration 172, loss = 0.05714085\n",
      "Iteration 173, loss = 0.05638546\n",
      "Iteration 174, loss = 0.05605275\n",
      "Iteration 175, loss = 0.05595080\n",
      "Iteration 176, loss = 0.05539067\n",
      "Iteration 177, loss = 0.05498843\n",
      "Iteration 178, loss = 0.05472411\n",
      "Iteration 179, loss = 0.05420156\n",
      "Iteration 180, loss = 0.05363115\n",
      "Iteration 181, loss = 0.05408844\n",
      "Iteration 182, loss = 0.05306655\n",
      "Iteration 183, loss = 0.05287983\n",
      "Iteration 184, loss = 0.05225467\n",
      "Iteration 185, loss = 0.05144876\n",
      "Iteration 186, loss = 0.05136355\n",
      "Iteration 187, loss = 0.05077813\n",
      "Iteration 188, loss = 0.05030896\n",
      "Iteration 189, loss = 0.05007136\n",
      "Iteration 190, loss = 0.05032866\n",
      "Iteration 191, loss = 0.04941566\n",
      "Iteration 192, loss = 0.04883506\n",
      "Iteration 193, loss = 0.04864088\n",
      "Iteration 194, loss = 0.04860116\n",
      "Iteration 195, loss = 0.04820451\n",
      "Iteration 196, loss = 0.04770809\n",
      "Iteration 197, loss = 0.04704418\n",
      "Iteration 198, loss = 0.04664964\n",
      "Iteration 199, loss = 0.04608704\n",
      "Iteration 200, loss = 0.04636430\n",
      "Iteration 1, loss = 0.69846700\n",
      "Iteration 2, loss = 0.47787953\n",
      "Iteration 3, loss = 0.38612170\n",
      "Iteration 4, loss = 0.34514749\n",
      "Iteration 5, loss = 0.32148170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.30493880\n",
      "Iteration 7, loss = 0.29243284\n",
      "Iteration 8, loss = 0.28251235\n",
      "Iteration 9, loss = 0.27442259\n",
      "Iteration 10, loss = 0.26749588\n",
      "Iteration 11, loss = 0.26147774\n",
      "Iteration 12, loss = 0.25572134\n",
      "Iteration 13, loss = 0.25089514\n",
      "Iteration 14, loss = 0.24597346\n",
      "Iteration 15, loss = 0.24143691\n",
      "Iteration 16, loss = 0.23730587\n",
      "Iteration 17, loss = 0.23328644\n",
      "Iteration 18, loss = 0.22942978\n",
      "Iteration 19, loss = 0.22563918\n",
      "Iteration 20, loss = 0.22216836\n",
      "Iteration 21, loss = 0.21877883\n",
      "Iteration 22, loss = 0.21555414\n",
      "Iteration 23, loss = 0.21232675\n",
      "Iteration 24, loss = 0.20885089\n",
      "Iteration 25, loss = 0.20574154\n",
      "Iteration 26, loss = 0.20280808\n",
      "Iteration 27, loss = 0.19999936\n",
      "Iteration 28, loss = 0.19680016\n",
      "Iteration 29, loss = 0.19455797\n",
      "Iteration 30, loss = 0.19195540\n",
      "Iteration 31, loss = 0.18893745\n",
      "Iteration 32, loss = 0.18657822\n",
      "Iteration 33, loss = 0.18403770\n",
      "Iteration 34, loss = 0.18198069\n",
      "Iteration 35, loss = 0.17975209\n",
      "Iteration 36, loss = 0.17715991\n",
      "Iteration 37, loss = 0.17525464\n",
      "Iteration 38, loss = 0.17306269\n",
      "Iteration 39, loss = 0.17092250\n",
      "Iteration 40, loss = 0.16890031\n",
      "Iteration 41, loss = 0.16661873\n",
      "Iteration 42, loss = 0.16486288\n",
      "Iteration 43, loss = 0.16321557\n",
      "Iteration 44, loss = 0.16138112\n",
      "Iteration 45, loss = 0.15953057\n",
      "Iteration 46, loss = 0.15776835\n",
      "Iteration 47, loss = 0.15608674\n",
      "Iteration 48, loss = 0.15436558\n",
      "Iteration 49, loss = 0.15298149\n",
      "Iteration 50, loss = 0.15152954\n",
      "Iteration 51, loss = 0.14998592\n",
      "Iteration 52, loss = 0.14848481\n",
      "Iteration 53, loss = 0.14704792\n",
      "Iteration 54, loss = 0.14548795\n",
      "Iteration 55, loss = 0.14398436\n",
      "Iteration 56, loss = 0.14265029\n",
      "Iteration 57, loss = 0.14168628\n",
      "Iteration 58, loss = 0.14048993\n",
      "Iteration 59, loss = 0.13901077\n",
      "Iteration 60, loss = 0.13783417\n",
      "Iteration 61, loss = 0.13691952\n",
      "Iteration 62, loss = 0.13563859\n",
      "Iteration 63, loss = 0.13428941\n",
      "Iteration 64, loss = 0.13320370\n",
      "Iteration 65, loss = 0.13276823\n",
      "Iteration 66, loss = 0.13108628\n",
      "Iteration 67, loss = 0.13022115\n",
      "Iteration 68, loss = 0.12894043\n",
      "Iteration 69, loss = 0.12812853\n",
      "Iteration 70, loss = 0.12708000\n",
      "Iteration 71, loss = 0.12628761\n",
      "Iteration 72, loss = 0.12483700\n",
      "Iteration 73, loss = 0.12427032\n",
      "Iteration 74, loss = 0.12321030\n",
      "Iteration 75, loss = 0.12246929\n",
      "Iteration 76, loss = 0.12133355\n",
      "Iteration 77, loss = 0.12058633\n",
      "Iteration 78, loss = 0.11985738\n",
      "Iteration 79, loss = 0.11852941\n",
      "Iteration 80, loss = 0.11825558\n",
      "Iteration 81, loss = 0.11727822\n",
      "Iteration 82, loss = 0.11662348\n",
      "Iteration 83, loss = 0.11567595\n",
      "Iteration 84, loss = 0.11444804\n",
      "Iteration 85, loss = 0.11343155\n",
      "Iteration 86, loss = 0.11317493\n",
      "Iteration 87, loss = 0.11195839\n",
      "Iteration 88, loss = 0.11122380\n",
      "Iteration 89, loss = 0.11029590\n",
      "Iteration 90, loss = 0.10983043\n",
      "Iteration 91, loss = 0.10885739\n",
      "Iteration 92, loss = 0.10804383\n",
      "Iteration 93, loss = 0.10712165\n",
      "Iteration 94, loss = 0.10665692\n",
      "Iteration 95, loss = 0.10620734\n",
      "Iteration 96, loss = 0.10605869\n",
      "Iteration 97, loss = 0.10425085\n",
      "Iteration 98, loss = 0.10391637\n",
      "Iteration 99, loss = 0.10323478\n",
      "Iteration 100, loss = 0.10294893\n",
      "Iteration 101, loss = 0.10216603\n",
      "Iteration 102, loss = 0.10165918\n",
      "Iteration 103, loss = 0.10047882\n",
      "Iteration 104, loss = 0.09973564\n",
      "Iteration 105, loss = 0.09929953\n",
      "Iteration 106, loss = 0.09836958\n",
      "Iteration 107, loss = 0.09775348\n",
      "Iteration 108, loss = 0.09728768\n",
      "Iteration 109, loss = 0.09666014\n",
      "Iteration 110, loss = 0.09600634\n",
      "Iteration 111, loss = 0.09496011\n",
      "Iteration 112, loss = 0.09465964\n",
      "Iteration 113, loss = 0.09387905\n",
      "Iteration 114, loss = 0.09373653\n",
      "Iteration 115, loss = 0.09271038\n",
      "Iteration 116, loss = 0.09215550\n",
      "Iteration 117, loss = 0.09142932\n",
      "Iteration 118, loss = 0.09058043\n",
      "Iteration 119, loss = 0.09025555\n",
      "Iteration 120, loss = 0.08982593\n",
      "Iteration 121, loss = 0.08929966\n",
      "Iteration 122, loss = 0.08879049\n",
      "Iteration 123, loss = 0.08821174\n",
      "Iteration 124, loss = 0.08732063\n",
      "Iteration 125, loss = 0.08678022\n",
      "Iteration 126, loss = 0.08657272\n",
      "Iteration 127, loss = 0.08595937\n",
      "Iteration 128, loss = 0.08527365\n",
      "Iteration 129, loss = 0.08474480\n",
      "Iteration 130, loss = 0.08450602\n",
      "Iteration 131, loss = 0.08359905\n",
      "Iteration 132, loss = 0.08299148\n",
      "Iteration 133, loss = 0.08226126\n",
      "Iteration 134, loss = 0.08193759\n",
      "Iteration 135, loss = 0.08109141\n",
      "Iteration 136, loss = 0.08057057\n",
      "Iteration 137, loss = 0.08026247\n",
      "Iteration 138, loss = 0.07974791\n",
      "Iteration 139, loss = 0.07888025\n",
      "Iteration 140, loss = 0.07871179\n",
      "Iteration 141, loss = 0.07806500\n",
      "Iteration 142, loss = 0.07763736\n",
      "Iteration 143, loss = 0.07709613\n",
      "Iteration 144, loss = 0.07653973\n",
      "Iteration 145, loss = 0.07611416\n",
      "Iteration 146, loss = 0.07556843\n",
      "Iteration 147, loss = 0.07537495\n",
      "Iteration 148, loss = 0.07464453\n",
      "Iteration 149, loss = 0.07409793\n",
      "Iteration 150, loss = 0.07403005\n",
      "Iteration 151, loss = 0.07322479\n",
      "Iteration 152, loss = 0.07293383\n",
      "Iteration 153, loss = 0.07182009\n",
      "Iteration 154, loss = 0.07156791\n",
      "Iteration 155, loss = 0.07100500\n",
      "Iteration 156, loss = 0.07041522\n",
      "Iteration 157, loss = 0.07026143\n",
      "Iteration 158, loss = 0.06990398\n",
      "Iteration 159, loss = 0.06926722\n",
      "Iteration 160, loss = 0.06873725\n",
      "Iteration 161, loss = 0.06823467\n",
      "Iteration 162, loss = 0.06772781\n",
      "Iteration 163, loss = 0.06698618\n",
      "Iteration 164, loss = 0.06663618\n",
      "Iteration 165, loss = 0.06639775\n",
      "Iteration 166, loss = 0.06552491\n",
      "Iteration 167, loss = 0.06534213\n",
      "Iteration 168, loss = 0.06593972\n",
      "Iteration 169, loss = 0.06414584\n",
      "Iteration 170, loss = 0.06398283\n",
      "Iteration 171, loss = 0.06346421\n",
      "Iteration 172, loss = 0.06338808\n",
      "Iteration 173, loss = 0.06261036\n",
      "Iteration 174, loss = 0.06187584\n",
      "Iteration 175, loss = 0.06160117\n",
      "Iteration 176, loss = 0.06157301\n",
      "Iteration 177, loss = 0.06100556\n",
      "Iteration 178, loss = 0.06022622\n",
      "Iteration 179, loss = 0.06015298\n",
      "Iteration 180, loss = 0.05955130\n",
      "Iteration 181, loss = 0.05933257\n",
      "Iteration 182, loss = 0.05865306\n",
      "Iteration 183, loss = 0.05855036\n",
      "Iteration 184, loss = 0.05754297\n",
      "Iteration 185, loss = 0.05731307\n",
      "Iteration 186, loss = 0.05684439\n",
      "Iteration 187, loss = 0.05656784\n",
      "Iteration 188, loss = 0.05590794\n",
      "Iteration 189, loss = 0.05550657\n",
      "Iteration 190, loss = 0.05582921\n",
      "Iteration 191, loss = 0.05522903\n",
      "Iteration 192, loss = 0.05467024\n",
      "Iteration 193, loss = 0.05370722\n",
      "Iteration 194, loss = 0.05400399\n",
      "Iteration 195, loss = 0.05341604\n",
      "Iteration 196, loss = 0.05290122\n",
      "Iteration 197, loss = 0.05249112\n",
      "Iteration 198, loss = 0.05179507\n",
      "Iteration 199, loss = 0.05149110\n",
      "Iteration 200, loss = 0.05142554\n",
      "Iteration 1, loss = 0.66072642\n",
      "Iteration 2, loss = 0.45384049\n",
      "Iteration 3, loss = 0.39843211\n",
      "Iteration 4, loss = 0.37801514\n",
      "Iteration 5, loss = 0.36294880\n",
      "Iteration 6, loss = 0.35006174\n",
      "Iteration 7, loss = 0.33919536\n",
      "Iteration 8, loss = 0.32965520\n",
      "Iteration 9, loss = 0.32148843\n",
      "Iteration 10, loss = 0.31439209\n",
      "Iteration 11, loss = 0.30843790\n",
      "Iteration 12, loss = 0.30305412\n",
      "Iteration 13, loss = 0.29860204\n",
      "Iteration 14, loss = 0.29448462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.29093622\n",
      "Iteration 16, loss = 0.28806679\n",
      "Iteration 17, loss = 0.28545666\n",
      "Iteration 18, loss = 0.28271827\n",
      "Iteration 19, loss = 0.28041418\n",
      "Iteration 20, loss = 0.27827745\n",
      "Iteration 21, loss = 0.27639770\n",
      "Iteration 22, loss = 0.27451141\n",
      "Iteration 23, loss = 0.27273515\n",
      "Iteration 24, loss = 0.27113487\n",
      "Iteration 25, loss = 0.26974798\n",
      "Iteration 26, loss = 0.26817513\n",
      "Iteration 27, loss = 0.26657993\n",
      "Iteration 28, loss = 0.26541723\n",
      "Iteration 29, loss = 0.26398723\n",
      "Iteration 30, loss = 0.26248079\n",
      "Iteration 31, loss = 0.26120225\n",
      "Iteration 32, loss = 0.25987556\n",
      "Iteration 33, loss = 0.25878071\n",
      "Iteration 34, loss = 0.25781494\n",
      "Iteration 35, loss = 0.25647326\n",
      "Iteration 36, loss = 0.25541839\n",
      "Iteration 37, loss = 0.25421494\n",
      "Iteration 38, loss = 0.25323000\n",
      "Iteration 39, loss = 0.25223061\n",
      "Iteration 40, loss = 0.25120790\n",
      "Iteration 41, loss = 0.25028610\n",
      "Iteration 42, loss = 0.24943046\n",
      "Iteration 43, loss = 0.24850509\n",
      "Iteration 44, loss = 0.24776378\n",
      "Iteration 45, loss = 0.24709496\n",
      "Iteration 46, loss = 0.24615728\n",
      "Iteration 47, loss = 0.24551104\n",
      "Iteration 48, loss = 0.24468300\n",
      "Iteration 49, loss = 0.24396804\n",
      "Iteration 50, loss = 0.24333145\n",
      "Iteration 51, loss = 0.24274580\n",
      "Iteration 52, loss = 0.24226554\n",
      "Iteration 53, loss = 0.24182178\n",
      "Iteration 54, loss = 0.24138908\n",
      "Iteration 55, loss = 0.24077426\n",
      "Iteration 56, loss = 0.24044929\n",
      "Iteration 57, loss = 0.24003552\n",
      "Iteration 58, loss = 0.23984155\n",
      "Iteration 59, loss = 0.23916482\n",
      "Iteration 60, loss = 0.23894467\n",
      "Iteration 61, loss = 0.23894427\n",
      "Iteration 62, loss = 0.23817770\n",
      "Iteration 63, loss = 0.23795912\n",
      "Iteration 64, loss = 0.23780771\n",
      "Iteration 65, loss = 0.23735231\n",
      "Iteration 66, loss = 0.23778949\n",
      "Iteration 67, loss = 0.23699766\n",
      "Iteration 68, loss = 0.23665770\n",
      "Iteration 69, loss = 0.23666143\n",
      "Iteration 70, loss = 0.23610424\n",
      "Iteration 71, loss = 0.23578679\n",
      "Iteration 72, loss = 0.23527006\n",
      "Iteration 73, loss = 0.23510583\n",
      "Iteration 74, loss = 0.23492705\n",
      "Iteration 75, loss = 0.23481778\n",
      "Iteration 76, loss = 0.23451119\n",
      "Iteration 77, loss = 0.23395345\n",
      "Iteration 78, loss = 0.23365109\n",
      "Iteration 79, loss = 0.23344682\n",
      "Iteration 80, loss = 0.23336213\n",
      "Iteration 81, loss = 0.23301856\n",
      "Iteration 82, loss = 0.23261326\n",
      "Iteration 83, loss = 0.23230559\n",
      "Iteration 84, loss = 0.23207949\n",
      "Iteration 85, loss = 0.23155231\n",
      "Iteration 86, loss = 0.23131357\n",
      "Iteration 87, loss = 0.23075880\n",
      "Iteration 88, loss = 0.23044202\n",
      "Iteration 89, loss = 0.22996945\n",
      "Iteration 90, loss = 0.22961754\n",
      "Iteration 91, loss = 0.22949414\n",
      "Iteration 92, loss = 0.22893668\n",
      "Iteration 93, loss = 0.22883581\n",
      "Iteration 94, loss = 0.22813763\n",
      "Iteration 95, loss = 0.22765899\n",
      "Iteration 96, loss = 0.22765935\n",
      "Iteration 97, loss = 0.22686869\n",
      "Iteration 98, loss = 0.22682154\n",
      "Iteration 99, loss = 0.22573052\n",
      "Iteration 100, loss = 0.22565545\n",
      "Iteration 101, loss = 0.22495385\n",
      "Iteration 102, loss = 0.22456386\n",
      "Iteration 103, loss = 0.22411140\n",
      "Iteration 104, loss = 0.22355226\n",
      "Iteration 105, loss = 0.22303337\n",
      "Iteration 106, loss = 0.22241995\n",
      "Iteration 107, loss = 0.22196270\n",
      "Iteration 108, loss = 0.22136822\n",
      "Iteration 109, loss = 0.22085104\n",
      "Iteration 110, loss = 0.22035271\n",
      "Iteration 111, loss = 0.21974899\n",
      "Iteration 112, loss = 0.21933545\n",
      "Iteration 113, loss = 0.21863536\n",
      "Iteration 114, loss = 0.21826063\n",
      "Iteration 115, loss = 0.21758509\n",
      "Iteration 116, loss = 0.21689649\n",
      "Iteration 117, loss = 0.21628202\n",
      "Iteration 118, loss = 0.21570369\n",
      "Iteration 119, loss = 0.21529097\n",
      "Iteration 120, loss = 0.21460569\n",
      "Iteration 121, loss = 0.21407166\n",
      "Iteration 122, loss = 0.21330243\n",
      "Iteration 123, loss = 0.21330947\n",
      "Iteration 124, loss = 0.21268968\n",
      "Iteration 125, loss = 0.21150732\n",
      "Iteration 126, loss = 0.21081294\n",
      "Iteration 127, loss = 0.21032178\n",
      "Iteration 128, loss = 0.20978819\n",
      "Iteration 129, loss = 0.20868734\n",
      "Iteration 130, loss = 0.20829022\n",
      "Iteration 131, loss = 0.20778386\n",
      "Iteration 132, loss = 0.20726559\n",
      "Iteration 133, loss = 0.20623918\n",
      "Iteration 134, loss = 0.20571466\n",
      "Iteration 135, loss = 0.20495696\n",
      "Iteration 136, loss = 0.20434613\n",
      "Iteration 137, loss = 0.20368628\n",
      "Iteration 138, loss = 0.20286401\n",
      "Iteration 139, loss = 0.20232066\n",
      "Iteration 140, loss = 0.20166990\n",
      "Iteration 141, loss = 0.20102763\n",
      "Iteration 142, loss = 0.20047165\n",
      "Iteration 143, loss = 0.19960557\n",
      "Iteration 144, loss = 0.19927584\n",
      "Iteration 145, loss = 0.19845465\n",
      "Iteration 146, loss = 0.19767713\n",
      "Iteration 147, loss = 0.19729297\n",
      "Iteration 148, loss = 0.19636406\n",
      "Iteration 149, loss = 0.19600829\n",
      "Iteration 150, loss = 0.19502615\n",
      "Iteration 151, loss = 0.19449422\n",
      "Iteration 152, loss = 0.19372968\n",
      "Iteration 153, loss = 0.19307991\n",
      "Iteration 154, loss = 0.19224672\n",
      "Iteration 155, loss = 0.19160435\n",
      "Iteration 156, loss = 0.19184915\n",
      "Iteration 157, loss = 0.19051047\n",
      "Iteration 158, loss = 0.18992510\n",
      "Iteration 159, loss = 0.18902550\n",
      "Iteration 160, loss = 0.18847439\n",
      "Iteration 161, loss = 0.18759906\n",
      "Iteration 162, loss = 0.18712330\n",
      "Iteration 163, loss = 0.18647517\n",
      "Iteration 164, loss = 0.18563436\n",
      "Iteration 165, loss = 0.18526968\n",
      "Iteration 166, loss = 0.18429818\n",
      "Iteration 167, loss = 0.18401041\n",
      "Iteration 168, loss = 0.18312550\n",
      "Iteration 169, loss = 0.18254829\n",
      "Iteration 170, loss = 0.18211995\n",
      "Iteration 171, loss = 0.18145239\n",
      "Iteration 172, loss = 0.18083236\n",
      "Iteration 173, loss = 0.18032091\n",
      "Iteration 174, loss = 0.17948524\n",
      "Iteration 175, loss = 0.17892295\n",
      "Iteration 176, loss = 0.17834124\n",
      "Iteration 177, loss = 0.17790517\n",
      "Iteration 178, loss = 0.17709739\n",
      "Iteration 179, loss = 0.17671621\n",
      "Iteration 180, loss = 0.17573012\n",
      "Iteration 181, loss = 0.17552080\n",
      "Iteration 182, loss = 0.17502754\n",
      "Iteration 183, loss = 0.17427404\n",
      "Iteration 184, loss = 0.17382303\n",
      "Iteration 185, loss = 0.17347370\n",
      "Iteration 186, loss = 0.17256786\n",
      "Iteration 187, loss = 0.17181787\n",
      "Iteration 188, loss = 0.17131150\n",
      "Iteration 189, loss = 0.17079205\n",
      "Iteration 190, loss = 0.17048351\n",
      "Iteration 191, loss = 0.17010176\n",
      "Iteration 192, loss = 0.16914136\n",
      "Iteration 193, loss = 0.16855984\n",
      "Iteration 194, loss = 0.16809669\n",
      "Iteration 195, loss = 0.16782906\n",
      "Iteration 196, loss = 0.16713752\n",
      "Iteration 197, loss = 0.16681469\n",
      "Iteration 198, loss = 0.16635742\n",
      "Iteration 199, loss = 0.16602298\n",
      "Iteration 200, loss = 0.16506833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Applied MLP Neural Network Classifier model to 'dropped' bank churn X and y training data\n",
    "mlpmodelparams_dropped , mlpmodelScore_dropped, mlpmodelTime_dropped = mlpmodel(X_train_dropped, y_train_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate MLP Neural Network Classifier Model Build Time with GridSearchCV : 285.9 seconds\n",
      "MLP Neural Network Classifier Model Best Parameters given Dropped Dataset: {'activation': 'logistic', 'alpha': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'power_t': 0.1, 'solver': 'adam'}\n",
      "MLP Neural Network Classifier Model Best Parameter Score given Dropped Dataset: 0.9163959698114159\n"
     ]
    }
   ],
   "source": [
    "print(f\"Approximate MLP Neural Network Classifier Model Build Time with GridSearchCV : {round(mlpmodelTime_dropped , 2)} seconds\")\n",
    "print(f\"MLP Neural Network Classifier Model Best Parameters given Dropped Dataset: {mlpmodelparams_dropped}\")\n",
    "print(f\"MLP Neural Network Classifier Model Best Parameter Score given Dropped Dataset: {mlpmodelScore_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373169302870533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       churn       0.83      0.71      0.76       244\n",
      " not_churned       0.95      0.98      0.96      1463\n",
      "\n",
      "    accuracy                           0.94      1707\n",
      "   macro avg       0.89      0.84      0.86      1707\n",
      "weighted avg       0.93      0.94      0.94      1707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tpooz\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Applying best params for MLP Neural Network model model for X_train_dropped and y_train_dropped\n",
    "mlpmodel_dropped = MLPClassifier(hidden_layer_sizes = (100,), activation='logistic', alpha=0.0001, beta_1 = 0.9, beta_2=0.999, learning_rate='constant', learning_rate_init=0.001, power_t=0.1, solver='adam', random_state=random_state)\n",
    "mlpmodel_dropped.fit(X_train_dropped, y_train_dropped.values.ravel())\n",
    "y_pred_mlpmodel_dropped = mlpmodel_dropped.predict(X_test_dropped)\n",
    "\n",
    "print(accuracy_score(y_test_dropped, y_pred_mlpmodel_dropped))\n",
    "print(classification_report(y_test_dropped, y_pred_mlpmodel_dropped, target_names=['churn', 'not_churned']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note to mention regarding apply the MLP Neural Network model, it seems further tuning needs to be implemented in order to further optimized predictability of the model, however this will be noted down in future work in terms of potentially returning a more fine tuned Neural Network model for assessing predictability for bank churn datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Compile and Assess Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Retrieve ROC AUC Scores and Plot Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further assess each model's performance, the ROC AUC scores as well as plotted curves will be produced. This will provide another metric to assess overall model performing predictibility power regarding the specified test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retreiving each model's predicted values given associated test values compared to.\n",
    "y_pred_log_missing\n",
    "y_pred_log_dropped\n",
    "y_pred_randForest_missing\n",
    "y_pred_randForest_dropped\n",
    "y_pred_gradBoost_missing\n",
    "y_pred_gradBoost_dropped\n",
    "y_pred_svc_missing\n",
    "y_pred_svc_dropped\n",
    "y_pred_adaBoost_decision_missing\n",
    "y_pred_adaBoost_decision_dropped\n",
    "y_pred_adaBoost_gradient_missing\n",
    "y_pred_adaBoost_gradient_dropped\n",
    "y_pred_mlpmodel_missing\n",
    "y_pred_mlpmodel_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designed function from sklearn documentation to retrieve ROC AUC value and Curve\n",
    "def roccurveplot(y_true , y_predict, name):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true , y_predict)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc , estimator_name=name)\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7716877246633403\n"
     ]
    }
   ],
   "source": [
    "#Plot ROC curve for Logistic Regression Model for missing and dropped datasets\n",
    "roc_auc_log_missing = roccurveplot(y_test_missing,y_pred_log_missing, 'Logistic Regression w/ Missing Dataset')\n",
    "print(roc_auc_log_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776534294006253\n"
     ]
    }
   ],
   "source": [
    "roc_auc_log_dropped = roccurveplot(y_test_dropped,y_pred_log_dropped, 'Logistic Regression w/ Dropped Dataset')\n",
    "print(roc_auc_log_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8300156929019797\n"
     ]
    }
   ],
   "source": [
    "roc_auc_randForest_missing = roccurveplot(y_test_missing,y_pred_randForest_missing, 'Random Forest Classifier w/ Missing Dataset')\n",
    "print(roc_auc_randForest_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835722409600753\n"
     ]
    }
   ],
   "source": [
    "roc_auc_randForest_dropped = roccurveplot(y_test_dropped,y_pred_randForest_dropped, 'Random Forest Classifier w/ Dropped Dataset')\n",
    "print(roc_auc_randForest_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9192056977305649\n"
     ]
    }
   ],
   "source": [
    "roc_auc_gradBoost_missing = roccurveplot(y_test_missing,y_pred_gradBoost_missing, 'Gradient Boosting Classifier w/ Missing Dataset')\n",
    "print(roc_auc_gradBoost_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9132480978900307\n"
     ]
    }
   ],
   "source": [
    "roc_auc_gradBoost_dropped = roccurveplot(y_test_dropped,y_pred_gradBoost_dropped, 'Gradient Boosting Classifier w/ Dropped Dataset')\n",
    "print(roc_auc_gradBoost_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjRElEQVR4nO3dd1iT5/4G8DsJJGwQkCVbxT2hKHhcdVWtHm3rqNaBo7XWulo9+rNHrT2tra1KrdUuxXFcraO1rbW1dYsTwX2cKA4QQdkykjy/P5DUSECChBfC/bmuXJp3JHfeAPnmeZ/neWVCCAEiIiIiMyGXOgARERFRRWJxQ0RERGaFxQ0RERGZFRY3REREZFZY3BAREZFZYXFDREREZoXFDREREZkVC6kDVDatVos7d+7A3t4eMplM6jhERERUBkIIZGZmwsvLC3J56W0zNa64uXPnDnx8fKSOQUREROVw8+ZNeHt7l7pNjStu7O3tARQeHAcHB4nTEBERUVlkZGTAx8dH9zlemhpX3BSdinJwcGBxQ0REVM2UpUsJOxQTERGRWWFxQ0RERGaFxQ0RERGZFRY3REREZFZY3BAREZFZYXFDREREZoXFDREREZkVFjdERERkVljcEBERkVlhcUNERERmRdLiZv/+/ejTpw+8vLwgk8nw448/PnWfffv2ITg4GFZWVggMDMRXX31l+qBERERUbUha3GRnZ6NFixZYunRpmbaPj49Hr1690L59e8TGxuL//u//MHHiRGzZssXESYmIiKi6kPTCmT179kTPnj3LvP1XX30FX19fREZGAgAaNWqEEydO4LPPPsPLL79sopRERET0NBqtQE6+Gtl5GhRotPBxtpEsS7W6Kvjhw4fRvXt3vWU9evTAihUrUFBQAEtLy2L75OXlIS8vT3c/IyPD5DmJiIiqMiEE8tRa5ORrkJ2nRna+uvDfvKL7hpfn5GuQladGTr4aWXmaR8VM4fqHBRrd49dxssahGc9L9vqqVXGTlJQEd3d3vWXu7u5Qq9VISUmBp6dnsX3mz5+P999/v7IiEhERVTitViCn4FHBUVRsFBUW+frLCwsPA4VInv72aq0wSVYLuQwKucwkj13mDJI+eznIZPoHTAhhcHmRmTNnYurUqbr7GRkZ8PHxMV1AIiKq8fLV2sdaPjQGWkD+bvnIeqLoyMkr3jqSk695+pOWk7WlArYqBWxVFrBRWsBOpXj0rwVslIXLi9bbKi0e/Vu0/NE65d//VyrkJX4mV5ZqVdx4eHggKSlJb1lycjIsLCzg4uJicB+VSgWVSlUZ8YiIqBoSQhSensl/7LTM44WJgdM0fxcgfxcijxcxBRrTtIrIZYCt6u/Cw+5RQVK4TAEbg+v0i4+/i5TCIkbqVhZTqFbFTVhYGH7++We9ZX/88QdCQkIM9rchIiLzU6DRGjwd8+Rpmpw8/daRYoXIo31zCjQQpqlFYGUp1xUWuoJDVULriF5ryGP3lX8XJSoL6VtFqgNJi5usrCxcuXJFdz8+Ph5xcXFwdnaGr68vZs6cidu3b2PNmjUAgHHjxmHp0qWYOnUqxo4di8OHD2PFihXYsGGDVC+BiIhKIYTAwwLNYy0g+oWIoVMwWY+WZz/2/8eLknyN1iRZ5TL8XYioFAZbR2yfeprm79M7tkoFLBScK1cKkhY3J06cQOfOnXX3i/rGjBgxAqtWrUJiYiISEhJ06wMCArBjxw5MmTIFX375Jby8vLBkyRIOAyciqiBqjfbvlo/HOqJmlXCa5slt9EffFBYopmoVUVrInyhAnig4VAZaRJTFT80U/d/Kkq0i5kImhKl+7KqmjIwMODo6Ij09HQ4ODlLHISIqNyEEcgu0+p1VDfxfdzrGwGmaJ/uW5KlN0yoie9Qq8vepmcJ+IE+epinWCqI0fJrGRqWAJVtFahRjPr+rVZ8bIqLqTKMVBuYTeWL4roHTNCUVLjn5GmhMNJxXqZDrOpzaPXGaprTTMcX7lxT+a2WhgNwMO65S1cTihojIgKJJzgxOXJZnuINqSdsUFTG5BaZpFQEAW6X+SJniHVKLn6Z5snApWmejtIDSgq0iVH2xuCEis6B91CpSVHCU2jpi4DRNsdE3JmwVsZDL9IbzGporxODpmBKWW1uyVYTocSxuiKjSCSGQr9EWH0FTwqgZQ/OI/P3/wv0en/q9ohVOcvbE8N0nO6sWK0yeaB157DSNykJhsqxExOKGiMqgaOp3Q6djSh0p8+j/lTn1u0Iug+1j84noFyCGR808Ptz3yf4l5jrJGZE5Y3FDZIby1VoDp2D+Pl1jsHUkX/2oADEwGZoJp363spSXPI/IY60hxVtACltSnuzEyknOiIjFDZHESpr63WDrSP6jyc1KumbNo/UmneSstLlCSjlN83driIXeUGC2ihBRRWNxQ1TJNFqBSRtjcSz+vq6oMdVsU6qiSc5KnC9EvygxdF2ax0/TsFWEiKoDFjdElWzP/5Lxy+nEYstlMsBO+XhHVcMjaAxdl8bQRGg2Sk5yRkQ1E4sbokoWFR0PAHitrS/Gtg/UFSmc+p2IqGKwuCGqRBeTMnHoSirkMuDNTvVQx8la6khERGaHbdZElWjVo1abHk08WNgQEZkIixuiSvIgOx9bT94GAES0C5A4DRGR+WJxQ1RJNhxPQJ5aiyZeDnjOv5bUcYiIzBaLG6JKUKDRYu3hGwAKW23YcZiIyHRY3BBVgt/PJSExPReudkr0aeEpdRwiIrPG4oaoEkQdug4AGNLGjxdNJCIyMRY3RCZ2+lYaYm48gKVChtfa+kodh4jI7LG4ITKxolabF5t7wc3eStowREQ1AIsbIhNKzsjFL6fvAAAi2vlLG4aIqIZgcUNkQv89moACjUCwXy0093aSOg4RUY3A4obIRPLUGqw/WjT821/aMERENQiLGyIT+flUIlKy8uHpaIUeTTykjkNEVGOwuCEyASEEog4VXkdqWJgfLBX8VSMiqiz8i0tkAsevP8C5OxmwspTj1ec4/JuIqDKxuCEygaJWm/6t6qCWrVLiNERENQuLG6IKdutBDn4/lwQAGBnOq38TEVU2FjdEFWzt4RvQCqBdPRc08LCXOg4RUY3D4oaoAuXkq7HhWAIAIIKtNkREkmBxQ1SBtp68jYxcNfxcbPB8Qzep4xAR1UgsbogqiBACq6KvAwBGhPlDLpdJG4iIqIZicUNUQQ5cTsGV5CzYqSwwIMRb6jhERDUWixuiClLUavNKsDfsrSylDUNEVIOxuCGqAPEp2dj9v2TIZMDIcH+p4xAR1WgsbogqwOpHrTbPN3CDv6uttGGIiGo4FjdEzygjtwA/nLgJAIhox+HfRERSY3FD9Ix+OHEL2fkaBLnboV09F6njEBHVeCxuiJ6BRit0p6RGhgdAJuPwbyIiqbG4IXoGu/+XjIT7OXC0tkT/VnWkjkNERGBxQ/RMiq7+PTjUB9ZKhcRpiIgIYHFDVG7/S8pA9NVUKOQyDA/zlzoOERE9wuKGqJxWHboOAOjRxB11nKylDUNERDosbojK4X52PrbF3gbA4d9ERFUNixuicthwLAF5ai2a1nFAiF8tqeMQEdFjWNwQGalAo8XawzcAABEc/k1EVOWwuCEy0s6zSUjKyIWrnQovtvCUOg4RET2BxQ2RkYqGfw9t4wuVBYd/ExFVNSxuiIxw6mYaTiakwVIhw9C2vlLHISIiA1jcEBmhqNWmT3MvuNlbSZyGiIgMYXFDVEbJGbn49UwiAA7/JiKqyljcEJXRf4/cQIFGIMSvFpp5O0odh4iISsDihqgMcgs0WHc0AQBbbYiIqjoWN0Rl8POpO0jNzoeXoxV6NHGXOg4REZWCxQ3RUwghEPXoOlLDwvxhoeCvDRFRVca/0kRPcSz+Ps4nZsDKUo5XQ32kjkNERE/B4oboKYpabfq38oaTjVLaMERE9FQsbohKcfN+Dv44nwQAiGjnL20YIiIqE8mLm2XLliEgIABWVlYIDg7GgQMHSt1+3bp1aNGiBWxsbODp6YmIiAikpqZWUlqqadYeuQGtAP5RzxVB7vZSxyEiojKQtLjZtGkTJk+ejFmzZiE2Nhbt27dHz549kZCQYHD7gwcPYvjw4Rg9ejTOnTuHH374AcePH8eYMWMqOTnVBDn5amw8VjT821/aMEREVGaSFjeLFi3C6NGjMWbMGDRq1AiRkZHw8fHB8uXLDW5/5MgR+Pv7Y+LEiQgICMA//vEPvPHGGzhx4kSJz5GXl4eMjAy9G1FZbDl5Gxm5avi72KBzAzep4xARURlJVtzk5+cjJiYG3bt311vevXt3REdHG9wnPDwct27dwo4dOyCEwN27d7F582b07t27xOeZP38+HB0ddTcfH452oacTQmDVo+tIjQj3h1wukzgRERGVlWTFTUpKCjQaDdzd9SdEc3d3R1JSksF9wsPDsW7dOgwaNAhKpRIeHh5wcnLCF198UeLzzJw5E+np6brbzZs3K/R1kHk6cDkFV+9lw05lgVeCvaWOQ0RERpC8Q7FMpv+NWAhRbFmR8+fPY+LEiZg9ezZiYmKwc+dOxMfHY9y4cSU+vkqlgoODg96N6GmKrv49IMQb9laWEqchIiJjWEj1xK6urlAoFMVaaZKTk4u15hSZP38+2rVrh2nTpgEAmjdvDltbW7Rv3x7/+c9/4OnpafLcZP6u3cvCnov3IJMBI8P9pY5DRERGkqzlRqlUIjg4GLt27dJbvmvXLoSHhxvcJycnB3K5fmSFQgGgsMWHqCKsjr4OAOjS0A1+LrbShiEiIqNJelpq6tSp+O6777By5UpcuHABU6ZMQUJCgu4008yZMzF8+HDd9n369MHWrVuxfPlyXLt2DYcOHcLEiRMRGhoKLy8vqV4GmZGM3AJsjrkFgFf/JiKqriQ7LQUAgwYNQmpqKubNm4fExEQ0bdoUO3bsgJ+fHwAgMTFRb86bkSNHIjMzE0uXLsU777wDJycnPP/88/jkk0+keglkZr4/fhPZ+RoEudshvK6L1HGIiKgcZKKGnc/JyMiAo6Mj0tPT2bmY9Gi0Ap0+24Ob9x/io/7NMKSNr9SRiIjoEWM+vyUfLUVUVfx14S5u3n8IJxtL9G9VR+o4RERUTixuiB4puvr34Od8Ya1USBuGiIjKjcUNEYALiRk4fC0VCrkMw8P8pI5DRETPgMUNEYBVj1ptXmjiAS8na2nDEBHRM2FxQzXe/ex8/Bh3GwCv/k1EZA5Y3FCNt+FYAvLUWjSr44hgv1pSxyEiomfE4oZqtAKNFmsP3wBQ2GpT0nXNiIio+mBxQzXab2eTkJSRC1c7FXo357XJiIjMAYsbqtGKrv79WltfqCw4/JuIyBywuKEaK+5mGmIT0qBUyDG0DYd/ExGZCxY3VGMVtdq82MITte1VEqchIqKKwuKGaqS7Gbn49XQiAGAUr/5NRGRWWNxQjfTfIzeg1go8518LTes4Sh2HiIgqEIsbqnFyCzRYfzQBABDBVhsiIrPD4oZqnO2n7iA1Ox91nKzRvbG71HGIiKiCsbihGkUIobv697AwP1go+CtARGRu+JedapSj8fdxITEDVpZyDH7OR+o4RERkAixuqEYpGv79UmtvONkoJU5DRESmwOKGaoyb93Ow6/xdAEBEuL+0YYiIyGRY3FCNsebwdWgF0L6+K+q720sdh4iITITFDdUI2XlqbDx+E0Dh1b+JiMh8sbihGmFr7G1k5qoR4GqLTkFuUschIiITYnFDZk+rFVj1qCPxiDA/yOUyiRMREZEpsbghs3fgSgqu3suGvcoCr4Rw+DcRkbljcUNmr2j494AQH9ipLCROQ0REpsbihsza1XtZ2HvxHmQyYES4n9RxiIioErC4IbO2Ovo6AKBLQzf4udhKG4aIiCoFixsyW+kPC7A55hYAXv2biKgmYXFDZuuHEzeRk69BA3d7hNd1kToOERFVEhY3ZJY0WoFVj05JjWznD5mMw7+JiGoKFjdklv68cBe3HjyEk40l+rWsI3UcIiKqRCxuyCwVDf9+NdQX1kqFxGmIiKgysbghs3MhMQNHrt2HQi7DsLYc/k1EVNOUq7hRq9X4888/8fXXXyMzMxMAcOfOHWRlZVVoOKLyKGq1eaGpB7ycrCVOQ0RElc3o6Vpv3LiBF154AQkJCcjLy0O3bt1gb2+PBQsWIDc3F1999ZUpchKVSWpWHn6MuwMAGMWrfxMR1UhGt9xMmjQJISEhePDgAayt//5W3L9/f/z1118VGo7IWBuOJSBfrUVzb0e09q0ldRwiIpKA0S03Bw8exKFDh6BUKvWW+/n54fbt2xUWjMhYBRot1h65AQCI4PBvIqIay+iWG61WC41GU2z5rVu3YG9vXyGhiMpjx5lE3M3IQ217FXo385I6DhERScTo4qZbt26IjIzU3ZfJZMjKysKcOXPQq1evisxGZJSoQ9cBAK+18YPSggMBiYhqKqNPSy1evBidO3dG48aNkZubiyFDhuDy5ctwdXXFhg0bTJGR6KliEx4g7mYalAo5hrTxlToOERFJyOjixsvLC3Fxcdi4cSNiYmKg1WoxevRoDB06VK+DMVFlKmq16dPCC7XtVdKGISIiSRld3Ozfvx/h4eGIiIhARESEbrlarcb+/fvRoUOHCg1I9DRJ6bnYcSYRQGFHYiIiqtmM7pjQuXNn3L9/v9jy9PR0dO7cuUJCERnjv0duQK0VCPV3RtM6jlLHISIiiRld3AghDA6xTU1Nha2tbYWEIiqr3AIN1h9LAMBWGyIiKlTm01IvvfQSgMLRUSNHjoRK9Xe/Bo1Gg9OnTyM8PLziExKVYnvcHdzPzkcdJ2t0a+wudRwiIqoCylzcODoWNvcLIWBvb6/XeVipVKJt27YYO3ZsxSckKoEQAisfXUdqeJgfLBQc/k1EREYUN1FRUQAAf39/vPvuuzwFRZI7cu0+/peUCWtLBQY/x+HfRERUyOjRUnPmzDFFDiKjFV39+6XWdeBoYylxGiIiqiqMLm4AYPPmzfj++++RkJCA/Px8vXUnT56skGBEpbl5Pwd/XrgLgB2JiYhIn9GdFJYsWYKIiAi4ubkhNjYWoaGhcHFxwbVr19CzZ09TZCQqZs3h69AKoH19V9Rz4zXNiIjob0YXN8uWLcM333yDpUuXQqlUYvr06di1axcmTpyI9PR0U2Qk0pOdp8bG4zcBAKPaBUichoiIqhqji5uEhATdkG9ra2tkZmYCAIYNG8ZrS1Gl2HryFjJz1Qh0tUXHoNpSxyEioirG6OLGw8MDqampAAA/Pz8cOXIEABAfHw8hRMWmI3qCVisQFX0dADAi3B9yefEJJYmIqGYzurh5/vnn8fPPPwMARo8ejSlTpqBbt24YNGgQ+vfvX+EBiR63//I9XLuXDXuVBV4O9pY6DhERVUFGj5b65ptvoNVqAQDjxo2Ds7MzDh48iD59+mDcuHEVHpDocUVX/x4Q4gM7VbkG+xERkZkz+tNBLpdDLv+7wWfgwIEYOHAgAOD27duoU6dOxaUjesyV5Czsu3QPMhkwMtxf6jhERFRFVch89UlJSXj77bdRr149o/ddtmwZAgICYGVlheDgYBw4cKDU7fPy8jBr1iz4+flBpVKhbt26WLlyZXmjUzWy+lFfmy4N3eHrYiNtGCIiqrLKXNykpaVh6NChqF27Nry8vLBkyRJotVrMnj0bgYGBOHLkiNFFxqZNmzB58mTMmjULsbGxaN++PXr27ImEhIQS9xk4cCD++usvrFixAhcvXsSGDRvQsGFDo56Xqp/0hwXYcvIWAGAUJ+0jIqJSyEQZhziNHz8eP//8MwYNGoSdO3fiwoUL6NGjB3JzczFnzhx07NjR6Cdv06YNWrdujeXLl+uWNWrUCP369cP8+fOLbb9z504MHjwY165dg7Ozc5meIy8vD3l5ebr7GRkZ8PHxQXp6OhwcHIzOTNL4dv81fLjjAhq422Pn5PaQyThKioioJsnIyICjo2OZPr/L3HLz66+/IioqCp999hm2b98OIQSCgoKwe/fuchU2+fn5iImJQffu3fWWd+/eHdHR0Qb32b59O0JCQrBgwQLUqVMHQUFBePfdd/Hw4cMSn2f+/PlwdHTU3Xx8fIzOStLSaAVWH74OoPBSCyxsiIioNGXuUHznzh00btwYABAYGAgrKyuMGTOm3E+ckpICjUYDd3d3veXu7u5ISkoyuM+1a9dw8OBBWFlZYdu2bUhJScH48eNx//79Ek+JzZw5E1OnTtXdL2q5oepj1/m7uPXgIWrZWKJfK3ZYJyKi0pW5uNFqtbC0/PvKywqFAra2ts8c4Mlv4UKIEr+Za7VayGQyrFu3Do6OjgCARYsW4ZVXXsGXX34Ja2vrYvuoVCqoVKpnzknSKbr696uhvrCyVEichoiIqroyFzdCCIwcOVJXKOTm5mLcuHHFCpytW7eW6fFcXV2hUCiKtdIkJycXa80p4unpiTp16ugKG6Cwj44QArdu3UL9+vXL+nKomjh3Jx1H4+9DIZdhWJif1HGIiKgaKHOfmxEjRsDNzU3Xd+W1116Dl5eXXn+Wx4uOp1EqlQgODsauXbv0lu/atUt37aontWvXDnfu3EFWVpZu2aVLlyCXy+HtzdlqzdGqR5P29WzqAU/H4i1zRERETypzy01UVFSFP/nUqVMxbNgwhISEICwsDN988w0SEhJ0Mx3PnDkTt2/fxpo1awAAQ4YMwQcffICIiAi8//77SElJwbRp0zBq1CiDp6SoekvNysNPp+4AACJ49W8iIiojSeevHzRoEFJTUzFv3jwkJiaiadOm2LFjB/z8Ck8/JCYm6s15Y2dnh127duHtt99GSEgIXFxcMHDgQPznP/+R6iWQCa0/moB8tRYtvB3R2tdJ6jhERFRNlHmeG3NhzDh5kk6+Wot/fLIbyZl5iBzUkqOkiIhqOJPMc0NUmX47m4jkzDy42avQq5mn1HGIiKgaYXFDVdLKRx2JX2vrB6UFf0yJiKjs+KlBVc7JhAc4dTMNSoUcQ9r4Sh2HiIiqmXIVN2vXrkW7du3g5eWFGzduAAAiIyPx008/VWg4qpmiHrXa9G3pBVc7TsBIRETGMbq4Wb58OaZOnYpevXohLS0NGo0GAODk5ITIyMiKzkc1TFJ6Ln47kwig8DpSRERExjK6uPniiy/w7bffYtasWVAo/p4KPyQkBGfOnKnQcFTzrD1yHWqtQGiAM5p4lX1SSCIioiJGFzfx8fFo1apVseUqlQrZ2dkVEopqptwCDdYfLZzXaBRbbYiIqJyMLm4CAgIQFxdXbPlvv/2mu2o4UXn8FHcbD3IKUMfJGt0ae0gdh4iIqimjZyieNm0a3nrrLeTm5kIIgWPHjmHDhg2YP38+vvvuO1NkpBpACKHrSDwi3A8KueErwxMRET2N0cVNREQE1Go1pk+fjpycHAwZMgR16tTB559/jsGDB5siI9UAR67dx/+SMmFtqcCgEA7/JiKi8ivXtaXGjh2LsWPHIiUlBVqtFm5ubhWdi2qYqEPxAICXg+vA0cZS4jRERFSdGd3n5v3338fVq1cBAK6urixs6JndvJ+DXRfuAgBGhvPq30RE9GyMLm62bNmCoKAgtG3bFkuXLsW9e/dMkYtqkNXR1yEE0CGoNuq52Ukdh4iIqjmji5vTp0/j9OnTeP7557Fo0SLUqVMHvXr1wvr165GTk2OKjGTGsvPU2HTiJgBO2kdERBWjXJdfaNKkCT766CNcu3YNe/bsQUBAACZPngwPDw7fJeNsOXkLmblqBLraomP92lLHISIiM/DMF860tbWFtbU1lEolCgoKKiIT1RBarcAq3fBvf8g5/JuIiCpAuYqb+Ph4fPjhh2jcuDFCQkJw8uRJzJ07F0lJSRWdj8zYvsv3cC0lG/YqC7wc7C11HCIiMhNGDwUPCwvDsWPH0KxZM0REROjmuSEyVtGkfQOf84GdqlyzEhARERVj9CdK586d8d1336FJkyamyEM1xJXkLOy/dA8yGTAizF/qOEREZEaMLm4++ugjU+SgGmZVdOGkfV0bucPXxUbiNEREZE7KVNxMnToVH3zwAWxtbTF16tRSt120aFGFBCPzlZ5TgC0xtwFw+DcREVW8MhU3sbGxupFQsbGxJg1E5m/TiQQ8LNCgoYc9wgJdpI5DRERmpkzFzZ49ewz+n8hYao0Wq6NvAChstZHJOPybiIgqltFDwUeNGoXMzMxiy7OzszFq1KgKCUXm688Ld3E77SFq2Vjiny05yo6IiCqe0cXN6tWr8fDhw2LLHz58iDVr1lRIKDJfKx8N/x7SxhdWlgppwxARkVkq82ipjIwMCCEghEBmZiasrKx06zQaDXbs2MErhFOpzt1Jx7H4+7CQyzCsrb/UcYiIyEyVubhxcnKCTCaDTCZDUFBQsfUymQzvv/9+hYYj81I0aV/PZp7wcLQqfWMiIqJyKnNxs2fPHggh8Pzzz2PLli1wdnbWrVMqlfDz84OXl5dJQlL1l5KVh+1xdwBw+DcREZlWmYubjh07Aii8rpSvry9HuZBR1h9NQL5GixY+TmjtW0vqOEREZMbKVNycPn0aTZs2hVwuR3p6Os6cOVPits2bN6+wcGQe8tVarD1SOPx7FFttiIjIxMpU3LRs2RJJSUlwc3NDy5YtIZPJIIQotp1MJoNGo6nwkFS97TiTiHuZeXCzV6FnU0+p4xARkZkrU3ETHx+P2rVr6/5PVFZCCEQdKvyZGdbWD0oLo2cfICIiMkqZihs/Pz+D/yd6mpMJaTh1Kx1KCzmGtPGVOg4REdUA5ZrE79dff9Xdnz59OpycnBAeHo4bN25UaDiq/opabf7ZwgsudiqJ0xARUU1gdHHz0UcfwdraGgBw+PBhLF26FAsWLICrqyumTJlS4QGp+kpMf4jfziYBACLaBUichoiIaooyDwUvcvPmTdSrVw8A8OOPP+KVV17B66+/jnbt2qFTp04VnY+qsbWHb0CjFWgT4IzGXg5SxyEiohrC6JYbOzs7pKamAgD++OMPdO3aFQBgZWVl8JpTVDPlFmiw4VgCALbaEBFR5TK65aZbt24YM2YMWrVqhUuXLqF3794AgHPnzsHf37+i81E19VPcbTzIKYB3LWt0a+wudRwiIqpBjG65+fLLLxEWFoZ79+5hy5YtcHFxAQDExMTg1VdfrfCAVP0UDv++DgAYEeYPhZyzWRMRUeWRCUOz8ZmxjIwMODo6Ij09HQ4O7AdiCtFXUzDk26OwUSpweGYXOFpbSh2JiIiqOWM+v40+LQUAaWlpWLFiBS5cuACZTIZGjRph9OjRcHR0LFdgMi9FrTYvt/ZmYUNERJXO6NNSJ06cQN26dbF48WLcv38fKSkpWLx4MerWrYuTJ0+aIiNVIwmpOfjzwl0AwEheR4qIiCRgdMvNlClT0LdvX3z77bewsCjcXa1WY8yYMZg8eTL2799f4SGp+lh9+DqEADoG1Ubd2nZSxyEiohrI6OLmxIkTeoUNAFhYWGD69OkICQmp0HBUvWTlqfH98ZsA2GpDRETSMfq0lIODAxISEootv3nzJuzt7SskFFVPW2JuITNPjUBXW3SsX1vqOEREVEMZXdwMGjQIo0ePxqZNm3Dz5k3cunULGzduxJgxYzgUvAbTagVWRV8HUNhqI+fwbyIikojRp6U+++wzyGQyDB8+HGq1GgBgaWmJN998Ex9//HGFB6TqYd+le4hPyYa9lQVebu0tdRwiIqrBjC5ulEolPv/8c8yfPx9Xr16FEAL16tWDjY2NKfJRNbHy0dW/B4X4wFZVrhkGiIiIKkSZT0vl5OTgrbfeQp06deDm5oYxY8bA09MTzZs3Z2FTw11JzsSByymQy4AR4f5SxyEiohquzMXNnDlzsGrVKvTu3RuDBw/Grl278Oabb5oyG1UTRZP2dW3kDh9nFrpERCStMp8/2Lp1K1asWIHBgwcDAF577TW0a9cOGo0GCoXCZAGpakvPKcDWk7cB8OrfRERUNZS55ebmzZto37697n5oaCgsLCxw584dkwSj6mHj8QQ8LNCgoYc92gY6Sx2HiIio7MWNRqOBUqnUW2ZhYaEbMUU1j1qjxZrDNwAAo9oFQCbj8G8iIpJemU9LCSEwcuRIqFQq3bLc3FyMGzcOtra2umVbt26t2IRUZe06fxe30x7C2VaJvi29pI5DREQEwIjiZsSIEcWWvfbaaxUahqqXoo7EQ0J9YWXJfldERFQ1lLm4iYqKMmUOqmbO3k7Hsev3YSGXYViYn9RxiIiIdIy+/EJFW7ZsGQICAmBlZYXg4GAcOHCgTPsdOnQIFhYWaNmypWkDkkFFrTa9mnnC3cFK2jBERESPkbS42bRpEyZPnoxZs2YhNjYW7du3R8+ePQ1emPNx6enpGD58OLp06VJJSelx9zLz8POpwlFyEbz6NxERVTGSFjeLFi3C6NGjMWbMGDRq1AiRkZHw8fHB8uXLS93vjTfewJAhQxAWFlZJSelx648mIF+jRUsfJ7TyrSV1HCIiIj2SFTf5+fmIiYlB9+7d9ZZ3794d0dHRJe4XFRWFq1evYs6cOWV6nry8PGRkZOjdqPzy1Vr892jh8G+22hARUVUkWXGTkpICjUYDd3d3veXu7u5ISkoyuM/ly5cxY8YMrFu3DhYWZesLPX/+fDg6OupuPj4+z5y9Jvv1zB3cy8yDu4MKvZp5Sh2HiIiomHIVN2vXrkW7du3g5eWFGzcKv8VHRkbip59+Mvqxnpz4TQhhcDI4jUaDIUOG4P3330dQUFCZH3/mzJlIT0/X3W7evGl0RiokhNB1JB7W1g+WCsn7oxMRERVj9KfT8uXLMXXqVPTq1QtpaWnQaDQAACcnJ0RGRpb5cVxdXaFQKIq10iQnJxdrzQGAzMxMnDhxAhMmTICFhQUsLCwwb948nDp1ChYWFti9e7fB51GpVHBwcNC7UfmcTHiA07fSobSQ49VQX6njEBERGWR0cfPFF1/g22+/xaxZs/QumBkSEoIzZ86U+XGUSiWCg4Oxa9cuveW7du1CeHh4se0dHBxw5swZxMXF6W7jxo1DgwYNEBcXhzZt2hj7UshIKx+12vRr6QUXO1XpGxMREUmkzJP4FYmPj0erVq2KLVepVMjOzjbqsaZOnYphw4YhJCQEYWFh+Oabb5CQkIBx48YBKDyldPv2baxZswZyuRxNmzbV29/NzQ1WVlbFllPFu5P2EDvPFray8erfRERUlRld3AQEBCAuLg5+fvqz0v72229o3LixUY81aNAgpKamYt68eUhMTETTpk2xY8cO3WMnJiY+dc4bqhz/PXIDGq1A20BnNPLkqT0iIqq6ZEIIYcwOUVFR+Pe//42FCxdi9OjR+O6773D16lXMnz8f3333HQYPHmyqrBUiIyMDjo6OSE9PZ/+bMsot0CBs/l94kFOAr4cFo0cTD6kjERFRDWPM57fRLTcRERFQq9WYPn06cnJyMGTIENSpUweff/55lS9sqHx+jL2NBzkF8HG2RtdGxTt7ExERVSVGFzcAMHbsWIwdOxYpKSnQarVwc3Or6FxURTw+/HtEmD8U8uLD9ImIiKqSchU3RVxdXSsqB1VRh6+m4uLdTNgoFRgQwgkQiYio6itXh2JDk+wVuXbt2jMFoqqlaPj3y6294WhtKW0YIiKiMjC6uJk8ebLe/YKCAsTGxmLnzp2YNm1aReWiKuBGajb++t9dAMBIXkeKiIiqCaOLm0mTJhlc/uWXX+LEiRPPHIiqjtXRNyAE0DGoNurWtpM6DhERUZlU2MWBevbsiS1btlTUw5HEsvLU+OFE4XW4ePVvIiKqTiqsuNm8eTOcnZ0r6uFIYptP3ERmnhqBtW3RoX5tqeMQERGVmdGnpVq1aqXXoVgIgaSkJNy7dw/Lli2r0HAkDa1WYPXhwqu9R4T7Q87h30REVI0YXdz069dP775cLkft2rXRqVMnNGzYsKJykYT2XkpGfEo27K0s8FJrb6njEBERGcWo4katVsPf3x89evSAhwen4DdXRZP2DX7OB7aqZ5oKiYiIqNIZ1efGwsICb775JvLy8kyVhyR2+W4mDlxOgVwGDA/zlzoOERGR0YzuUNymTRvExsaaIgtVAVHR1wEA3Rq7w8fZRtowRERE5WD0OYfx48fjnXfewa1btxAcHAxbW1u99c2bN6+wcFS50nLysfXkLQBARLsAidMQERGVT5mLm1GjRiEyMhKDBg0CAEycOFG3TiaTQQgBmUwGjUZT8SmpUmw8fhO5BVo08nRAmwAO6yciouqpzMXN6tWr8fHHHyM+Pt6UeUgiao0Wax6dkopo51/q9cOIiIiqsjIXN0IIAICfn5/JwpB0/jh/F3fSc+Fiq0TfFl5SxyEiIio3ozoU89u8+Yo6VNgiN6SNL6wsFRKnISIiKj+jOhQHBQU9tcC5f//+MwWiynf2djqOX38AC7kMr7VlyxwREVVvRhU377//PhwdHU2VhSSy8lGrTe/mnnB3sJI4DRER0bMxqrgZPHgw3NzcTJWFJHAvMw+/nEoEwOHfRERkHsrc54b9bczTuqM3kK/RopWvE1r6OEkdh4iI6JmVubgpGi1F5iNPrcF/jyQAYKsNERGZjzKfltJqtabMQRL49XQiUrLy4OFghZ5NeSFUIiIyD0ZfW4rMgxBCd/XvYWF+sFTwR4GIiMwDP9FqqJgbD3DmdjpUFnK8GuordRwiIqIKw+Kmhiq6+ne/lnXgbKuUNgwREVEFYnFTA91Je4idZ5MAABH/8Jc2DBERUQVjcVMDrT1yAxqtQFigCxp6OEgdh4iIqEKxuKlhHuZrsOFY0fBvf2nDEBERmQCLmxrmx7jbSMspgI+zNbo0cpc6DhERUYVjcVODFA7/LryO1IgwfyjknHWaiIjMD4ubGiT6aiou3c2CjVKBASE+UschIiIyCRY3NUhRq80rwd5wtLaUOA0REZFpsLipIW6kZuOv/yUDAEaE+0sbhoiIyIRY3NQQq6KvQwigU4PaqFvbTuo4REREJsPipgbIzC3ADyduAeDVv4mIyPyxuKkBNsfcQlaeGnVr26JDfVep4xAREZkUixszp9UKrH50HamR7QIgk3H4NxERmTcWN2Zuz8VkXE/NgYOVBV5uXUfqOERERCbH4sbMRR26DgAYHOoLG6WFtGGIiIgqAYsbM3bpbiYOXkmBXAYMD/OTOg4REVGlYHFjxopabbo39oB3LRtpwxAREVUSFjdmKi0nH9tii4Z/+0sbhoiIqBKxuDFTG47dRG6BFo09HRAa4Cx1HCIiokrD4sYMqTVarD18HUBhqw2HfxMRUU3C4sYM/X7uLu6k58LFVok+LbykjkNERFSpWNyYoaKrfw9t4wsrS4XEaYiIiCoXixszc+ZWOk7ceABLhQyvteXwbyIiqnlY3JiZolab3s084eZgJXEaIiKiysfixowkZ+bi59N3APDq30REVHOxuDEj644koEAj0NrXCS18nKSOQ0REJAkWN2YiT63BuqM3ALDVhoiIajYWN2bil1OJSMnKh4eDFV5o6iF1HCIiIsmwuDEDQgisir4OABgW5gdLBd9WIiKqufgpaAZibjzAmdvpUFnIMSTUV+o4REREkpK8uFm2bBkCAgJgZWWF4OBgHDhwoMRtt27dim7duqF27dpwcHBAWFgYfv/990pMWzUVXf27f6s6qGWrlDYMERGRxCQtbjZt2oTJkydj1qxZiI2NRfv27dGzZ08kJCQY3H7//v3o1q0bduzYgZiYGHTu3Bl9+vRBbGxsJSevOu6kPcTOc0kAgJG8+jcRERFkQggh1ZO3adMGrVu3xvLly3XLGjVqhH79+mH+/PlleowmTZpg0KBBmD17dpm2z8jIgKOjI9LT0+Hg4FCu3FXJx7/9D1/tu4rwui5YP7at1HGIiIhMwpjPb8labvLz8xETE4Pu3bvrLe/evTuio6PL9BharRaZmZlwdnYucZu8vDxkZGTo3czFw3wNNhwrbOUaGe4vbRgiIqIqQrLiJiUlBRqNBu7u7nrL3d3dkZSUVKbHWLhwIbKzszFw4MASt5k/fz4cHR11Nx8fn2fKXZVsi72N9IcF8HG2RpdG7k/fgYiIqAaQvEOxTCbTuy+EKLbMkA0bNmDu3LnYtGkT3NzcStxu5syZSE9P191u3rz5zJmrgsLh34XXkRoR5g+F/OnHjIiIqCawkOqJXV1doVAoirXSJCcnF2vNedKmTZswevRo/PDDD+jatWup26pUKqhUqmfOW9UcupKKS3ezYKtUYOBz5tMaRURE9Kwka7lRKpUIDg7Grl279Jbv2rUL4eHhJe63YcMGjBw5EuvXr0fv3r1NHbPKKrr69yvB3nCwspQ4DRERUdUhWcsNAEydOhXDhg1DSEgIwsLC8M033yAhIQHjxo0DUHhK6fbt21izZg2AwsJm+PDh+Pzzz9G2bVtdq4+1tTUcHR0lex2V7XpKNnZfTAYAjGBHYiIiIj2SFjeDBg1Camoq5s2bh8TERDRt2hQ7duyAn58fACAxMVFvzpuvv/4aarUab731Ft566y3d8hEjRmDVqlWVHV8yq6KvQwigc4PaCKxtJ3UcIiKiKkXSeW6kUN3nucnMLUDY/N3IylNjzahQdAiqLXUkIiIik6sW89xQ+fxw4hay8tSo52aH9vVdpY5DRERU5bC4qUY0WoHVh68DKJy0ryxD5omIiGoaFjfVyJ7/JeNGag4crCzwUus6UschIiKqkljcVCNRjybtezXUFzZKSfuCExERVVksbqqJi0mZOHQlFXIZMCzMT+o4REREVRaLm2qi6FILPZp4wLuWjcRpiIiIqi4WN9XAg+x8bD15GwAQ0S5A4jRERERVG4ubamDD8QTkqbVo4uWA5/xrSR2HiIioSmNxU8UVaLRYe/gGgMJWGw7/JiIiKh2Lmyru93NJSEzPhaudEn1aeEodh4iIqMpjcVPFRR26DgAY0sYPKguFtGGIiIiqARY3VdjpW2mIufEAlgoZXmvrK3UcIiKiaoHFTRVW1GrzYnMvuNlbSRuGiIiommBxU0UlZ+Til9N3AAAR7fylDUNERFSNsLipov57NAEFGoFgv1po7u0kdRwiIqJqg8VNFZSn1mD90aLh3/7ShiEiIqpmWNxUQb+cSkRKVj48Ha3Qo4mH1HGIiIiqFRY3VYwQQnf172FhfrBU8C0iIiIyBj85q5gTNx7g7O0MWFnK8epzHP5NRERkLBY3VUzUocJWm/6t6qCWrVLiNERERNUPi5sq5HbaQ/x+7i4AYES4v7RhiIiIqikWN1XImsPXodEKhNd1QUMPB6njEBERVUssbqqInHw1Nh67CaDw6t9ERERUPixuqohtsbeR/rAAvs42eL6hm9RxiIiIqi0WN1WAEAKrHl1HakS4PxRymbSBiIiIqjEWN1XAwSspuJycBVulAgNCvKWOQ0REVK2xuKkCiq7+PSDEBw5WltKGISIiquZY3EgsPiUbu/+XDJmMw7+JiIgqAosbia2Ovg4A6NzADQGuttKGISIiMgMsbiSUkVuAH04UDf/2lzYMERGRmWBxI6EfTtxCdr4G9d3s8I96rlLHISIiMgssbiSi0QrdKamR7fwhk3H4NxERUUVgcSOR3f9LRsL9HDhaW+KlVhz+TUREVFFY3Eik6Orfg0N9YK1USJyGiIjIfLC4kcD/kjIQfTUVCrkMw8P8pY5DRERkVljcSKDoUgs9mrijjpO1tGGIiIjMDIubSnY/Ox/bYm8D4NW/iYiITIHFTSXbcCwBeWotmtZxQIhfLanjEBERmR0WN5WoQKPF2sM3AAAR4QEc/k1ERGQCLG4q0c6zSUjKyIWrnQovtvCUOg4REZFZYnFTiYqGfw9t4wuVBYd/ExERmQKLm0py6mYaTiakwVIhw9C2vlLHISIiMlssbipJUatNn+ZecLO3kjgNERGR+WJxUwmSM3Lx65lEABz+TUREZGoWUgeoCf57NAEFGoEQv1po5u1ocBshBNRqNTQaTSWnIyIiqhosLS2hUDx7n1QWNyaWp9Zg/dFHw79LaLXJz89HYmIicnJyKjMaERFRlSKTyeDt7Q07O7tnehwWNyb286lEpGTlw9PRCj2auBdbr9VqER8fD4VCAS8vLyiVSs5/Q0RENY4QAvfu3cOtW7dQv379Z2rBYXFjQkIIXUfiYWF+sFAU7+KUn58PrVYLHx8f2NjYVHZEIiKiKqN27dq4fv06CgoKnqm4YYdiEzp+/QHO3cmAlaUcrz5X+vBvuZxvBRER1WwVdeaCn6gmVNRq079VHdSyVUqchoiIqGZgcWMitx7k4PdzSQCAkeEc/k1ERFRZWNyYyNrDN6AVQLt6LmjgYS91HKIyk8lk+PHHH03+PHv37oVMJkNaWppu2Y8//oh69epBoVBg8uTJWLVqFZycnEyeRQpz585Fy5Ytn/lxzPkYSa1Dhw5Yv3691DHMRl5eHnx9fRETE2Py52JxYwI5+WpsOJYAoPDq3+YqOTkZb7zxBnx9faFSqeDh4YEePXrg8OHDUkcrM0MfsE/asmULFAoFEhISDK5v2LAhJk6c+MxZ/P39ERkZ+cyPU5qkpCS8/fbbCAwMhEqlgo+PD/r06YO//vrLpM9rSHh4OBITE+Ho+PfcT2+88QZeeeUV3Lx5Ex988AEGDRqES5cuVXq2kqxatQpt27YtcZ1MJkOjRo2Krfv+++8hk8ng7++vW/buu+9WyHGvrGNU9PpkMhkUCgVq1aqFNm3aYN68eUhPTzfqsa5fvw6ZTIa4uDjThC2BMYXgL7/8gqSkJAwePLjYuo8++ggKhQIff/xxsXUlFa1paWmQyWTYu3ev3vItW7agU6dOcHR0hJ2dHZo3b4558+bh/v37ZcpZHnl5eXj77bfh6uoKW1tb9O3bF7du3Sp1H7Vajffeew8BAQGwtrZGYGAg5s2bB61WCwAoKCjAv/71LzRr1gy2trbw8vLC8OHDcefOHd1jqFQqvPvuu/jXv/5lstdWhMWNCWw9eRsZuWr4udjg+YZuUscxmZdffhmnTp3C6tWrcenSJWzfvh2dOnUy6S9lRSooKCjTdn379oWLiwtWr15dbN2hQ4dw8eJFjB49uqLjlVt+fr7B5devX0dwcDB2796NBQsW4MyZM9i5cyc6d+6Mt956q5JTAkqlEh4eHroOhFlZWUhOTkaPHj3g5eUFe3t7WFtbw83t2X6Hyvo+l8X27dvxz3/+s8T1tra2SE5OLlbgr1y5Er6++oMK7Ozs4OLi8syZKuIYlZWDgwMSExNx69YtREdH4/XXX8eaNWvQsmVLvQ8xc7BkyRJEREQYHOwRFRWF6dOnY+XKlc/0HLNmzcKgQYPw3HPP4bfffsPZs2excOFCnDp1CmvXrn2mxy7N5MmTsW3bNmzcuBEHDx5EVlYWXnzxxVInkf3kk0/w1VdfYenSpbhw4QIWLFiATz/9FF988QUAICcnBydPnsS///1vnDx5Elu3bsWlS5fQt29fvccZOnQoDhw4gAsXLpjs9QEARA2Tnp4uAIj09HSTPL5WqxVdFu4Vfv/6Raw4cO2p2z98+FCcP39ePHz4UO8xsvMKJLlptdoyvc4HDx4IAGLv3r0lbhMfHy8AiNjY2GL77dmzRwghxJ49ewQA8csvv4jmzZsLlUolQkNDxenTp3X7REVFCUdHR7Ft2zZRv359oVKpRNeuXUVCQoLe8y1btkwEBgYKS0tLERQUJNasWaO3HoBYvny56Nu3r7CxsRHDhw8XAPRuI0aMMPhapk6dKgIDA4sdn1GjRong4GAhhBBpaWli7Nixonbt2sLe3l507txZxMXF6W3/008/ieDgYKFSqYSLi4vo37+/EEKIjh07FstSZPPmzaJx48ZCqVQKPz8/8dlnn+k9pp+fn/jggw/EiBEjhIODgxg+fLjB19CzZ09Rp04dkZWVVWzdgwcP9I7Ttm3bdPenT58u6tevL6ytrUVAQIB47733RH5+vm59XFyc6NSpk7CzsxP29vaidevW4vjx40IIIa5fvy5efPFF4eTkJGxsbETjxo3Fr7/+KoT4+71/8OCB7v+P3/bs2aN77x+3fft20bp1a6FSqURAQICYO3euKCgo0Mv/+Ps8e/bsYq93yZIlomnTprr727ZtEwDE0qVLdcu6d+8uZsyYobv/8OFDYWtrK86ePWvw+BZlnTBhghgzZoxu+c2bN4VKpRIzZswQfn5+uuVz5swRLVq00N3fs2ePeO6554SNjY1wdHQU4eHh4vr16089xk8eo6LHXbNmjfDz8xMODg5i0KBBIiMjQ7dNRkaGGDJkiLCxsREeHh5i0aJFomPHjmLSpEkGX5uh5yly9+5d4erqKoYOHapb9ttvv4l27doJR0dH4ezsLHr37i2uXLmiW//ke92xY0chhBDHjh0TXbt2FS4uLsLBwUF06NBBxMTE6D3fnDlzhI+Pj1AqlcLT01O8/fbbunV5eXli2rRpwsvLS9jY2IjQ0NBif2sev82ZM8fga713756QyWQG3+u9e/eKOnXqiPz8fOHl5SX27dtXLN/j72uRJ//2HT16VAAQkZGRBjM8/jtZkdLS0oSlpaXYuHGjbtnt27eFXC4XO3fuLHG/3r17i1GjRukte+mll8Rrr71W4j7Hjh0TAMSNGzf0lnfq1En8+9//NriPoc/EIsZ8fnOemwp24HIKriRnwU5lgQEh3uV6jIcFGjSe/XsFJyub8/N6wEb59B8LOzs72NnZ4ccff0Tbtm2hUqme6XmnTZuGzz//HB4eHvi///s/9O3bF5cuXYKlpSWAwm8FH374IVavXg2lUonx48dj8ODBOHToEABg27ZtmDRpEiIjI9G1a1f88ssviIiIgLe3Nzp37qx7njlz5mD+/PlYvHgxFAoF/vnPf+Lll1/GxYsX4eDgAGtra4P5Ro8ejUWLFmHfvn3o1KkTACA7Oxvff/89FixYACEEevfuDWdnZ+zYsQOOjo74+uuv0aVLF1y6dAnOzs749ddf8dJLL2HWrFlYu3Yt8vPz8euvvwIAtm7dihYtWuD111/H2LFjdc8bExODgQMHYu7cuRg0aBCio6Mxfvx4uLi4YOTIkbrtPv30U/z73//Ge++9ZzD//fv3sXPnTnz44YewtbUttr60pnp7e3usWrUKXl5eOHPmDMaOHQt7e3tMnz4dQOE3sVatWmH58uVQKBSIi4vTvW9vvfUW8vPzsX//ftja2uL8+fMGZx4NDw/HxYsX0aBBA2zZsgXh4eFwdnbG9evX9bb7/fff8dprr2HJkiVo3749rl69itdffx1A4Xtb5Mn3+UmdOnXCpEmTkJKSAldXV+zbt0/371tvvQW1Wo3o6GhMmTJFt89ff/0FDw8PNGnSpMRjBRT+rHTo0AGff/45bGxssGrVKrzwwgtwdy8+iWcRtVqNfv36YezYsdiwYQPy8/Nx7NgxXatWacfYkKtXr+LHH3/EL7/8ggcPHmDgwIH4+OOP8eGHHwIApk6dikOHDmH79u1wd3fH7NmzcfLkyXL1AXJzc8PQoUOxcuVKaDQaKBQKZGdnY+rUqWjWrBmys7Mxe/Zs9O/fH3FxcZDL5Th27BhCQ0Px559/okmTJlAqC0eTZmZmYsSIEViyZAkAYOHChejVqxcuX74Me3t7bN68GYsXL8bGjRvRpEkTJCUl4dSpU7osERERuH79OjZu3AgvLy9s27YNL7zwAs6cOYPw8HBERkZi9uzZuHjxIgCUOAvuwYMHYWNjY/AU44oVK/Dqq6/C0tISr776KlasWIEOHToYfdzWrVsHOzs7jB8/3uD60n4nmzRpghs3bpS43s/PD+fOnTO4LiYmBgUFBejevbtumZeXF5o2bYro6Gj06NHD4H7/+Mc/8NVXX+HSpUsICgrCqVOncPDgwVJPpaenp0MmkxV7LaGhoThw4ECJ+1WIp5Y/Jvbll18Kf39/oVKpROvWrcX+/ftL3X7v3r1639qWL19u1POZuuVm5Mqjwu9fv4g5Pxn+dvckQ1Vqdl6B8PvXL5LcsvMKSkmrb/PmzaJWrVrCyspKhIeHi5kzZ4pTp07p1hvTcvP4t4jU1FRhbW0tNm3aJIQo/MYIQBw5ckS3zYULFwQAcfToUSGEEOHh4WLs2LF6+QYMGCB69eqluw9ATJ48WW+bx1sPnqZNmzZ6rSIrV64U1tbW4sGDB+Kvv/4SDg4OIjc3V2+funXriq+//loIIURYWJjet9sn+fn5icWLF+stGzJkiOjWrZvesmnTponGjRvr7devX79Ssxd9S9y6dWup2wlRvOXmSQsWLNC1VgkhhL29vVi1apXBbZs1aybmzp1rcN2Tx/7Jnw0hircWtG/fXnz00Ud6j7N27Vrh6empl//J9/lJWq1WuLq6is2bNwshhGjZsqWYP3++cHNzE0IIER0dLSwsLERmZqZun7Fjx4qpU6eW+JiPZ23ZsqVYvXq10Gq1om7duuKnn34SixcvLrHlJjU1tdSW0NKOsaGWGxsbG72WmmnTpok2bdoIIQpbbSwtLcUPP/ygW5+WliZsbGzK1XIjhBDLly8XAMTdu3cNrk9OThYAxJkzZ4QQhv82GKJWq4W9vb34+eefhRBCLFy4UAQFBem1HBa5cuWKkMlk4vbt23rLu3TpImbOnPnU1/C4xYsXi8DAwGLL09PThY2Nja5FNjY2VtjY2Oh9npS15aZnz56iefPmT81iyPXr18Xly5dLvBW1+Bmybt06oVQqiy3v1q2beP3110vcT6vVihkzZgiZTCYsLCyETCYr9rv4uIcPH4rg4GCDf/M+//xz4e/vX+J+1b7lZtOmTZg8eTKWLVuGdu3a4euvv0bPnj1x/vz5YuenASA+Ph69evXC2LFj8d///heHDh3C+PHjUbt2bbz88ssSvAJ91+5lYc/Fe5DJgJHh/uV+HGtLBc7PM1w9m5q1ZdlnhHz55ZfRu3dvHDhwAIcPH8bOnTuxYMECfPfdd3qtCmURFham+7+zszMaNGigd07WwsICISEhuvsNGzaEk5MTLly4gNDQUFy4cEH3Db5Iu3bt8Pnnn+ste/wxjDV69GhMnjwZS5cuhb29PVauXImXXnoJTk5OiImJQVZWVrE+FA8fPsTVq1cBAHFxcXqtMmVx4cKFYn082rVrh8jISN235LK8LiEEgPJNkLV582ZERkbiypUryMrKglqthoODg2791KlTMWbMGKxduxZdu3bFgAEDULduXQDAxIkT8eabb+KPP/5A165d8fLLL6N58+ZGZygSExOD48eP61ogAECj0SA3Nxc5OTm6Wb6fdjxkMhk6dOiAvXv3okuXLjh37hzGjRuHzz77DBcuXMDevXvRunVr3Td7IQR+/vlnbNy4sUw5R40ahaioKPj6+iIrKwu9evXC0qVLS9ze2dkZI0eORI8ePdCtWzd07doVAwcOhKenJ4DSj7Eh/v7+sLf/e5Smp6cnkpOTAQDXrl1DQUEBQkNDdesdHR3RoEGDMr02Q578+bp69Sr+/e9/48iRI0hJSdF1Ok1ISEDTpk1LfJzk5GTMnj0bu3fvxt27d6HRaJCTk6PrzD9gwABERkYiMDAQL7zwAnr16oU+ffrAwsICJ0+ehBACQUFBeo+Zl5dndN+mhw8fwsrKqtjy9evXIzAwEC1atAAAtGzZEoGBgdi4cWOxvz9PI4Qo94R1fn5+5dqvNE/Ls2nTJvz3v//F+vXr0aRJE8TFxWHy5Mnw8vLCiBEj9LYtKCjA4MGDodVqsWzZsmKPZW1tbfJrKUraoXjRokUYPXo0xowZg0aNGiEyMhI+Pj5Yvny5we2/+uor+Pr6IjIyEo0aNcKYMWMwatQofPbZZ5Wc3LDV0dcBAM83cIO/a/Gm/7KSyWSwUVpIcjP2l83KygrdunXD7NmzER0djZEjR+pODxR1xCv6wwcY17nzySyGsj2+7Mn1hn5ZDZ2SKavBgwdDJpNh06ZNuHLlCg4ePKjrSKzVauHp6Ym4uDi928WLFzFt2jQAKPGUV2kMvYbHj2eRp72u+vXrQyaTGd2J78iRIxg8eDB69uyJX375BbGxsZg1a5Zep+W5c+fi3Llz6N27N3bv3o3GjRtj27ZtAIAxY8bg2rVrGDZsGM6cOYOQkBBdB8Ty0Gq1eP/99/WO8ZkzZ3D58mW9D6OyvM+dOnXC3r17ceDAAbRo0QJOTk7o0KED9u3bh7179+pOPwLAsWPHkJ+fj3/84x9lyjl06FAcOXIEc+fOxfDhw2Fh8fTvkVFRUTh8+DDCw8OxadMmBAUF4ciRIwBKP8aGPHnKSiaT6QqMkgpdQz9XZXXhwgU4ODjoiog+ffogNTUV3377LY4ePYqjR48CKLmze5GRI0ciJiYGkZGRiI6ORlxcHFxcXHT7+fj44OLFi/jyyy9hbW2N8ePHo0OHDigoKIBWq4VCoUBMTIzez8eFCxeKfcl5GldXVzx48KDY8pUrV+LcuXOwsLDQ3c6dO4cVK1botnFwcDA4eqxoRGbR6MCgoCBcvXq1XB3emzRpousaYOhW2qlTDw8P5OfnF3t9ycnJpZ46nTZtGmbMmIHBgwejWbNmGDZsGKZMmYL58+frbVdQUICBAwciPj4eu3bt0vsiVOT+/fuoXbu2ka/aOJIVN/n5+YiJidE77wcA3bt3R3R0tMF9Dh8+XGz7Hj164MSJEyX+gOTl5SEjI0PvZgoZuQXYHFM4lK6kq3/XBI0bN0Z2djYA6H54ExMTdetLGvpZ9EccAB48eIBLly6hYcOGumVqtRonTpzQ3b948SLS0tJ02zRq1AgHDx7Ue8zo6GiD58wfV3Suv7RRAkXs7e0xYMAAREVFYeXKlQgMDNR9ALZu3RpJSUmwsLBAvXr19G6urq4AgObNm5c69FepVBbL0bhxY4OvKygoyKjrrjg7O6NHjx748ssvde/P40oaCn/o0CH4+flh1qxZCAkJQf369Q2e6w8KCsKUKVPwxx9/4KWXXkJUVJRunY+PD8aNG4etW7finXfewbffflvm3E9q3bo1Ll68WOwY16tXz+hLmHTq1Annzp3D5s2bde9jx44d8eeffyI6OhodO3bUbfvTTz+hd+/eZT7mzs7O6Nu3L/bt24dRo0aVOVOrVq0wc+ZMREdHo2nTpnpzrJR2jI1Rt25dWFpa4tixY7plGRkZuHz5crkeLzk5GevXr0e/fv0gl8uRmpqKCxcu4L333kOXLl3QqFGjYh+kJf3eHThwABMnTkSvXr3QpEkTqFQqpKSk6G1jbW2Nvn37YsmSJdi7dy8OHz6MM2fOoFWrVtBoNEhOTi72s+Hh4aF73rL8rrdq1QpJSUl6uc+cOYMTJ05g7969esXT/v37cfz4cZw9exZAYavyrVu3kJSUpPeYx48fh1wuR7169QAAQ4YMQVZWlsGWDaDk30kA2LFjR7EvUo/fduzYUeK+wcHBsLS0xK5du3TLEhMTcfbsWYSHh5e4X05OTrHfMYVCoSuagb8Lm8uXL+PPP/8sscXs7NmzaNWqVYnPVREkOy2VkpICjUZTrFJ0d3cv9kNRJCkpyeD2arUaKSkpuibcx82fPx/vv/9+xQUvQUJqDlztVahjIUe7es8+vLOqS01NxYABAzBq1Cg0b94c9vb2OHHiBBYsWKA7jWJtbY22bdvi448/hr+/P1JSUkrs8Dpv3jy4uLjA3d0ds2bNgqurK/r166dbb2lpibfffhtLliyBpaUlJkyYgLZt2+qa1qdNm4aBAweidevW6NKlC37++Wds3boVf/75Z6mvw8/PDzKZDL/88gt69eoFa2vrEjsZAoWnptq3b4/z58/j3Xff1X377dq1K8LCwtCvXz988sknaNCgAe7cuYMdO3agX79+CAkJwZw5c9ClSxfUrVsXgwcPhlqtxm+//abrmOvv74/9+/dj8ODBUKlUcHV1xTvvvIPnnntON+fL4cOHsXTp0hL/IJZm2bJlCA8PR2hoKObNm4fmzZtDrVZj165dWL58ucFWnXr16iEhIQEbN27Ec889h19//VWvxeDhw4eYNm0aXnnlFQQEBODWrVs4fvy47jTx5MmT0bNnTwQFBeHBgwfYvXv3UwvO0syePRsvvvgifHx8MGDAAMjlcpw+fRpnzpzBf/7zH6Meq2nTpnBxccG6devw008/ASgseN555x0A0Gul2b59u9F/R1atWoVly5aV6ZRIfHw8vvnmG/Tt2xdeXl64ePEiLl26hOHDhz/1GBvL3t4eI0aMwLRp0+Ds7Aw3NzfMmTMHcrn8qS23QggkJSVBCIG0tDQcPnwYH330ERwdHXVzvtSqVQsuLi745ptv4OnpiYSEBMyYMUPvcdzc3GBtbY2dO3fC29sbVlZWcHR0RL169bB27VqEhIQgIyMD06ZN02vxXLVqFTQaDdq0aQMbGxusXbsW1tbW8PPzg4uLC4YOHYrhw4dj4cKFaNWqFVJSUrB79240a9YMvXr1gr+/P7KysvDXX3+hRYsWsLGxMXjB4latWqF27do4dOgQXnzxRQCFHYlDQ0MNdh4OCwvDihUrsHjxYnTv3h2NGjXC4MGD8eGHH8LLywunT5/Gu+++i3HjxulOF7Zp0wbTp0/HO++8g9u3b6N///7w8vLClStX8NVXX+Ef//gHJk2aZPB9eJbTUo6Ojhg9ejTeeecduLi4wNnZGe+++y6aNWuGrl276rbr0qUL+vfvjwkTJgAobI378MMP4evriyZNmiA2NhaLFi3SFe9qtRqvvPIKTp48iV9++QUajUb3We7s7KwraIHCIvaDDz4o92sok6f2yjGR27dvCwAiOjpab/l//vMf0aBBA4P71K9fv1gHpoMHDwoAIjEx0eA+ubm5Ij09XXe7efOmyToUazRacSctx6h9Sus8VZXl5uaKGTNmiNatWwtHR0dhY2MjGjRoIN577z2Rk/P3MTh//rxo27atsLa2Fi1bthR//PGHwQ7FP//8s2jSpIlQKpXiueee0xtCXdQJcMuWLSIwMFAolUrx/PPPF+s0V5ah4IY6ys6bN094eHgImUxW4lDwxzVo0EDI5XJx8+ZNveUZGRni7bffFl5eXsLS0lL4+PiIoUOH6g1Z37Jli2jZsqVQKpXC1dVVvPTSS7p1hw8f1g2Hf/xXs2gouKWlpfD19RWffvqp3vMa6ohckjt37oi33npL+Pn5CaVSKerUqSP69u2r14n3yeM0bdo04eLiIuzs7MSgQYPE4sWLdZ0y8/LyxODBg3VDc728vMSECRN0P88TJkwQdevWFSqVStSuXVsMGzZMpKSkCCHK16FYCCF27twpwsPDhbW1tXBwcBChoaHim2++KTF/aV5++WWhUCh0fw+0Wq1wdnYWISEhum2uXLkiVCqVXudiQ57WWbW0DsVJSUmiX79+wtPTUzfkf/bs2UKj0Tz1GJc0FLy05zY0FDw0NFRv6Luh14dHQ6hlMplwdHQUoaGhYt68ecX+nu7atUs0atRIqFQq0bx5c7F3795i78u3334rfHx8hFwu1w0FP3nypAgJCREqlUrUr19f/PDDD3o/39u2bRNt2rQRDg4OwtbWVrRt21b8+eefusfMz88Xs2fPFv7+/sLS0lJ4eHiI/v37600tMW7cOOHi4lLqUHAhhJgxY4YYPHiwEKLw59zFxUUsWLDA4LYLFy4Urq6uIi8vTwghRGJiooiIiBB+fn7C2tpaNGzYUMybN6/YgAMhhNi0aZPo0KGDsLe3F7a2tqJ58+Zi3rx5JhsKLkTh586ECROEs7OzsLa2Fi+++GKxqTX8/Pz0jk9GRoaYNGmS8PX1FVZWViIwMFDMmjVL95qLOokbuj3+Ox0dHS2cnJz0PieezFYRHYolK27y8vKEQqEoNnpj4sSJokOHDgb3ad++vZg4caLesq1btwoLCwuDvecNMfVoKWNV1+KmopRltFJZRzgQmcLChQtFz549pY5hUllZWcLR0VF89913UkepMpKSkoSLi0upI4/IeK+88or48MMPS1xfUcWNZH1ulEolgoOD9c77AcCuXbtKPO8XFhZWbPs//vgDISEhpc75QERUXt7e3pg5c6bUMSpUbGwsNmzYgKtXr+LkyZMYOnQoAJQ6+3JN4+7ujhUrVpR42RUyXl5eHlq0aKE3f5SpSDoUfOrUqRg2bBhCQkIQFhaGb775BgkJCRg3bhwAYObMmbh9+zbWrFkDABg3bhyWLl2KqVOnYuzYsTh8+DBWrFiBDRs2SPkyiMiMDRw4UOoIJvHZZ5/h4sWLui+aBw4c0HV+p0Is9iqWSqUqsd9lRZO0uBk0aBBSU1Mxb948JCYmomnTptixY4eus1RiYqJe1RwQEIAdO3ZgypQp+PLLL+Hl5YUlS5ZUiTluqHw6der01CGoI0eONHreHCIqWatWrSrlysxEUpGJp32ymJmMjAw4OjoiPT3d4Pj7ypabm4v4+HgEBAQYnDSKiIiopijtM9GYz29eFbyKqGE1JhERUTEV9VnI4kZij18YkoiIqCYrmo3amElKDeFVwSWmUCjg5OSku+6LjY1Nua83QkREVF1ptVrcu3cPNjY2ZbpkSWlY3FQBRVODFxU4RERENZFcLoevr+8zf8lncVMFyGQyeHp6ws3NrVwXUSMiIjIHSqXS6OvEGcLipgpRKBTPfJ6RiIiopmOHYiIiIjIrLG6IiIjIrLC4ISIiIrNS4/rcFE0QlJGRIXESIiIiKquiz+2yTPRX44qbzMxMAICPj4/ESYiIiMhYmZmZcHR0LHWbGndtKa1Wizt37sDe3r7CJ8vLyMiAj48Pbt68WSWuW2WueJwrB49z5eBxrjw81pXDVMdZCIHMzEx4eXk9dbh4jWu5kcvl8Pb2NulzODg48BenEvA4Vw4e58rB41x5eKwrhymO89NabIqwQzERERGZFRY3REREZFZY3FQglUqFOXPmQKVSSR3FrPE4Vw4e58rB41x5eKwrR1U4zjWuQzERERGZN7bcEBERkVlhcUNERERmhcUNERERmRUWN0RERGRWWNwYadmyZQgICICVlRWCg4Nx4MCBUrfft28fgoODYWVlhcDAQHz11VeVlLR6M+Y4b926Fd26dUPt2rXh4OCAsLAw/P7775WYtvoy9ue5yKFDh2BhYYGWLVuaNqCZMPY45+XlYdasWfDz84NKpULdunWxcuXKSkpbfRl7nNetW4cWLVrAxsYGnp6eiIiIQGpqaiWlrZ7279+PPn36wMvLCzKZDD/++ONT95Hkc1BQmW3cuFFYWlqKb7/9Vpw/f15MmjRJ2Nraihs3bhjc/tq1a8LGxkZMmjRJnD9/Xnz77bfC0tJSbN68uZKTVy/GHudJkyaJTz75RBw7dkxcunRJzJw5U1haWoqTJ09WcvLqxdjjXCQtLU0EBgaK7t27ixYtWlRO2GqsPMe5b9++ok2bNmLXrl0iPj5eHD16VBw6dKgSU1c/xh7nAwcOCLlcLj7//HNx7do1ceDAAdGkSRPRr1+/Sk5evezYsUPMmjVLbNmyRQAQ27ZtK3V7qT4HWdwYITQ0VIwbN05vWcOGDcWMGTMMbj99+nTRsGFDvWVvvPGGaNu2rckymgNjj7MhjRs3Fu+//35FRzMr5T3OgwYNEu+9956YM2cOi5syMPY4//bbb8LR0VGkpqZWRjyzYexx/vTTT0VgYKDesiVLlghvb2+TZTQ3ZSlupPoc5GmpMsrPz0dMTAy6d++ut7x79+6Ijo42uM/hw4eLbd+jRw+cOHECBQUFJstanZXnOD9Jq9UiMzMTzs7OpohoFsp7nKOionD16lXMmTPH1BHNQnmO8/bt2xESEoIFCxagTp06CAoKwrvvvouHDx9WRuRqqTzHOTw8HLdu3cKOHTsghMDdu3exefNm9O7duzIi1xhSfQ7WuAtnlldKSgo0Gg3c3d31lru7uyMpKcngPklJSQa3V6vVSElJgaenp8nyVlflOc5PWrhwIbKzszFw4EBTRDQL5TnOly9fxowZM3DgwAFYWPBPR1mU5zhfu3YNBw8ehJWVFbZt24aUlBSMHz8e9+/fZ7+bEpTnOIeHh2PdunUYNGgQcnNzoVar0bdvX3zxxReVEbnGkOpzkC03RpLJZHr3hRDFlj1te0PLSZ+xx7nIhg0bMHfuXGzatAlubm6mimc2ynqcNRoNhgwZgvfffx9BQUGVFc9sGPPzrNVqIZPJsG7dOoSGhqJXr15YtGgRVq1axdabpzDmOJ8/fx4TJ07E7NmzERMTg507dyI+Ph7jxo2rjKg1ihSfg/z6VUaurq5QKBTFvgUkJycXq0qLeHh4GNzewsICLi4uJstanZXnOBfZtGkTRo8ejR9++AFdu3Y1Zcxqz9jjnJmZiRMnTiA2NhYTJkwAUPghLISAhYUF/vjjDzz//POVkr06Kc/Ps6enJ+rUqQNHR0fdskaNGkEIgVu3bqF+/fomzVwdlec4z58/H+3atcO0adMAAM2bN4etrS3at2+P//znP2xZryBSfQ6y5aaMlEolgoODsWvXLr3lu3btQnh4uMF9wsLCim3/xx9/ICQkBJaWlibLWp2V5zgDhS02I0eOxPr163nOvAyMPc4ODg44c+YM4uLidLdx48ahQYMGiIuLQ5s2bSorerVSnp/ndu3a4c6dO8jKytItu3TpEuRyOby9vU2at7oqz3HOycmBXK7/EahQKAD83bJAz06yz0GTdlc2M0VDDVesWCHOnz8vJk+eLGxtbcX169eFEELMmDFDDBs2TLd90RC4KVOmiPPnz4sVK1ZwKHgZGHuc169fLywsLMSXX34pEhMTdbe0tDSpXkK1YOxxfhJHS5WNscc5MzNTeHt7i1deeUWcO3dO7Nu3T9SvX1+MGTNGqpdQLRh7nKOiooSFhYVYtmyZuHr1qjh48KAICQkRoaGhUr2EaiEzM1PExsaK2NhYAUAsWrRIxMbG6obcV5XPQRY3Rvryyy+Fn5+fUCqVonXr1mLfvn26dSNGjBAdO3bU237v3r2iVatWQqlUCn9/f7F8+fJKTlw9GXOcO3bsKAAUu40YMaLyg1czxv48P47FTdkZe5wvXLggunbtKqytrYW3t7eYOnWqyMnJqeTU1Y+xx3nJkiWicePGwtraWnh6eoqhQ4eKW7duVXLq6mXPnj2l/r2tKp+DMiHY/kZERETmg31uiIiIyKywuCEiIiKzwuKGiIiIzAqLGyIiIjIrLG6IiIjIrLC4ISIiIrPC4oaIiIjMCosbIiIiMissbohIz6pVq+Dk5CR1jHLz9/dHZGRkqdvMnTsXLVu2rJQ8RFT5WNwQmaGRI0dCJpMVu125ckXqaFi1apVeJk9PTwwcOBDx8fEV8vjHjx/H66+/rrsvk8nw448/6m3z7rvv4q+//qqQ5yvJk6/T3d0dffr0wblz54x+nOpcbBJJgcUNkZl64YUXkJiYqHcLCAiQOhaAwquMJyYm4s6dO1i/fj3i4uLQt29faDSaZ37s2rVrw8bGptRt7Ozs4OLi8szP9TSPv85ff/0V2dnZ6N27N/Lz803+3EQ1GYsbIjOlUqng4eGhd1MoFFi0aBGaNWsGW1tb+Pj4YPz48cjKyirxcU6dOoXOnTvD3t4eDg4OCA4OxokTJ3Tro6Oj0aFDB1hbW8PHxwcTJ05EdnZ2qdlkMhk8PDzg6emJzp07Y86cOTh79qyuZWn58uWoW7culEolGjRogLVr1+rtP3fuXPj6+kKlUsHLywsTJ07UrXv8tJS/vz8AoH///pDJZLr7j5+W+v3332FlZYW0tDS955g4cSI6duxYYa8zJCQEU6ZMwY0bN3Dx4kXdNqW9H3v37kVERATS09N1LUBz584FAOTn52P69OmoU6cObG1t0aZNG+zdu7fUPEQ1BYsbohpGLpdjyZIlOHv2LFavXo3du3dj+vTpJW4/dOhQeHt74/jx44iJicGMGTNgaWkJADhz5gx69OiBl156CadPn8amTZtw8OBBTJgwwahM1tbWAICCggJs27YNkyZNwjvvvIOzZ8/ijTfeQEREBPbs2QMA2Lx5MxYvXoyvv/4aly9fxo8//ohmzZoZfNzjx48DAKKiopCYmKi7/7iuXbvCyckJW7Zs0S3TaDT4/vvvMXTo0Ap7nWlpaVi/fj0A6I4fUPr7ER4ejsjISF0LUGJiIt59910AQEREBA4dOoSNGzfi9OnTGDBgAF544QVcvny5zJmIzJbJrztORJVuxIgRQqFQCFtbW93tlVdeMbjt999/L1xcXHT3o6KihKOjo+6+vb29WLVqlcF9hw0bJl5//XW9ZQcOHBByuVw8fPjQ4D5PPv7NmzdF27Zthbe3t8jLyxPh4eFi7NixevsMGDBA9OrVSwghxMKFC0VQUJDIz883+Ph+fn5i8eLFuvsAxLZt2/S2mTNnjmjRooXu/sSJE8Xzzz+vu//7778LpVIp7t+//0yvE4CwtbUVNjY2AoAAIPr27Wtw+yJPez+EEOLKlStCJpOJ27dv6y3v0qWLmDlzZqmPT1QTWEhbWhGRqXTu3BnLly/X3be1tQUA7NmzBx999BHOnz+PjIwMqNVq5ObmIjs7W7fN46ZOnYoxY8Zg7dq16Nq1KwYMGIC6desCAGJiYnDlyhWsW7dOt70QAlqtFvHx8WjUqJHBbOnp6bCzs4MQAjk5OWjdujW2bt0KpVKJCxcu6HUIBoB27drh888/BwAMGDAAkZGRCAwMxAsvvIBevXqhT58+sLAo/5+zoUOHIiwsDHfu3IGXlxfWrVuHXr16oVatWs/0Ou3t7XHy5Emo1Wrs27cPn376Kb766iu9bYx9PwDg5MmTEEIgKChIb3leXl6l9CUiqupY3BCZKVtbW9SrV09v2Y0bN9CrVy+MGzcOH3zwAZydnXHw4EGMHj0aBQUFBh9n7ty5GDJkCH799Vf89ttvmDNnDjZu3Ij+/ftDq9XijTfe0OvzUsTX17fEbEUf+nK5HO7u7sU+xGUymd59IYRumY+PDy5evIhdu3bhzz//xPjx4/Hpp59i3759eqd7jBEaGoq6deti48aNePPNN7Ft2zZERUXp1pf3dcrlct170LBhQyQlJWHQoEHYv38/gPK9H0V5FAoFYmJioFAo9NbZ2dkZ9dqJzBGLG6Ia5MSJE1Cr1Vi4cCHk8sIud99///1T9wsKCkJQUBCmTJmCV199FVFRUejfvz9at26Nc+fOFSuinubxD/0nNWrUCAcPHsTw4cN1y6Kjo/VaR6ytrdG3b1/07dsXb731Fho2bIgzZ86gdevWxR7P0tKyTKOwhgwZgnXr1sHb2xtyuRy9e/fWrSvv63zSlClTsGjRImzbtg39+/cv0/uhVCqL5W/VqhU0Gg2Sk5PRvn37Z8pEZI7YoZioBqlbty7UajW++OILXLt2DWvXri12muRxDx8+xIQJE7B3717cuHEDhw4dwvHjx3WFxr/+9S8cPnwYb731FuLi4nD58mVs374db7/9drkzTps2DatWrcJXX32Fy5cvY9GiRdi6dauuI+2qVauwYsUKnD17VvcarK2t4efnZ/Dx/P398ddffyEpKQkPHjwo8XmHDh2KkydP4sMPP8Qrr7wCKysr3bqKep0ODg4YM2YM5syZAyFEmd4Pf39/ZGVl4a+//kJKSgpycnIQFBSEoUOHYvjw4di6dSvi4+Nx/PhxfPLJJ9ixY4dRmYjMkpQdfojINEaMGCH++c9/Gly3aNEi4enpKaytrUWPHj3EmjVrBADx4MEDIYR+B9a8vDwxePBg4ePjI5RKpfDy8hITJkzQ60R77Ngx0a1bN2FnZydsbW1F8+bNxYcfflhiNkMdZJ+0bNkyERgYKCwtLUVQUJBYs2aNbt22bdtEmzZthIODg7C1tRVt27YVf/75p279kx2Kt2/fLurVqycsLCyEn5+fEKJ4h+Iizz33nAAgdu/eXWxdRb3OGzduCAsLC7Fp0yYhxNPfDyGEGDdunHBxcREAxJw5c4QQQuTn54vZs2cLf39/YWlpKTw8PET//v3F6dOnS8xEVFPIhBBC2vKKiIiIqOLwtBQRERGZFRY3REREZFZY3BAREZFZYXFDREREZoXFDREREZkVFjdERERkVljcEBERkVlhcUNERERmhcUNERERmRUWN0RERGRWWNwQERGRWfl/ajwxjOwQZnsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8231389827780461\n"
     ]
    }
   ],
   "source": [
    "roc_auc_svc_missing = roccurveplot(y_test_missing,y_pred_svc_missing, 'Support Vector Classifier w/ Missing Dataset')\n",
    "print(roc_auc_svc_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8295664645966631\n"
     ]
    }
   ],
   "source": [
    "roc_auc_svc_dropped = roccurveplot(y_test_dropped,y_pred_svc_dropped, 'Support Vector Classifier w/ Dropped Dataset')\n",
    "print(roc_auc_svc_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8972845914480391\n"
     ]
    }
   ],
   "source": [
    "roc_auc_adaBoost_randForest_missing = roccurveplot(y_test_missing,y_pred_adaBoost_decision_missing, 'AdaBoost RandomForest w/ Missing Dataset')\n",
    "print(roc_auc_adaBoost_randForest_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfuklEQVR4nO3deVxU9f4/8NcMMMMOsgqKiAsqmqaQil5zuQpq3yxb1CxTU2+m5VZZ5v25dCvvbTGtXFpMr2Vqi7ZcTcV9LRUhF3BHcQFZVECQbeb9+4NmYliUQYbDDK/n4zGPB/OZc868z2dmznlzPp/P+ahEREBERERkI9RKB0BERERUk5jcEBERkU1hckNEREQ2hckNERER2RQmN0RERGRTmNwQERGRTWFyQ0RERDbFXukAapter8fVq1fh5uYGlUqldDhERERUBSKCnJwcBAYGQq2+87WZepfcXL16FUFBQUqHQURERNVw6dIlNG7c+I7L1Lvkxs3NDUBJ5bi7uyscDREREVVFdnY2goKCjOfxO6l3yY2hKcrd3Z3JDRERkZWpSpcSdigmIiIim8LkhoiIiGwKkxsiIiKyKUxuiIiIyKYwuSEiIiKbwuSGiIiIbAqTGyIiIrIpTG6IiIjIpjC5ISIiIpvC5IaIiIhsiqLJze7du/Hwww8jMDAQKpUKP/74413X2bVrF8LDw+Ho6IhmzZph6dKllg+UiIiIrIaiyU1ubi46dOiATz75pErLJyUlYeDAgejRowfi4uLwxhtvYNKkSfjhhx8sHCkRERFZC0UnzhwwYAAGDBhQ5eWXLl2KJk2aYMGCBQCANm3a4PDhw3j//ffx+OOPWyhKIiIi2yAiEAH0IhAAIoCgpMzk7z+X1QuAMuV6EeOyKFdWsp5apUKgp5Ni+2lVs4IfOHAAUVFRJmXR0dFYtmwZioqK4ODgUG6dgoICFBQUGJ9nZ2dbPE4iunciFR9MRUpeL3swFQCiL39wNqwPY1nJMnr56z3wZ/lfB/xSB/4KTgL6MuuZbO/PMhjLqri9klVKld1he2X3scwJqtLtlVoPUj7mCreHsvt7l+2ZxFY6LtNy/Z8bLP3++jL7W7quK9xeqbj/+ozL10O57d1pf0t9npWf1Es2YFJ2h/oz3fcy+1u6Xst9r0t/7mW+Q5VtD5XHXZv83LQ4OLNv7b5pKVaV3KSmpsLf39+kzN/fH8XFxcjIyEBAQEC5debNm4e5c+fWVohkJWIvXseOk+nl/ntBlQ+mf51ogL8ObhVtr/wBS0wPeGVOQIDpsvpSfwOmB8W7HUD1fwZjEre+ku2VWg9Sfl+Msd6tHkrvb1W2V8H+EpF1UKkAtUoF1Z9/q6ACVIDWQdnxSlaV3ACASqUyeW44QJctN5gxYwamTZtmfJ6dnY2goCDLBUh1XmGxHmP+exg384qUDoVqSclBt+Q4YTwIlzkglz5Io9Ty6jLLwrjsX+sZt60y3Z4KJdtEmfdX/7ksSpepTdeD4b2rELchdpRZVn23uE3+Lr3d0nGb7othvTvVA0ovWzrucsuWrvOK4650e5XFrb7L9vDX51F6fyre3l/7iDL7qFbdYXsVfS9K/V3R9sp/L0q2U/Z7YRqj6T6izOul68zwWZb9Pquggkpd/ntmsr0KvwMqM36Btc+qkpuGDRsiNTXVpCwtLQ329vbw9vaucB2tVgutVlsb4ZGV+O18Jm7mFcHT2QGDOzYy/eEaD4ymB7aKTzpVP3iXrFbmYFtqe2UPRJUebO+0PZQ96Ziud6eDd0UHZMPBu/RBuOzBG6j4BFHpyaDUeqb1YHoQ/iv2Mif1qpwMytVZ3T4IE1HNs6rkJjIyEr/88otJ2ZYtWxAREVFhfxuiimw6UZIgD7wvALMfbqtwNEREVNMUbRS7desW4uPjER8fD6BkqHd8fDySk5MBlDQpPfvss8blx48fj4sXL2LatGlITEzEl19+iWXLluGVV15RInyyQjq9YMuJawCA/m0bKhwNERFZgqJXbg4fPozevXsbnxv6xowcORIrVqxASkqKMdEBgJCQEGzcuBFTp07FokWLEBgYiI8++ojDwKnKjiTfQMatArg72qNrs4qbMomIyLopmtz06tXL2CG4IitWrChX1rNnTxw5csSCUZEt+/VYSZNU3zb+0Nhz9hEiIlvEozvVGyKCzX/2t4luxyYpIiJbxeSG6o3jV7Jx5eZtODnY4cGWvkqHQ0REFsLkhuqNTSdSAAC9WvnCSWOncDRERGQpTG6o3th0vKRJqj+bpIiIbBqTG6oXzqbl4Fx6LjR2avRp7ad0OEREZEFMbqheMFy16d7CG26OvOEjEZEtY3JD9YLhrsRskiIisn1MbsjmXbqeh+NXsqFWldzfhoiIbBuTG7J5hnvbdA7xgrcrJ1ElIrJ1TG7I5hmSG84lRURUPzC5IZuWlpOPwxdvAACimNwQEdULTG7IpsUkXIMI0CHIE4GeTkqHQ0REtYDJDdk04437eNWGiKjeYHJDNisrrwgHzmUCAKLbcpQUEVF9weSGbNa2k9dQrBe08ndDM19XpcMhIqJawuSGbJahSSqaN+4jIqpXmNyQTcotKMau0+kA2N+GiKi+YXJDNmnX6XQUFOvRxMsZbQLclA6HiIhqEZMbsknGUVLtGkKlUikcDRER1SYmN2RzCop12H4yDQAQzSYpIqJ6h8kN2Zz9ZzNxq6AY/u5adAzyVDocIiKqZUxuyOYYR0m1bQi1mk1SRET1DZMbsinFOj1iEq8B4CgpIqL6iskN2ZRDF27gem4hPJ0d0DnES+lwiIhIAUxuyKZsPlHSJNWvjT/s7fj1JiKqj3j0J5uh14vJEHAiIqqfmNyQzTh6JQup2flw0dihewsfpcMhIiKFMLkhm2G4atO7tR8cHewUjoaIiJTC5IZsgohg0/EUAGySIiKq75jckE04fe0WLmTmQWOvRq9WfkqHQ0RECmJyQzbB0CT1YEsfuGrtFY6GiIiUxOSGbMKmE3/dlZiIiOo3Jjdk9S5m5iIxJRt2ahX6tvFXOhwiIlIYkxuyeoYb93Vt5oUGLhqFoyEiIqUxuSGr96vhxn1skiIiIjC5ISuXmpWPuOSbAIAoJjdERAQmN2TltiSUXLXp1MQT/u6OCkdDRER1AZMbsmqcS4qIiMpickNW63puIX5Pug4A6N82QOFoiIiormByQ1Zra+I16PSCsAB3NPF2VjocIiKqI5jckNXazCYpIiKqAJMbskq3Coqx50wGACY3RERkiskNWaUdJ9NQqNOjmY8LWvq5Kh0OERHVIUxuyCoZ55Jq1xAqlUrhaIiIqC5hckNWJ79Ihx0n0wDwrsRERFQekxuyOnvPZCCvUIcAD0e0b+yhdDhERFTHMLkhq2NskmrLJikiIiqPyQ1ZlSKdHlsTrwHgKCkiIqoYkxuyKgeTruNmXhG8XTR4oKmX0uEQEVEdxOSGrIphLql+Yf6wU7NJioiIymNyQ1ZDrxdsLjUEnIiIqCJMbshqxF26gbScArhp7dGtubfS4RARUR3F5IashqFJqk8bP2jt7RSOhoiI6iomN2QVRMQ4BJw37iMiojthckNWISElG5eu34ajgxo9W/kqHQ4REdVhTG7IKmz+s0mqZ6gvnDX2CkdDRER1GZMbsgrGJimOkiIiortQPLlZvHgxQkJC4OjoiPDwcOzZs+eOy69atQodOnSAs7MzAgICMHr0aGRmZtZStKSEc+m3cPraLdirVejT2l/pcIiIqI5TNLlZu3YtpkyZgpkzZyIuLg49evTAgAEDkJycXOHye/fuxbPPPosxY8bgxIkT+O6773Do0CGMHTu2liOn2mS4t023Fj7wcHJQOBoiIqrrFE1u5s+fjzFjxmDs2LFo06YNFixYgKCgICxZsqTC5X/77Tc0bdoUkyZNQkhICP72t7/h+eefx+HDhyt9j4KCAmRnZ5s8yLoY+ttwlBQREVWFYslNYWEhYmNjERUVZVIeFRWF/fv3V7hOt27dcPnyZWzcuBEigmvXruH777/HQw89VOn7zJs3Dx4eHsZHUFBQje4HWdaVm7fxx+UsqFQlUy4QERHdjWLJTUZGBnQ6Hfz9TU9Y/v7+SE1NrXCdbt26YdWqVRg6dCg0Gg0aNmwIT09PfPzxx5W+z4wZM5CVlWV8XLp0qUb3gyxry59NUg8Ee8HXTatwNEREZA0U71CsUplOfigi5coMEhISMGnSJMyaNQuxsbHYtGkTkpKSMH78+Eq3r9Vq4e7ubvIg62G4KzHnkiIioqpS7IYhPj4+sLOzK3eVJi0trdzVHIN58+ahe/fuePXVVwEA7du3h4uLC3r06IG33noLAQEBFo+bak/GrQIcunAdABDdlk1SRERUNYpdudFoNAgPD0dMTIxJeUxMDLp161bhOnl5eVCrTUO2syuZY0hELBMoKWZrwjXoBbivkQcaN3BWOhwiIrISijZLTZs2DV988QW+/PJLJCYmYurUqUhOTjY2M82YMQPPPvuscfmHH34Y69atw5IlS3D+/Hns27cPkyZNQufOnREYGKjUbpCF8MZ9RERUHYrex37o0KHIzMzEm2++iZSUFLRr1w4bN25EcHAwACAlJcXknjejRo1CTk4OPvnkE7z88svw9PREnz598J///EepXSALyc4vwr6zGQCAaA4BJyIiM6iknrXnZGdnw8PDA1lZWexcXIf9FH8Fk9fEo4WfK7ZO66l0OEREpDBzzt+Kj5Yiqsivx3jjPiIiqh4mN1Tn3C7UYefpNADsb0NEROZjckN1zq7T6cgv0qORpxPaBrLpkIiIzMPkhuqczaVGSVV2Q0ciIqLKMLmhOqWwWI+tidcAAAPYJEVERNXA5IbqlAPnM5GTXwxfNy06NWmgdDhERGSFmNxQnWKYSyoqzB9qNZukiIjIfExuqM7Q6QUxCbwrMRER3RsmN1RnxF68gYxbhXB3tEfXZt5Kh0NERFaKyQ3VGYYmqb5h/nCw41eTiIiqh2cQqhNE5K8h4LwrMRER3QMmN1QnHL+SjSs3b8PJwQ4PhvoqHQ4REVkxJjdUJ2w6kQIA6N3aF44OdgpHQ0RE1ozJDdUJhv420WySIiKie8TkhhR3Ni0H59JzobFTo09rP6XDISIiK8fkhhRnuGrTvYU33BwdFI6GiIisHZMbUtymE7xxHxER1RwmN6SoS9fzcPxKNtQqoG8bf6XDISIiG8DkhhRluLdN5xAveLtqFY6GiIhsAZMbUpShvw1v3EdERDWFyQ0pJi07H7HJNwAA0exvQ0RENYTJDSlmS8I1iAD3B3kiwMNJ6XCIiMhGMLkhxWzmKCkiIrIAJjekiJt5hThwLhMA70pMREQ1i8kNKWJbYhqK9YLWDd0Q4uOidDhERGRDmNyQIgw37uNVGyIiqmlMbqjW5RYUY/fpdADsb0NERDWPyQ3Vul2n01FQrEewtzNaN3RTOhwiIrIxTG6o1pW+cZ9KpVI4GiIisjVMbqhWFRTrsP1kGgDeuI+IiCyDyQ3Vqv1nM3GroBj+7lrc39hT6XCIiMgGMbmhWmVokopu2xBqNZukiIio5jG5oVpTrNMjJvEaAE6USUREllOt5Ka4uBhbt27Fp59+ipycHADA1atXcevWrRoNjmzLoQs3cD23EJ7ODugc4qV0OEREZKPszV3h4sWL6N+/P5KTk1FQUIB+/frBzc0N7777LvLz87F06VJLxEk2wDCXVL82/rC340VDIiKyDLPPMJMnT0ZERARu3LgBJ6e/ZnIePHgwtm3bVqPBke3Q6+WvIeAcJUVERBZk9pWbvXv3Yt++fdBoNCblwcHBuHLlSo0FRrblj8s3kZqdDxeNHbq38FE6HCIismFmX7nR6/XQ6XTlyi9fvgw3N95tlipmmEuqd2s/ODrYKRwNERHZMrOTm379+mHBggXG5yqVCrdu3cLs2bMxcODAmoyNbISIYPOfTVID2gUoHA0REdk6s5ulPvzwQ/Tu3RthYWHIz8/H8OHDcebMGfj4+GD16tWWiJGs3KlrObiQmQeNvRq9WvkqHQ4REdk4s5ObwMBAxMfHY82aNYiNjYVer8eYMWPw9NNPm3QwJjIwdCR+sKUvXLRmf+WIiIjMYvaZZvfu3ejWrRtGjx6N0aNHG8uLi4uxe/duPPjggzUaIFk/jpIiIqLaZHafm969e+P69evlyrOystC7d+8aCYpsx4WMXJxMzYGdWoW+bfyUDoeIiOoBs5MbEYFKVX5OoMzMTLi4uNRIUGQ7DDfui2zmDU9nzV2WJiIiundVbpZ67LHHAJSMjho1ahS0Wq3xNZ1Oh6NHj6Jbt241HyFZNcMQ8Gg2SRERUS2pcnLj4eEBoOTKjZubm0nnYY1Gg65du2LcuHE1HyFZrdSsfMQl34RKBUSH+SsdDhER1RNVTm6WL18OAGjatCleeeUVNkHRXW1JKLlq06lJA/i5OyocDRER1Rdmj5aaPXu2JeIgG2QcJdWWTVJERFR7qnXTke+//x7ffvstkpOTUVhYaPLakSNHaiQwsm7Xcwvxe1LJqLpoJjdERFSLzB4t9dFHH2H06NHw8/NDXFwcOnfuDG9vb5w/fx4DBgywRIxkhbYmXoNOLwgLcEcTb2elwyEionrE7ORm8eLF+Oyzz/DJJ59Ao9Fg+vTpiImJwaRJk5CVlWWJGMkKbeaN+4iISCFmJzfJycnGId9OTk7IyckBAIwYMYJzSxEA4FZBMfacyQDA5IaIiGqf2clNw4YNkZmZCQAIDg7Gb7/9BgBISkqCiNRsdGSVdpxMQ6FOj2Y+Lmjp56p0OEREVM+Yndz06dMHv/zyCwBgzJgxmDp1Kvr164ehQ4di8ODBNR4gWR/DKKnodg0rvJs1ERGRJZk9Wuqzzz6DXq8HAIwfPx5eXl7Yu3cvHn74YYwfP77GAyTrkl+kw45TaQA4BJyIiJRhdnKjVquhVv91wWfIkCEYMmQIAODKlSto1KhRzUVHVmfPmQzkFeoQ6OGI9o09lA6HiIjqIbObpSqSmpqKl156CS1atDB73cWLFyMkJASOjo4IDw/Hnj177rh8QUEBZs6cieDgYGi1WjRv3hxffvlldUOnGsYmKSIiUlqVk5ubN2/i6aefhq+vLwIDA/HRRx9Br9dj1qxZaNasGX777Tezk4y1a9diypQpmDlzJuLi4tCjRw8MGDAAycnJla4zZMgQbNu2DcuWLcOpU6ewevVqtG7d2qz3Jcso0umxNfEaADZJERGRclRSxSFOEyZMwC+//IKhQ4di06ZNSExMRHR0NPLz8zF79mz07NnT7Dfv0qULOnXqhCVLlhjL2rRpg0cffRTz5s0rt/ymTZswbNgwnD9/Hl5eXlV6j4KCAhQUFBifZ2dnIygoCFlZWXB3dzc7Zqrc3jMZeGbZ7/B20eDgzL6wU/PKDRER1Yzs7Gx4eHhU6fxd5Ss3GzZswPLly/H+++/j559/hoggNDQU27dvr1ZiU1hYiNjYWERFRZmUR0VFYf/+/RWu8/PPPyMiIgLvvvsuGjVqhNDQULzyyiu4fft2pe8zb948eHh4GB9BQUFmx0pVs+lECgAgqq0/ExsiIlJMlTsUX716FWFhYQCAZs2awdHREWPHjq32G2dkZECn08Hf39+k3N/fH6mpqRWuc/78eezduxeOjo5Yv349MjIyMGHCBFy/fr3SJrEZM2Zg2rRpxueGKzdUs/R6weYTJU1SnEuKiIiUVOXkRq/Xw8HBwfjczs4OLi4u9xxA2U6nIlJpR1S9Xg+VSoVVq1bBw6NkJM78+fPxxBNPYNGiRXByciq3jlarhVarvec46c7iLt1Aek4B3LT26NbcR+lwiIioHqtyciMiGDVqlDFRyM/Px/jx48slOOvWravS9nx8fGBnZ1fuKk1aWlq5qzkGAQEBaNSokTGxAUr66IgILl++jJYtW1Z1d6iGGUZJ/b2NHzT2NTIIj4iIqFqqfBYaOXIk/Pz8jH1XnnnmGQQGBpr0ZymddNyNRqNBeHg4YmJiTMpjYmKMc1eV1b17d1y9ehW3bt0ylp0+fRpqtRqNGzeu8ntTzRIRbDrBiTKJiKhuqPKVm+XLl9f4m0+bNg0jRoxAREQEIiMj8dlnnyE5Odl4p+MZM2bgypUrWLlyJQBg+PDh+Ne//oXRo0dj7ty5yMjIwKuvvornnnuuwiYpqh0JKdm4dP02HB3UeDDUV+lwiIionjP7DsU1aejQocjMzMSbb76JlJQUtGvXDhs3bkRwcDAAICUlxeSeN66uroiJicFLL72EiIgIeHt7Y8iQIXjrrbeU2gUCsPnPJqmeob5w1ij6lSIiIqr6fW5shTnj5Klqoj7chdPXbuHDoR0wuCObB4mIqOZZ5D43RBU5l34Lp6/dgr1ahT6tK+4ITkREVJuY3NA92fxnR+JuLXzg4eRwl6WJiIgsj8kN3RNDfxvOJUVERHVFtZKbr776Ct27d0dgYCAuXrwIAFiwYAF++umnGg2O6rYrN2/jj8tZUKmAfmFskiIiorrB7ORmyZIlmDZtGgYOHIibN29Cp9MBADw9PbFgwYKajo/qMMNVmweCveDrxrtAExFR3WB2cvPxxx/j888/x8yZM2FnZ2csj4iIwLFjx2o0OKrbDDfui+aN+4iIqA4xO7lJSkpCx44dy5VrtVrk5ubWSFBU96XnFODQhesAeFdiIiKqW8xObkJCQhAfH1+u/NdffzXOGk62b2viNYgA7Rt7oJEn7w5NRER1h9m3k3311VcxceJE5OfnQ0Rw8OBBrF69GvPmzcMXX3xhiRipDjJMlBnNUVJERFTHmJ3cjB49GsXFxZg+fTry8vIwfPhwNGrUCAsXLsSwYcMsESPVMVm3i7D/XAYANkkREVHdU62JgMaNG4dx48YhIyMDer0efn5+NR0X1WE7TqahSCdo6eeK5r6uSodDRERkwuw+N3PnzsW5c+cAAD4+Pkxs6iFDkxSv2hARUV1kdnLzww8/IDQ0FF27dsUnn3yC9PR0S8RFddTtQh12nk4DwP42RERUN5md3Bw9ehRHjx5Fnz59MH/+fDRq1AgDBw7EN998g7y8PEvESHXIrtPpyC/So3EDJ7QN5KzqRERU91Rr+oW2bdvinXfewfnz57Fjxw6EhIRgypQpaNiQ/8nbOsNEmf3bNoRKpVI4GiIiovLueeJMFxcXODk5QaPRoKioqCZiojqqsFiPrYnXALC/DRER1V3VSm6SkpLw9ttvIywsDBEREThy5AjmzJmD1NTUmo6P6pAD5zORk18MXzctOjVpoHQ4REREFTJ7KHhkZCQOHjyI++67D6NHjzbe54Zsn2GUVFSYP9RqNkkREVHdZHZy07t3b3zxxRdo27atJeKhOkqnF8QkcAg4ERHVfWYnN++8844l4qA6LvbiDWTcKoS7oz26NvNWOhwiIqJKVSm5mTZtGv71r3/BxcUF06ZNu+Oy8+fPr5HAqG759XgKAKBvmD8c7O65HzoREZHFVCm5iYuLM46EiouLs2hAVPeICDYf/2sIOBERUV1WpeRmx44dFf5N9cOxK1m4mpUPZ40dHgz1VTocIiKiOzK7feG5555DTk5OufLc3Fw899xzNRIU1S2GUVK9W/nB0cFO4WiIiIjuzOzk5r///S9u375drvz27dtYuXJljQRFdYeIGJObaI6SIiIiK1Dl0VLZ2dkQEYgIcnJy4OjoaHxNp9Nh48aNnCHcBp1Nu4XzGbnQ2KnRuxWbpIiIqO6rcnLj6ekJlUoFlUqF0NDQcq+rVCrMnTu3RoMj5Rmu2vytpQ/cHB0UjoaIiOjuqpzc7NixAyKCPn364IcffoCXl5fxNY1Gg+DgYAQGBlokSFLOphMcJUVERNalyslNz549AZTMK9WkSRPOCF0PXLqehxNXs6FWldzfhoiIyBpUKbk5evQo2rVrB7VajaysLBw7dqzSZdu3b19jwZGyNv951aZLiDe8XDQKR0NERFQ1VUpu7r//fqSmpsLPzw/3338/VCoVRKTcciqVCjqdrsaDJGUY+ttwLikiIrImVUpukpKS4Ovra/ybbF9adj5ik28AAKLaskmKiIisR5WSm+Dg4Ar/Jtu1JeEaRID7gzwR4OGkdDhERERVVq2b+G3YsMH4fPr06fD09ES3bt1w8eLFGg2OlGPob8MmKSIisjZmJzfvvPMOnJxK/pM/cOAAPvnkE7z77rvw8fHB1KlTazxAqn038wpx4FwmACCaQ8CJiMjKVHkouMGlS5fQokULAMCPP/6IJ554Av/4xz/QvXt39OrVq6bjIwVsS0xDsV7QuqEbQnxclA6HiIjILGZfuXF1dUVmZsl/9Vu2bEHfvn0BAI6OjhXOOUXWx3DjPl61ISIia2T2lZt+/fph7Nix6NixI06fPo2HHnoIAHDixAk0bdq0puOjWpZbUIzdp9MBsL8NERFZJ7Ov3CxatAiRkZFIT0/HDz/8AG9vbwBAbGwsnnrqqRoPkGrXzlPpKCjWI9jbGa0buikdDhERkdnMvnLj6emJTz75pFw5J820DaXnkuIUG0REZI3MTm4A4ObNm1i2bBkSExOhUqnQpk0bjBkzBh4eHjUdH9Wi/CIdtideA8AmKSIisl5mN0sdPnwYzZs3x4cffojr168jIyMDH374IZo3b44jR45YIkaqJfvPZSC3UIeG7o7o0NhT6XCIiIiqxewrN1OnTsWgQYPw+eefw96+ZPXi4mKMHTsWU6ZMwe7du2s8SKodhrmkotv6Q61mkxQREVkns5Obw4cPmyQ2AGBvb4/p06cjIiKiRoOj2lOs0yMmoaRJKppNUkREZMXMbpZyd3dHcnJyufJLly7BzY2ja6zVwQvXcSOvCA2cHdC5qZfS4RAREVWb2cnN0KFDMWbMGKxduxaXLl3C5cuXsWbNGowdO5ZDwa3Y5j+bpPqF+cPezuyvBRERUZ1hdrPU+++/D5VKhWeffRbFxcUAAAcHB7zwwgv497//XeMBkuXp9YLNJzhKioiIbIPZyY1Go8HChQsxb948nDt3DiKCFi1awNnZ2RLxUS344/JNpGbnw1Vrj27NfZQOh4iI6J5Uuf0hLy8PEydORKNGjeDn54exY8ciICAA7du3Z2Jj5Qw37uvd2g+ODnYKR0NERHRvqpzczJ49GytWrMBDDz2EYcOGISYmBi+88IIlY6NaICLG/jb9OVEmERHZgCo3S61btw7Lli3DsGHDAADPPPMMunfvDp1OBzs7/rdvrU5dy8GFzDxo7NXo1cpX6XCIiIjuWZWv3Fy6dAk9evQwPu/cuTPs7e1x9epViwRGtcNw474HW/rCRVut2TiIiIjqlConNzqdDhqNxqTM3t7eOGKKrJMhueEoKSIishVV/lddRDBq1ChotVpjWX5+PsaPHw8XFxdj2bp162o2QrKYCxm5OJmaAzu1Cn3b+CkdDhERUY2ocnIzcuTIcmXPPPNMjQZDtWvzn6OkIpt5w9NZc5eliYiIrEOVk5vly5dbMg5SwK+GiTLZJEVERDZE8fvsL168GCEhIXB0dER4eDj27NlTpfX27dsHe3t73H///ZYN0EalZN1G/KWbUKmA6DB/pcMhIiKqMYomN2vXrsWUKVMwc+ZMxMXFoUePHhgwYECFE3OWlpWVhWeffRZ///vfaylS27Plz+kWwps0gJ+7o8LREBER1RxFk5v58+djzJgxGDt2LNq0aYMFCxYgKCgIS5YsueN6zz//PIYPH47IyMhaitT2cJQUERHZKsWSm8LCQsTGxiIqKsqkPCoqCvv37690veXLl+PcuXOYPXt2ld6noKAA2dnZJo/67npuIX5PygQARPOuxEREZGMUS24yMjKg0+ng72/a38Pf3x+pqakVrnPmzBm8/vrrWLVqFeztq9YXet68efDw8DA+goKC7jl2a7c14Rr0ArQNdEeQF+cFIyIi21Kt5Oarr75C9+7dERgYiIsXLwIAFixYgJ9++snsbalUKpPnIlKuDCi5ieDw4cMxd+5chIaGVnn7M2bMQFZWlvFx6dIls2O0NYaJMjmXFBER2SKzk5slS5Zg2rRpGDhwIG7evAmdTgcA8PT0xIIFC6q8HR8fH9jZ2ZW7SpOWllbuag4A5OTk4PDhw3jxxRdhb28Pe3t7vPnmm/jjjz9gb2+P7du3V/g+Wq0W7u7uJo/6LCe/CHvPZABgfxsiIrJNZic3H3/8MT7//HPMnDnTZMLMiIgIHDt2rMrb0Wg0CA8PR0xMjEl5TEwMunXrVm55d3d3HDt2DPHx8cbH+PHj0apVK8THx6NLly7m7kq9tONUOgp1ejTzdUELP1elwyEiIqpxZs+UmJSUhI4dO5Yr12q1yM3NNWtb06ZNw4gRIxAREYHIyEh89tlnSE5Oxvjx4wGUNClduXIFK1euhFqtRrt27UzW9/Pzg6OjY7lyqtzm4381SVXU/EdERGTtzE5uQkJCEB8fj+DgYJPyX3/9FWFhYWZta+jQocjMzMSbb76JlJQUtGvXDhs3bjRuOyUl5a73vKGqyy/SYcepNABskiIiItulEhExZ4Xly5fj//2//4cPPvgAY8aMwRdffIFz585h3rx5+OKLLzBs2DBLxVojsrOz4eHhgaysrHrX/yYm4RrGrTyMQA9H7Hu9D6/cEBGR1TDn/G32lZvRo0ejuLgY06dPR15eHoYPH45GjRph4cKFdT6xqe82lZpLiokNERHZKrOTGwAYN24cxo0bh4yMDOj1evj5+dV0XFTDinR6bE0smXKBQ8CJiMiWVSu5MfDx8ampOMjCfj9/HVm3i+DtokFEUy+lwyEiIrKYanUovlOTxvnz5+8pILKMTSdSAABRbf1hp2aTFBER2S6zk5spU6aYPC8qKkJcXBw2bdqEV199tabiohqk1ws2/zkLOOeSIiIiW2d2cjN58uQKyxctWoTDhw/fc0BU8+Iu3UB6TgHctPbo1pxNiUREZNtqbOLMAQMG4IcffqipzVEN+vVYySipv7fxg8ZesblSiYiIakWNnem+//57eHmxo2pdIyJ/TZTZLkDhaIiIiCzP7Gapjh07mnQoFhGkpqYiPT0dixcvrtHg6N6duJqNyzduw9FBjZ6hvkqHQ0REZHFmJzePPvqoyXO1Wg1fX1/06tULrVu3rqm4qIZs/vOqTa9QPzhp7O6yNBERkfUzK7kpLi5G06ZNER0djYYNOerGGhjuSsy5pIiIqL4wq8+Nvb09XnjhBRQUFFgqHqpBZ9Nu4UzaLTjYqdC7Ne8iTURE9YPZHYq7dOmCuLg4S8RCNczQJNWtuQ88nBwUjoaIiKh2mN3nZsKECXj55Zdx+fJlhIeHw8XFxeT19u3b11hwdG82n2CTFBER1T9VTm6ee+45LFiwAEOHDgUATJo0yfiaSqWCiEClUkGn09V8lGS2Kzdv4+jlLKhUQL8wf6XDISIiqjVVTm7++9//4t///jeSkpIsGQ/VkM1/diR+oKkXfFy1CkdDRERUe6qc3IgIACA4ONhiwVDNMd64j3NJERFRPWNWh+I7zQZOdUd6TgEOXbgOAIhmfxsiIqpnzOpQHBoaetcE5/r16/cUEN27rYnXIAK0b+yBRp5OSodDRERUq8xKbubOnQsPDw9LxUI1xHDjvmg2SRERUT1kVnIzbNgw+PnxZnB1WdbtIuw/lwGAQ8CJiKh+qnKfG/a3sQ47TqahSCdo6eeK5r6uSodDRERU66qc3BhGS1HdxrmkiIiovqtys5Rer7dkHFQD8gqLsfN0GgD2tyEiovrL7LmlqO7afTod+UV6BHk5oW2gu9LhEBERKYLJjQ0xNkm1bcg+UkREVG8xubERhcV6bEssaZJifxsiIqrPmNzYiP3nMpBTUAxfNy06BjVQOhwiIiLFMLmxEZtPGG7c5w+1mk1SRERUfzG5sQE6vWDLiWsAgP5tAxSOhoiISFlMbmzA4QvXkZlbCA8nB3Rp5qV0OERERIpicmMDNv3ZJNW3jT8c7PiREhFR/cYzoZUTEWzmXYmJiIiMmNxYuWNXsnA1Kx/OGjv0aOmjdDhERESKY3Jj5Qw37uvdyg+ODnYKR0NERKQ8JjdWTESMyU00m6SIiIgAMLmxamfTbuF8Ri40dmr0buWrdDhERER1ApMbK2a4avO3lj5wc3RQOBoiIqK6gcmNFTMMAe/flk1SREREBkxurNSl63k4cTUbahXQN8xf6XCIiIjqDCY3VsrQJNUlxBteLhqFoyEiIqo7mNxYKUOT1ID72CRFRERUGpMbK5SWnY/YizcAAFFhTG6IiIhKY3JjhTYnlMwA3rGJJxp6OCocDRERUd3C5MYKGeeS4igpIiKicpjcWJmbeYU4cD4TABDN5IaIiKgcJjdWZmtiGnR6QeuGbmjq46J0OERERHUOkxsrYxgC3p9zSREREVWIyY0VyS0oxu4z6QCY3BAREVWGyY0V2XkqHYXFejT1dkYrfzelwyEiIqqTmNxYEcON+6LbNYRKpVI4GiIiorqJyY2VyC/SYXtiyf1tOASciIiockxurMT+cxnILdShobsjOjT2VDocIiKiOovJjZUwjJKKbusPtZpNUkRERJVhcmMFinV6xPw55UI0R0kRERHdEZMbK3DwwnXcyCtCA2cHdG7qpXQ4REREdZriyc3ixYsREhICR0dHhIeHY8+ePZUuu27dOvTr1w++vr5wd3dHZGQkNm/eXIvRKsMwl1S/MH/Y2yn+kREREdVpip4p165diylTpmDmzJmIi4tDjx49MGDAACQnJ1e4/O7du9GvXz9s3LgRsbGx6N27Nx5++GHExcXVcuS1R68XbD7x5ygpNkkRERHdlUpERKk379KlCzp16oQlS5YYy9q0aYNHH30U8+bNq9I22rZti6FDh2LWrFlVWj47OxseHh7IysqCu7t7teKuTUeSb+CxxfvhqrVH7P/rC629ndIhERER1Tpzzt+KXbkpLCxEbGwsoqKiTMqjoqKwf//+Km1Dr9cjJycHXl6V90MpKChAdna2ycOaGJqk+rT2Y2JDRERUBYolNxkZGdDpdPD39zcp9/f3R2pqapW28cEHHyA3NxdDhgypdJl58+bBw8PD+AgKCrqnuGuTiBjvSswmKSIioqpRvHdq2WkERKRKUwusXr0ac+bMwdq1a+Hn51fpcjNmzEBWVpbxcenSpXuOubacTM3Bxcw8aO3V6Bnqq3Q4REREVsFeqTf28fGBnZ1duas0aWlp5a7mlLV27VqMGTMG3333Hfr27XvHZbVaLbRa7T3HqwTDjfseDPWFi1axj4qIiMiqKHblRqPRIDw8HDExMSblMTEx6NatW6XrrV69GqNGjcI333yDhx56yNJhKmqzoUmKc0kRERFVmaKXA6ZNm4YRI0YgIiICkZGR+Oyzz5CcnIzx48cDKGlSunLlClauXAmgJLF59tlnsXDhQnTt2tV41cfJyQkeHh6K7YclJGXk4mRqDuzVKvy9TeXNbkRERGRK0eRm6NChyMzMxJtvvomUlBS0a9cOGzduRHBwMAAgJSXF5J43n376KYqLizFx4kRMnDjRWD5y5EisWLGitsO3KMNVm8jm3vB01igcDRERkfVQ9D43SrCW+9w8umgf4i/dxFuPtsMzXYOVDoeIiEhRVnGfG6pcStZtxF+6CZUKiAq7c+dqIiIiMsXkpg7a8ud0C+FNGsDP3VHhaIiIiKwLk5s6yDAEnDfuIyIiMh+Tmzrmem4hfk/KBABEcwg4ERGR2Zjc1DFbE65BL0DbQHcEeTkrHQ4REZHVYXJTx2zijfuIiIjuCZObOiQnvwh7z2QAYH8bIiKi6mJyU4fsOJWOQp0ezXxd0MLPVelwiIiIrBKTmzpk0/EUACVNUlWZGZ2IiIjKY3JTR+QX6bDjZDoAYEC7AIWjISIisl5MbuqI3afTcbtIh0aeTmjXqO5OC0FERFTXMbmpIwyjpKLZJEVERHRPmNzUAUU6PbYmlEy5wFFSRERE94bJTR3w2/lMZOcXw8dVg/DgBkqHQ0REZNWY3NQBhrmk+oU1hJ2aTVJERET3gsmNwnR6weYTbJIiIiKqKUxuFBaXfAMZtwrg5miPyGbeSodDRERk9ZjcKMzQJNW3jT809vw4iIiI7hXPpgoSEZMh4ERERHTvmNwo6MTVbFy+cRuODmr0DPVVOhwiIiKbwORGQZv/vGrTK9QPTho7haMhIiKyDUxuFGTob8NRUkRERDWHyY1Czqbdwpm0W3CwU6F3az+lwyEiIrIZTG4UYmiS6tbcBx5ODgpHQ0REZDuY3CjEkNywSYqIiKhmMblRwOUbeTh6OQtqFdAvzF/pcIiIiGwKkxsFGKZbeKCpF3xctQpHQ0REZFuY3ChgM0dJERERWQyTm1qWnlOAQxevA+BdiYmIiCyByU0ti0m4BhGgQ2MPBHo6KR0OERGRzWFyU8uMc0mxSYqIiMgimNzUoqzbRdh/NgMA0J9NUkRERBbB5KYWbT95DcV6Qai/K5r5uiodDhERkU1iclOLjHNJ8aoNERGRxTC5qSV5hcXYdTodAPvbEBERWRKTm1qy+3Q68ov0CPJyQliAu9LhEBER2SwmN7WkdJOUSqVSOBoiIiLbxeSmFhQW67EtMQ0A70pMRERkaUxuasH+cxnIKSiGr5sWHYMaKB0OERGRTWNyUws2G27c19YfajWbpIiIiCyJyY2F6fSCLX/OAt6/bYDC0RAREdk+JjcWdvjCdWTmFsLDyQFdmnkpHQ4REZHNY3JjYYa5pPq28YeDHaubiIjI0ni2tSARweY/h4AP4CgpIiKiWmGvdAC27OjlLFzNyoezxg5/a+kDEUFxcTF0Op3SoREREdU5Dg4OsLOzu+ftMLmxIEOTVO/WflCLDsnJl5GXl6dwVERERHWTSqVC48aN4ep6b5NLM7mxEBH5667EYf5ISkqCnZ0dAgMDodFoeJdiIiKiUkQE6enpuHz5Mlq2bHlPV3CY3FjImbRbSMrIhcZOje7NPHHtSjaCgoLg7OysdGhERER1kq+vLy5cuICioqJ7Sm7YodhCDFdterT0gbOm5ANSq1ndRERElampVg2ebS3EkNxEc5QUERFRrWJyYwHJmXlISMmGnVqFvm38lQ6HiIioXmFyYwGGuaS6hHjBy0WjcDS1b86cObj//vuVDsNmsX6ppu3cuRMqlQo3b95UOpQ6Y/v27WjdujX0er3SodiMTz75BIMGDaqV92JyYwGGIeD9baRJav/+/bCzs0P//v0t9h5NmzaFSqWCSqUyjiobM2YMbty4YbH3LKuqB3jDcoaHt7c3+vTpg3379tVOoLVkzpw5JvtpeGzdulXRmCyV2F28eBFarRbZ2dnlXrtw4YJJHbi5uaFt27aYOHEizpw5Y5F46prSv1EnJyc0bdoUQ4YMwfbt283e1qhRo/Doo4/WfJB30bRpUyxYsKBKy06fPh0zZ84s11fy9u3baNCgAby8vHD79u1y66lUKvz444/lyqdMmYJevXqZlKWmpuKll15Cs2bNoNVqERQUhIcffhjbtm2r6i5Vy65duxAeHg5HR0c0a9YMS5cuves6hw4dwt///nd4enqiQYMGiIqKQnx8vMkymzdvRteuXeHm5gZfX188/vjjSEpKMr4+btw4HDp0CHv37q3pXSqHyU0NS8vOR+zFkhNyVJhtJDdffvklXnrpJezduxfJyckWe58333wTKSkpSE5OxqpVq7B7925MmjTJYu93r06dOoWUlBTs3LkTvr6+eOihh5CWlqZ0WDWqbdu2SElJMXk8+OCD1dpWYWFhDUdXs3766Sf06tUL7u7ulS6zdetWpKSk4I8//sA777yDxMREdOjQ4Y4no6KiIkuEqwjDb/TUqVNYuXIlPD090bdvX7z99ttKh1aj9u/fjzNnzuDJJ58s99oPP/yAdu3aISwsDOvWrav2e1y4cAHh4eHYvn073n33XRw7dgybNm1C7969MXHixHsJ/46SkpIwcOBA9OjRA3FxcXjjjTcwadIk/PDDD5Wuk5OTg+joaDRp0gS///479u7dC3d3d0RHRxu/3+fPn8cjjzyCPn36ID4+Hps3b0ZGRgYee+wx43a0Wi2GDx+Ojz/+2GL7ZyT1TFZWlgCQrKwsi2x/5YELEvza/+TRRXuNZbdv35aEhAS5ffu2iIjo9XrJLShS5KHX683an1u3bombm5ucPHlShg4dKnPnzi23zLx588TPz09cXV3lueeek9dee006dOhgfP3gwYPSt29f8fb2Fnd3d3nwwQclNjbWZBvBwcHy4YcfmpS9+eabEhYWZlL2/fffS1hYmGg0GgkODpb333/f5PXr16/LiBEjxNPTU5ycnKR///5y+vRp4+sXLlyQ//u//xNPT09xdnaWsLAw2bBhgyQlJQkAk8fIkSMrrJMdO3YIALlx44ax7OjRowJAfv75Z2PZV199JeHh4eLq6ir+/v7y1FNPybVr18ptZ+vWrRIeHi5OTk4SGRkpJ0+eNKt+dTqdzJ07Vxo1aiQajUY6dOggv/76q/F1w76tXbtW/va3v4mjo6NERETIqVOn5ODBgxIeHi4uLi4SHR0taWlpxvVmz55t8j5lHT16VHr37i2Ojo7i5eUl48aNk5ycHOPrI0eOlEceeUTeeecdCQgIkODgYBERuXz5sgwZMkQ8PT3Fy8tLBg0aJElJSSb18sADD4izs7N4eHhIt27d5MKFC7J8+fJyn9Hy5csrjEulUkl6erqIlHwnVCqVPPHEE8Zl3nnnHenatavJen369JFPPvmkwn011GFcXJxJuU6nk169eklwcLAUFxeb1NuyZcskJCREVCqV6PV6uXjxogwaNEhcXFzEzc1NnnzySUlNTS1X30uXLpXGjRuLk5OTPPHEEybfM0OdzpkzR3x9fcXNzU3+8Y9/SEFBgXEZvV4v//nPfyQkJEQcHR2lffv28t1335nEvWHDBmnZsqU4OjpKr169jHVb+r3Kqug3KiIya9YsUavVxu9tcXGxPPfcc9K0aVNxdHSU0NBQWbBggcl+lv0cd+zYISIi06dPl5YtW4qTk5OEhITIP//5TyksLDSuGx8fL7169RJXV1dxc3OTTp06yaFDh4yv79u3T3r06CGOjo7SuHFjeemll+TWrVsiItKzZ89y71uZl156yeT7UlqvXr1k6dKlsmTJEundu3e51wHI+vXry5VPnjxZevbsaXw+YMAAadSokTG+0u70Odyr6dOnS+vWrU3Knn/++XK/h9IOHTokACQ5OdlYZjjmnT17VkREvvvuO7G3txedTmdc5ueffxaVSmXyGe7cuVM0Go3k5eVV+F5lz5elmXP+ZnJTw57+/DcJfu1/snTnWWNZ2Q8rt6BIgl/7nyKP3IIis/Zn2bJlEhERISIiv/zyizRt2tQkQVq7dq1oNBr5/PPP5eTJkzJz5kxxc3MzOSlu27ZNvvrqK0lISJCEhAQZM2aM+Pv7S3Z2tnGZsgfOy5cvS+fOnWX06NHGssOHD4tarZY333xTTp06JcuXLxcnJyeTE9ygQYOkTZs2snv3bomPj5fo6Ghp0aKF8cf10EMPSb9+/eTo0aNy7tw5+eWXX2TXrl1SXFwsP/zwgwCQU6dOSUpKity8ebPCOimb3OTm5srUqVMFgElSsWzZMtm4caOcO3dODhw4IF27dpUBAwaU206XLl1k586dcuLECenRo4d069bNrPqdP3++uLu7y+rVq+XkyZMyffp0cXBwMCZ1hhNz69atZdOmTZKQkCBdu3aVTp06Sa9evWTv3r1y5MgRadGihYwfP9643TslN7m5uRIYGCiPPfaYHDt2TLZt2yYhISEmCeHIkSPF1dVVRowYIcePH5djx45Jbm6utGzZUp577jk5evSoJCQkyPDhw6VVq1ZSUFAgRUVF4uHhIa+88oqcPXtWEhISZMWKFXLx4kXJy8uTl19+Wdq2bSspKSmSkpJS4QFSr9eLj4+PfP/99yIi8uOPP4qPj4/4+fkZl4mKipLXXnvN+PzGjRvi4OBgcvAurbLkRkRk/fr1AkB+//13Y70ZksUjR47IH3/8IXq9Xjp27Ch/+9vf5PDhw/Lbb79Jp06dTE52hvX69OkjcXFxsmvXLmnRooUMHz68XJ0OHTpUjh8/Lv/73//E19dX3njjDeMyb7zxhvGzPnfunCxfvly0Wq3s3LlTRESSk5NFq9XK5MmT5eTJk/L111+Lv79/tZObzMxMUalU8p///EdERAoLC2XWrFly8OBBOX/+vHz99dfi7Owsa9euFRGRnJwcGTJkiPTv39/4ORqSs3/961+yb98+SUpKkp9//ln8/f2N2xURadu2rTzzzDOSmJgop0+flm+//Vbi4+NFpORk6+rqKh9++KGcPn1a9u3bJx07dpRRo0YZ42zcuLG8+eabxvetTIcOHeTf//53ufKzZ8+KVquV69evS2Zmpmi1Wjl37pzJMlVJbgx19s4771QaQ2W+/vprcXFxuePj66+/rnT9Hj16yKRJk0zK1q1bJ/b29iZJSGnZ2dni4+Mjs2fPloKCAsnLy5PJkydL27Ztpaio5JySlJQkWq1WvvjiCykuLpabN2/Kk08+KdHR0SbbunXrlqhUKuP3sSybSW4WLVokTZs2Fa1WK506dZLdu3ffcfmdO3dKp06dRKvVSkhIiCxZssSs97NkcnMjt0Cazdggwa/9T5LS/8rGrTm56datm/G/rqKiIvHx8ZGYmBjj65GRkSYnRBGRLl263PE//uLiYnFzc5NffvnFWBYcHCwajUZcXFzE0dHReNIvfbAdPny49OvXz2Rbr776qvHqzunTpwWA7Nu3z/h6RkaGODk5ybfffisiIvfdd5/MmTOnwrgquiJzp+UMBxKVSiUAJDw8vNKDg0jJFSwAxqsbpa/cGGzYsEEAGL8rVanfwMBAefvtt02WeeCBB2TChAki8teJ+YsvvjC+vnr1agEg27ZtM5bNmzdPWrVqZXw+e/ZsUavVJgfNBx54QEREPvvsM2nQoIHJf50bNmwQtVptvBoxcuRI8ff3N7mqsGzZMmnVqpVJglxQUCBOTk6yefNmyczMFACVHvjudjXJ4LHHHpMXX3xRRESmTJkiL7/8svj4+MiJEyekqKhIXF1dTRLRVatWSadOnSrd3p2Sm8TEROOVMUOMDg4OJlfBtmzZInZ2dibJ04kTJwSAHDx40LienZ2dXLp0ybjMr7/+Kmq12ngiHjlypHh5eUlubq5xmSVLloirq6vodDq5deuWODo6yv79+01iHDNmjDz11FMiIjJjxgxp06aNyWfw2muvVTu5ERHx9/eXF154odJ1J0yYII8//rjxueEK1N28++67Eh4ebnzu5uYmK1asqHDZESNGyD/+8Q+Tsj179oharTb+nu60D6V5eHjIypUry5W/8cYb8uijjxqfP/LIIzJz5kyTZaqS3Pz+++8CQNatW3fXWMrKzs6WM2fO3PFR+h/Hslq2bFnueLFv3z4BIFevXq10vePHj0vz5s1FrVaLWq2W1q1by8WLF02W2bVrl/j5+YmdnZ0AkMjIyAq/Uw0aNKj0c6yp5EbROxSvXbsWU6ZMweLFi9G9e3d8+umnGDBgABISEtCkSZNyyxvaCseNG4evv/4a+/btw4QJE4wdl5S2NTENOr2gdUM3NPVxqXQ5Jwc7JLwZXYuRmb53VZ06dQoHDx40tivb29tj6NCh+PLLL9G3b18AQGJiIsaPH2+yXmRkJHbs2GF8npaWhlmzZmH79u24du0adDod8vLyyvXfefXVVzFq1CiICC5duoQ33ngDDz30EHbv3g07OzskJibikUceMVmne/fuWLBgAXQ6HRITE2Fvb48uXboYX/f29karVq2QmJgIAJg0aRJeeOEFbNmyBX379sXjjz+O9u3bV7lOStuzZw9cXFwQFxeH1157DStWrICDg4Px9bi4OMyZMwfx8fG4fv26cdRFcnIywsLCjMuVfv+AgABjnTVp0uSu9ZudnY2rV6+ie/fu5erljz/+MCkr/T7+/iW3KLjvvvtMysr2GWrVqhV+/vln43OtVgsAxr4mLi5/fc+7d+8OvV6PU6dOmWxfo/lrxGBsbCzOnj0LNzc3k/fJz8/HuXPnEBUVhVGjRiE6Ohr9+vVD3759MWTIEGO9VFWvXr3w2WefASjpPPmvf/0LSUlJ2LVrF7KysnD79m2TOvvpp5+qPYpDRACY3nwsODgYvr6+xueJiYkICgpCUFCQsSwsLAyenp5ITEzEAw88AABo0qQJGjdubFwmMjLSWKcNG5b04evQoYPJnc4jIyNx69YtXLp0CWlpacjPz0e/fv1MYiwsLETHjh2NsXTt2tUk3sjIyGrte+k6KL29pUuX4osvvsDFixdx+/ZtFBYWVqkj+Pfff48FCxbg7NmzuHXrFoqLi036QE2bNg1jx47FV199hb59++LJJ59E8+bNAfz13Vq1apVJXHq9HklJSWjTpk2V9+f27dtwdHQ0KdPpdPjvf/+LhQsXGsueeeYZTJ06FXPnzjXrbroVfWeqys3Nrdzvx1xl3/du8dy+fRvPPfccunfvjtWrV0On0+H999/HwIEDcejQITg5OSE1NRVjx47FyJEj8dRTTyEnJwezZs3CE088gZiYGJNtOzk5WXyeRUWTm/nz52PMmDEYO3YsAGDBggXYvHkzlixZgnnz5pVbfunSpWjSpImxt3ubNm1w+PBhvP/++3UiuTHOJXWXUVIqlQrOmro/88WyZctQXFyMRo0aGctEBA4ODrhx4wYaNGhQpe2MGjUK6enpWLBgAYKDg6HVahEZGVmug6mPjw9atGgBAGjZsiUWLFhgPJH37du33AHUEE9Ff5ddxrDe2LFjER0djQ0bNmDLli2YN28ePvjgA7z00ktV2pfSQkJC4OnpidDQUOTn52Pw4ME4fvw4tFotcnNzERUVhaioKHz99dfw9fVFcnIyoqOjy+136YTIEKe5w08rqpeyZRW9T9mysu+r0WiMn8ndtl9RLKWTH6Bkv8LDw01OQAaGZGD58uWYNGkSNm3ahLVr1+Kf//wnYmJi0LVr1wrfryK9evXC5MmTcfbsWRw/fhw9evTAuXPnsGvXLty8eRPh4eHGE0RRURE2bdqEGTNmVHn7pRkS55CQEGNZ2f2urL7uVI/AX3VZlZNg6c9vw4YNJr9b4K/EtLLfSXVlZmYiPT3duP/ffvstpk6dig8++ACRkZFwc3PDe++9h99///2O2/ntt98wbNgwzJ07F9HR0fDw8MCaNWvwwQcfGJeZM2cOhg8fjg0bNuDXX3/F7NmzsWbNGgwePBh6vR7PP/98hYMQKvpn+U58fHzKjdTcvHkzrly5gqFDh5qU63Q6bNmyBQMGDABQknxkZWWV2+bNmzfh4eEBoOT4plKpkJiYaPaosVWrVuH555+/4zKffvopnn766Qpfa9iwIVJTU03K0tLSYG9vD29v7wrX+eabb3DhwgUcOHDAOHrsm2++QYMGDfDTTz9h2LBhWLRoEdzd3fHuu+8a1/v6668RFBSE33//3eT3e/36dZPk3xIUGy1VWFiI2NhYREVFmZRHRUVh//79Fa5z4MCBcstHR0fj8OHDlY5IKCgoQHZ2tsnDEnILirH7TDoA2xgCXlxcjJUrV+KDDz5AfHy88fHHH38gODjYeHJq06YNfvvtN5N1yz7fs2cPJk2ahIEDB6Jt27bQarXIyMi4awyG/4QMwy3DwsLKDSHcv38/QkNDYWdnh7CwMBQXF5scRDMzM3H69GmT/9qCgoIwfvx4rFu3Di+//DI+//xzADBeYdDpdFWqo9JGjBgBvV6PxYsXAwBOnjyJjIwM/Pvf/0aPHj3QunXrao2kulv9uru7IzAwsMJ6Mec/VXOFhYUhPj4eubm5xrJ9+/ZBrVYjNDS00vU6deqEM2fOwM/PDy1atDB5GA78ANCxY0fMmDED+/fvR7t27fDNN98AKPmMqvL5tGvXDt7e3njrrbfQoUMHuLu7o2fPnti1axd27tyJnj17GpfdsWMHPD09qzXEXK/X46OPPkJISIjxykhFwsLCkJycjEuXLhnLEhISkJWVZfI5JScn4+rVq8bnhpNJ6Tr9448/TIYg//bbb3B1dUXjxo0RFhYGrVaL5OTkcvVruGoUFhZ219+sORYuXAi1Wm08Se/ZswfdunXDhAkT0LFjR7Ro0QLnzp0zWaeiz3Hfvn0IDg7GzJkzERERgZYtW+LixYvl3i80NBRTp07Fli1b8Nhjj2H58uUASr5bJ06cKLffLVq0MP62q/r96dixIxISEkzKli1bhmHDhpkcD+Pj4/H0009j2bJlxuVat26NQ4cOmawrIoiNjUWrVq0AAF5eXoiOjsaiRYtMfkMGd7odxaBBg8rFUPZxp6uQkZGRiImJMSnbsmULIiIiTP7ZKS0vLw9qtdokyTY8NyTUeXl55a5eGZ6X/qfp3LlzyM/Pv+PvpUbcteHKQq5cuVKuf4SIyNtvvy2hoaEVrlOdtsKKeubDAn1uTlzJku7/3ia93ttRbkTSndoQ66r169eLRqOpsFPtG2+8Iffff7+IiKxZs0a0Wq0sW7ZMTp06JbNmzSrX4fX++++Xfv36SUJCgvz222/So0cPcXJyMmn7Dg4ONnb0u3r1qvz+++/Ss2dP8fHxkYyMDBERiY2NNelQvGLFinIdih955BEJCwuTPXv2SHx8vPTv39+kQ/HkyZNl06ZNcv78eYmNjZXOnTvLkCFDRKSkE7NKpZIVK1ZIWlqaycif0irrm/PRRx+Jn5+f5ObmSlpammg0Gnn11Vfl3Llz8tNPP0loaKhJv42KthMXFycAjKOHqlK/H374obi7u8uaNWvk5MmT8tprr1XYobh0f5GK3nv58uXi4eFhfH63DsUBAQHy+OOPy7Fjx2T79u3SrFmzch2Ky/arMHQo7tWrl+zevVvOnz8vO3fulEmTJsmlS5fk/Pnz8vrrr8v+/fvlwoULsnnzZvHy8pLFixeLSEnfGBcXF4mLi5P09HTJz8+vMD6Rkn43dnZ28sorr4hISUdjLy8vsbOzkw0bNhiXmzhxorF/TmUMdbh161ZJSUkxfqa9e/cWJycn2b59+x3rzdChuEePHhIbGyu///67hIeHV9ihuG/fvhIfHy+7d++W0NBQGTZsmEmdurq6ylNPPSUnTpyQjRs3ir+/v7z++uvGZWbOnCne3t6yYsUKOXv2rBw5ckQ++eQTYx+HixcvikajkalTp8rJkydl1apV0rBhwyr1uTH8RpOTk2XXrl0ybtw4UalUJp1vFyxYIO7u7rJp0yY5deqU/POf/xR3d3eTOnn77belSZMmcvLkSUlPT5fCwkL58ccfxd7eXlavXi1nz56VhQsXipeXl/E7mZeXJxMnTpQdO3bIhQsXZO/evdK8eXOZPn26iIj88ccf4uTkJBMmTJC4uDg5ffq0/PTTTyafbb9+/WTQoEFy+fJl42i6inz00UcmfX3S0tLEwcHBpJ+WwZYtW0z6WK1du1YcHR3l448/llOnTkl8fLxMmDBBnJyc5MKFC8b1zp8/Lw0bNpSwsDD5/vvv5fTp05KQkCALFy4sN5qpJp0/f16cnZ1l6tSpkpCQIMuWLRMHBwdjB3yRkg7GpfvfJSYmilarlRdeeEESEhLk+PHj8swzz4iHh4fx3Ltt2zZRqVQyd+5cOX36tMTGxkp0dLQEBwebdPxfvny5NGvWrNL4rL5DsSG5Kdvx7a233jKp1NJatmxZrnf53r17BUClPd/z8/MlKyvL+Lh06ZLFOhTr9XpJyy5/sLXG5Ob//u//ZODAgRW+FhsbKwCMw7nffvtt8fHxEVdXVxk5cqRMnz7d5EB25MgRiYiIEK1WKy1btpTvvvuuXMe+4OBgk+TT19dXBg4cWK4Dp2EouIODgzRp0kTee+89k9cNQ8E9PDzEyclJoqOjTYaCv/jii9K8eXPRarXi6+srI0aMMCZPIiXDzxs2bCgqlcqsoeAiJaMAGjRoYBzd8c033xg7y0dGRsrPP/9sdnJTlfotPRTcwcGh0qHgNZnciFR9KHhZKSkp8uyzz4qPj49otVpp1qyZjBs3TrKysiQ1NVUeffRRCQgIMA73nzVrlnF4aX5+vjz++OPi6elZ6VBwg48//lgAyP/+9z9j2SOPPCJ2dnYmv/+goCCTTvIVKXurAGdnZ2nTpo1MmDBBzpw5Y7JsZfVW1aHgixcvlsDAQHF0dJTHHntMrl+/Xq5OZ82aJd7e3uLq6ipjx441SfL0er0sXLhQWrVqJQ4ODuLr6yvR0dGya9cu4zK//PKLtGjRQrRarfTo0UO+/PLLKiU3hv3XaDTSpEkTGTJkiEliJ1LyGY0aNUo8PDzE09NTXnjhBXn99ddN6iQtLU369esnrq6uJkPBX331VeN+DR06VD788EPjd7KgoECGDRsmQUFBotFoJDAwUF588UWT4+rBgweN23VxcZH27dub/EN84MABad++vWi12jsOBb9+/bo4OTkZh7e///774unpWeGAgaKiIvHy8pIPPvjAWLZmzRqJiIgQd3d38fPzk+joaDl8+HC5da9evSoTJ040Dqho1KiRDBo0yFgflrJz507p2LGjaDQaadq0abmBOYZbA5S2ZcsW6d69u3h4eEiDBg2kT58+cuDAAZNlVq9eLR07dhQXFxfx9fWVQYMGSWJioskyUVFRMm/evEpjq6nkRiVSww2wVVRYWAhnZ2d89913GDx4sLF88uTJiI+Px65du8qt8+CDD6Jjx44mHbrWr1+PIUOGIC8vr9JLaqVlZ2fDw8MDWVlZd7xZV03Kz89HUlISQkJCynVSIyLlHDlyBH369EF6enqVjh+WNGfOHPz444/l7vpa2qhRo3Dz5s0K74BLNWv69OnIysrCp59+qnQoNuP48eP4+9//jtOnT5s0Q5d2p/OlOedvxfrcaDQahIeHl2v7i4mJQbdu3SpcpzpthURElSkuLsbHH3/M4weVM3PmTAQHB1erDx5V7OrVq1i5cmWliU1NUnTIzrRp0zBixAhEREQgMjISn332GZKTk41DX2fMmIErV65g5cqVAIDx48fjk08+wbRp0zBu3DgcOHAAy5Ytw+rVq5XcDSKyUp07d0bnzp2VDoPqIA8PD7zxxhtKh2FTyg4IsiRFk5uhQ4ciMzPTOF9Ju3btsHHjRgQHBwOAcZ4hg5CQEGzcuBFTp07FokWLEBgYiI8++qhODAMnIroXc+bMwZw5c+64zIoVK2olFiJrp1ifG6Wwzw0REVHdZPV9buqjepZHEhERmaWmzpNMbmqBobOipW83TUREZM0Md3A3ZzqLitT9OQBsgJ2dHTw9PY13qHV2dq7WnCJERES2Sq/XIz09Hc7OzrC3v7f0hMlNLTFMeledW/ATERHVB2q1Gk2aNLnnCwBMbmqJSqVCQEAA/Pz8Kp0Hi4iIqD7TaDTGyTnvBZObWmZnZ3fPbYlERERUOXYoJiIiIpvC5IaIiIhsCpMbIiIisin1rs+N4QZB2dnZCkdCREREVWU4b1flRn/1LrnJyckBAAQFBSkcCREREZkrJyfnrjOL17u5pfR6Pa5evQo3N7cav5FednY2goKCcOnSpVqbt6o+Yj3XDtZz7WA91x7Wde2wVD2LCHJychAYGHjX4eL17sqNWq1G48aNLfoe7u7u/OHUAtZz7WA91w7Wc+1hXdcOS9Tz3a7YGLBDMREREdkUJjdERERkU5jc1CCtVovZs2dDq9UqHYpNYz3XDtZz7WA91x7Wde2oC/Vc7zoUExERkW3jlRsiIiKyKUxuiIiIyKYwuSEiIiKbwuSGiIiIbAqTGzMtXrwYISEhcHR0RHh4OPbs2XPH5Xft2oXw8HA4OjqiWbNmWLp0aS1Fat3Mqed169ahX79+8PX1hbu7OyIjI7F58+ZajNZ6mft9Nti3bx/s7e1x//33WzZAG2FuPRcUFGDmzJkIDg6GVqtF8+bN8eWXX9ZStNbL3HpetWoVOnToAGdnZwQEBGD06NHIzMyspWit0+7du/Hwww8jMDAQKpUKP/74413XUeQ8KFRla9asEQcHB/n8888lISFBJk+eLC4uLnLx4sUKlz9//rw4OzvL5MmTJSEhQT7//HNxcHCQ77//vpYjty7m1vPkyZPlP//5jxw8eFBOnz4tM2bMEAcHBzly5EgtR25dzK1ng5s3b0qzZs0kKipKOnToUDvBWrHq1POgQYOkS5cuEhMTI0lJSfL777/Lvn37ajFq62NuPe/Zs0fUarUsXLhQzp8/L3v27JG2bdvKo48+WsuRW5eNGzfKzJkz5YcffhAAsn79+jsur9R5kMmNGTp37izjx483KWvdurW8/vrrFS4/ffp0ad26tUnZ888/L127drVYjLbA3HquSFhYmMydO7emQ7Mp1a3noUOHyj//+U+ZPXs2k5sqMLeef/31V/Hw8JDMzMzaCM9mmFvP7733njRr1syk7KOPPpLGjRtbLEZbU5XkRqnzIJulqqiwsBCxsbGIiooyKY+KisL+/fsrXOfAgQPllo+Ojsbhw4dRVFRksVitWXXquSy9Xo+cnBx4eXlZIkSbUN16Xr58Oc6dO4fZs2dbOkSbUJ16/vnnnxEREYF3330XjRo1QmhoKF555RXcvn27NkK2StWp527duuHy5cvYuHEjRATXrl3D999/j4ceeqg2Qq43lDoP1ruJM6srIyMDOp0O/v7+JuX+/v5ITU2tcJ3U1NQKly8uLkZGRgYCAgIsFq+1qk49l/XBBx8gNzcXQ4YMsUSINqE69XzmzBm8/vrr2LNnD+zteeioiurU8/nz57F37144Ojpi/fr1yMjIwIQJE3D9+nX2u6lEdeq5W7duWLVqFYYOHYr8/HwUFxdj0KBB+Pjjj2sj5HpDqfMgr9yYSaVSmTwXkXJld1u+onIyZW49G6xevRpz5szB2rVr4efnZ6nwbEZV61mn02H48OGYO3cuQkNDays8m2HO91mv10OlUmHVqlXo3LkzBg4ciPnz52PFihW8enMX5tRzQkICJk2ahFmzZiE2NhabNm1CUlISxo8fXxuh1itKnAf571cV+fj4wM7Ortx/AWlpaeWyUoOGDRtWuLy9vT28vb0tFqs1q049G6xduxZjxozBd999h759+1oyTKtnbj3n5OTg8OHDiIuLw4svvgig5CQsIrC3t8eWLVvQp0+fWondmlTn+xwQEIBGjRrBw8PDWNamTRuICC5fvoyWLVtaNGZrVJ16njdvHrp3745XX30VANC+fXu4uLigR48eeOutt3hlvYYodR7klZsq0mg0CA8PR0xMjEl5TEwMunXrVuE6kZGR5ZbfsmULIiIi4ODgYLFYrVl16hkouWIzatQofPPNN2wzrwJz69nd3R3Hjh1DfHy88TF+/Hi0atUK8fHx6NKlS22FblWq833u3r07rl69ilu3bhnLTp8+DbVajcaNG1s0XmtVnXrOy8uDWm16CrSzswPw15UFuneKnQct2l3ZxhiGGi5btkwSEhJkypQp4uLiIhcuXBARkddff11GjBhhXN4wBG7q1KmSkJAgy5Yt41DwKjC3nr/55huxt7eXRYsWSUpKivFx8+ZNpXbBKphbz2VxtFTVmFvPOTk50rhxY3niiSfkxIkTsmvXLmnZsqWMHTtWqV2wCubW8/Lly8Xe3l4WL14s586dk71790pERIR07txZqV2wCjk5ORIXFydxcXECQObPny9xcXHGIfd15TzI5MZMixYtkuDgYNFoNNKpUyfZtWuX8bWRI0dKz549TZbfuXOndOzYUTQajTRt2lSWLFlSyxFbJ3PquWfPngKg3GPkyJG1H7iVMff7XBqTm6ozt54TExOlb9++4uTkJI0bN5Zp06ZJXl5eLUdtfcyt548++kjCwsLEyclJAgIC5Omnn5bLly/XctTWZceOHXc83taV86BKhNffiIiIyHawzw0RERHZFCY3REREZFOY3BAREZFNYXJDRERENoXJDREREdkUJjdERERkU5jcEBERkU1hckNEREQ2hckNEZlYsWIFPD09lQ6j2po2bYoFCxbccZk5c+bg/vvvr5V4iKj2MbkhskGjRo2CSqUq9zh79qzSoWHFihUmMQUEBGDIkCFISkqqke0fOnQI//jHP4zPVSoVfvzxR5NlXnnlFWzbtq1G3q8yZffT398fDz/8ME6cOGH2dqw52SRSApMbIhvVv39/pKSkmDxCQkKUDgtAySzjKSkpuHr1Kr755hvEx8dj0KBB0Ol097xtX19fODs733EZV1dXeHt73/N73U3p/dywYQNyc3Px0EMPobCw0OLvTVSfMbkhslFarRYNGzY0edjZ2WH+/Pm477774OLigqCgIEyYMAG3bt2qdDt//PEHevfuDTc3N7i7uyM8PByHDx82vr5//348+OCDcHJyQlBQECZNmoTc3Nw7xqZSqdCwYUMEBASgd+/emD17No4fP268srRkyRI0b94cGo0GrVq1wldffWWy/pw5c9CkSRNotVoEBgZi0qRJxtdKN0s1bdoUADB48GCoVCrj89LNUps3b4ajoyNu3rxp8h6TJk1Cz549a2w/IyIiMHXqVFy8eBGnTp0yLnOnz2Pnzp0YPXo0srKyjFeA5syZAwAoLCzE9OnT0ahRI7i4uKBLly7YuXPnHeMhqi+Y3BDVM2q1Gh999BGOHz+O//73v9i+fTumT59e6fJPP/00GjdujEOHDiE2Nhavv/46HBwcAADHjh1DdHQ0HnvsMRw9ehRr167F3r178eKLL5oVk5OTEwCgqKgI69evx+TJk/Hyyy/j+PHjeP755zF69Gjs2LEDAPD999/jww8/xKeffoozZ87gxx9/xH333Vfhdg8dOgQAWL58OVJSUozPS+vbty88PT3xww8/GMt0Oh2+/fZbPP300zW2nzdv3sQ333wDAMb6A+78eXTr1g0LFiwwXgFKSUnBK6+8AgAYPXo09u3bhzVr1uDo0aN48skn0b9/f5w5c6bKMRHZLIvPO05EtW7kyJFiZ2cnLi4uxscTTzxR4bLffvuteHt7G58vX75cPDw8jM/d3NxkxYoVFa47YsQI+cc//mFStmfPHlGr1XL79u0K1ym7/UuXLknXrl2lcePGUlBQIN26dZNx48aZrPPkk0/KwIEDRUTkgw8+kNDQUCksLKxw+8HBwfLhhx8anwOQ9evXmywze/Zs6dChg/H5pEmTpE+fPsbnmzdvFo1GI9evX7+n/QQgLi4u4uzsLAAEgAwaNKjC5Q3u9nmIiJw9e1ZUKpVcuXLFpPzvf/+7zJgx447bJ6oP7JVNrYjIUnr37o0lS5YYn7u4uAAAduzYgXfeeQcJCQnIzs5GcXEx8vPzkZuba1ymtGnTpmHs2LH46quv0LdvXzz55JNo3rw5ACA2NhZnz57FqlWrjMuLCPR6PZKSktCmTZsKY8vKyoKrqytEBHl5eejUqRPWrVsHjUaDxMREkw7BANC9e3csXLgQAPDkk09iwYIFaNasGfr374+BAwfi4Ycfhr199Q9nTz/9NCIjI3H16lUEBgZi1apVGDhwIBo0aHBP++nm5oYjR46guLgYu3btwnvvvYelS5eaLGPu5wEAR44cgYggNDTUpLygoKBW+hIR1XVMbohslIuLC1q0aGFSdvHiRQwcOBDjx4/Hv/71L3h5eWHv3r0YM2YMioqKKtzOnDlzMHz4cGzYsAG//vorZs+ejTVr1mDw4MHQ6/V4/vnnTfq8GDRp0qTS2AwnfbVaDX9//3IncZVKZfJcRIxlQUFBOHXqFGJiYrB161ZMmDAB7733Hnbt2mXS3GOOzp07o3nz5lizZg1eeOEFrF+/HsuXLze+Xt39VKvVxs+gdevWSE1NxdChQ7F7924A1fs8DPHY2dkhNjYWdnZ2Jq+5urqate9EtojJDVE9cvjwYRQXF+ODDz6AWl3S5e7bb7+963qhoaEIDQ3F1KlT8dRTT2H58uUYPHgwOnXqhBMnTpRLou6m9Em/rDZt2mDv3r149tlnjWX79+83uTri5OSEQYMGYdCgQZg4cSJat26NY8eOoVOnTuW25+DgUKVRWMOHD8eqVavQuHFjqNVqPPTQQ8bXqrufZU2dOhXz58/H+vXrMXjw4Cp9HhqNplz8HTt2hE6nQ1paGnr06HFPMRHZInYoJqpHmjdvjuLiYnz88cc4f/48vvrqq3LNJKXdvn0bL774Inbu3ImLFy9i3759OHTokDHReO2113DgwAFMnDgR8fHxOHPmDH7++We89NJL1Y7x1VdfxYoVK7B06VKcOXMG8+fPx7p164wdaVesWIFly5bh+PHjxn1wcnJCcHBwhdtr2rQptm3bhtTUVNy4caPS93366adx5MgRvP3223jiiSfg6OhofK2m9tPd3R1jx47F7NmzISJV+jyaNm2KW7duYdu2bcjIyEBeXh5CQ0Px9NNP49lnn8W6deuQlJSEQ4cO4T//+Q82btxoVkxENknJDj9EZBkjR46URx55pMLX5s+fLwEBAeLk5CTR0dGycuVKASA3btwQEdMOrAUFBTJs2DAJCgoSjUYjgYGB8uKLL5p0oj148KD069dPXF1dxcXFRdq3by9vv/12pbFV1EG2rMWLF0uzZs3EwcFBQkNDZeXKlcbX1q9fL126dBF3d3dxcXGRrl27ytatW42vl+1Q/PPPP0uLFi3E3t5egoODRaR8h2KDBx54QADI9u3by71WU/t58eJFsbe3l7Vr14rI3T8PEZHx48eLt7e3AJDZs2eLiEhhYaHMmjVLmjZtKg4ODtKwYUMZPHiwHD16tNKYiOoLlYiIsukVERERUc1hsxQRERHZFCY3REREZFOY3BAREZFNYXJDRERENoXJDREREdkUJjdERERkU5jcEBERkU1hckNEREQ2hckNERER2RQmN0RERGRTmNwQERGRTfn/pu7szV9977AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878755196485999\n"
     ]
    }
   ],
   "source": [
    "roc_auc_adaBoost_randForest_dropped = roccurveplot(y_test_dropped,y_pred_adaBoost_decision_dropped, 'AdaBoost RandomForest w/ Dropped Dataset')\n",
    "print(roc_auc_adaBoost_randForest_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9222249047695692\n"
     ]
    }
   ],
   "source": [
    "roc_auc_adaBoost_gradBoost_missing = roccurveplot(y_test_missing,y_pred_adaBoost_gradient_missing, 'AdaBoost GradientBoost w/ Missing Dataset')\n",
    "print(roc_auc_adaBoost_gradBoost_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9221283462008225\n"
     ]
    }
   ],
   "source": [
    "roc_auc_adaBoost_gradBoost_dropped = roccurveplot(y_test_dropped,y_pred_adaBoost_gradient_dropped, 'AdaBoost GradientBoost w/ Dropped Dataset')\n",
    "print(roc_auc_adaBoost_gradBoost_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8486788454316219\n"
     ]
    }
   ],
   "source": [
    "roc_auc_mlpmodel_missing = roccurveplot(y_test_missing,y_pred_mlpmodel_missing, 'MLP Neural Network Model w/ Missing Dataset')\n",
    "print(roc_auc_mlpmodel_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZvUlEQVR4nO3de1xT9f8H8NcYDBAFEkFAuXhDxbvgDTUveTftoonXvJeZmlqaZt9Qu/irFNG8lGWYJah5KS0vkfdrCkJqmlcSVAjxAghy2z6/P3An5gZsOBgbr+fjsYfu7Jyz987GznvnvPaZTAghQERERGQhrExdABEREZExsbkhIiIii8LmhoiIiCwKmxsiIiKyKGxuiIiIyKKwuSEiIiKLwuaGiIiILIq1qQsobyqVCrdv30a1atUgk8lMXQ4RERHpQQiBjIwMeHp6wsqq+GMzla65uX37Nry8vExdBhEREZVCYmIiateuXew8la65qVatGoCCjePo6GjiaoiIiEgf6enp8PLykvbjxal0zY36VJSjoyObGyIiIjOjT6SEgWIiIiKyKGxuiIiIyKKwuSEiIiKLwuaGiIiILAqbGyIiIrIobG6IiIjIorC5ISIiIovC5oaIiIgsCpsbIiIisihsboiIiMiimLS5OXz4MAYMGABPT0/IZDL89NNPJS5z6NAhBAQEwM7ODnXr1sWXX35Z9oUSERGR2TBpc5OZmYkWLVpgxYoVes0fHx+Pfv36oXPnzoiNjcV7772HadOmYevWrWVcKREREZkLk/5wZt++fdG3b1+95//yyy/h7e2NsLAwAEDjxo0RHR2NxYsXY9CgQWVUJRERERVFpRLIVaqQk69CnlKF3HwVVEKg9jNVTFaTWf0q+IkTJ9CrVy+Nab1798batWuRl5cHGxsbrWVycnKQk5MjXU9PTy/zOomIiIxJiIIGIk8pkJtf0EDkPdFQ5CpVyMtXIUf53+2F/y2Y9/HySmWh/6s01qkxTVlo2uPln7xPpUpo1evhZIcTc58zwZYqYFbNTXJyMmrWrKkxrWbNmsjPz0dqaio8PDy0llm0aBEWLFhQXiUSEZGZEkIgXyW0d/BPNAW5+UJqJHJ1zluo+dBqNEShRkNHA1FEo5Gn1G4gKiqF3ArWcplJazCr5gYAZDLNDSaE0Dldbe7cuZg5c6Z0PT09HV5eXmVXIBERFUvXaYwij0ToOOqg8+hCcY2GUoW8fKHVaGitS6mCMJMewtpKBhu5FRTWVrCRW8HWWv1/GRTWVlDIraTbFep/rXVMKzyftRUUj5cvPJ+NtRVs5ZrLa95n4XXJitwfl+v2MXUBhnB3d0dycrLGtJSUFFhbW8PFxUXnMra2trC1tS2P8oiIKgz1aYzcJ5qCnCd2+oWbAd2NhvZpjMINiK5G48kjFQX3+Xj5Ik5jVEQyGTSagOJ3+ppNga36/zqW02401A2J/L/mRNd9FmpG5FambyAqMrNqbjp06ICdO3dqTPvtt98QGBioM29DRFSWtE5jaB1NEBqnMXQ2AzqOHmg3Go8bhEJHKHTdpzmfxii8Uy/56MKTRynksLGW6Ty6oH+jIdO6T7lVxTgKQYYzaXPz8OFDXL16VboeHx+PuLg4VK9eHd7e3pg7dy5u3bqF9evXAwAmTZqEFStWYObMmZg4cSJOnDiBtWvXIjIy0lQPgYjKgfJxA5GT/+RRAVWRO32Now6FjhoUdaRCY5o0n9DRaDxe/+P74mkMyziNQZbFpM1NdHQ0unXrJl1XZ2NGjx6NdevWISkpCQkJCdLtderUwa5duzBjxgysXLkSnp6eWL58Ob8GTmQEuk5jFL3TLyI8qXXEQse3K/Q4JVLQVPA0Bk9jEJWOTAhz+dxhHOnp6XByckJaWhocHR1NXQ5VMurTGEWHIguaAp3hyaIClDobDe3TGCUFMXkag6cxiCoyQ/bfZpW5ITIXYb9fxvbYW8jJ+6+B4GkMnsYgovLB5obIyJLSHmH5vivQ50wKT2MQERkfmxsiI9t8+iZUAmjt7YwFA5vyNAYRUTljc0NkREqVwKbTBSH40UG+aFbbycQVERFVPib9VXAiS3Pocgpup2XjmSo26N3E3dTlEBFVSmxuiIwo4o+CozaDWteGnY3cxNUQEVVObG6IjCQp7RH2/50CABja1tvE1RARVV5sboiMZNPpRKgE0K5OddR3q2rqcoiIKi02N0RGUBAkTgQADG/HozZERKbE5obICA5eSkESg8RERBUCmxsiI4g8xSAxEVFFweaG6CndfvBfkHgYT0kREZkcmxuip7Q5+r8gcT1XBomJiEyNzQ3RU8hXqhgkJiKqYNjcED2FQ5fvSEHiPk0ZJCYiqgjY3BA9BfWIxIMDasPWmkFiIqKKgM0NUSndfvAIBy5xRGIiooqGzQ1RKalHJG5fl0FiIqKKhM0NUSnkK1XYHF0QJB7GozZERBUKmxuiUjh4iUFiIqKKis0NUSmoRyRmkJiIqOJhc0NkoMJBYp6SIiKqeNjcEBmocJC4LoPEREQVDpsbIgNojkjsY+JqiIhIFzY3RAY4eOkOktOzUd1Bgd5Napq6HCIi0oHNDZEBIhgkJiKq8NjcEOnp1oNHOKgekbiNl4mrISKiorC5IdKTOkjcoa4Lg8RERBUYmxsiPeQrVdj8OEg8rB2//k1EVJGxuSHSwwEGiYmIzAabGyI9cERiIiLzweaGqAQMEhMRmRc2N0QlYJCYiMi8sLkhKkbBiMQFp6SGM0hMRGQW2NwQFePApTv4Nz0H1R0U6MUgMRGRWWBzQ1SMiD9uAABeYZCYiMhssLkhKsKtB49w8PIdAMDQtjwlRURkLtjcEBVh06kECAEE1XNBnRoOpi6HiIj0xOaGSId8pQqboh+PSMyjNkREZoXNDZEO+/9Owb/pOXBxUKB3E3dTl0NERAZgc0OkQ+ERiRXW/DMhIjInfNcmesLN+1kMEhMRmTE2N0RP2Hw6kUFiIiIzxuaGqJDCQWKOSExEZJ7Y3BAVUjhI3MufQWIiInPE5oaokAh1kDiQQWIiInPFd2+ix27ez8Khx0HiYW14SoqIyFyxuSF6TB0k7ljfBb4MEhMRmS02N0TgiMRERJaEzQ0RGCQmIrIkbG6IwCAxEZEl4bs4VXoMEhMRWRY2N1TpbWKQmIjIorC5oUotX6nCptMMEhMRWRI2N1Sp7fs7BSkZDBITEVkSNjdUqUUySExEZHH4bk6VVuI9BomJiCyRyZubVatWoU6dOrCzs0NAQACOHDlS7PwbNmxAixYtUKVKFXh4eGDs2LG4e/duOVVLlmRzNIPERESWyKTNzaZNmzB9+nTMmzcPsbGx6Ny5M/r27YuEhASd8x89ehSvvvoqxo8fj7/++gs//vgjTp8+jQkTJpRz5WTu8goFiYe39TFxNUREZEwmbW5CQ0Mxfvx4TJgwAY0bN0ZYWBi8vLywevVqnfOfPHkSvr6+mDZtGurUqYNOnTrh9ddfR3R0dJH3kZOTg/T0dI0L0f7HQeIaVRXo6V/T1OUQEZERmay5yc3NRUxMDHr16qUxvVevXjh+/LjOZYKCgnDz5k3s2rULQgj8+++/2LJlC/r371/k/SxatAhOTk7SxcvLy6iPg8xTxB+Pg8QBXgwSExFZGJO9q6empkKpVKJmTc1PzTVr1kRycrLOZYKCgrBhwwYEBwdDoVDA3d0dzs7O+OKLL4q8n7lz5yItLU26JCYmGvVxkPlJvJeFw1cKgsRD27DZJSKyNCb/yCqTyTSuCyG0pqlduHAB06ZNwwcffICYmBjs2bMH8fHxmDRpUpHrt7W1haOjo8aFKjf1iMSd6tdgkJiIyAJZm+qOa9SoAblcrnWUJiUlRetojtqiRYvQsWNHzJo1CwDQvHlzODg4oHPnzvjoo4/g4eFR5nWTectTqrA5miMSExFZMpMduVEoFAgICEBUVJTG9KioKAQFBelcJisrC1ZWmiXL5XIABUd8iEqy7yKDxEREls6kp6VmzpyJb775Bt9++y0uXryIGTNmICEhQTrNNHfuXLz66qvS/AMGDMC2bduwevVqXL9+HceOHcO0adPQtm1beHp6muphkBmRRiRmkJiIyGKZ7LQUAAQHB+Pu3btYuHAhkpKS0LRpU+zatQs+PgXjjiQlJWmMeTNmzBhkZGRgxYoVePvtt+Hs7Izu3bvj008/NdVDIDNSOEg8rC2DxERElkomKtn5nPT0dDg5OSEtLY3h4kpm8d5LWHHgKjrVr4EfJrQzdTlERGQAQ/bfPC5PlUKeUoVNj4PEw9sxSExEZMnY3FClsO9iCu48DhL3aMwgMRGRJWNzQ5VCxOMg8SuBDBITEVk6vsuTxUu8l4UjHJGYiKjSYHNDFm/j6QQIAXRuUAM+LhyRmIjI0rG5IYtWMCLxTQAckZiIqLJgc0MWbd/Ffx8HiW05IjERUSXB5oYsWsSpgq9/vxJYGzZyvtyJiCoDvtuTxSocJB7WhqekiIgqCzY3ZLEKB4m9XaqYuhwiIionbG7IIhUOEg9nkJiIqFJhc0MWqXCQuAeDxERElQqbG7JIG/4oGJF4CIPERESVDt/1yeIUBIlTAQBDGSQmIqp02NyQxYl8/DtSDBITEVVObG7IojBITEREbG7Iovx+4V+kPmSQmIioMmNzQxYl4hSDxERElR3f/cliJNz9L0jMH8kkIqq82NyQxdh4+r8gsVd1BomJiCorNjdkEQoHiUe041EbIqLKjM0NWQR1kNi1mi2ea8wgMRFRZcbmhiwCg8RERKTGvQCZPXWQWCbjiMRERMTmhizAf0FiVwaJiYiIzQ2ZN80Rib1MXA0REVUEbG7IrDFITERET2JzQ2aNQWIiInoS9wZkthgkJiIiXdjckNmKZJCYiIh0YHNDZik3X4UfoxMBMEhMRESa2NyQWfr94r9IfZjLIDEREWlhc0NmKZJBYiIiKkKp9gr5+fn4/fff8dVXXyEjIwMAcPv2bTx8+NCoxRHpcuNuJoPERERUJGtDF7hx4wb69OmDhIQE5OTkoGfPnqhWrRo+++wzZGdn48svvyyLOokkG08XZG0YJCYiIl0MPnLz1ltvITAwEPfv34e9vb00/aWXXsK+ffuMWhzRkzSDxDxqQ0RE2gw+cnP06FEcO3YMCoVCY7qPjw9u3bpltMKIdNEMEruZuhwiIqqADD5yo1KpoFQqtabfvHkT1apVM0pRREWJ+KMgSBwc6MUgMRER6WTw3qFnz54ICwuTrstkMjx8+BAhISHo16+fMWsj0nDjbiaOXi0IEge34dg2RESkm8GnpZYuXYpu3brB398f2dnZGD58OK5cuYIaNWogMjKyLGokAgBEnirI2jzLIDERERXD4ObG09MTcXFx2LhxI2JiYqBSqTB+/HiMGDFCI2BMZEy5+SpsiSloboYxSExERMUwuLk5fPgwgoKCMHbsWIwdO1aanp+fj8OHD+PZZ581aoFEABB1oSBI7MYgMRERlcDgzE23bt1w7949relpaWno1q2bUYoietJ/IxIzSExERMUzeC8hhIBMJtOafvfuXTg4OBilKKLC/kllkJiIiPSn92mpl19+GUDBt6PGjBkDW1tb6TalUomzZ88iKCjI+BVSpacekZhBYiIi0ofezY2TkxOAgiM31apV0wgPKxQKtG/fHhMnTjR+hVSpFQ4SD2/HIDEREZVM7+YmPDwcAODr64t33nmHp6CoXBQOEndvxCAxERGVzOBvS4WEhJRFHUQ6RZy6AaAga8MgMRER6cPg5gYAtmzZgs2bNyMhIQG5ubkat505c8YohRH9k5qJY1fvMkhMREQGMfij8PLlyzF27Fi4ubkhNjYWbdu2hYuLC65fv46+ffuWRY1USUWeLvj6dxc/V9R+hkFiIiLSj8HNzapVq7BmzRqsWLECCoUCs2fPRlRUFKZNm4a0tLSyqJEqodx8FbZE3wTAEYmJiMgwBjc3CQkJ0le+7e3tkZGRAQAYNWoUf1uKjOa3C8m4m/l4RGIGiYmIyAAGNzfu7u64e/cuAMDHxwcnT54EAMTHx0MIYdzqqNJSj0gc3MYL1gwSExGRAQzea3Tv3h07d+4EAIwfPx4zZsxAz549ERwcjJdeesnoBVLlwyAxERE9DYO/LbVmzRqoVCoAwKRJk1C9enUcPXoUAwYMwKRJk4xeIFU+DBITEdHTMLi5sbKygpXVfwd8hgwZgiFDhgAAbt26hVq1ahmvOqp0CgeJhzNITEREpWCUMENycjKmTp2K+vXrG7zsqlWrUKdOHdjZ2SEgIABHjhwpdv6cnBzMmzcPPj4+sLW1Rb169fDtt9+WtnSqYNRB4pqOHJGYiIhKR+/m5sGDBxgxYgRcXV3h6emJ5cuXQ6VS4YMPPkDdunVx8uRJg5uMTZs2Yfr06Zg3bx5iY2PRuXNn9O3bFwkJCUUuM2TIEOzbtw9r167FpUuXEBkZiUaNGhl0v1RxRfzxOEgcyCAxERGVjkzo+RWnyZMnY+fOnQgODsaePXtw8eJF9O7dG9nZ2QgJCUGXLl0MvvN27dqhdevWWL16tTStcePGePHFF7Fo0SKt+ffs2YOhQ4fi+vXrqF69ul73kZOTg5ycHOl6eno6vLy8kJaWBkdHR4NrprITn5qJbosPQiYDjszuxrwNERFJ0tPT4eTkpNf+W++Pxr/++ivCw8OxePFi7NixA0II+Pn5Yf/+/aVqbHJzcxETE4NevXppTO/VqxeOHz+uc5kdO3YgMDAQn332GWrVqgU/Pz+88847ePToUZH3s2jRIjg5OUkXLy9++6ai2vj4699dGSQmIqKnoHeg+Pbt2/D39wcA1K1bF3Z2dpgwYUKp7zg1NRVKpRI1a9bUmF6zZk0kJyfrXOb69es4evQo7OzssH37dqSmpmLy5Mm4d+9ekafE5s6di5kzZ0rX1UduqGLJyVfixxiOSExERE9P7+ZGpVLBxsZGui6Xy+Hg4PDUBchkMo3rQgitaYVrkMlk2LBhA5ycnAAAoaGhGDx4MFauXAl7e3utZWxtbWFra/vUdVLZ+u2vf3GPQWIiIjICvZsbIQTGjBkjNQrZ2dmYNGmSVoOzbds2vdZXo0YNyOVyraM0KSkpWkdz1Dw8PFCrVi2psQEKMjpCCNy8eRMNGjTQ9+FQBSONSMwgMRERPSW99yKjR4+Gm5ublF0ZOXIkPD09NfIshZuOkigUCgQEBCAqKkpjelRUlPTbVU/q2LEjbt++jYcPH0rTLl++DCsrK9SuXVvv+6aKJT41E8evPR6RmKekiIjoKel95CY8PNzodz5z5kyMGjUKgYGB6NChA9asWYOEhARppOO5c+fi1q1bWL9+PQBg+PDh+PDDDzF27FgsWLAAqampmDVrFsaNG6fzlBSZh8JB4lrOfB6JiOjpGDxCsTEFBwfj7t27WLhwIZKSktC0aVPs2rULPj4+AICkpCSNMW+qVq2KqKgoTJ06FYGBgXBxccGQIUPw0Ucfmeoh0FMqHCQe3s7HxNUQEZEl0HucG0thyPfkqezt/PM2pkbGwt3RDkff7ca8DRER6VQm49wQlQX1iMRD2jBITERExsG9CZnM9TsPceL6XVjJgOA2HHuIiIiMg80Nmcym04kAgK4N3RgkJiIioylVc/P999+jY8eO8PT0xI0bNwAAYWFh+Pnnn41aHFkujkhMRERlxeDmZvXq1Zg5cyb69euHBw8eQKlUAgCcnZ0RFhZm7PrIQqlHJHZ3tEO3hq6mLoeIiCyIwc3NF198ga+//hrz5s2DXC6XpgcGBuLcuXNGLY4sF4PERERUVgzeq8THx6NVq1Za021tbZGZmWmUosiyMUhMRERlyeDmpk6dOoiLi9Oavnv3bulXw4mKs5FBYiIiKkMGj1A8a9YsvPnmm8jOzoYQAqdOnUJkZCQWLVqEb775pixqJAuSk6/EFvWIxAwSExFRGTC4uRk7dizy8/Mxe/ZsZGVlYfjw4ahVqxaWLVuGoUOHlkWNZEH2FgoSd2WQmIiIykCpfltq4sSJmDhxIlJTU6FSqeDm5mbsushCRTJITEREZczgvcuCBQtw7do1AECNGjXY2JDeGCQmIqLyYHBzs3XrVvj5+aF9+/ZYsWIF7ty5UxZ1kQVikJiIiMqDwc3N2bNncfbsWXTv3h2hoaGoVasW+vXrh4iICGRlZZVFjWQBGCQmIqLyUqrQQ5MmTfDJJ5/g+vXrOHDgAOrUqYPp06fD3d3d2PWRhWCQmIiIystTJzodHBxgb28PhUKBvLw8Y9REFijij4LfIAtmkJiIiMpYqfYy8fHx+Pjjj+Hv74/AwECcOXMG8+fPR3JysrHrIwtw/c5DnLx+j0FiIiIqFwZ/FbxDhw44deoUmjVrhrFjx0rj3BAVJfJUwde/uzV0gyeDxEREVMYMbm66deuGb775Bk2aNCmLesjCFA4SD2OQmIiIyoHBzc0nn3xSFnWQhdpzPhn3s/Lg4cQgMRERlQ+9mpuZM2fiww8/hIODA2bOnFnsvKGhoUYpjCyD+pTUkEAGiYmIqHzo1dzExsZK34SKjY0t04LIclxjkJiIiExAr+bmwIEDOv9PVJyNDBITEZEJGHyeYNy4ccjIyNCanpmZiXHjxhmlKDJ/2XmFRiRuxyAxERGVH4Obm++++w6PHj3Smv7o0SOsX7/eKEWR+dv7139B4i5+DBITEVH50fvbUunp6RBCQAiBjIwM2NnZSbcplUrs2rWLvxBOkog/Ck5JcURiIiIqb3o3N87OzpDJZJDJZPDz89O6XSaTYcGCBUYtjszTtTsP8Uc8g8RERGQaejc3Bw4cgBAC3bt3x9atW1G9enXpNoVCAR8fH3h6epZJkWReIh8fteneyA0eTgwSExFR+dK7uenSpQuAgt+V8vb2hkwmK7OiyHxl5ymx9QxHJCYiItPRq7k5e/YsmjZtCisrK6SlpeHcuXNFztu8eXOjFUfmp3CQuGtDZrCIiKj86dXctGzZEsnJyXBzc0PLli0hk8kghNCaTyaTQalUGr1IMh+Fg8RyKx7dIyKi8qdXcxMfHw9XV1fp/0S6XE1hkJiIiExPr+bGx8dH5/+JClOPSMwgMRERmVKpBvH79ddfpeuzZ8+Gs7MzgoKCcOPGDaMWR+YjO0+JLWc4IjEREZmewc3NJ598Anv7gk/lJ06cwIoVK/DZZ5+hRo0amDFjhtELJPOw969kPMjKg6eTHbr4MUhMRESmo/dXwdUSExNRv359AMBPP/2EwYMH47XXXkPHjh3RtWtXY9dHZmKDFCT2ZpCYiIhMyuAjN1WrVsXdu3cBAL/99ht69OgBALCzs9P5m1Nk+a6mPMSpx0HiIW1qm7ocIiKq5Aw+ctOzZ09MmDABrVq1wuXLl9G/f38AwF9//QVfX19j10dmIFIKEtdkkJiIiEzO4CM3K1euRIcOHXDnzh1s3boVLi4uAICYmBgMGzbM6AVSxVZ4ROLh7fj1byIiMj2Z0DUanwVLT0+Hk5MT0tLS4OjoaOpyzN5PsbcwfVMcPJ3scOTd7szbEBFRmTBk/23waSkAePDgAdauXYuLFy9CJpOhcePGGD9+PJycnEpVMJmviFMMEhMRUcVi8Gmp6Oho1KtXD0uXLsW9e/eQmpqKpUuXol69ejhz5kxZ1EgV1NWUDClIzBGJiYioojD4yM2MGTMwcOBAfP3117C2Llg8Pz8fEyZMwPTp03H48GGjF0kVU+SpRAAFQWJ3JzsTV0NERFTA4OYmOjpao7EBAGtra8yePRuBgYFGLY4qrsJB4hEckZiIiCoQg09LOTo6IiEhQWt6YmIiqlWrZpSiqOLbc75gROJazvZ41s/V1OUQERFJDG5ugoODMX78eGzatAmJiYm4efMmNm7ciAkTJvCr4JVIhDQisReDxEREVKEYfFpq8eLFkMlkePXVV5Gfnw8AsLGxwRtvvIH/+7//M3qBVPFcTcnAqX/uQW4lw5BABomJiKhiMbi5USgUWLZsGRYtWoRr165BCIH69eujSpUqZVEfVUD/BYndGCQmIqIKR+/TUllZWXjzzTdRq1YtuLm5YcKECfDw8EDz5s3Z2FQiGiMSt2WQmIiIKh69m5uQkBCsW7cO/fv3x9ChQxEVFYU33nijLGujCohBYiIiquj0Pi21bds2rF27FkOHDgUAjBw5Eh07doRSqYRcLi+zAqliYZCYiIgqOr2P3CQmJqJz587S9bZt28La2hq3b98uk8Ko4mGQmIiIzIHezY1SqYRCodCYZm1tLX1jiixfxB8MEhMRUcWn92kpIQTGjBkDW1tbaVp2djYmTZoEBwcHadq2bduMWyFVCBpBYo5ITEREFZjezc3o0aO1po0cOdKoxVDFtft8EtIePQ4SN2CQmIiIKi69m5vw8PCyrIMquMjHp6SGMkhMREQVnME/v2Bsq1atQp06dWBnZ4eAgAAcOXJEr+WOHTsGa2trtGzZsmwLJFz5t1CQuA2DxEREVLGZtLnZtGkTpk+fjnnz5iE2NhadO3dG3759df4wZ2FpaWl49dVX8dxzz5VTpZWbekTi5xq5oaYjg8RERFSxmbS5CQ0Nxfjx4zFhwgQ0btwYYWFh8PLywurVq4td7vXXX8fw4cPRoUOHcqq08iocJB7GIDEREZkBkzU3ubm5iImJQa9evTSm9+rVC8ePHy9yufDwcFy7dg0hISF63U9OTg7S09M1LqQ/BomJiMjcmKy5SU1NhVKpRM2aNTWm16xZE8nJyTqXuXLlCubMmYMNGzbA2lq/LPSiRYvg5OQkXby8mBkxhHpEYgaJiYjIXJSqufn+++/RsWNHeHp64saNGwCAsLAw/PzzzwavSybT3GEKIbSmAQWDCA4fPhwLFiyAn5+f3uufO3cu0tLSpEtiYqLBNVZWV/7NwOl/7jNITEREZsXg5mb16tWYOXMm+vXrhwcPHkCpVAIAnJ2dERYWpvd6atSoAblcrnWUJiUlRetoDgBkZGQgOjoaU6ZMgbW1NaytrbFw4UL8+eefsLa2xv79+3Xej62tLRwdHTUupJ+IUwVHbRgkJiIic2Jwc/PFF1/g66+/xrx58zR+MDMwMBDnzp3Tez0KhQIBAQGIiorSmB4VFYWgoCCt+R0dHXHu3DnExcVJl0mTJqFhw4aIi4tDu3btDH0oVIzsPCW2nbkFgEFiIiIyL3oP4qcWHx+PVq1aaU23tbVFZmamQeuaOXMmRo0ahcDAQHTo0AFr1qxBQkICJk2aBKDglNKtW7ewfv16WFlZoWnTphrLu7m5wc7OTms6Pb1d5xgkJiIi82Rwc1OnTh3ExcXBx8dHY/ru3bvh7+9v0LqCg4Nx9+5dLFy4EElJSWjatCl27dolrTspKanEMW+obESeYpCYiIjMk0wIIQxZIDw8HP/73/+wZMkSjB8/Ht988w2uXbuGRYsW4ZtvvsHQoUPLqlajSE9Ph5OTE9LS0pi/KcLlfzPQa+lhyK1kOD6nO/M2RERkcobsvw0+cjN27Fjk5+dj9uzZyMrKwvDhw1GrVi0sW7aswjc2pJ9IBomJiMiMGdzcAMDEiRMxceJEpKamQqVSwc3Nzdh1kYlk5ymxNaZgROLhDBITEZEZKlVzo1ajRg1j1UEVxK5zSUjPzkctZ3t0ZpCYiIjMUKkCxboG2VO7fv36UxVEpqUekXhYWwaJiYjIPBnc3EyfPl3jel5eHmJjY7Fnzx7MmjXLWHWRCVz+NwPRNwpGJH4lkCMSExGReTK4uXnrrbd0Tl+5ciWio6OfuiAyHfVRmx6NGSQmIiLzZbQfzuzbty+2bt1qrNVROSsYkbggSDysLYPERERkvozW3GzZsgXVq1c31uqonP169r8gMUckJiIic2bwaalWrVppBIqFEEhOTsadO3ewatUqoxZH5Uc9ts2wtl6wYpCYiIjMmMHNzYsvvqhx3crKCq6urujatSsaNWpkrLqoHBUOEg9hkJiIiMycQc1Nfn4+fH190bt3b7i7u5dVTVTOCgeJ3RgkJiIiM2dQ5sba2hpvvPEGcnJyyqoeKmeFg8TD2/mUMDcREVHFZ3CguF27doiNjS2LWsgE1EHi2s/Yo3N9jjhNRETmz+DMzeTJk/H222/j5s2bCAgIgIODg8btzZs3N1pxVPYipCCxN4PERERkEfRubsaNG4ewsDAEBwcDAKZNmybdJpPJIISATCaDUqk0fpVUJi4lZyDmxn1YW8nwSkBtU5dDRERkFHo3N9999x3+7//+D/Hx8WVZD5Uj9de/ezSuySAxERFZDL2bGyEEAMDHh6FTS/Aot9CIxO04IjEREVkOgwLFxf0aOJmXX88xSExERJbJoECxn59fiQ3OvXv3nqogKh+RDBITEZGFMqi5WbBgAZycnMqqFionGkHiQAaJiYjIshjU3AwdOhRubm5lVQuVE40gcTUGiYmIyLLonblh3sYyPMpVYqs0IjGDxEREZHn0bm7U35Yi8/bruSRkZOfDq7o9OjFITEREFkjv01Iqlaos66ByEvHHDQDA0DYMEhMRkWUy+LelyHz9nZyOMwkPGCQmIiKLxuamEtl4KhEA0NOfQWIiIrJcbG4qicJB4mFtGSQmIiLLxeamkmCQmIiIKgs2N5UEg8RERFRZsLmpBBgkJiKiyoTNTSUQ+UfBiMQMEhMRUWXA5sbCPcpVYlvsLQAckZiIiCoHNjcW7pezt5GRnQ/v6lXQsR6DxEREZPnY3Fg49Y9kDm3rxSAxERFVCmxuLFjhIPHgAAaJiYiocmBzY8HUQeJeTRgkJiKiyoPNjYUqHCTmiMRERFSZsLmxUAwSExFRZcXmxkJFMEhMRESVFJsbC3QxKR2xDBITEVElxebGAqm//s0gMRERVUZsbizMo1wltp9hkJiIiCovNjcWZufZ28jIYZCYiIgqLzY3FoYjEhMRUWXH5saCFA4SvxLgZepyiIiITILNjQUpHCR2rWZr4mqIiIhMg82NhcjKzZeCxMPb+pi4GiIiItNhc2MhfjmbJAWJg+q5mLocIiIik2FzYyEiHv9I5rC23gwSExFRpcbmxgJcuJ2OuESOSExERASwubEI6iBx7ybuDBITEVGlx+bGzGXl5uOnWI5ITEREpMbmxsz98mdBkNjHhUFiIiIigM2N2YtQj0jchkFiIiIigM2NWVMHiW3kMrwSyCAxERERwObGrEkjEvu7o0ZVBomJiIiACtDcrFq1CnXq1IGdnR0CAgJw5MiRIufdtm0bevbsCVdXVzg6OqJDhw7Yu3dvOVZbcRQOEg9vxyAxERGRmkmbm02bNmH69OmYN28eYmNj0blzZ/Tt2xcJCQk65z98+DB69uyJXbt2ISYmBt26dcOAAQMQGxtbzpWbXuEgcYe6DBITERGpyYQQwlR33q5dO7Ru3RqrV6+WpjVu3BgvvvgiFi1apNc6mjRpguDgYHzwwQd6zZ+eng4nJyekpaXB0dGxVHVXBC+sPIY/Ex9gTt9GmNSlnqnLISIiKlOG7L9NduQmNzcXMTEx6NWrl8b0Xr164fjx43qtQ6VSISMjA9WrVy9ynpycHKSnp2tczN1ft9Pw5+MgMUckJiIi0mSy5iY1NRVKpRI1a9bUmF6zZk0kJyfrtY4lS5YgMzMTQ4YMKXKeRYsWwcnJSbp4eXk9Vd0VgRQkbsIgMRER0ZNMHiiWyTTHZhFCaE3TJTIyEvPnz8emTZvg5uZW5Hxz585FWlqadElMTHzqmk2pIEh8GwAwnCMSExERabE21R3XqFEDcrlc6yhNSkqK1tGcJ23atAnjx4/Hjz/+iB49ehQ7r62tLWxtLefoxs4/b+NhTj58GSQmIiLSyWRHbhQKBQICAhAVFaUxPSoqCkFBQUUuFxkZiTFjxiAiIgL9+/cv6zIrnIhTBUeehrbliMRERES6mOzIDQDMnDkTo0aNQmBgIDp06IA1a9YgISEBkyZNAlBwSunWrVtYv349gILG5tVXX8WyZcvQvn176aiPvb09nJycTPY4yguDxERERCUzaXMTHByMu3fvYuHChUhKSkLTpk2xa9cu+Pj4AACSkpI0xrz56quvkJ+fjzfffBNvvvmmNH306NFYt25deZdf7hgkJiIiKplJx7kxBXMd5yYzJx/tPtmHhzn5iJjQDkH1a5i6JCIionJjFuPckGF+OftfkLg9g8RERERFYnNjJiL+KDglNYxBYiIiomKxuTED52+l4c+babCRyzCIQWIiIqJisbkxAxtPFxy16c0gMRERUYnY3FRwmTkckZiIiMgQbG4quMJB4g71GCQmIiIqCZubCq5wkFif39wiIiKq7NjcVGCFg8QckZiIiEg/bG4qMPWIxL2buMOFQWIiIiK9sLmpoDJz8vFz3OMgcTsGiYmIiPTF5qaC2vlnQZC4Tg0HdOCIxERERHpjc1NBqU9JDWvrxSAxERGRAdjcVEDqILFCboVBrRkkJiIiMgSbmwpIChI3ZZCYiIjIUGxuKpjCQeJhbb1MXA0REZH5YXNTwTBITERE9HTY3FQwEQwSExERPRU2NxXI+VtpOPs4SDw4gKekiIiISoPNTQUSUShIXN1BYeJqiIiIzBObmwoiMycfP8feAgAMb8sRiYmIiEqLzU0FsePP28jMVaJODQe0r1vd1OUQERGZLTY3FQRHJCYiIjIONjcVAIPERERExsPmpgJgkJiIiMh42NyY2EMGiYmIiIyKzY2J7XwcJK7LIDEREZFRsLkxsYg/1EFibwaJiYiIjIDNjQmdu5mGc7cKgsSDAmqbuhwiIiKLwObGhNRB4j4MEhMRERkNmxsTeZiTjx1xBUHiYQwSExERGQ2bGxPZEccgMRERUVlgc2Mi/41IzCAxERGRMbG5MQEGiYmIiMoOmxsTYJCYiIio7LC5KWeFg8TD2zFITEREZGxsbsqZFCR2dUC7OgwSExERGRubm3IWceoGgILfkWKQmIiIyPjY3JSjczfTcP5WOhRyK7zcmkFiIiKissDmphypj9r0bcYgMRERUVlhc1NOHubk4+e42wA4IjEREVFZYnNTTn6Ou4UsBomJiIjKHJubcqIekZhBYiIiorLF5qYcnL35QAoSD2KQmIiIqEyxuSkH6qM2fZu54xkGiYmIiMqUtakLMBdCCOTn50OpVBq0XGZOHk5d/Re1qskxPMAd2dnZZVQhERGRebOxsYFcLn/q9bC50UNubi6SkpKQlZVl8LKZOfmY08kFNnIZHPMfID7+gfELJCIisgAymQy1a9dG1apVn2o9bG5KoFKpEB8fD7lcDk9PTygUCr0DwUII3LibBZt8JVyr2XFsGyIioiIIIXDnzh3cvHkTDRo0eKojOGxuSpCbmwuVSgUvLy9UqVLFoGWzcvORCzmsbKzh5lwV1nJGnIiIiIri6uqKf/75B3l5eU/V3HBvqycrK8M31b3MXACAk70NGxsiIqISGGuoFO5xy4hSpcKDrDwA4OkoIiKicsTmpow8yMqDSgjYWsvhoHj65DcRERHph81NGRBCSKekqjvoH0Am0/jnn38gk8kQFxdn6lLKnK+vL8LCwkxdhhZD65o/fz5atmxZZvVURhX1tWEqubm5qF+/Po4dO2bqUixGSkoKXF1dcevWrTK/LzY3ZeBRnhKP8pSQyWR4poqNyeoYM2YMZDIZJk2apHXb5MmTIZPJMGbMGI35X3zxxSLX5+vrC5lMBplMhipVqqBp06b46quviq1BJpPBzs4ON27c0Jj+4osvatx3Rde1a1fIZDJs3LhRY3pYWBh8fX0NWpdMJsNPP/1kvOLK0MGDBwtex888ozVG06lTp6TXg6Xo2rUrvvzyyyJvUz9eW1tb1KpVCwMGDMC2bdvKuUrTmD9/vvT4ra2tUaNGDTz77LMICwtDTk6OQetSv64ePHhQNsUWwZCmeM2aNfDx8UHHjh21bnvttdcgl8u13g+Aot9H4+LiIJPJ8M8//0jThBBYs2YN2rVrh6pVq8LZ2RmBgYEICwsr1dAj+rp//z5GjRoFJycnODk5YdSoUSU+Fw8fPsSUKVNQu3Zt2Nvbo3Hjxli9erXOeYUQ6Nu3r9Z7nZubG0aNGoWQkBAjPhrd2NyUgYoUJPby8sLGjRvx6NEjaVp2djYiIyPh7W34r5MvXLgQSUlJOHv2LF588UVMmjQJmzZtKnYZmUyGDz74wOD7elp5eXlGXZ+dnR3ef/99o6+3POTm5j7V8tWqVcP27ds1pn377beleg1VVPfu3cPx48cxYMCAIueZOHEikpKScPXqVWzduhX+/v4YOnQoXnvttWLXbY6vGV2aNGmCpKQkJCQk4MCBA3jllVewaNEiBAUFISMjw9TlGdUXX3yBCRMmaE3PysrCpk2bMGvWLKxdu/ap7mPUqFGYPn06XnjhBRw4cABxcXH43//+h59//hm//fbbU627OMOHD0dcXBz27NmDPXv2IC4uDqNGjSp2mRkzZmDPnj344YcfcPHiRcyYMQNTp07Fzz//rDVvWFhYkR96xo4diw0bNuD+/ftGeSxFYXNTCkIIZOXm67xkZOciOS0b2XlK2NtYFTlfaS9CCINqbd26Nby9vTU+XW7btg1eXl5o1aqVwY+9WrVqcHd3R/369fHRRx+hQYMGJR6FmDp1Kn744QecO3euyHmEEPjss89Qt25d2Nvbo0WLFtiyZYt0+7p16+Ds7KyxzE8//aTxB6T+VPbtt9+ibt26sLW1hRACe/bsQadOneDs7AwXFxc8//zzuHbtmsGPfdiwYUhLS8PXX39d7Hw7d+5EQEAA7OzsULduXSxYsAD5+fkAIB3leemllyCTyeDr64u0tDTI5XLExMRI26J69epo06aNtM7IyEh4eHhI18+dO4fu3bvD3t4eLi4ueO211/Dw4UPpdvWnx0WLFsHT0xN+fn46aw0PD4eTkxOioqKKfUyjR4/Gt99+K11/9OgRNm7ciNGjR2vNu3XrVjRp0gS2trbw9fXFkiVLNG5PSUnBgAEDYG9vjzp16mDDhg1a60hLS8Nrr70GNzc3ODo6onv37vjzzz+LrbGwgIAAjft98cUXYW1tjfT0dABAcnIyZDIZLl26JM3z66+/okWLFqhVq1aR661SpQrc3d3h5eWF9u3b49NPP8VXX32Fr7/+Gr///juA/05zbt68GV27doWdnR1++OEHqFQqLFy4ELVr14atrS1atmyJPXv2SOtWL7dx40YEBQXBzs4OTZo0wcGDB6V51Ec81LXa2dmhXbt2Wn9bx48fx7PPPgt7e3t4eXlh2rRpyMzMlG7X5znQxdraGu7u7vD09ESzZs0wdepUHDp0COfPn8enn34qzffDDz8gMDBQer8YPnw4UlJSpMfZrVs3AMAzzzyjcQS5pL/V3NxcTJkyBR4eHrCzs4Ovry8WLVok3V7c62bdunVYsGAB/vzzT+kI1Lp163Q+zjNnzuDq1avo37+/1m0//vgj/P39MXfuXBw7dkzjSIwhNm/ejA0bNiAyMhLvvfce2rRpA19fX7zwwgvYv3+/tI2M7eLFi9izZw+++eYbdOjQAR06dMDXX3+NX375RePv4UknTpzA6NGj0bVrV/j6+uK1115DixYtEB0drTHfn3/+idDQUI33i8KaNWsGd3d3rQ9LxsZxbkrhUZ4S/h/sNcl9X1jYG1UUhj1tY8eORXh4OEaMGAGg4BP3uHHjNN40S8vOzq7ET6VBQUG4dOkS5s6di19++UXnPO+//z62bduG1atXo0GDBjh8+DBGjhwJV1dXdOnSRe96rl69is2bN2Pr1q3SGAmZmZmYOXMmmjVrhszMTHzwwQd46aWXEBcXZ9BX/B0dHfHee+9h4cKFGD16NBwcHLTm2bt3L0aOHInly5ejc+fOuHbtmvSpPiQkBKdPn4abmxvCw8PRp08fyOVyODk5oWXLljh48CACAgJw9uxZAMDZs2eRnp4OR0dHHDx4UNoOWVlZ6NOnD9q3b4/Tp08jJSUFEyZMwJQpUzTerPft2wdHR0dERUXpbIoXL16MRYsWYe/evWjfvn2xj33UqFH4/PPPkZCQAG9vb2zduhW+vr5o3bq1xnwxMTEYMmQI5s+fj+DgYBw/fhyTJ0+Gi4uLtAMbM2YMEhMTsX//figUCkybNk3a8QEFzV3//v1RvXp17Nq1C05OTvjqq6/w3HPP4fLly6hevXqJz1XXrl1x8OBBvP322xBC4MiRI3jmmWdw9OhR9OvXDwcOHIC7uzsaNmwoLbNjxw688MILJa77SaNHj8bbb7+Nbdu2oUePHtL0d999F0uWLEF4eDhsbW2xbNkyLFmyBF999RVatWqFb7/9FgMHDsRff/2FBg0aSMvNmjULYWFh8Pf3R2hoKAYOHIj4+Hi4uLhozLNs2TK4u7vjvffew8CBA3H58mXY2Njg3Llz6N27Nz788EOsXbsWd+7cwZQpUzBlyhSEh4fr9RwYolGjRujbty+2bduGjz76CEBBE/Lhhx+iYcOGSElJwYwZMzBmzBjs2rULXl5e2Lp1KwYNGoRLly7B0dER9vb2AEr+W12+fDl27NiBzZs3w9vbG4mJiUhMTARQ8usmODgY58+fx549e6RG1MnJSedjOnz4MPz8/ODo6Kh129q1azFy5Eg4OTmhX79+CA8Px4IFCwzebhs2bEDDhg11vuZkMlmRtQEocfTezp07Y/fu3TpvO3HiBJycnNCuXTtpWvv27eHk5ITjx49r/E0U1qlTJ+zYsQPjxo2Dp6cnDh48iMuXL2PZsmXSPFlZWRg2bBhWrFgBd3f3Iutr27Ytjhw5gnHjxhX7OJ6GyY/crFq1CnXq1IGdnR0CAgJw5MiRYuc/dOiQxqfios6P039GjRqFo0eP4p9//sGNGzdw7NgxjBw58qnWmZ+fj3Xr1uHcuXN47rnnSpx/0aJF2LNnj87nNzMzU+r0e/fujbp162LMmDEYOXJkiZmeJ+Xm5uL7779Hq1at0Lx5c8hkMgwaNAgvv/wyGjRogJYtW2Lt2rU4d+4cLly4YNC6gYKskp2dHUJDQ3Xe/vHHH2POnDkYPXo06tati549e+LDDz+UHoerqysAwNnZGe7u7tJ19c4YKPh0/txzz6Fp06Y4evSoNK1r164ACt4UHz16hPXr16Np06bo3r07VqxYge+//x7//vuvVIuDgwO++eYbNGnSBE2bNtWoc+7cuQgNDcXBgwdLbGyAgnPlffv2lZondYP8pNDQUDz33HP43//+Bz8/P4wZMwZTpkzB559/DgC4fPkydu/eLX1qDAgIwNq1azVOmx44cADnzp3Djz/+iMDAQDRo0ACLFy+Gs7OzxtG84nTt2hVHjhyBSqXC2bNnIZfLMWrUKI1tXLhpzsnJwd69e0vV3FhZWcHPz0/rE/z06dPx8ssvo06dOvD09MTixYvx7rvvYujQoWjYsCE+/fRTtGzZUivEO2XKFAwaNEjKNDg5OWmd/ggJCUHPnj3RrFkzfPfdd/j333+lT8Kff/45hg8fjunTp6NBgwYICgrC8uXLsX79emRnZ+v1HBiqUaNGGo9/3Lhx6Nu3L+rWrYv27dtj+fLl2L17Nx4+fAi5XC41qG5ubnB3d5d25CX9rSYkJKBBgwbo1KkTfHx80KlTJwwbNgxAya8be3t7VK1aVTr65O7uLjVVT/rnn3/g6empNf3KlSs4efIkgoODAQAjR45EeHg4VCqVwdvsypUrRTYSJYmLiyv28s033xS5bHJyMtzc3LSmu7m5ITk5ucjlli9fDn9/f9SuXRsKhQJ9+vTBqlWr0KlTJ2meGTNmICgoqMS/o1q1apX6iJe+THrkZtOmTZg+fTpWrVqFjh074quvvkLfvn1x4cIFnefy4+Pj0a9fP0ycOBE//PADjh07hsmTJ8PV1RWDBg0qt7rtbeS4sLC31vSs3Hxcv5MJmUyGhjXLZkRiexvDv1Zeo0YN9O/fH99995306aZGjRqluv93330X77//PnJycqBQKDBr1iy8/vrrJS7n7++PV199Fe+++y6OHz+ucduFCxeQnZ2Nnj17akzPzc01+NSZj4+P1DCoXbt2Df/73/9w8uRJpKamSm9ECQkJWjv9ktja2mLhwoWYMmUK3njjDa3bY2JicPr0aXz88cfSNKVSiezsbGRlZRU5ynXXrl2xdu1aqFQqHDp0CM899xy8vb1x6NAhtG7dGpcvX5Z2xhcvXkSLFi00jhx17NgRKpUKly5dQs2aNQEUHP5VKLTHWFqyZAkyMzMRHR2NunXr6v3Yx40bh7feegsjR47EiRMn8OOPP2o1qxcvXtR6Y+vYsSPCwsKgVCpx8eJFWFtbIzAwULq9UaNGGqccY2Ji8PDhQ40jFUDBqTB9Tyc+++yzyMjIQGxsLI4dO4YuXbqgW7du0pGFgwcPYvr06dL8+/fvh4uLC5o1a6bX+p8khNDKGBR+jOnp6bh9+7ZWOLVjx45ap9s6dOgg/V+9rS5evFjkPNWrV0fDhg2leWJiYnD16lWNU01CCOmnZC5fvlzic2CoJx9/bGws5s+fj7i4ONy7d0/jb87f37/I9ZT0tzpmzBj07NkTDRs2RJ8+ffD888+jV69e0uN+2tdN4WXs7Oy0pq9duxa9e/eW3j/79euH8ePH4/fff5fq0Jeu14y+6tevX6rl1HTdb0n1LF++HCdPnsSOHTvg4+ODw4cPY/LkyfDw8ECPHj2wY8cO7N+/H7GxsSXev729fZkGpgETNzehoaEYP368FNoKCwvD3r17sXr1ao3zqGpffvklvL29pU86jRs3RnR0NBYvXlyuzY1MJtN5auheZi7sbOR4pooCjvYVa+C+cePGYcqUKQCAlStXlno9s2bNwpgxY1ClShV4eHgY9Me5YMEC+Pn5aWV01G9gv/76q1bewdbWFkDBp+MnT63oOh2m61TRgAED4OXlha+//hqenp5QqVRo2rRpqUO2I0eOxOLFi/HRRx9pfVNKpVJhwYIFePnll7WW0/VmqabeGZ85cwZHjhzBhx9+CC8vL3zyySdo2bIl3Nzc0LhxYwDFvwkVnq5rWwAFh6x//fVXbN68GXPmzCnp4Ur69euH119/HePHj8eAAQO0diJF1Vb4eVP/v7jXjUqlgoeHh87TpvrugAuf6jt+/Di6d++Ozp07Iy4uDleuXMHly5elI2FA6U9JAQXN65UrVzQyUoDu7a9r2+jzN2TIPCqVCq+//jqmTZumNY+3t7eUqzDmt9wuXryIOnXqACg4EturVy/06tULP/zwA1xdXZGQkIDevXuX+DdX0t9q69atER8fj927d+P333/HkCFD0KNHD2zZssUorxu1GjVqaOWYlEol1q9fj+TkZFhbW2tMX7t2rdTcODo6an07FID0bST1USo/Pz+tplVfT3Nayt3dXeMIr9qdO3ekD0ZPevToEd577z1s375dyiE1b94ccXFxWLx4MXr06IH9+/fj2rVrWtt60KBB6Ny5s8bzcu/ePa0PocZmsuYmNzcXMTExWm+uvXr10vpkr3bixAmt7rh3795Yu3Yt8vLyYGOj/bXrnJwcja8pqgOFxlbRRyTu06eP9AbRu7f2USd91ahRo9SfGry8vDBlyhS89957qFevnjTd398ftra2SEhIKDJf4+rqioyMDGRmZko7DX3Gpbl79y4uXryIr776Cp07dwYA6VRPaVlZWWHRokV4+eWXtY7etG7dGpcuXSp2G9nY2ECpVGpMU++MV6xYAZlMBn9/f3h6eiI2Nha//PKLxnbx9/fHd999p7Etjh07Jp0eKUnbtm0xdepU9O7dG3K5HLNmzdLrcatP7Xz22WdFvnH6+/trbd/jx4/Dz88PcrkcjRs3Rn5+PqKjo9G2bVsAwKVLlzS+htq6dWtpB2Lo1+wL69q1Kw4cOIA//vgDCxcuhLOzM/z9/fHRRx9pNYs7d+7E+vXrS3U/3333He7fv1/sByxHR0d4enri6NGjePbZZ6Xpx48fl7aD2smTJ6V58vPzERMTI30wKTyP+uj2/fv3cfnyZTRq1AhAwfb766+/inwN6vMcGOLvv//Gnj17MHfuXOl6amoq/u///g9eXl4AoBU6VR9RLPx3oO/fqqOjI4KDgxEcHIzBgwejT58+uHfvnl6vG4VCofW3p0urVq2wevVqjeZz165d0tHAwr959Pfff2PEiBG4e/cuXFxc0KhRI0RGRiI7O1vjA83p06fh6uqKZ555BkDBN5aGDh2Kn3/+WauxFkIgPT29yNxNSe99RZ1uAwqO+qWlpeHUqVPS8//HH38gLS0NQUFBOpfJy8tDXl6eVkZRLpdLH07nzJmj9e2yZs2aYenSpVrfQDx//rzGh4syIUzk1q1bAoA4duyYxvSPP/5Y+Pn56VymQYMG4uOPP9aYduzYMQFA3L59W+cyISEhAoDWJS0tTa86Hz16JC5cuCAePXpU7HxZOXniYlKa+DspXahUKr3WXdZGjx4tXnjhBel6WlqaxuN+4YUXxOjRozXm79q1q4iNjdW43LhxQwghhI+Pj1i6dKlBNQAQ27dvl67fvXtXODk5CTs7O437njdvnnBxcRHr1q0TV69eFWfOnBErVqwQ69atk5ZzcHAQ06ZNE1euXBEbNmwQnp6eovBLOCQkRLRo0ULj/pVKpXBxcREjR44UV65cEfv27RNt2rTRqCs+Pl4AELGxsUU+ji5duoi33npLY1rnzp2FnZ2d8PHxkabt2bNHWFtbi5CQEHH+/Hlx4cIFsXHjRjFv3jxpngYNGog33nhDJCUliXv37knTZ86cKeRyuRg8eLA0rWXLlkIul4uVK1dK0zIzM4WHh4cYNGiQOHfunNi/f7+oW7eu1nNZ+LlXK/wcHj16VFStWlWEhoYW+bgPHDggAIj79+8LIYTIyckRd+7ckV7j27dv13gOYmJihJWVlVi4cKG4dOmSWLdunbC3txfh4eHSPH369BHNmzcXJ0+eFNHR0aJTp07C3t5eqkulUolOnTqJFi1aiD179oj4+Hhx7NgxMW/ePHH69GkhhO7n+kk7duwQcrlcuLq6SvVOnz5dyOVy8corr0jznT59Wjg7O4u8vLxi19elSxcxceJEkZSUJBITE8XJkyfF7NmzhY2NjXjjjTek+Yp6PS1dulQ4OjqKjRs3ir///lu8++67wsbGRly+fFljOW9vb7Ft2zZx8eJF8dprr4mqVauKO3fuaDwfTZo0Eb///rs4d+6cGDhwoPD29hY5OTlCCCH+/PNPYW9vLyZPnixiY2PF5cuXxc8//yymTJmi93OgS0hIiGjSpIlISkoSt27dEmfPnhXLly8Xbm5uok2bNiIjI0MIIURKSopQKBRi1qxZ4tq1a+Lnn38Wfn5+Gtvk5s2bQiaTiXXr1omUlBSRkZGh199qaGioiIyMFBcvXhSXLl0S48ePF+7u7kKpVOr1utmwYYNwcHAQsbGx4s6dOyI7O1vnY01NTRUKhUKcO3dOmvbCCy+I4OBgrXlVKpWoVauWCAsLE0II8eDBA+Hu7i4GDx4sTp8+La5evSq+//578cwzz4jPPvtMY7ng4GBhb28vPvnkE3H69Gnxzz//iJ07d4ru3btrvG8am/r5P3HihDhx4oRo1qyZeP755zXmadiwodi2bZt0vUuXLqJJkybiwIED4vr16yI8PFzY2dmJVatWFXk/T77/C1Hw/mVvby8OHz6sc5ni9rlpaWl6779N3twcP35cY/pHH30kGjZsqHOZBg0aiE8++URj2tGjRwUAkZSUpHOZ7OxsaaeelpYmEhMTy6S5EaLgxZqTp9RrveWhqB2cmq7mRlcjqJ7HGM2NEEJ88sknGusVomDbLVu2TDRs2FDY2NgIV1dX0bt3b3Ho0CFpnu3bt4v69esLOzs78fzzz4s1a9aU2NwIIURUVJRo3LixsLW1Fc2bNxcHDx40SnNz/PhxAUCjuRGioMEJCgoS9vb2wtHRUbRt21asWbNGun3Hjh2ifv36wtraWmPZnTt3CgBixYoV0rS33npLABDnz5/XuI+zZ8+Kbt26CTs7O1G9enUxceJEaecihH7NjRBCHDp0SDg4OIhly5bpfNxPNjdPerK5EUKILVu2CH9/f2FjYyO8vb3F559/rnF7UlKS6N+/v7C1tRXe3t5i/fr1WnWlp6eLqVOnCk9PT2FjYyO8vLzEiBEjREJCghBCv+bmwYMHWs2iut7C2/j9998XI0aMKHZdQhS8BtR/EwqFQnh4eIjnn39eYwcgRNGvJ6VSKRYsWCBq1aolbGxsRIsWLcTu3bu1louIiBDt2rUTCoVCNG7cWOzbt0+aR/187Ny5UzRp0kQoFArRpk0bERcXp3Ffp06dEj179hRVq1YVDg4Oonnz5hofDPV5Dp5U+IOiXC4X1atXF506dRJLly7VahIiIiKEr6+vsLW1FR06dBA7duzQ2iYLFy4U7u7uQiaTSe8FJf2trlmzRrRs2VI4ODgIR0dH8dxzz4kzZ85I6yzpdZOdnS0GDRoknJ2dBQCNpvtJQ4cOFXPmzBFCCJGcnCysra3F5s2bdc47depU0axZM+n6lStXxKBBg0StWrWEg4ODaNasmVixYoVQKjX3D0qlUqxevVq0adNGVKlSRTg6OoqAgACxbNkykZWVVWRtT+vu3btixIgRolq1aqJatWpixIgRWn/jT26fpKQkMWbMGOHp6Sns7OxEw4YNxZIlS4r9MK/r/T8iIqLIfbwQFtDc5OTkCLlcrvXGMG3aNPHss8/qXKZz585i2rRpGtO2bdsmrK2tRW5url73a8jGEcKw5oaIzE+zZs3Epk2bTF2GXk12Sc0mGc/Zs2eFm5ubSE9PN3UpFqVNmzZiw4YNRd5urObGZF8FVygUCAgI0Bo8LCoqqsjzfh06dNCa/7fffkNgYKDOvA0RUXFyc3MxaNAg9O3b19SlUAXTrFkzfPbZZ2X+leXKJCUlBYMHD5a+vl+WTPptqZkzZ2LUqFEIDAxEhw4dsGbNGiQkJEi/hTR37lzcunVLCvpNmjQJK1aswMyZMzFx4kScOHECa9euRWRkpCkfBhGZKYVCUS6/c0PmSdcI3FR6bm5umD17drncl0mbm+DgYNy9e1f6vaKmTZti165d8PHxAQDpN0zU6tSpg127dmHGjBlYuXIlPD09sXz58nL9GjgRUVnw9fUt8edVunbtavBPsBBVRjJRyf5S1F+vS0tL0zm09pOys7MRHx8vjaJMREREZaO4fa4h+2+T//yCuahkPSAREVG5M9a+ls1NCdRB5bIeKpqIiKiyUw82W3igxNLgr4KXQC6Xw9nZWfrF3CpVqhh12HIiIiIq+OmQO3fuoEqVKho/cVEabG70oP7pdnWDQ0RERMZnZWUFb2/vpz6IwOZGDzKZDB4eHnBzc9P5Y41ERET09BQKhdZvWJUGmxsDyOXypz4PSERERGWLgWIiIiKyKGxuiIiIyKKwuSEiIiKLUukyN+oBgtLT001cCREREelLvd/WZ6C/StfcZGRkAAC8vLxMXAkREREZKiMjA05OTsXOU+l+W0qlUuH27duoVq2a0QfjS09Ph5eXFxITE/X63SoqHW7n8sHtXD64ncsPt3X5KKvtLIRARkYGPD09S/y6eKU7cmNlZYXatWuX6X04OjryD6cccDuXD27n8sHtXH64rctHWWznko7YqDFQTERERBaFzQ0RERFZFDY3RmRra4uQkBDY2tqauhSLxu1cPridywe3c/nhti4fFWE7V7pAMREREVk2HrkhIiIii8LmhoiIiCwKmxsiIiKyKGxuiIiIyKKwuTHQqlWrUKdOHdjZ2SEgIABHjhwpdv5Dhw4hICAAdnZ2qFu3Lr788styqtS8GbKdt23bhp49e8LV1RWOjo7o0KED9u7dW47Vmi9DX89qx44dg7W1NVq2bFm2BVoIQ7dzTk4O5s2bBx8fH9ja2qJevXr49ttvy6la82Xodt6wYQNatGiBKlWqwMPDA2PHjsXdu3fLqVrzdPjwYQwYMACenp6QyWT46aefSlzGJPtBQXrbuHGjsLGxEV9//bW4cOGCeOutt4SDg4O4ceOGzvmvX78uqlSpIt566y1x4cIF8fXXXwsbGxuxZcuWcq7cvBi6nd966y3x6aefilOnTonLly+LuXPnChsbG3HmzJlyrty8GLqd1R48eCDq1q0revXqJVq0aFE+xZqx0mzngQMHinbt2omoqCgRHx8v/vjjD3Hs2LFyrNr8GLqdjxw5IqysrMSyZcvE9evXxZEjR0STJk3Eiy++WM6Vm5ddu3aJefPmia1btwoAYvv27cXOb6r9IJsbA7Rt21ZMmjRJY1qjRo3EnDlzdM4/e/Zs0ahRI41pr7/+umjfvn2Z1WgJDN3Ouvj7+4sFCxYYuzSLUtrtHBwcLN5//30REhLC5kYPhm7n3bt3CycnJ3H37t3yKM9iGLqdP//8c1G3bl2NacuXLxe1a9cusxotjT7Njan2gzwtpafc3FzExMSgV69eGtN79eqF48eP61zmxIkTWvP37t0b0dHRyMvLK7NazVlptvOTVCoVMjIyUL169bIo0SKUdjuHh4fj2rVrCAkJKesSLUJptvOOHTsQGBiIzz77DLVq1YKfnx/eeecdPHr0qDxKNkul2c5BQUG4efMmdu3aBSEE/v33X2zZsgX9+/cvj5IrDVPtByvdD2eWVmpqKpRKJWrWrKkxvWbNmkhOTta5THJyss758/PzkZqaCg8PjzKr11yVZjs/acmSJcjMzMSQIUPKokSLUJrtfOXKFcyZMwdHjhyBtTXfOvRRmu18/fp1HD16FHZ2dti+fTtSU1MxefJk3Lt3j7mbIpRmOwcFBWHDhg0IDg5GdnY28vPzMXDgQHzxxRflUXKlYar9II/cGEgmk2lcF0JoTStpfl3TSZOh21ktMjIS8+fPx6ZNm+Dm5lZW5VkMfbezUqnE8OHDsWDBAvj5+ZVXeRbDkNezSqWCTCbDhg0b0LZtW/Tr1w+hoaFYt24dj96UwJDtfOHCBUybNg0ffPABYmJisGfPHsTHx2PSpEnlUWqlYor9ID9+6alGjRqQy+VanwJSUlK0ulI1d3d3nfNbW1vDxcWlzGo1Z6XZzmqbNm3C+PHj8eOPP6JHjx5lWabZM3Q7Z2RkIDo6GrGxsZgyZQqAgp2wEALW1tb47bff0L1793Kp3ZyU5vXs4eGBWrVqwcnJSZrWuHFjCCFw8+ZNNGjQoExrNkel2c6LFi1Cx44dMWvWLABA8+bN4eDggM6dO+Ojjz7ikXUjMdV+kEdu9KRQKBAQEICoqCiN6VFRUQgKCtK5TIcOHbTm/+233xAYGAgbG5syq9WclWY7AwVHbMaMGYOIiAieM9eDodvZ0dER586dQ1xcnHSZNGkSGjZsiLi4OLRr1668SjcrpXk9d+zYEbdv38bDhw+laZcvX4aVlRVq165dpvWaq9Js56ysLFhZae4C5XI5gP+OLNDTM9l+sEzjyhZG/VXDtWvXigsXLojp06cLBwcH8c8//wghhJgzZ44YNWqUNL/6K3AzZswQFy5cEGvXruVXwfVg6HaOiIgQ1tbWYuXKlSIpKUm6PHjwwFQPwSwYup2fxG9L6cfQ7ZyRkSFq164tBg8eLP766y9x6NAh0aBBAzFhwgRTPQSzYOh2Dg8PF9bW1mLVqlXi2rVr4ujRoyIwMFC0bdvWVA/BLGRkZIjY2FgRGxsrAIjQ0FARGxsrfeW+ouwH2dwYaOXKlcLHx0coFArRunVrcejQIem20aNHiy5dumjMf/DgQdGqVSuhUCiEr6+vWL16dTlXbJ4M2c5dunQRALQuo0ePLv/CzYyhr+fC2Nzoz9DtfPHiRdGjRw9hb28vateuLWbOnCmysrLKuWrzY+h2Xr58ufD39xf29vbCw8NDjBgxQty8ebOcqzYvBw4cKPb9tqLsB2VC8PgbERERWQ5mboiIiMiisLkhIiIii8LmhoiIiCwKmxsiIiKyKGxuiIiIyKKwuSEiIiKLwuaGiIiILAqbGyIiIrIobG6ISMO6devg7Oxs6jJKzdfXF2FhYcXOM3/+fLRs2bJc6iGi8sfmhsgCjRkzBjKZTOty9epVU5eGdevWadTk4eGBIUOGID4+3ijrP336NF577TXpukwmw08//aQxzzvvvIN9+/YZ5f6K8uTjrFmzJgYMGIC//vrL4PWYc7NJZApsbogsVJ8+fZCUlKRxqVOnjqnLAlDwK+NJSUm4ffs2IiIiEBcXh4EDB0KpVD71ul1dXVGlSpVi56latSpcXFye+r5KUvhx/vrrr8jMzET//v2Rm5tb5vdNVJmxuSGyULa2tnB3d9e4yOVyhIaGolmzZnBwcICXlxcmT56Mhw8fFrmeP//8E926dUO1atXg6OiIgIAAREdHS7cfP34czz77LOzt7eHl5YVp06YhMzOz2NpkMhnc3d3h4eGBbt26ISQkBOfPn5eOLK1evRr16tWDQqFAw4YN8f3332ssP3/+fHh7e8PW1haenp6YNm2adFvh01K+vr4AgJdeegkymUy6Xvi01N69e2FnZ4cHDx5o3Me0adPQpUsXoz3OwMBAzJgxAzdu3MClS5ekeYp7Pg4ePIixY8ciLS1NOgI0f/58AEBubi5mz56NWrVqwcHBAe3atcPBgweLrYeosmBzQ1TJWFlZYfny5Th//jy+++477N+/H7Nnzy5y/hEjRqB27do4ffo0YmJiMGfOHNjY2AAAzp07h969e+Pll1/G2bNnsWnTJhw9ehRTpkwxqCZ7e3sAQF5eHrZv34633noLb7/9Ns6fP4/XX38dY8eOxYEDBwAAW7ZswdKlS/HVV1/hypUr+Omnn9CsWTOd6z19+jQAIDw8HElJSdL1wnr06AFnZ2ds3bpVmqZUKrF582aMGDHCaI/zwYMHiIiIAABp+wHFPx9BQUEICwuTjgAlJSXhnXfeAQCMHTsWx44dw8aNG3H27Fm88sor6NOnD65cuaJ3TUQWq8x/d5yIyt3o0aOFXC4XDg4O0mXw4ME65928ebNwcXGRroeHhwsnJyfperVq1cS6det0Ljtq1Cjx2muvaUw7cuSIsLKyEo8ePdK5zJPrT0xMFO3btxe1a9cWOTk5IigoSEycOFFjmVdeeUX069dPCCHEkiVLhJ+fn8jNzdW5fh8fH7F06VLpOgCxfft2jXlCQkJEixYtpOvTpk0T3bt3l67v3btXKBQKce/evad6nACEg4ODqFKligAgAIiBAwfqnF+tpOdDCCGuXr0qZDKZuHXrlsb05557TsydO7fY9RNVBtamba2IqKx069YNq1evlq47ODgAAA4cOIBPPvkEFy5cQHp6OvLz85GdnY3MzExpnsJmzpyJCRMm4Pvvv0ePHj3wyiuvoF69egCAmJgYXL16FRs2bJDmF0JApVIhPj4ejRs31llbWloaqlatCiEEsrKy0Lp1a2zbtg0KhQIXL17UCAQDQMeOHbFs2TIAwCuvvIKwsDDUrVsXffr0Qb9+/TBgwABYW5f+7WzEiBHo0KEDbt++DU9PT2zYsAH9+vXDM88881SPs1q1ajhz5gzy8/Nx6NAhfP755/jyyy815jH0+QCAM2fOQAgBPz8/jek5OTnlkiUiqujY3BBZKAcHB9SvX19j2o0bN9CvXz9MmjQJH374IapXr46jR49i/PjxyMvL07me+fPnY/jw4fj111+xe/duhISEYOPGjXjppZegUqnw+uuva2Re1Ly9vYusTb3Tt7KyQs2aNbV24jKZTOO6EEKa5uXlhUuXLiEqKgq///47Jk+ejM8//xyHDh3SON1jiLZt26JevXrYuHEj3njjDWzfvh3h4eHS7aV9nFZWVtJz0KhRIyQnJyM4OBiHDx8GULrnQ12PXC5HTEwM5HK5xm1Vq1Y16LETWSI2N0SVSHR0NPLz87FkyRJYWRVE7jZv3lzicn5+fvDz88OMGTMwbNgwhIeH46WXXkLr1q3x119/aTVRJSm8039S48aNcfToUbz66qvStOPHj2scHbG3t8fAgQMxcOBAvPnmm2jUqBHOnTuH1q1ba63PxsZGr29hDR8+HBs2bEDt2rVhZWWF/v37S7eV9nE+acaMGQgNDcX27dvx0ksv6fV8KBQKrfpbtWoFpVKJlJQUdO7c+alqIrJEDBQTVSL16tVDfn4+vvjiC1y/fh3ff/+91mmSwh49eoQpU6bg4MGDuHHjBo4dO4bTp09Ljca7776LEydO4M0330RcXByuXLmCHTt2YOrUqaWucdasWVi3bh2+/PJLXLlyBaGhodi2bZsUpF23bh3Wrl2L8+fPS4/B3t4ePj4+Otfn6+uLffv2ITk5Gffv3y/yfkeMGIEzZ87g448/xuDBg2FnZyfdZqzH6ejoiAkTJiAkJARCCL2eD19fXzx8+BD79u1DamoqsrKy4OfnhxEjRuDVV1/Ftm3bEB8fj9OnT+PTTz/Frl27DKqJyCKZMvBDRGVj9OjR4oUXXtB5W2hoqPDw8BD29vaid+/eYv369QKAuH//vhBCM8Cak5Mjhg4dKry8vIRCoRCenp5iypQpGiHaU6dOiZ49e4qqVasKBwcH0bx5c/Hxxx8XWZuugOyTVq1aJerWrStsbGyEn5+fWL9+vXTb9u3bRbt27YSjo6NwcHAQ7du3F7///rt0+5OB4h07doj69esLa2tr4ePjI4TQDhSrtWnTRgAQ+/fv17rNWI/zxo0bwtraWmzatEkIUfLzIYQQkyZNEi4uLgKACAkJEUIIkZubKz744APh6+srbGxshLu7u3jppZfE2bNni6yJqLKQCSGEadsrIiIiIuPhaSkiIiKyKGxuiIiIyKKwuSEiIiKLwuaGiIiILAqbGyIiIrIobG6IiIjIorC5ISIiIovC5oaIiIgsCpsbIiIisihsboiIiMiisLkhIiIii/L/CIOoSpv272UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842204710733615\n"
     ]
    }
   ],
   "source": [
    "roc_auc_mlpmodel_dropped = roccurveplot(y_test_dropped,y_pred_mlpmodel_dropped, 'MLP Neural Network Model w/ Dropped Dataset')\n",
    "print(roc_auc_mlpmodel_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 Create and Compare Results DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compiling all the metrics used in order to adequantely compare each one of the predictive models created for each missing and dropped dataset trained dataset values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Missing Dataframe to compare results of precision, recall, f1 score, accuracy, and roc_auc scores for each model.\n",
    "missing_results = {'Model Name':['Logistic Regression', 'Random Forest Classifier', 'Gradient Boosting Classifier', 'Support Vector Classifier', 'AdaBoost w/ Random Forest Classifier', 'AdaBoost w/ Gradient Boosting Classifier', 'MLP Neural Network Classifier'], 'Accuracy_score':[accuracy_score(y_test_missing , y_pred_log_missing), accuracy_score(y_test_missing , y_pred_randForest_missing), accuracy_score(y_test_missing , y_pred_gradBoost_missing), accuracy_score(y_test_missing , y_pred_svc_missing), accuracy_score(y_test_missing , y_pred_adaBoost_decision_missing), accuracy_score(y_test_missing , y_pred_adaBoost_gradient_missing), accuracy_score(y_test_missing, y_pred_mlpmodel_missing)] , 'Precision_score': [precision_score(y_test_missing, y_pred_log_missing, pos_label=0) , precision_score(y_test_missing, y_pred_randForest_missing, pos_label=0) , precision_score(y_test_missing, y_pred_gradBoost_missing, pos_label=0) , precision_score(y_test_missing, y_pred_svc_missing, pos_label=0) , precision_score(y_test_missing, y_pred_adaBoost_decision_missing, pos_label=0), precision_score(y_test_missing, y_pred_adaBoost_gradient_missing, pos_label=0), precision_score(y_test_missing,y_pred_mlpmodel_missing, pos_label=0)] , 'Recall_score':[recall_score(y_test_missing, y_pred_log_missing, pos_label=0), recall_score(y_test_missing, y_pred_randForest_missing, pos_label=0), recall_score(y_test_missing, y_pred_gradBoost_missing, pos_label=0), recall_score(y_test_missing, y_pred_svc_missing, pos_label=0), recall_score(y_test_missing, y_pred_adaBoost_decision_missing, pos_label=0), recall_score(y_test_missing, y_pred_adaBoost_gradient_missing, pos_label=0), recall_score(y_test_missing,y_pred_mlpmodel_missing, pos_label=0)], 'f1_score':[f1_score(y_test_missing, y_pred_log_missing, pos_label=0), f1_score(y_test_missing, y_pred_randForest_missing, pos_label=0), f1_score(y_test_missing, y_pred_gradBoost_missing, pos_label=0) , f1_score(y_test_missing, y_pred_svc_missing, pos_label=0), f1_score(y_test_missing, y_pred_adaBoost_decision_missing, pos_label=0), f1_score(y_test_missing, y_pred_adaBoost_gradient_missing, pos_label=0), f1_score(y_test_missing,y_pred_mlpmodel_missing)], 'ROC_AUC_score':[roc_auc_log_missing, roc_auc_randForest_missing, roc_auc_gradBoost_missing, roc_auc_svc_missing, roc_auc_adaBoost_randForest_missing, roc_auc_adaBoost_gradBoost_missing, roc_auc_mlpmodel_missing], 'Training Time (Sec)': [round(time_log_missing,2), round(randForestTime_missing,2), round(gradBoostTime_missing,2), round(svcTime_missing , 2) , round(adaBoostTime_decision_missing , 2) , round(adaBoostTime_gradient_missing , 2), round(mlpmodelTime_missing, 2)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy_score</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>ROC_AUC_score</th>\n",
       "      <th>Training Time (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.906211</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.580556</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.771688</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.939120</td>\n",
       "      <td>0.886861</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.766562</td>\n",
       "      <td>0.830016</td>\n",
       "      <td>60.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.967914</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>798.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>0.915673</td>\n",
       "      <td>0.725948</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.708393</td>\n",
       "      <td>0.823139</td>\n",
       "      <td>79.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost w/ Random Forest Classifier</td>\n",
       "      <td>0.963801</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.802778</td>\n",
       "      <td>0.867868</td>\n",
       "      <td>0.897285</td>\n",
       "      <td>1461.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost w/ Gradient Boosting Classifier</td>\n",
       "      <td>0.969148</td>\n",
       "      <td>0.930514</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.891462</td>\n",
       "      <td>0.922225</td>\n",
       "      <td>202.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP Neural Network Classifier</td>\n",
       "      <td>0.933772</td>\n",
       "      <td>0.806154</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.961456</td>\n",
       "      <td>0.848679</td>\n",
       "      <td>695.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model Name  Accuracy_score  Precision_score  \\\n",
       "0                       Logistic Regression        0.906211         0.730769   \n",
       "1                  Random Forest Classifier        0.939120         0.886861   \n",
       "2              Gradient Boosting Classifier        0.967914         0.927273   \n",
       "3                 Support Vector Classifier        0.915673         0.725948   \n",
       "4      AdaBoost w/ Random Forest Classifier        0.963801         0.944444   \n",
       "5  AdaBoost w/ Gradient Boosting Classifier        0.969148         0.930514   \n",
       "6             MLP Neural Network Classifier        0.933772         0.806154   \n",
       "\n",
       "   Recall_score  f1_score  ROC_AUC_score  Training Time (Sec)  \n",
       "0      0.580556  0.647059       0.771688                 9.15  \n",
       "1      0.675000  0.766562       0.830016                60.82  \n",
       "2      0.850000  0.886957       0.919206               798.64  \n",
       "3      0.691667  0.708393       0.823139                79.55  \n",
       "4      0.802778  0.867868       0.897285              1461.56  \n",
       "5      0.855556  0.891462       0.922225               202.42  \n",
       "6      0.727778  0.961456       0.848679               695.39  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_results_df = pd.DataFrame(missing_results)\n",
    "missing_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dropped Dataframe to compare results of precision, recall, f1 score, accuracy, and roc_auc scores for each model.\n",
    "dropped_results = {'Model Name':['Logistic Regression', 'Random Forest Classifier', 'Gradient Boosting Classifier', 'Support Vector Classifier', 'AdaBoost w/ Random Forest Classifier', 'AdaBoost w/ Gradient Boosting Classifier','MLP Neural Network Classifier'], 'Accuracy_score':[accuracy_score(y_test_dropped , y_pred_log_dropped), accuracy_score(y_test_dropped , y_pred_randForest_dropped), accuracy_score(y_test_dropped , y_pred_gradBoost_dropped), accuracy_score(y_test_dropped , y_pred_svc_dropped), accuracy_score(y_test_dropped , y_pred_adaBoost_decision_dropped), accuracy_score(y_test_dropped , y_pred_adaBoost_gradient_dropped), accuracy_score(y_test_dropped, y_pred_mlpmodel_dropped) ] , 'Precision_score': [precision_score(y_test_dropped, y_pred_log_dropped, pos_label=0) , precision_score(y_test_dropped, y_pred_randForest_dropped, pos_label=0) , precision_score(y_test_dropped, y_pred_gradBoost_dropped, pos_label=0) , precision_score(y_test_dropped, y_pred_svc_dropped, pos_label=0) , precision_score(y_test_dropped, y_pred_adaBoost_decision_dropped, pos_label=0), precision_score(y_test_dropped, y_pred_adaBoost_gradient_dropped, pos_label=0),  precision_score(y_test_dropped,y_pred_mlpmodel_dropped, pos_label=0)] , 'Recall_score':[recall_score(y_test_dropped, y_pred_log_dropped, pos_label=0), recall_score(y_test_dropped, y_pred_randForest_dropped, pos_label=0), recall_score(y_test_dropped, y_pred_gradBoost_dropped, pos_label=0), recall_score(y_test_dropped, y_pred_svc_dropped, pos_label=0), recall_score(y_test_dropped, y_pred_adaBoost_decision_dropped, pos_label=0), recall_score(y_test_dropped, y_pred_adaBoost_gradient_dropped, pos_label=0), recall_score(y_test_dropped,y_pred_mlpmodel_dropped, pos_label=0) ], 'f1_score':[f1_score(y_test_dropped, y_pred_log_dropped, pos_label=0), f1_score(y_test_dropped, y_pred_randForest_dropped, pos_label=0), f1_score(y_test_dropped, y_pred_gradBoost_dropped, pos_label=0) , f1_score(y_test_dropped, y_pred_svc_dropped, pos_label=0), f1_score(y_test_dropped, y_pred_adaBoost_decision_dropped, pos_label=0), f1_score(y_test_dropped, y_pred_adaBoost_gradient_dropped, pos_label=0), f1_score(y_test_dropped,y_pred_mlpmodel_dropped) ], 'ROC_AUC_score':[roc_auc_log_dropped, roc_auc_randForest_dropped, roc_auc_gradBoost_dropped, roc_auc_svc_dropped, roc_auc_adaBoost_randForest_dropped, roc_auc_adaBoost_gradBoost_dropped, roc_auc_mlpmodel_dropped], 'Training Time (Sec)': [round(time_log_dropped,2), round(randForestTime_dropped,2), round(gradBoostTime_dropped,2), round(svcTime_dropped, 2) , round(adaBoostTime_decision_dropped, 2) , round(adaBoostTime_gradient_dropped, 2), round(mlpmodelTime_dropped, 2)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy_score</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>ROC_AUC_score</th>\n",
       "      <th>Training Time (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.914470</td>\n",
       "      <td>0.760638</td>\n",
       "      <td>0.586066</td>\n",
       "      <td>0.662037</td>\n",
       "      <td>0.777653</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.949619</td>\n",
       "      <td>0.959302</td>\n",
       "      <td>0.676230</td>\n",
       "      <td>0.793269</td>\n",
       "      <td>0.835722</td>\n",
       "      <td>50.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.968366</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>572.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>0.930287</td>\n",
       "      <td>0.796209</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.829566</td>\n",
       "      <td>37.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost w/ Random Forest Classifier</td>\n",
       "      <td>0.961921</td>\n",
       "      <td>0.963731</td>\n",
       "      <td>0.762295</td>\n",
       "      <td>0.851259</td>\n",
       "      <td>0.878755</td>\n",
       "      <td>1109.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost w/ Gradient Boosting Classifier</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.922128</td>\n",
       "      <td>877.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP Neural Network Classifier</td>\n",
       "      <td>0.937317</td>\n",
       "      <td>0.827751</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.963864</td>\n",
       "      <td>0.842205</td>\n",
       "      <td>285.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model Name  Accuracy_score  Precision_score  \\\n",
       "0                       Logistic Regression        0.914470         0.760638   \n",
       "1                  Random Forest Classifier        0.949619         0.959302   \n",
       "2              Gradient Boosting Classifier        0.968366         0.935780   \n",
       "3                 Support Vector Classifier        0.930287         0.796209   \n",
       "4      AdaBoost w/ Random Forest Classifier        0.961921         0.963731   \n",
       "5  AdaBoost w/ Gradient Boosting Classifier        0.971880         0.945455   \n",
       "6             MLP Neural Network Classifier        0.937317         0.827751   \n",
       "\n",
       "   Recall_score  f1_score  ROC_AUC_score  Training Time (Sec)  \n",
       "0      0.586066  0.662037       0.777653                 9.02  \n",
       "1      0.676230  0.793269       0.835722                50.89  \n",
       "2      0.836066  0.883117       0.913248               572.62  \n",
       "3      0.688525  0.738462       0.829566                37.78  \n",
       "4      0.762295  0.851259       0.878755              1109.92  \n",
       "5      0.852459  0.896552       0.922128               877.93  \n",
       "6      0.709016  0.963864       0.842205               285.90  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped_results_df = pd.DataFrame(dropped_results)\n",
    "dropped_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the above comprehensive results, it seems that Logistic Regression though had the shortest training time, turned to be the worst performer for both the missing and dropped valued datasets as per predicting label of churn or '0'. The best performer overall for both the dropped and missing datasets seems to be Gradient Boosting Classifer when applied with the AdaBoost Classifier model. However, the steps involved in terms of reiterating the results will involve also retreiving the best hyperparameters for the base Gradient Boosting model so this might not be the best intuitive model in order to reproduce. However, once finding the best hyperparameters for the base Gradient Boosting model, the AdaBoost Classifier slighlty improves its prior sole Gradient Boosting counterpart as per predicting values of churn within the datasets. Though in terms of ease of reproducibility and scale, just accessing the individual Gradient Boosting Model seems sufficient for prediciting churn for the bank churn datasets. \n",
    "\n",
    "Some notes to mention, the AdaBoost Classifier with the Gradient Boosting Base model in the missing dataset seemed to take less training time as compared to its individual counterpart; however in the dropped dataset, the training time was higher than its individual Gradient Boosting base model counterpart which might be due to the AdaBoost needing to correct more values in the dropped versus in the missing dataset. Also it seemed the Random Forest Classifier had better performance when the values of the missing data were dropped as compared to imputing them with the categorical variable of 'missing'. This same phenomena is seen as well with the Logistic Regression and Support Vector Classifier models. It seems as well looking at the data for the MLP Neural Network Classifier models, that there needs to be further tuning potentially to test all facotrs for the model in order to potentially increase performance. However given the time constraints and only really allowing the model to make 200 iterations in order to create its solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Save Data & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the missing and dropped results dataframe to file\n",
    "datapath = 'C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets'\n",
    "missing_results_df.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/bank_churn_missing_results.csv') \n",
    "dropped_results_df.to_csv('C:/Users/tpooz/OneDrive/Desktop/Data_Science_BootCamp_2023/SpringBoard_Github/Bank-Churnrate/0_Datasets/bank_churn_dropped_results.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After developing different models (Logistic Regression, Random Forest, Gradient Boosting, Support Vector Classifier, AdaBoost Classifier with RandomForest Base and AdaBoost Classifier with Gradient Boosting Base) and comparing each of the models results with the associated true test value sets, it seems the best performer for both the missing and dropped datasets is AdaBoost Classifier model with a Hyperparameterized Gradient Boosting model base, though this model is only slightly better performer than its individual Gradient Boosting Base. Therefore, in terms of ease of reproducibility and scale, having just the Gradient Boosting Model might be sufficient to offer generalizable predictability of churn for the bank churn datasets.\n",
    "\n",
    "However, potential future work will be to further fine tune the Multi-Layer Perceptron Neural Network Classifier model in order to see if the model with further tuned hyperparameters can outperform the Gradient Boosting model with the Ada Boost Classifier. But for the case of moving forward with a model to place into production, the individual Gradient Boosting classifier model again seems sufficient in terms of predictability for the bank churn datasets.\n",
    "\n",
    "In terms of differences between whether dropping the null values or imputing the null values with the categorical variable 'missing', overall the recall score seemed to slighlty lower than the imputed 'missing' dataset which could be due to the size of the dataset being smaller so the models had less confidence in terms of accurately predicting churn labels. Also the ROC_AUC_score seemed to overall be slightly lower than the 'missing' imputed dataset though it did seem to be higher for Random Forest and Logistic Regression. The training time overall was shorter in the dropped dataset versus the missing expect for the mentioned AdaBoost Classifier with Gradient Boosting Base. It seems there is a mixed response as per effectiveness of dropping versus imputing the missing values. Dropping the rows lead to Logistic Regression, Random Forest, and Support Vector Classifier models having slightly better performance than its imputed counterpart, however performance slightly suffered in terms of dropped versus missing for Gradient Boosting, and the AdaBoost Classifiers counterpart where missing dataset had a better performance in terms of recall in predicting 0 label or churn, f1_score, and ROC_AUC_score. Therefore, imputing 'missing' values with a categorical variable of 'missing' will be added as well for future datasets prior to prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
